[
  {
    "objectID": "3_figure.html",
    "href": "3_figure.html",
    "title": "Figure",
    "section": "",
    "text": "Date\n\n\nTitle\n\n\nAuthor\n\n\n\n\n\n\nFeb 29, 2024\n\n\n실험결과정리\n\n\nSEOYEON CHOI\n\n\n\n\nDec 29, 2023\n\n\nFigure for dashboard\n\n\nSEOYEON CHOI\n\n\n\n\nJul 20, 2023\n\n\nData management Figure for ITSTGCN\n\n\nSEOYEON CHOI\n\n\n\n\n\nNo matching items",
    "crumbs": [
      "Main_Blog",
      "**Result**",
      "**Figure**"
    ]
  },
  {
    "objectID": "1_studies.html",
    "href": "1_studies.html",
    "title": "Studies",
    "section": "",
    "text": "Date\n\n\nTitle\n\n\nAuthor\n\n\n\n\n\n\nMay 16, 2024\n\n\n논문 라이팅\n\n\nSEOYEON CHOI\n\n\n\n\nMar 12, 2024\n\n\nToy example / Algorithm\n\n\nSEOYEON CHOI\n\n\n\n\nSep 20, 2023\n\n\n[IT-STGCN]논문리비전_수정\n\n\nSEOYEON CHOI\n\n\n\n\nMay 11, 2023\n\n\nPyG Geometric Temporal CPU vs GPU\n\n\nSEOYEON CHOI\n\n\n\n\n\nNo matching items",
    "crumbs": [
      "Main_Blog",
      "**Studies**"
    ]
  },
  {
    "objectID": "3_table.html",
    "href": "3_table.html",
    "title": "Table",
    "section": "",
    "text": "Date\n\n\nTitle\n\n\nAuthor\n\n\n\n\n\n\nJan 8, 2024\n\n\nResearch Sections of Experiments\n\n\nSEOYEON CHOI\n\n\n\n\nJul 5, 2023\n\n\nData management for ITSTGCN\n\n\nSEOYEON CHOI\n\n\n\n\n\nNo matching items",
    "crumbs": [
      "Main_Blog",
      "**Result**",
      "**Table**"
    ]
  },
  {
    "objectID": "posts/2_research/2023-06-20-TGCN_simulation_table_reshape.html",
    "href": "posts/2_research/2023-06-20-TGCN_simulation_table_reshape.html",
    "title": "TGCN_Simulation Tables_reshape",
    "section": "",
    "text": "Simulation Tables"
  },
  {
    "objectID": "posts/2_research/2023-06-20-TGCN_simulation_table_reshape.html#baseline",
    "href": "posts/2_research/2023-06-20-TGCN_simulation_table_reshape.html#baseline",
    "title": "TGCN_Simulation Tables_reshape",
    "section": "Baseline",
    "text": "Baseline\n\npd.merge(data.query(\"dataset=='fivenodes' and mtype!='rand' and mtype!='block'\").groupby(['nof_filters','method','lags'])['mse'].mean().reset_index(),\n         data.query(\"dataset=='fivenodes' and mtype!='rand' and mtype!='block'\").groupby(['nof_filters','method','lags'])['mse'].std().reset_index(),\n         on=['method','nof_filters','lags']).rename(columns={'mse_x':'mean','mse_y':'std'}).round(3)\n\n\n\n\n\n\n\n\nnof_filters\nmethod\nlags\nmean\nstd\n\n\n\n\n0\n12\nIT-STGCN\n2\n1.084\n0.017\n\n\n1\n12\nSTGCN\n2\n1.085\n0.014"
  },
  {
    "objectID": "posts/2_research/2023-06-20-TGCN_simulation_table_reshape.html#random",
    "href": "posts/2_research/2023-06-20-TGCN_simulation_table_reshape.html#random",
    "title": "TGCN_Simulation Tables_reshape",
    "section": "Random",
    "text": "Random\n\npd.merge(data.query(\"dataset=='fivenodes' and mtype=='rand'\").groupby(['mrate','nof_filters','method','lags'])['mse'].mean().reset_index(),\n         data.query(\"dataset=='fivenodes' and mtype=='rand'\").groupby(['mrate','nof_filters','method','lags'])['mse'].std().reset_index(),\n         on=['method','nof_filters','mrate','lags']).rename(columns={'mse_x':'mean','mse_y':'std'}).round(3).query(\"nof_filters==12\")\n\n\n\n\n\n\n\n\nmrate\nnof_filters\nmethod\nlags\nmean\nstd\n\n\n\n\n0\n0.7\n12\nIT-STGCN\n2\n1.119\n0.043\n\n\n1\n0.7\n12\nSTGCN\n2\n1.156\n0.070\n\n\n2\n0.8\n12\nIT-STGCN\n2\n1.132\n0.051\n\n\n3\n0.8\n12\nSTGCN\n2\n1.169\n0.065"
  },
  {
    "objectID": "posts/2_research/2023-06-20-TGCN_simulation_table_reshape.html#block",
    "href": "posts/2_research/2023-06-20-TGCN_simulation_table_reshape.html#block",
    "title": "TGCN_Simulation Tables_reshape",
    "section": "Block",
    "text": "Block\n\npd.merge(data.query(\"dataset=='fivenodes' and mtype=='block'\").groupby(['mrate','nof_filters','method'])['mse'].mean().reset_index(),\n         data.query(\"dataset=='fivenodes' and mtype=='block'\").groupby(['mrate','nof_filters','method'])['mse'].std().reset_index(),\n         on=['method','nof_filters','mrate']).rename(columns={'mse_x':'mean','mse_y':'std'}).round(3)\n\n\n\n\n\n\n\n\nmrate\nnof_filters\nmethod\nmean\nstd\n\n\n\n\n0\n0.125\n12\nIT-STGCN\n1.090\n0.015\n\n\n1\n0.125\n12\nSTGCN\n1.099\n0.018"
  },
  {
    "objectID": "posts/2_research/2023-06-20-TGCN_simulation_table_reshape.html#baseline-1",
    "href": "posts/2_research/2023-06-20-TGCN_simulation_table_reshape.html#baseline-1",
    "title": "TGCN_Simulation Tables_reshape",
    "section": "Baseline",
    "text": "Baseline\n\npd.merge(data.query(\"dataset=='chickenpox' and mtype!='rand' and mtype!='block'\").groupby(['nof_filters','method'])['mse'].mean().reset_index(),\n         data.query(\"dataset=='chickenpox' and mtype!='rand' and mtype!='block'\").groupby(['nof_filters','method'])['mse'].std().reset_index(),\n         on=['method','nof_filters']).rename(columns={'mse_x':'mean','mse_y':'std'}).round(3).query(\"nof_filters==16\")\n\n\n\n\n\n\n\n\nnof_filters\nmethod\nmean\nstd"
  },
  {
    "objectID": "posts/2_research/2023-06-20-TGCN_simulation_table_reshape.html#random-1",
    "href": "posts/2_research/2023-06-20-TGCN_simulation_table_reshape.html#random-1",
    "title": "TGCN_Simulation Tables_reshape",
    "section": "Random",
    "text": "Random\n\npd.merge(data.query(\"dataset=='chickenpox' and mtype=='rand'\").groupby(['mrate','inter_method','nof_filters','method'])['mse'].mean().reset_index(),\n         data.query(\"dataset=='chickenpox' and mtype=='rand'\").groupby(['mrate','inter_method','nof_filters','method'])['mse'].std().reset_index(),\n         on=['method','inter_method','mrate','nof_filters']).rename(columns={'mse_x':'mean','mse_y':'std'}).round(3)\n\n\n\n\n\n\n\n\nmrate\ninter_method\nnof_filters\nmethod\nmean\nstd\n\n\n\n\n0\n0.3\nlinear\n12\nIT-STGCN\n1.042\n0.020\n\n\n1\n0.3\nlinear\n12\nSTGCN\n1.054\n0.015\n\n\n2\n0.8\nlinear\n12\nIT-STGCN\n1.183\n0.028\n\n\n3\n0.8\nlinear\n12\nSTGCN\n1.466\n0.064"
  },
  {
    "objectID": "posts/2_research/2023-06-20-TGCN_simulation_table_reshape.html#block-1",
    "href": "posts/2_research/2023-06-20-TGCN_simulation_table_reshape.html#block-1",
    "title": "TGCN_Simulation Tables_reshape",
    "section": "Block",
    "text": "Block\n\npd.merge(data.query(\"dataset=='chickenpox' and mtype=='block'\").groupby(['inter_method','mrate','nof_filters','method'])['mse'].mean().reset_index(),\n         data.query(\"dataset=='chickenpox' and mtype=='block'\").groupby(['inter_method','mrate','nof_filters','method'])['mse'].std().reset_index(),\n         on=['method','inter_method','mrate','nof_filters']).rename(columns={'mse_x':'mean','mse_y':'std'})\n\n\n\n\n\n\n\n\ninter_method\nmrate\nnof_filters\nmethod\nmean\nstd\n\n\n\n\n0\nlinear\n0.28777\n12\nIT-STGCN\n1.064651\n0.030813\n\n\n1\nlinear\n0.28777\n12\nSTGCN\n1.082494\n0.028106\n\n\n2\nnearest\n0.28777\n12\nIT-STGCN\n1.069594\n0.028391\n\n\n3\nnearest\n0.28777\n12\nSTGCN\n1.079100\n0.027403"
  },
  {
    "objectID": "posts/2_research/2023-06-20-TGCN_simulation_table_reshape.html#baseline-2",
    "href": "posts/2_research/2023-06-20-TGCN_simulation_table_reshape.html#baseline-2",
    "title": "TGCN_Simulation Tables_reshape",
    "section": "Baseline",
    "text": "Baseline\n\npd.merge(data.query(\"dataset=='pedalme' and mtype!='rand' and mtype!='block'\").groupby(['lags','nof_filters','method'])['mse'].mean().reset_index(),\n         data.query(\"dataset=='pedalme' and mtype!='rand' and mtype!='block'\").groupby(['lags','nof_filters','method'])['mse'].std().reset_index(),\n         on=['method','lags','nof_filters']).rename(columns={'mse_x':'mean','mse_y':'std'}).round(3).query(\"lags==4\")\n\n\n\n\n\n\n\n\nlags\nnof_filters\nmethod\nmean\nstd\n\n\n\n\n0\n4\n12\nIT-STGCN\n1.341\n0.067\n\n\n1\n4\n12\nSTGCN\n1.274\n0.067"
  },
  {
    "objectID": "posts/2_research/2023-06-20-TGCN_simulation_table_reshape.html#random-2",
    "href": "posts/2_research/2023-06-20-TGCN_simulation_table_reshape.html#random-2",
    "title": "TGCN_Simulation Tables_reshape",
    "section": "Random",
    "text": "Random\n\npd.merge(data.query(\"dataset=='pedalme' and mtype=='rand'\").groupby(['mrate','lags','inter_method','method'])['mse'].mean().reset_index(),\n         data.query(\"dataset=='pedalme' and mtype=='rand'\").groupby(['mrate','lags','inter_method','method'])['mse'].std().reset_index(),\n         on=['method','mrate','lags','inter_method']).rename(columns={'mse_x':'mean','mse_y':'std'}).round(3)\n\n\n\n\n\n\n\n\nmrate\nlags\ninter_method\nmethod\nmean\nstd\n\n\n\n\n0\n0.3\n4\nlinear\nIT-STGCN\n1.280\n0.070\n\n\n1\n0.3\n4\nlinear\nSTGCN\n1.302\n0.112\n\n\n2\n0.3\n4\nnearest\nIT-STGCN\n1.248\n0.074\n\n\n3\n0.3\n4\nnearest\nSTGCN\n1.291\n0.111\n\n\n4\n0.6\n4\nlinear\nIT-STGCN\n1.257\n0.048\n\n\n5\n0.6\n4\nlinear\nSTGCN\n1.257\n0.072\n\n\n6\n0.6\n4\nnearest\nIT-STGCN\n1.260\n0.072\n\n\n7\n0.6\n4\nnearest\nSTGCN\n1.301\n0.090"
  },
  {
    "objectID": "posts/2_research/2023-06-20-TGCN_simulation_table_reshape.html#block-2",
    "href": "posts/2_research/2023-06-20-TGCN_simulation_table_reshape.html#block-2",
    "title": "TGCN_Simulation Tables_reshape",
    "section": "Block",
    "text": "Block\n\npd.merge(data.query(\"dataset=='pedalme' and mtype=='block'\").groupby(['mrate','lags','inter_method','method'])['mse'].mean().reset_index(),\n         data.query(\"dataset=='pedalme' and mtype=='block'\").groupby(['mrate','lags','inter_method','method'])['mse'].std().reset_index(),\n         on=['method','mrate','lags','inter_method']).rename(columns={'mse_x':'mean','mse_y':'std'}).round(3).query(\"lags==4\")\n\n\n\n\n\n\n\n\nmrate\nlags\ninter_method\nmethod\nmean\nstd\n\n\n\n\n0\n0.286\n4\nlinear\nIT-STGCN\n1.278\n0.056\n\n\n1\n0.286\n4\nlinear\nSTGCN\n1.244\n0.071\n\n\n2\n0.286\n4\nnearest\nIT-STGCN\n1.262\n0.066\n\n\n3\n0.286\n4\nnearest\nSTGCN\n1.232\n0.069"
  },
  {
    "objectID": "posts/2_research/2023-06-20-TGCN_simulation_table_reshape.html#w_st",
    "href": "posts/2_research/2023-06-20-TGCN_simulation_table_reshape.html#w_st",
    "title": "TGCN_Simulation Tables_reshape",
    "section": "W_st",
    "text": "W_st\n\npd.merge(data_pedal2.query(\"mtype=='rand'\").groupby(['mrate','lags','inter_method','method'])['mse'].mean().reset_index(),\n         data_pedal2.query(\"mtype=='rand'\").groupby(['mrate','lags','inter_method','method'])['mse'].std().reset_index(),\n         on=['method','mrate','lags','inter_method']).rename(columns={'mse_x':'mean','mse_y':'std'}).round(3).query(\"lags==4\")\n\n\n\n\n\n\n\n\nmrate\nlags\ninter_method\nmethod\nmean\nstd\n\n\n\n\n0\n0.3\n4\nlinear\nIT-STGCN\n1.320\n0.164\n\n\n1\n0.3\n4\nlinear\nSTGCN\n1.287\n0.126\n\n\n2\n0.3\n4\nnearest\nIT-STGCN\n1.276\n0.105\n\n\n3\n0.3\n4\nnearest\nSTGCN\n1.313\n0.101\n\n\n4\n0.6\n4\nlinear\nIT-STGCN\n1.304\n0.129\n\n\n5\n0.6\n4\nlinear\nSTGCN\n1.299\n0.076\n\n\n6\n0.6\n4\nnearest\nIT-STGCN\n1.338\n0.202\n\n\n7\n0.6\n4\nnearest\nSTGCN\n1.297\n0.093\n\n\n\n\n\n\n\n\npd.merge(data_pedal2.query(\"mtype=='block'\").groupby(['mrate','lags','inter_method','method'])['mse'].mean().reset_index(),\n         data_pedal2.query(\"mtype=='block'\").groupby(['mrate','lags','inter_method','method'])['mse'].std().reset_index(),\n         on=['method','mrate','lags','inter_method']).rename(columns={'mse_x':'mean','mse_y':'std'}).round(3).query(\"lags==4\")\n\n\n\n\n\n\n\n\nmrate\nlags\ninter_method\nmethod\nmean\nstd\n\n\n\n\n0\n0.286\n4\nlinear\nIT-STGCN\n1.243\n0.110\n\n\n1\n0.286\n4\nlinear\nSTGCN\n1.176\n0.068\n\n\n2\n0.286\n4\nnearest\nIT-STGCN\n1.237\n0.083\n\n\n3\n0.286\n4\nnearest\nSTGCN\n1.258\n0.064"
  },
  {
    "objectID": "posts/2_research/2023-06-20-TGCN_simulation_table_reshape.html#baseline-3",
    "href": "posts/2_research/2023-06-20-TGCN_simulation_table_reshape.html#baseline-3",
    "title": "TGCN_Simulation Tables_reshape",
    "section": "Baseline",
    "text": "Baseline\n\npd.merge(data.query(\"dataset=='wikimath' and mrate==0\").groupby(['lags','nof_filters','method'])['mse'].mean().reset_index(),\n         data.query(\"dataset=='wikimath' and mrate==0\").groupby(['lags','nof_filters','method'])['mse'].std().reset_index(),\n         on=['lags','nof_filters','method']).rename(columns={'mse_x':'mean','mse_y':'std'}).round(3)\n\n\n\n\n\n\n\n\nlags\nnof_filters\nmethod\nmean\nstd\n\n\n\n\n0\n8\n12\nIT-STGCN\n0.730\n0.030\n\n\n1\n8\n12\nSTGCN\n0.732\n0.035"
  },
  {
    "objectID": "posts/2_research/2023-06-20-TGCN_simulation_table_reshape.html#random-3",
    "href": "posts/2_research/2023-06-20-TGCN_simulation_table_reshape.html#random-3",
    "title": "TGCN_Simulation Tables_reshape",
    "section": "Random",
    "text": "Random\n\npd.merge(data.query(\"dataset=='wikimath' and mtype=='rand'\").groupby(['mrate','lags','method'])['mse'].mean().reset_index(),\n         data.query(\"dataset=='wikimath' and mtype=='rand'\").groupby(['mrate','lags','method'])['mse'].std().reset_index(),\n         on=['method','mrate','lags']).rename(columns={'mse_x':'mean','mse_y':'std'}).round(3)\n\n\n\n\n\n\n\n\nmrate\nlags\nmethod\nmean\nstd\n\n\n\n\n0\n0.3\n8\nIT-STGCN\n0.739\n0.040\n\n\n1\n0.3\n8\nSTGCN\n0.734\n0.027\n\n\n2\n0.5\n8\nIT-STGCN\n0.757\n0.046\n\n\n3\n0.5\n8\nSTGCN\n0.757\n0.034\n\n\n4\n0.6\n8\nIT-STGCN\n0.742\n0.030\n\n\n5\n0.6\n8\nSTGCN\n0.774\n0.025\n\n\n6\n0.8\n8\nIT-STGCN\n0.771\n0.020\n\n\n7\n0.8\n8\nSTGCN\n0.827\n0.030"
  },
  {
    "objectID": "posts/2_research/2023-06-20-TGCN_simulation_table_reshape.html#block-3",
    "href": "posts/2_research/2023-06-20-TGCN_simulation_table_reshape.html#block-3",
    "title": "TGCN_Simulation Tables_reshape",
    "section": "Block",
    "text": "Block\n\npd.merge(data.query(\"dataset=='wikimath' and mtype=='block'\").groupby(['mrate','lags','method'])['mse'].mean().reset_index(),\n         data.query(\"dataset=='wikimath' and mtype=='block'\").groupby(['mrate','lags','method'])['mse'].std().reset_index(),\n         on=['method','mrate','lags']).rename(columns={'mse_x':'mean','mse_y':'std'})\n\n\n\n\n\n\n\n\nmrate\nlags\nmethod\nmean\nstd\n\n\n\n\n0\n0.119837\n8\nIT-STGCN\n0.747581\n0.045961\n\n\n1\n0.119837\n8\nSTGCN\n0.741134\n0.046269"
  },
  {
    "objectID": "posts/2_research/2023-06-20-TGCN_simulation_table_reshape.html#missing-values-on-the-same-nodes",
    "href": "posts/2_research/2023-06-20-TGCN_simulation_table_reshape.html#missing-values-on-the-same-nodes",
    "title": "TGCN_Simulation Tables_reshape",
    "section": "missing values on the same nodes",
    "text": "missing values on the same nodes\n\npd.merge(data_wiki_GSO.groupby(['mrate','lags','method'])['mse'].mean().reset_index(),\n        data_wiki_GSO.groupby(['mrate','lags','method'])['mse'].std().reset_index(),\n         on=['method','mrate','lags']).rename(columns={'mse_x':'mean','mse_y':'std'}).round(3)"
  },
  {
    "objectID": "posts/2_research/2023-06-20-TGCN_simulation_table_reshape.html#baseline-4",
    "href": "posts/2_research/2023-06-20-TGCN_simulation_table_reshape.html#baseline-4",
    "title": "TGCN_Simulation Tables_reshape",
    "section": "Baseline",
    "text": "Baseline\n\npd.merge(data.query(\"dataset=='windmillsmall' and mrate==0\").groupby(['lags','method'])['mse'].mean().reset_index(),\n         data.query(\"dataset=='windmillsmall' and mrate==0\").groupby(['lags','method'])['mse'].std().reset_index(),\n         on=['method','lags']).rename(columns={'mse_x':'mean','mse_y':'std'}).round(3)\n\n\n\n\n\n\n\n\nlags\nmethod\nmean\nstd\n\n\n\n\n0\n8\nIT-STGCN\n0.992\n0.012\n\n\n1\n8\nSTGCN\n0.990\n0.007"
  },
  {
    "objectID": "posts/2_research/2023-06-20-TGCN_simulation_table_reshape.html#random-4",
    "href": "posts/2_research/2023-06-20-TGCN_simulation_table_reshape.html#random-4",
    "title": "TGCN_Simulation Tables_reshape",
    "section": "Random",
    "text": "Random\n\npd.merge(data.query(\"dataset=='windmillsmall' and mtype=='rand'\").groupby(['mrate','lags','method'])['mse'].mean().reset_index(),\n         data.query(\"dataset=='windmillsmall' and mtype=='rand'\").groupby(['mrate','lags','method'])['mse'].std().reset_index(),\n         on=['method','mrate','lags']).rename(columns={'mse_x':'mean','mse_y':'std'}).round(3)\n\n\n\n\n\n\n\n\nmrate\nlags\nmethod\nmean\nstd\n\n\n\n\n0\n0.7\n8\nIT-STGCN\n1.071\n0.010\n\n\n1\n0.7\n8\nSTGCN\n1.305\n0.039"
  },
  {
    "objectID": "posts/2_research/2023-06-20-TGCN_simulation_table_reshape.html#block-4",
    "href": "posts/2_research/2023-06-20-TGCN_simulation_table_reshape.html#block-4",
    "title": "TGCN_Simulation Tables_reshape",
    "section": "Block",
    "text": "Block\n\npd.merge(data.query(\"dataset=='windmillsmall' and mtype=='block'\").groupby(['mrate','lags','method'])['mse'].mean().reset_index(),\n         data.query(\"dataset=='windmillsmall' and mtype=='block'\").groupby(['mrate','lags','method'])['mse'].std().reset_index(),\n         on=['method','mrate','lags']).rename(columns={'mse_x':'mean','mse_y':'std'}).round(3)\n\n\n\n\n\n\n\n\nmrate\nlags\nmethod\nmean\nstd\n\n\n\n\n0\n0.081\n8\nIT-STGCN\n0.992\n0.015\n\n\n1\n0.081\n8\nSTGCN\n0.999\n0.013"
  },
  {
    "objectID": "posts/2_research/2023-06-20-TGCN_simulation_table_reshape.html#baseline-5",
    "href": "posts/2_research/2023-06-20-TGCN_simulation_table_reshape.html#baseline-5",
    "title": "TGCN_Simulation Tables_reshape",
    "section": "Baseline",
    "text": "Baseline\n\npd.merge(data.query(\"dataset=='monte' and mrate==0\").groupby(['lags','method'])['mse'].mean().reset_index(),\n         data.query(\"dataset=='monte' and mrate==0\").groupby(['lags','method'])['mse'].std().reset_index(),\n         on=['method','lags']).rename(columns={'mse_x':'mean','mse_y':'std'}).round(3)\n\n\n\n\n\n\n\n\nlags\nmethod\nmean\nstd\n\n\n\n\n0\n4\nIT-STGCN\n0.984\n0.007\n\n\n1\n4\nSTGCN\n0.982\n0.006"
  },
  {
    "objectID": "posts/2_research/2023-06-20-TGCN_simulation_table_reshape.html#random-5",
    "href": "posts/2_research/2023-06-20-TGCN_simulation_table_reshape.html#random-5",
    "title": "TGCN_Simulation Tables_reshape",
    "section": "Random",
    "text": "Random\n\npd.merge(data.query(\"dataset=='monte' and mtype=='rand'\").groupby(['mrate','lags','inter_method','method'])['mse'].mean().reset_index(),\n         data.query(\"dataset=='monte' and mtype=='rand'\").groupby(['mrate','lags','inter_method','method'])['mse'].std().reset_index(),\n         on=['mrate','inter_method','method','mrate','lags']).rename(columns={'mse_x':'mean','mse_y':'std'})\n\n\n\n\n\n\n\n\nmrate\nlags\ninter_method\nmethod\nmean\nstd\n\n\n\n\n0\n0.3\n4\nnearest\nIT-STGCN\n0.987441\n0.007021\n\n\n1\n0.3\n4\nnearest\nSTGCN\n1.034595\n0.012626\n\n\n2\n0.5\n4\nnearest\nIT-STGCN\n0.990051\n0.009426\n\n\n3\n0.5\n4\nnearest\nSTGCN\n1.099853\n0.036318\n\n\n4\n0.7\n4\nnearest\nIT-STGCN\n1.029527\n0.009532\n\n\n5\n0.7\n4\nnearest\nSTGCN\n1.182811\n0.057392\n\n\n6\n0.8\n4\nnearest\nIT-STGCN\n1.072795\n0.024438\n\n\n7\n0.8\n4\nnearest\nSTGCN\n1.217952\n0.085842"
  },
  {
    "objectID": "posts/2_research/2023-06-20-TGCN_simulation_table_reshape.html#block-5",
    "href": "posts/2_research/2023-06-20-TGCN_simulation_table_reshape.html#block-5",
    "title": "TGCN_Simulation Tables_reshape",
    "section": "Block",
    "text": "Block\n\npd.merge(data.query(\"dataset=='monte' and mtype=='block'\").groupby(['mrate','lags','inter_method','method'])['mse'].mean().reset_index(),\n         data.query(\"dataset=='monte' and mtype=='block'\").groupby(['mrate','lags','inter_method','method'])['mse'].std().reset_index(),\n         on=['method','mrate','inter_method','lags']).rename(columns={'mse_x':'mean','mse_y':'std'})\n\n\n\n\n\n\n\n\nmrate\nlags\ninter_method\nmethod\nmean\nstd\n\n\n\n\n0\n0.149142\n4\nnearest\nIT-STGCN\n0.983640\n0.006550\n\n\n1\n0.149142\n4\nnearest\nSTGCN\n0.985485\n0.005308"
  },
  {
    "objectID": "posts/2_research/2023-01-11-Algorithm_EX_1.html",
    "href": "posts/2_research/2023-01-11-Algorithm_EX_1.html",
    "title": "GCN Algorithm Example 1",
    "section": "",
    "text": "Our method; GNAR Dataset Example(fiveVTS, fiveNet)"
  },
  {
    "objectID": "posts/2_research/2023-01-11-Algorithm_EX_1.html#데이터-일부-missing-처리",
    "href": "posts/2_research/2023-01-11-Algorithm_EX_1.html#데이터-일부-missing-처리",
    "title": "GCN Algorithm Example 1",
    "section": "데이터 일부 missing 처리",
    "text": "데이터 일부 missing 처리\n\n1) Block 처리\n\n[1] ST-GCN\n\n%%R\nfiveVTS0 &lt;- fiveVTS\nfiveVTS0[50:150, 3] &lt;- NA\n\n\nplt.plot(fiveVTS0[:,2])\n\n\n\n\n\n\n\n\n\nT = 200\nN = 5 # number of Nodes\nE = fiveNet_edge\nV = np.array([1,2,3,4,5])\nt = np.arange(0,T)\nnode_features = 1\n\n\nf = torch.tensor(fiveVTS0).reshape(200,5,1).float()\n\n\nX = f[:199,:,:]\ny = f[1:,:,:]\n\n\nedge_index = torch.tensor(E)\nedge_attr = torch.tensor(np.array([1,1,1,1,1,1,1,1,1,1]),dtype=torch.float32)\n\n\n_ee = enumerate(zip(X,y))\n\n\nmodel = RecurrentGCN(node_features=1, filters=4)\n\noptimizer = torch.optim.Adam(model.parameters(), lr=0.01)\n\nmodel.train()\n\nfor epoch in tqdm(range(50)):\n    for time, (xt,yt) in enumerate(zip(X,y)):\n        y_hat = model(xt, edge_index, edge_attr)\n        cost = torch.mean((y_hat-yt)**2)\n        cost.backward()\n        optimizer.step()\n        optimizer.zero_grad()\n\n100%|██████████| 50/50 [00:32&lt;00:00,  1.53it/s]\n\n\n\nyhat = torch.stack([model(xt, edge_index, edge_attr) for xt in X]).detach().numpy()\n\n\nplt.plot(yhat[:,0].data)\nplt.plot(yhat[:,1].data)\nplt.plot(yhat[:,2].data)\nplt.plot(yhat[:,3].data)\n\n\n\n\n\n\n\n\n\n\n\n2) Random missing values\n\n%%R\nset.seed(1)\nfiveVTSrandom &lt;- fiveVTS\nsampleindex = sort(sample(1:200, 100))\nfiveVTSrandom[sampleindex,3] &lt;- NA\n\n\n%R -o fiveVTSrandom\n%R -o sampleindex\n\n\nplt.plot(fiveVTSrandom[:,2],'o')\n\n\n\n\n\n\n\n\n\n\n3) By 2\n\n%%R\nfiveVTStwo &lt;- fiveVTS\nindextwo &lt;- rep(seq(1, by = 2, 200))\nfiveVTStwo[indextwo, 3] &lt;- NA\n\n\n%R -o fiveVTStwo\n%R -o indextwo\n\n\nplt.plot(fiveVTStwo[:,2],'o')"
  },
  {
    "objectID": "posts/2_research/2023-01-11-Algorithm_EX_1.html#mean",
    "href": "posts/2_research/2023-01-11-Algorithm_EX_1.html#mean",
    "title": "GCN Algorithm Example 1",
    "section": "1.1. Mean",
    "text": "1.1. Mean\n\n1) Block\n\nfiveVTS0_mean = fiveVTS0.copy()\n\n\nfiveVTS0_mean[49:150,2] = np.mean(fiveVTS0[:49,2].tolist()+fiveVTS0[150:,2].tolist())\n\n\nplt.plot(fiveVTS0_mean[:,2])\n\n\n\n\n\n\n\n\n\n\n2) Random missing values\n\nfiveVTSrandom_mean = fiveVTSrandom.copy()\n\n\ndf = pd.DataFrame(fiveVTSrandom[:,2])\nmean_value = df.mean() # finds the mean value of the column A\ndf = df.fillna(mean_value) # replace missing values with the mean value\n\n\nfiveVTSrandom_mean[:,2] = np.array(df).reshape(200,)\n\n\nplt.plot(fiveVTSrandom_mean[:,2])\n\n\n\n\n\n\n\n\n\n\n3) By 2\n\nfiveVTStwo_mean = fiveVTStwo.copy()\n\n\ndf = pd.DataFrame(fiveVTStwo[:,2])\nmean_value = df.mean() # finds the mean value of the column A\ndf = df.fillna(mean_value) # replace missing values with the mean value\n\n\nfiveVTStwo_mean[:,2] = np.array(df).reshape(200,)\n\n\nplt.plot(fiveVTStwo_mean[:,2])"
  },
  {
    "objectID": "posts/2_research/2023-01-11-Algorithm_EX_1.html#linear-interpolation",
    "href": "posts/2_research/2023-01-11-Algorithm_EX_1.html#linear-interpolation",
    "title": "GCN Algorithm Example 1",
    "section": "1.2. linear interpolation",
    "text": "1.2. linear interpolation\n\n1) Block\n\nfiveVTS0_linearinterpolation = fiveVTS0.copy()\n\n\n# Sample data points\nx = np.array([48,150])\ny = np.array([fiveVTS0_linearinterpolation[48,2],fiveVTS0_linearinterpolation[150,2]])\n\n# Create interpolating function\nf = interp1d(x, y, kind='linear')\n\n# Estimate y value for x = 2.5\ny_interp = f(range(49,150))\n\n\nfiveVTS0_linearinterpolation[49:150,2] = y_interp\n\n\nplt.plot(fiveVTS0_linearinterpolation[:,2])\n\n\n\n\n\n\n\n\n\n\n2) Random missing values\n\nfiveVTSrandom_linearinterpolation = fiveVTSrandom.copy()\n\n\n_df = pd.DataFrame(fiveVTSrandom_linearinterpolation[:,2])\n_df.interpolate(method='linear', inplace=True)\n_df = _df.fillna(0)\n\n\nfiveVTSrandom_linearinterpolation[:,2] = np.array(_df).reshape(200,)\n\n\nplt.plot(fiveVTSrandom_linearinterpolation[:,2])\n\n\n\n\n\n\n\n\n\n\n3) By 2\n\nfiveVTStwo_linearinterpolation = fiveVTStwo.copy()\n\n\n_df = pd.Series(fiveVTStwo_linearinterpolation[:,2])\n_df.interpolate(method='linear', inplace=True)\n_df = _df.fillna(0)\n\n\nfiveVTStwo_linearinterpolation[:,2] = _df\n\n\nplt.plot(fiveVTStwo_linearinterpolation[:,2])"
  },
  {
    "objectID": "posts/2_research/2023-01-11-Algorithm_EX_1.html#mean-1",
    "href": "posts/2_research/2023-01-11-Algorithm_EX_1.html#mean-1",
    "title": "GCN Algorithm Example 1",
    "section": "2.1. Mean",
    "text": "2.1. Mean\n\n1) Block\n\nf_mean = torch.tensor(fiveVTS0_mean).reshape(200,5,1).float()\n\n\nX_mean = f_mean[:199,:,:]\ny_mean = f_mean[1:,:,:]\n\n\nmodel = RecurrentGCN(node_features=1, filters=4)\n\noptimizer = torch.optim.Adam(model.parameters(), lr=0.01)\n\nmodel.train()\n\nfor epoch in tqdm(range(50)):\n    for time, (xt,yt) in enumerate(zip(X_mean,y_mean)):\n        y_hat = model(xt, edge_index, edge_attr)\n        cost = torch.mean((y_hat-yt)**2)\n        cost.backward()\n        optimizer.step()\n        optimizer.zero_grad()\n\n100%|██████████| 50/50 [00:32&lt;00:00,  1.56it/s]\n\n\n\nfhat_mean = torch.stack([model(xt, edge_index, edge_attr) for xt in X_mean]).detach().numpy()\n\n\nplt.plot(fhat_mean[:,2].data)\n\n\n\n\n\n\n\n\n\n\n2) Random missing values\n\nf_fiveVTSrandom_mean = torch.tensor(fiveVTSrandom_mean).reshape(200,5,1).float()\n\n\nX_fiveVTSrandom_mean = f_fiveVTSrandom_mean[:199,:,:]\ny_fiveVTSrandom_mean = f_fiveVTSrandom_mean[1:,:,:]\n\n\nmodel = RecurrentGCN(node_features=1, filters=4)\n\noptimizer = torch.optim.Adam(model.parameters(), lr=0.01)\n\nmodel.train()\n\nfor epoch in tqdm(range(50)):\n    for time, (xt,yt) in enumerate(zip(X_fiveVTSrandom_mean,y_fiveVTSrandom_mean)):\n        y_hat = model(xt, edge_index, edge_attr)\n        cost = torch.mean((y_hat-yt)**2)\n        cost.backward()\n        optimizer.step()\n        optimizer.zero_grad()\n\n100%|██████████| 50/50 [00:32&lt;00:00,  1.55it/s]\n\n\n\nfhat_fiveVTSrandom_mean = torch.stack([model(xt, edge_index, edge_attr) for xt in X_fiveVTSrandom_mean]).detach().numpy()\n\n\nplt.plot(fhat_fiveVTSrandom_mean[:,2].data)\n\n\n\n\n\n\n\n\n\n\n3) By 2\n\nf_fiveVTStwo_mean = torch.tensor(fiveVTStwo_mean).reshape(200,5,1).float()\n\n\nX_fiveVTStwo_mean = f_fiveVTStwo_mean[:199,:,:]\ny_fiveVTStwo_mean = f_fiveVTStwo_mean[1:,:,:]\n\n\nmodel = RecurrentGCN(node_features=1, filters=4)\n\noptimizer = torch.optim.Adam(model.parameters(), lr=0.01)\n\nmodel.train()\n\nfor epoch in tqdm(range(50)):\n    for time, (xt,yt) in enumerate(zip(X_fiveVTStwo_mean,y_fiveVTStwo_mean)):\n        y_hat = model(xt, edge_index, edge_attr)\n        cost = torch.mean((y_hat-yt)**2)\n        cost.backward()\n        optimizer.step()\n        optimizer.zero_grad()\n\n100%|██████████| 50/50 [00:32&lt;00:00,  1.54it/s]\n\n\n\nfhat_fiveVTStwo_mean = torch.stack([model(xt, edge_index, edge_attr) for xt in X_fiveVTStwo_mean]).detach().numpy()\n\n\nplt.plot(fhat_fiveVTStwo_mean[:,2].data)"
  },
  {
    "objectID": "posts/2_research/2023-01-11-Algorithm_EX_1.html#linear-interpolation-1",
    "href": "posts/2_research/2023-01-11-Algorithm_EX_1.html#linear-interpolation-1",
    "title": "GCN Algorithm Example 1",
    "section": "2.2. linear interpolation",
    "text": "2.2. linear interpolation\n\n1) Block\n\nf_linearinterpolation = torch.tensor(fiveVTS0_linearinterpolation).reshape(200,5,1).float()\n\n\nX_linearinterpolation = f_linearinterpolation[:199,:,:]\ny_linearinterpolation = f_linearinterpolation[1:,:,:]\n\n\nmodel = RecurrentGCN(node_features=1, filters=4)\n\noptimizer = torch.optim.Adam(model.parameters(), lr=0.01)\n\nmodel.train()\n\nfor epoch in tqdm(range(50)):\n    for time, (xt,yt) in enumerate(zip(X_linearinterpolation,y_linearinterpolation)):\n        y_hat = model(xt, edge_index, edge_attr)\n        cost = torch.mean((y_hat-yt)**2)\n        cost.backward()\n        optimizer.step()\n        optimizer.zero_grad()\n\n100%|██████████| 50/50 [00:32&lt;00:00,  1.55it/s]\n\n\n\nfhat_linearinterpolation = torch.stack([model(xt, edge_index, edge_attr) for xt in X_linearinterpolation]).detach().numpy()\n\n\nplt.plot(fhat_linearinterpolation[:,2].data)\n\n\n\n\n\n\n\n\n\n\n2) Random missing values\n\nf_fiveVTSrandom_linearinterpolation = torch.tensor(fiveVTSrandom_linearinterpolation).reshape(200,5,1).float()\n\n\nX_fiveVTSrandom_linearinterpolation = f_fiveVTSrandom_linearinterpolation[:199,:,:]\ny_fiveVTSrandom_linearinterpolation = f_fiveVTSrandom_linearinterpolation[1:,:,:]\n\n\nmodel = RecurrentGCN(node_features=1, filters=4)\n\noptimizer = torch.optim.Adam(model.parameters(), lr=0.01)\n\nmodel.train()\n\nfor epoch in tqdm(range(50)):\n    for time, (xt,yt) in enumerate(zip(X_fiveVTSrandom_linearinterpolation,y_fiveVTSrandom_linearinterpolation)):\n        y_hat = model(xt, edge_index, edge_attr)\n        cost = torch.mean((y_hat-yt)**2)\n        cost.backward()\n        optimizer.step()\n        optimizer.zero_grad()\n\n100%|██████████| 50/50 [00:32&lt;00:00,  1.55it/s]\n\n\n\nfhat_fiveVTSrandom_linearinterpolation = torch.stack([model(xt, edge_index, edge_attr) for xt in X_fiveVTSrandom_linearinterpolation]).detach().numpy()\n\n\nplt.plot(fhat_fiveVTSrandom_linearinterpolation[:,2].data)\n\n\n\n\n\n\n\n\n\n\n3) By 2\n\nf_fiveVTStwo_linearinterpolation = torch.tensor(fiveVTStwo_linearinterpolation).reshape(200,5,1).float()\n\n\nX_fiveVTStwo_linearinterpolation = f_fiveVTSrandom_linearinterpolation[:199,:,:]\ny_fiveVTStwo_linearinterpolation = f_fiveVTSrandom_linearinterpolation[1:,:,:]\n\n\nmodel = RecurrentGCN(node_features=1, filters=4)\n\noptimizer = torch.optim.Adam(model.parameters(), lr=0.01)\n\nmodel.train()\n\nfor epoch in tqdm(range(50)):\n    for time, (xt,yt) in enumerate(zip(X_fiveVTStwo_linearinterpolation,y_fiveVTStwo_linearinterpolation)):\n        y_hat = model(xt, edge_index, edge_attr)\n        cost = torch.mean((y_hat-yt)**2)\n        cost.backward()\n        optimizer.step()\n        optimizer.zero_grad()\n\n100%|██████████| 50/50 [00:32&lt;00:00,  1.55it/s]\n\n\n\nfhat_fiveVTStwo_linearinterpolation = torch.stack([model(xt, edge_index, edge_attr) for xt in X_fiveVTStwo_linearinterpolation]).detach().numpy()\n\n\nplt.plot(fhat_fiveVTStwo_linearinterpolation[:,2].data)"
  },
  {
    "objectID": "posts/2_research/2023-01-11-Algorithm_EX_1.html#원래-f",
    "href": "posts/2_research/2023-01-11-Algorithm_EX_1.html#원래-f",
    "title": "GCN Algorithm Example 1",
    "section": "2.3. 원래 f",
    "text": "2.3. 원래 f\n\nf = torch.tensor(fiveVTS).reshape(200,5,1).float()\n\n\nX = f[:199,:,:]\ny = f[1:,:,:]\n\n\nmodel = RecurrentGCN(node_features=1, filters=4)\n\noptimizer = torch.optim.Adam(model.parameters(), lr=0.01)\n\nmodel.train()\n\nfor epoch in tqdm(range(50)):\n    for time, (xt,yt) in enumerate(zip(X,y)):\n        y_hat = model(xt, edge_index, edge_attr)\n        cost = torch.mean((y_hat-yt)**2)\n        cost.backward()\n        optimizer.step()\n        optimizer.zero_grad()\n\n100%|██████████| 50/50 [00:32&lt;00:00,  1.55it/s]\n\n\n\nfhat_fiveVTS = torch.stack([model(xt, edge_index, edge_attr) for xt in X]).detach().numpy()\n\n\nplt.plot(fhat_fiveVTS[:,2].data)"
  },
  {
    "objectID": "posts/2_research/2023-01-11-Algorithm_EX_1.html#mean-2",
    "href": "posts/2_research/2023-01-11-Algorithm_EX_1.html#mean-2",
    "title": "GCN Algorithm Example 1",
    "section": "3.1. Mean",
    "text": "3.1. Mean\n\n3.1.1. Temporal\n\n1) Block\n\nw=np.zeros((5,199,199))\n\n\nfor k in range(5):\n    for i in range(199):\n        for j in range(199):\n            if i==j :\n                w[k,i,j] = 0\n            elif np.abs(i-j) &lt;= 1 : \n                w[k,i,j] = 1\n\n\nd = np.array([w[i].sum(axis=1) for i in range(5)])\nD= np.array([np.diag(d[i]) for i in range(5)])\nL = np.array([np.diag(1/np.sqrt(d[i])) @ (D[i]-w[i]) @ np.diag(1/np.sqrt(d[i])) for i in range(5)])\nlamb, Psi  = np.linalg.eigh(L)[0],np.linalg.eigh(L)[1]\nLamb = np.array([np.diag(lamb[i]) for i in range(5)])    \nfhatbar = np.hstack([Psi[i] @ fhat_mean[:,i] for i in range(5)])\n_fhatbar = fhatbar.reshape(5,199)\npower = _fhatbar**2 \nebayesthresh = importr('EbayesThresh').ebayesthresh\npower_threshed=np.array([np.array(ebayesthresh(FloatVector(_fhatbar[i]**2))) for i in range(5)])\nfhatbar_threshed = np.where(power_threshed&gt;0,_fhatbar,0)\nfhatbarhat = np.array([Psi[i] @ fhatbar_threshed[i] for i in range(5)])    \nfhatbarhat_mean_temporal = fhatbarhat.reshape(199,-1)\n\n\nplt.plot(fhatbarhat_mean_temporal[:,0])\nplt.plot(fhatbarhat_mean_temporal[:,1])\nplt.plot(fhatbarhat_mean_temporal[:,2])\nplt.plot(fhatbarhat_mean_temporal[:,3])\nplt.plot(fhatbarhat_mean_temporal[:,4])\n\n\n\n\n\n\n\n\n\n\n2) Random missing values\n\nw=np.zeros((5,199,199))\n\n\nfor k in range(5):\n    for i in range(199):\n        for j in range(199):\n            if i==j :\n                w[k,i,j] = 0\n            elif np.abs(i-j) &lt;= 1 : \n                w[k,i,j] = 1\n\n\nd = np.array([w[i].sum(axis=1) for i in range(5)])\nD= np.array([np.diag(d[i]) for i in range(5)])\nL = np.array([np.diag(1/np.sqrt(d[i])) @ (D[i]-w[i]) @ np.diag(1/np.sqrt(d[i])) for i in range(5)])\nlamb, Psi  = np.linalg.eigh(L)[0],np.linalg.eigh(L)[1]\nLamb = np.array([np.diag(lamb[i]) for i in range(5)])    \nfhatbar = np.hstack([Psi[i] @ fhat_fiveVTSrandom_mean[:,i] for i in range(5)])\n_fhatbar = fhatbar.reshape(5,199)\npower = _fhatbar**2 \nebayesthresh = importr('EbayesThresh').ebayesthresh\npower_threshed=np.array([np.array(ebayesthresh(FloatVector(_fhatbar[i]**2))) for i in range(5)])\nfhatbar_threshed = np.where(power_threshed&gt;0,_fhatbar,0)\nfhatbarhat = np.array([Psi[i] @ fhatbar_threshed[i] for i in range(5)])    \nfhatbarhat_random_mean_temporal = fhatbarhat.reshape(199,-1)\n\n\nplt.plot(fhatbarhat_random_mean_temporal[:,0])\nplt.plot(fhatbarhat_random_mean_temporal[:,1])\nplt.plot(fhatbarhat_random_mean_temporal[:,2])\nplt.plot(fhatbarhat_random_mean_temporal[:,3])\nplt.plot(fhatbarhat_random_mean_temporal[:,4])\n\n\n\n\n\n\n\n\n\n\n3) By 2\n\nw=np.zeros((5,199,199))\n\n\nfor k in range(5):\n    for i in range(199):\n        for j in range(199):\n            if i==j :\n                w[k,i,j] = 0\n            elif np.abs(i-j) &lt;= 1 : \n                w[k,i,j] = 1\n\n\nd = np.array([w[i].sum(axis=1) for i in range(5)])\nD= np.array([np.diag(d[i]) for i in range(5)])\nL = np.array([np.diag(1/np.sqrt(d[i])) @ (D[i]-w[i]) @ np.diag(1/np.sqrt(d[i])) for i in range(5)])\nlamb, Psi  = np.linalg.eigh(L)[0],np.linalg.eigh(L)[1]\nLamb = np.array([np.diag(lamb[i]) for i in range(5)])    \nfhatbar = np.hstack([Psi[i] @ fhat_fiveVTStwo_mean[:,i] for i in range(5)])\n_fhatbar = fhatbar.reshape(5,199)\npower = _fhatbar**2 \nebayesthresh = importr('EbayesThresh').ebayesthresh\npower_threshed=np.array([np.array(ebayesthresh(FloatVector(_fhatbar[i]**2))) for i in range(5)])\nfhatbar_threshed = np.where(power_threshed&gt;0,_fhatbar,0)\nfhatbarhat = np.array([Psi[i] @ fhatbar_threshed[i] for i in range(5)])    \nfhatbarhat_twomean_temporal = fhatbarhat.reshape(199,-1)\n\n\nplt.plot(fhatbarhat_twomean_temporal[:,0])\nplt.plot(fhatbarhat_twomean_temporal[:,1])\nplt.plot(fhatbarhat_twomean_temporal[:,2])\nplt.plot(fhatbarhat_twomean_temporal[:,3])\nplt.plot(fhatbarhat_twomean_temporal[:,4])\n\n\n\n\n\n\n\n\n\n\n\n3.1.2. Spatio\n\n1) Block\n\nw=np.zeros((5,5))\n\n\nfor i in range(5):\n    for j in range(5):\n        if i==j :\n            w[i,j] = 0\n        elif np.abs(i-j) &lt;= 1 : \n            w[i,j] = 1\n\n\nd = np.array(w.sum(axis=1))\nD = np.diag(d)\nL = np.array(np.diag(1/np.sqrt(d)) @ (D-w) @ np.diag(1/np.sqrt(d)))\nlamb, Psi = np.linalg.eigh(L)\nLamb = np.diag(lamb)\nfhatbar = Psi @ fhat_mean.reshape(5,199)\npower = fhatbar**2 \nebayesthresh = importr('EbayesThresh').ebayesthresh\npower_threshed=np.array([np.array(ebayesthresh(FloatVector(fhatbar[i]**2))) for i in range(5)])\nfhatbar_threshed = np.where(power_threshed&gt;0,fhatbar,0)\nfhatbarhat = Psi @ fhatbar_threshed\nfhatbarhat_mean_spatio = fhatbarhat.reshape(199,-1)\n\n\nplt.plot(fhatbarhat_mean_spatio[:,0])\nplt.plot(fhatbarhat_mean_spatio[:,1])\nplt.plot(fhatbarhat_mean_spatio[:,2])\nplt.plot(fhatbarhat_mean_spatio[:,3])\nplt.plot(fhatbarhat_mean_spatio[:,4])\n\n\n\n\n\n\n\n\n\n\n2) Random missing values\n\nw=np.zeros((5,5))\n\n\nfor i in range(5):\n    for j in range(5):\n        if i==j :\n            w[i,j] = 0\n        elif np.abs(i-j) &lt;= 1 : \n            w[i,j] = 1\n\n\nd = np.array(w.sum(axis=1))\nD = np.diag(d)\nL = np.array(np.diag(1/np.sqrt(d)) @ (D-w) @ np.diag(1/np.sqrt(d)))\nlamb, Psi = np.linalg.eigh(L)\nLamb = np.diag(lamb)\nfhatbar = Psi @ fhat_fiveVTSrandom_mean.reshape(5,199)\npower = fhatbar**2 \nebayesthresh = importr('EbayesThresh').ebayesthresh\npower_threshed=np.array([np.array(ebayesthresh(FloatVector(fhatbar[i]**2))) for i in range(5)])\nfhatbar_threshed = np.where(power_threshed&gt;0,fhatbar,0)\nfhatbarhat = Psi @ fhatbar_threshed\nfhatbarhat_random_mean_spatio = fhatbarhat.reshape(199,-1)\n\n\nplt.plot(fhatbarhat_random_mean_spatio[:,0])\nplt.plot(fhatbarhat_random_mean_spatio[:,1])\nplt.plot(fhatbarhat_random_mean_spatio[:,2])\nplt.plot(fhatbarhat_random_mean_spatio[:,3])\nplt.plot(fhatbarhat_random_mean_spatio[:,4])\n\n\n\n\n\n\n\n\n\n\n3) By 2\n\nw=np.zeros((5,5))\n\n\nfor i in range(5):\n    for j in range(5):\n        if i==j :\n            w[i,j] = 0\n        elif np.abs(i-j) &lt;= 1 : \n            w[i,j] = 1\n\n\nd = np.array(w.sum(axis=1))\nD = np.diag(d)\nL = np.array(np.diag(1/np.sqrt(d)) @ (D-w) @ np.diag(1/np.sqrt(d)))\nlamb, Psi = np.linalg.eigh(L)\nLamb = np.diag(lamb)\nfhatbar = Psi @ fhat_fiveVTStwo_mean.reshape(5,199)\npower = fhatbar**2 \nebayesthresh = importr('EbayesThresh').ebayesthresh\npower_threshed=np.array([np.array(ebayesthresh(FloatVector(fhatbar[i]**2))) for i in range(5)])\nfhatbar_threshed = np.where(power_threshed&gt;0,fhatbar,0)\nfhatbarhat = Psi @ fhatbar_threshed\nfhatbarhat_two_mean_spatio = fhatbarhat.reshape(199,-1)\n\n\nplt.plot(fhatbarhat_two_mean_spatio[:,0])\nplt.plot(fhatbarhat_two_mean_spatio[:,1])\nplt.plot(fhatbarhat_two_mean_spatio[:,2])\nplt.plot(fhatbarhat_two_mean_spatio[:,3])\nplt.plot(fhatbarhat_two_mean_spatio[:,4])\n\n\n\n\n\n\n\n\n\n\n\n3.1.3. Spatio-Temporal\n\n1) Block\n\nw=np.zeros((995,995))\n\n\nfor i in range(995):\n    for j in range(995):\n        if i==j :\n            w[i,j] = 0\n        elif np.abs(i-j) &lt;= 1 : \n            w[i,j] = 1\n\n\nd = np.array(w.sum(axis=1))\nD = np.diag(d)\nL = np.array(np.diag(1/np.sqrt(d)) @ (D-w) @ np.diag(1/np.sqrt(d)))\nlamb, Psi = np.linalg.eigh(L)\nLamb = np.diag(lamb)\nfhatbar = Psi @ fhat_mean.reshape(995,1)\npower = fhatbar**2 \nebayesthresh = importr('EbayesThresh').ebayesthresh\npower_threshed=np.array([np.array(ebayesthresh(FloatVector(fhatbar[i]**2))) for i in range(995)])\nfhatbar_threshed = np.where(power_threshed&gt;0,fhatbar,0)\nfhatbarhat = Psi @ fhatbar_threshed\nfhatbarhat_mean_spatio_temporal = fhatbarhat.reshape(199,5,1)\n\n\nplt.plot(fhatbarhat_mean_spatio_temporal[:,0])\nplt.plot(fhatbarhat_mean_spatio_temporal[:,1])\nplt.plot(fhatbarhat_mean_spatio_temporal[:,2])\nplt.plot(fhatbarhat_mean_spatio_temporal[:,3])\nplt.plot(fhatbarhat_mean_spatio_temporal[:,4])\n\n\n\n\n\n\n\n\n\n\n2) Random missing values\n\nw=np.zeros((995,995))\n\n\nfor i in range(995):\n    for j in range(995):\n        if i==j :\n            w[i,j] = 0\n        elif np.abs(i-j) &lt;= 1 : \n            w[i,j] = 1\n\n\nd = np.array(w.sum(axis=1))\nD = np.diag(d)\nL = np.array(np.diag(1/np.sqrt(d)) @ (D-w) @ np.diag(1/np.sqrt(d)))\nlamb, Psi = np.linalg.eigh(L)\nLamb = np.diag(lamb)\nfhatbar = Psi @ fhat_fiveVTSrandom_mean.reshape(995,1)\npower = fhatbar**2 \nebayesthresh = importr('EbayesThresh').ebayesthresh\npower_threshed=np.array([np.array(ebayesthresh(FloatVector(fhatbar[i]**2))) for i in range(995)])\nfhatbar_threshed = np.where(power_threshed&gt;0,fhatbar,0)\nfhatbarhat = Psi @ fhatbar_threshed\nfhatbarhat_random_mean_spatio_temporal = fhatbarhat.reshape(199,5,1)\n\n\nplt.plot(fhatbarhat_random_mean_spatio_temporal[:,0])\nplt.plot(fhatbarhat_random_mean_spatio_temporal[:,1])\nplt.plot(fhatbarhat_random_mean_spatio_temporal[:,2])\nplt.plot(fhatbarhat_random_mean_spatio_temporal[:,3])\nplt.plot(fhatbarhat_random_mean_spatio_temporal[:,4])\n\n\n\n\n\n\n\n\n\n\n3) By 2\n\nw=np.zeros((995,995))\n\n\nfor i in range(995):\n    for j in range(995):\n        if i==j :\n            w[i,j] = 0\n        elif np.abs(i-j) &lt;= 1 : \n            w[i,j] = 1\n\n\nd = np.array(w.sum(axis=1))\nD = np.diag(d)\nL = np.array(np.diag(1/np.sqrt(d)) @ (D-w) @ np.diag(1/np.sqrt(d)))\nlamb, Psi = np.linalg.eigh(L)\nLamb = np.diag(lamb)\nfhatbar = Psi @ fhat_fiveVTStwo_mean.reshape(995,1)\npower = fhatbar**2 \nebayesthresh = importr('EbayesThresh').ebayesthresh\npower_threshed=np.array([np.array(ebayesthresh(FloatVector(fhatbar[i]**2))) for i in range(995)])\nfhatbar_threshed = np.where(power_threshed&gt;0,fhatbar,0)\nfhatbarhat = Psi @ fhatbar_threshed\nfhatbarhat_two_mean_spatio_temporal = fhatbarhat.reshape(199,5,1)\n\n\nplt.plot(fhatbarhat_two_mean_spatio_temporal[:,0])\nplt.plot(fhatbarhat_two_mean_spatio_temporal[:,1])\nplt.plot(fhatbarhat_two_mean_spatio_temporal[:,2])\nplt.plot(fhatbarhat_two_mean_spatio_temporal[:,3])\nplt.plot(fhatbarhat_two_mean_spatio_temporal[:,4])"
  },
  {
    "objectID": "posts/2_research/2023-01-11-Algorithm_EX_1.html#linear-interpolation-2",
    "href": "posts/2_research/2023-01-11-Algorithm_EX_1.html#linear-interpolation-2",
    "title": "GCN Algorithm Example 1",
    "section": "3.2.linear interpolation",
    "text": "3.2.linear interpolation\n\n3.2.1. Temporal\n\n1) Block\n\nw=np.zeros((5,199,199))\n\n\nfor k in range(5):\n    for i in range(199):\n        for j in range(199):\n            if i==j :\n                w[k,i,j] = 0\n            elif np.abs(i-j) &lt;= 1 : \n                w[k,i,j] = 1\n\n\nd = np.array([w[i].sum(axis=1) for i in range(5)])\nD= np.array([np.diag(d[i]) for i in range(5)])\nL = np.array([np.diag(1/np.sqrt(d[i])) @ (D[i]-w[i]) @ np.diag(1/np.sqrt(d[i])) for i in range(5)])\nlamb, Psi  = np.linalg.eigh(L)[0],np.linalg.eigh(L)[1]\nLamb = np.array([np.diag(lamb[i]) for i in range(5)])\nfhatbar = np.hstack([Psi[i] @ fhat_linearinterpolation[:,i] for i in range(5)])\n_fhatbar = fhatbar.reshape(5,199)\npower = _fhatbar**2 \nebayesthresh = importr('EbayesThresh').ebayesthresh\npower_threshed=np.array([np.array(ebayesthresh(FloatVector(_fhatbar[i]**2))) for i in range(5)])    \nfhatbar_threshed = np.where(power_threshed&gt;0,_fhatbar,0)\nfhatbarhat = np.array([Psi[i] @ fhatbar_threshed[i] for i in range(5)])    \nfhatbarhat_linearinterpolation_temporal = fhatbarhat.reshape(199,-1)\n\n\nplt.plot(fhatbarhat_linearinterpolation_temporal[:,0])\nplt.plot(fhatbarhat_linearinterpolation_temporal[:,1])\nplt.plot(fhatbarhat_linearinterpolation_temporal[:,2])\nplt.plot(fhatbarhat_linearinterpolation_temporal[:,3])\nplt.plot(fhatbarhat_linearinterpolation_temporal[:,4])\n\n\n\n\n\n\n\n\n\n\n2) Random missing values\n\nw=np.zeros((5,199,199))\n\n\nfor k in range(5):\n    for i in range(199):\n        for j in range(199):\n            if i==j :\n                w[k,i,j] = 0\n            elif np.abs(i-j) &lt;= 1 : \n                w[k,i,j] = 1\n\n\nd = np.array([w[i].sum(axis=1) for i in range(5)])\nD= np.array([np.diag(d[i]) for i in range(5)])\nL = np.array([np.diag(1/np.sqrt(d[i])) @ (D[i]-w[i]) @ np.diag(1/np.sqrt(d[i])) for i in range(5)])\nlamb, Psi  = np.linalg.eigh(L)[0],np.linalg.eigh(L)[1]\nLamb = np.array([np.diag(lamb[i]) for i in range(5)])\nfhatbar = np.hstack([Psi[i] @ fhat_fiveVTSrandom_linearinterpolation[:,i] for i in range(5)])\n_fhatbar = fhatbar.reshape(5,199)\npower = _fhatbar**2 \nebayesthresh = importr('EbayesThresh').ebayesthresh\npower_threshed=np.array([np.array(ebayesthresh(FloatVector(_fhatbar[i]**2))) for i in range(5)])    \nfhatbar_threshed = np.where(power_threshed&gt;0,_fhatbar,0)\nfhatbarhat = np.array([Psi[i] @ fhatbar_threshed[i] for i in range(5)])    \nfhatbarhat_random_linearinterpolation_temporal = fhatbarhat.reshape(199,-1)\n\n\nplt.plot(fhatbarhat_random_linearinterpolation_temporal[:,0])\nplt.plot(fhatbarhat_random_linearinterpolation_temporal[:,1])\nplt.plot(fhatbarhat_random_linearinterpolation_temporal[:,2])\nplt.plot(fhatbarhat_random_linearinterpolation_temporal[:,3])\nplt.plot(fhatbarhat_random_linearinterpolation_temporal[:,4])\n\n\n\n\n\n\n\n\n\n\n3) By 2\n\nw=np.zeros((5,199,199))\n\n\nfor k in range(5):\n    for i in range(199):\n        for j in range(199):\n            if i==j :\n                w[k,i,j] = 0\n            elif np.abs(i-j) &lt;= 1 : \n                w[k,i,j] = 1\n\n\nd = np.array([w[i].sum(axis=1) for i in range(5)])\nD= np.array([np.diag(d[i]) for i in range(5)])\nL = np.array([np.diag(1/np.sqrt(d[i])) @ (D[i]-w[i]) @ np.diag(1/np.sqrt(d[i])) for i in range(5)])\nlamb, Psi  = np.linalg.eigh(L)[0],np.linalg.eigh(L)[1]\nLamb = np.array([np.diag(lamb[i]) for i in range(5)])\nfhatbar = np.hstack([Psi[i] @ fhat_fiveVTStwo_linearinterpolation[:,i] for i in range(5)])\n_fhatbar = fhatbar.reshape(5,199)\npower = _fhatbar**2 \nebayesthresh = importr('EbayesThresh').ebayesthresh\npower_threshed=np.array([np.array(ebayesthresh(FloatVector(_fhatbar[i]**2))) for i in range(5)])    \nfhatbar_threshed = np.where(power_threshed&gt;0,_fhatbar,0)\nfhatbarhat = np.array([Psi[i] @ fhatbar_threshed[i] for i in range(5)])    \nfhatbarhat_two_linearinterpolation_temporal = fhatbarhat.reshape(199,-1)\n\n\nplt.plot(fhatbarhat_two_linearinterpolation_temporal[:,0])\nplt.plot(fhatbarhat_two_linearinterpolation_temporal[:,1])\nplt.plot(fhatbarhat_two_linearinterpolation_temporal[:,2])\nplt.plot(fhatbarhat_two_linearinterpolation_temporal[:,3])\nplt.plot(fhatbarhat_two_linearinterpolation_temporal[:,4])\n\n\n\n\n\n\n\n\n\n\n\n3.2.2. Spatio\n\n1) Block\n\nw=np.zeros((5,5))\n\n\nfor i in range(5):\n    for j in range(5):\n        if i==j :\n            w[i,j] = 0\n        elif np.abs(i-j) &lt;= 1 : \n            w[i,j] = 1\n\n\nd = np.array(w.sum(axis=1))\nD = np.diag(d)\nL = np.array(np.diag(1/np.sqrt(d)) @ (D-w) @ np.diag(1/np.sqrt(d)))\nlamb, Psi = np.linalg.eigh(L)\nLamb = np.diag(lamb)\nfhatbar = Psi @ fhat_linearinterpolation.reshape(5,199)\npower = fhatbar**2 \nebayesthresh = importr('EbayesThresh').ebayesthresh\npower_threshed=np.array([np.array(ebayesthresh(FloatVector(fhatbar[i]**2))) for i in range(5)])    \nfhatbar_threshed = np.where(power_threshed&gt;0,fhatbar,0)\nfhatbarhat = Psi @ fhatbar_threshed\nfhatbarhat_linearinterpolation_spatio = fhatbarhat.reshape(199,-1)\n\n\nplt.plot(fhatbarhat_linearinterpolation_spatio[:,0])\nplt.plot(fhatbarhat_linearinterpolation_spatio[:,1])\nplt.plot(fhatbarhat_linearinterpolation_spatio[:,2])\nplt.plot(fhatbarhat_linearinterpolation_spatio[:,3])\nplt.plot(fhatbarhat_linearinterpolation_spatio[:,4])\n\n\n\n\n\n\n\n\n\n\n2) Random missing values\n\nw=np.zeros((5,5))\n\n\nfor i in range(5):\n    for j in range(5):\n        if i==j :\n            w[i,j] = 0\n        elif np.abs(i-j) &lt;= 1 : \n            w[i,j] = 1\n\n\nd = np.array(w.sum(axis=1))\nD = np.diag(d)\nL = np.array(np.diag(1/np.sqrt(d)) @ (D-w) @ np.diag(1/np.sqrt(d)))\nlamb, Psi = np.linalg.eigh(L)\nLamb = np.diag(lamb)\nfhatbar = Psi @ fhat_fiveVTSrandom_linearinterpolation.reshape(5,199)\npower = fhatbar**2 \nebayesthresh = importr('EbayesThresh').ebayesthresh\npower_threshed=np.array([np.array(ebayesthresh(FloatVector(fhatbar[i]**2))) for i in range(5)])    \nfhatbar_threshed = np.where(power_threshed&gt;0,fhatbar,0)\nfhatbarhat = Psi @ fhatbar_threshed\nfhatbarhat_random_linearinterpolation_spatio = fhatbarhat.reshape(199,-1)\n\n\nplt.plot(fhatbarhat_random_linearinterpolation_spatio[:,0])\nplt.plot(fhatbarhat_random_linearinterpolation_spatio[:,1])\nplt.plot(fhatbarhat_random_linearinterpolation_spatio[:,2])\nplt.plot(fhatbarhat_random_linearinterpolation_spatio[:,3])\nplt.plot(fhatbarhat_random_linearinterpolation_spatio[:,4])\n\n\n\n\n\n\n\n\n\n\n3) By 2\n\nw=np.zeros((5,5))\n\n\nfor i in range(5):\n    for j in range(5):\n        if i==j :\n            w[i,j] = 0\n        elif np.abs(i-j) &lt;= 1 : \n            w[i,j] = 1\n\n\nd = np.array(w.sum(axis=1))\nD = np.diag(d)\nL = np.array(np.diag(1/np.sqrt(d)) @ (D-w) @ np.diag(1/np.sqrt(d)))\nlamb, Psi = np.linalg.eigh(L)\nLamb = np.diag(lamb)\nfhatbar = Psi @ fhat_fiveVTStwo_linearinterpolation.reshape(5,199)\npower = fhatbar**2 \nebayesthresh = importr('EbayesThresh').ebayesthresh\npower_threshed=np.array([np.array(ebayesthresh(FloatVector(fhatbar[i]**2))) for i in range(5)])    \nfhatbar_threshed = np.where(power_threshed&gt;0,fhatbar,0)\nfhatbarhat = Psi @ fhatbar_threshed\nfhatbarhat_two_linearinterpolation_spatio = fhatbarhat.reshape(199,-1)\n\n\nplt.plot(fhatbarhat_two_linearinterpolation_spatio[:,0])\nplt.plot(fhatbarhat_two_linearinterpolation_spatio[:,1])\nplt.plot(fhatbarhat_two_linearinterpolation_spatio[:,2])\nplt.plot(fhatbarhat_two_linearinterpolation_spatio[:,3])\nplt.plot(fhatbarhat_two_linearinterpolation_spatio[:,4])\n\n\n\n\n\n\n\n\n\n\n\n3.2.3. Spatio-Temporal\n\n1) Block\n\nw=np.zeros((995,995))\n\n\nfor i in range(995):\n    for j in range(995):\n        if i==j :\n            w[i,j] = 0\n        elif np.abs(i-j) &lt;= 1 : \n            w[i,j] = 1\n\n\nd = np.array(w.sum(axis=1))\nD = np.diag(d)\nL = np.array(np.diag(1/np.sqrt(d)) @ (D-w) @ np.diag(1/np.sqrt(d)))\nlamb, Psi = np.linalg.eigh(L)\nLamb = np.diag(lamb)\nfhatbar = Psi @ fhat_linearinterpolation.reshape(995,1)\npower = fhatbar**2 \nebayesthresh = importr('EbayesThresh').ebayesthresh\npower_threshed=np.array([np.array(ebayesthresh(FloatVector(fhatbar[i]**2))) for i in range(995)])\nfhatbar_threshed = np.where(power_threshed&gt;0,fhatbar,0)\nfhatbarhat = Psi @ fhatbar_threshed\nfhat_linearinterpolation_spatio_temporal = fhatbarhat.reshape(199,5,1)\n\n\nplt.plot(fhat_linearinterpolation_spatio_temporal[:,0])\nplt.plot(fhat_linearinterpolation_spatio_temporal[:,1])\nplt.plot(fhat_linearinterpolation_spatio_temporal[:,2])\nplt.plot(fhat_linearinterpolation_spatio_temporal[:,3])\nplt.plot(fhat_linearinterpolation_spatio_temporal[:,4])\n\n\n\n\n\n\n\n\n\n\n2) Random missing values\n\nw=np.zeros((995,995))\n\n\nfor i in range(995):\n    for j in range(995):\n        if i==j :\n            w[i,j] = 0\n        elif np.abs(i-j) &lt;= 1 : \n            w[i,j] = 1\n\n\nd = np.array(w.sum(axis=1))\nD = np.diag(d)\nL = np.array(np.diag(1/np.sqrt(d)) @ (D-w) @ np.diag(1/np.sqrt(d)))\nlamb, Psi = np.linalg.eigh(L)\nLamb = np.diag(lamb)\nfhatbar = Psi @ fhat_fiveVTSrandom_linearinterpolation.reshape(995,1)\npower = fhatbar**2 \nebayesthresh = importr('EbayesThresh').ebayesthresh\npower_threshed=np.array([np.array(ebayesthresh(FloatVector(fhatbar[i]**2))) for i in range(995)])\nfhatbar_threshed = np.where(power_threshed&gt;0,fhatbar,0)\nfhatbarhat = Psi @ fhatbar_threshed\nfhat_random_linearinterpolation_spatio_temporal = fhatbarhat.reshape(199,5,1)\n\n\nplt.plot(fhat_random_linearinterpolation_spatio_temporal[:,0])\nplt.plot(fhat_random_linearinterpolation_spatio_temporal[:,1])\nplt.plot(fhat_random_linearinterpolation_spatio_temporal[:,2])\nplt.plot(fhat_random_linearinterpolation_spatio_temporal[:,3])\nplt.plot(fhat_random_linearinterpolation_spatio_temporal[:,4])\n\n\n\n\n\n\n\n\n\n\n3) By 2\n\nw=np.zeros((995,995))\n\n\nfor i in range(995):\n    for j in range(995):\n        if i==j :\n            w[i,j] = 0\n        elif np.abs(i-j) &lt;= 1 : \n            w[i,j] = 1\n\n\nd = np.array(w.sum(axis=1))\nD = np.diag(d)\nL = np.array(np.diag(1/np.sqrt(d)) @ (D-w) @ np.diag(1/np.sqrt(d)))\nlamb, Psi = np.linalg.eigh(L)\nLamb = np.diag(lamb)\nfhatbar = Psi @ fhat_fiveVTStwo_linearinterpolation.reshape(995,1)\npower = fhatbar**2 \nebayesthresh = importr('EbayesThresh').ebayesthresh\npower_threshed=np.array([np.array(ebayesthresh(FloatVector(fhatbar[i]**2))) for i in range(995)])\nfhatbar_threshed = np.where(power_threshed&gt;0,fhatbar,0)\nfhatbarhat = Psi @ fhatbar_threshed\nfhat_two_linearinterpolation_spatio_temporal = fhatbarhat.reshape(199,5,1)\n\n\nplt.plot(fhat_two_linearinterpolation_spatio_temporal[:,0])\nplt.plot(fhat_two_linearinterpolation_spatio_temporal[:,1])\nplt.plot(fhat_two_linearinterpolation_spatio_temporal[:,2])\nplt.plot(fhat_two_linearinterpolation_spatio_temporal[:,3])\nplt.plot(fhat_two_linearinterpolation_spatio_temporal[:,4])"
  },
  {
    "objectID": "posts/2_research/2023-01-11-Algorithm_EX_1.html#original",
    "href": "posts/2_research/2023-01-11-Algorithm_EX_1.html#original",
    "title": "GCN Algorithm Example 1",
    "section": "3.3. original",
    "text": "3.3. original\n\n1) Temporal\n\nw=np.zeros((5,199,199))\n\n\nfor k in range(5):\n    for i in range(199):\n        for j in range(199):\n            if i==j :\n                w[k,i,j] = 0\n            elif np.abs(i-j) &lt;= 1 : \n                w[k,i,j] = 1\n\n\nd = np.array([w[i].sum(axis=1) for i in range(5)])\nD= np.array([np.diag(d[i]) for i in range(5)])\nL = np.array([np.diag(1/np.sqrt(d[i])) @ (D[i]-w[i]) @ np.diag(1/np.sqrt(d[i])) for i in range(5)])\nlamb, Psi  = np.linalg.eigh(L)[0],np.linalg.eigh(L)[1]\nLamb = np.array([np.diag(lamb[i]) for i in range(5)])    \nfhatbar = np.hstack([Psi[i] @ fhat_fiveVTS[:,i] for i in range(5)])\n_fhatbar = fhatbar.reshape(5,199)\npower = _fhatbar**2 \nebayesthresh = importr('EbayesThresh').ebayesthresh\npower_threshed=np.array([np.array(ebayesthresh(FloatVector(_fhatbar[i]**2))) for i in range(5)])\nfhatbar_threshed = np.where(power_threshed&gt;0,_fhatbar,0)\nfhatbarhat = np.array([Psi[i] @ fhatbar_threshed[i] for i in range(5)])    \nfhatbarhat_temporal = fhatbarhat.reshape(199,-1)\n\n\nplt.plot(fhatbarhat_temporal[:,0])\nplt.plot(fhatbarhat_temporal[:,1])\nplt.plot(fhatbarhat_temporal[:,2])\nplt.plot(fhatbarhat_temporal[:,3])\nplt.plot(fhatbarhat_temporal[:,4])\n\n\n\n\n\n\n\n\n\n\n2) Spatio\n\nw=np.zeros((5,5))\n\n\nfor i in range(5):\n    for j in range(5):\n        if i==j :\n            w[i,j] = 0\n        elif np.abs(i-j) &lt;= 1 : \n            w[i,j] = 1\n\n\nd = np.array(w.sum(axis=1))\nD = np.diag(d)\nL = np.array(np.diag(1/np.sqrt(d)) @ (D-w) @ np.diag(1/np.sqrt(d)))\nlamb, Psi = np.linalg.eigh(L)\nLamb = np.diag(lamb)\nfhatbar = Psi @ fhat_fiveVTS.reshape(5,199)\npower = fhatbar**2 \nebayesthresh = importr('EbayesThresh').ebayesthresh\npower_threshed=np.array([np.array(ebayesthresh(FloatVector(fhatbar[i]**2))) for i in range(5)])    \nfhatbar_threshed = np.where(power_threshed&gt;0,fhatbar,0)\nfhatbarhat = Psi @ fhatbar_threshed\nfhatbarhat_spatio = fhatbarhat.reshape(199,-1)\n\n\nplt.plot(fhatbarhat_spatio[:,0])\nplt.plot(fhatbarhat_spatio[:,1])\nplt.plot(fhatbarhat_spatio[:,2])\nplt.plot(fhatbarhat_spatio[:,3])\nplt.plot(fhatbarhat_spatio[:,4])\n\n\n\n\n\n\n\n\n\n\n3) Spatio-Temporal\n\nw=np.zeros((995,995))\n\n\nfor i in range(995):\n    for j in range(995):\n        if i==j :\n            w[i,j] = 0\n        elif np.abs(i-j) &lt;= 1 : \n            w[i,j] = 1\n\n\nd = np.array(w.sum(axis=1))\nD = np.diag(d)\nL = np.array(np.diag(1/np.sqrt(d)) @ (D-w) @ np.diag(1/np.sqrt(d)))\nlamb, Psi = np.linalg.eigh(L)\nLamb = np.diag(lamb)\nfhatbar = Psi @ fhat_fiveVTS.reshape(995,1)\npower = fhatbar**2 \nebayesthresh = importr('EbayesThresh').ebayesthresh\npower_threshed=np.array([np.array(ebayesthresh(FloatVector(fhatbar[i]**2))) for i in range(995)])\nfhatbar_threshed = np.where(power_threshed&gt;0,fhatbar,0)\nfhatbarhat = Psi @ fhatbar_threshed\nfhat_spatio_temporal = fhatbarhat.reshape(199,5,1)\n\n\nplt.plot(fhat_spatio_temporal[:,0])\nplt.plot(fhat_spatio_temporal[:,1])\nplt.plot(fhat_spatio_temporal[:,2])\nplt.plot(fhat_spatio_temporal[:,3])\nplt.plot(fhat_spatio_temporal[:,4])"
  },
  {
    "objectID": "posts/2_research/2023-05-17-xx_GCONVGRU_simulation_table_reshape.html",
    "href": "posts/2_research/2023-05-17-xx_GCONVGRU_simulation_table_reshape.html",
    "title": "GConvGRU_Simulation Tables_reshape",
    "section": "",
    "text": "Simulation Tables"
  },
  {
    "objectID": "posts/2_research/2023-05-17-xx_GCONVGRU_simulation_table_reshape.html#baseline",
    "href": "posts/2_research/2023-05-17-xx_GCONVGRU_simulation_table_reshape.html#baseline",
    "title": "GConvGRU_Simulation Tables_reshape",
    "section": "Baseline",
    "text": "Baseline\n\npd.merge(data.query(\"dataset=='fivenodes' and mtype!='rand' and mtype!='block'\").groupby(['nof_filters','method','lags'])['mse'].mean().reset_index(),\n         data.query(\"dataset=='fivenodes' and mtype!='rand' and mtype!='block'\").groupby(['nof_filters','method','lags'])['mse'].std().reset_index(),\n         on=['method','nof_filters','lags']).rename(columns={'mse_x':'mean','mse_y':'std'}).round(3)\n\n\n\n\n\n\n\n\nnof_filters\nmethod\nlags\nmean\nstd\n\n\n\n\n0\n12\nIT-STGCN\n2\n0.732\n0.005\n\n\n1\n12\nSTGCN\n2\n0.732\n0.005"
  },
  {
    "objectID": "posts/2_research/2023-05-17-xx_GCONVGRU_simulation_table_reshape.html#random",
    "href": "posts/2_research/2023-05-17-xx_GCONVGRU_simulation_table_reshape.html#random",
    "title": "GConvGRU_Simulation Tables_reshape",
    "section": "Random",
    "text": "Random\n\npd.merge(data.query(\"dataset=='fivenodes' and mtype=='rand'\").groupby(['mrate','nof_filters','method','lags'])['mse'].mean().reset_index(),\n         data.query(\"dataset=='fivenodes' and mtype=='rand'\").groupby(['mrate','nof_filters','method','lags'])['mse'].std().reset_index(),\n         on=['method','nof_filters','mrate','lags']).rename(columns={'mse_x':'mean','mse_y':'std'}).round(3).query(\"nof_filters==12\")\n\n\n\n\n\n\n\n\nmrate\nnof_filters\nmethod\nlags\nmean\nstd\n\n\n\n\n0\n0.7\n12\nIT-STGCN\n2\n1.167\n0.059\n\n\n1\n0.7\n12\nSTGCN\n2\n2.077\n0.252\n\n\n2\n0.8\n12\nIT-STGCN\n2\n1.371\n0.097\n\n\n3\n0.8\n12\nSTGCN\n2\n2.432\n0.263"
  },
  {
    "objectID": "posts/2_research/2023-05-17-xx_GCONVGRU_simulation_table_reshape.html#block",
    "href": "posts/2_research/2023-05-17-xx_GCONVGRU_simulation_table_reshape.html#block",
    "title": "GConvGRU_Simulation Tables_reshape",
    "section": "Block",
    "text": "Block\n\npd.merge(data.query(\"dataset=='fivenodes' and mtype=='block'\").groupby(['mrate','nof_filters','method'])['mse'].mean().reset_index(),\n         data.query(\"dataset=='fivenodes' and mtype=='block'\").groupby(['mrate','nof_filters','method'])['mse'].std().reset_index(),\n         on=['method','nof_filters','mrate']).rename(columns={'mse_x':'mean','mse_y':'std'}).round(3)\n\n\n\n\n\n\n\n\nmrate\nnof_filters\nmethod\nmean\nstd\n\n\n\n\n0\n0.125\n12\nIT-STGCN\n1.160\n0.042\n\n\n1\n0.125\n12\nSTGCN\n1.215\n0.036"
  },
  {
    "objectID": "posts/2_research/2023-05-17-xx_GCONVGRU_simulation_table_reshape.html#baseline-1",
    "href": "posts/2_research/2023-05-17-xx_GCONVGRU_simulation_table_reshape.html#baseline-1",
    "title": "GConvGRU_Simulation Tables_reshape",
    "section": "Baseline",
    "text": "Baseline\n\npd.merge(data.query(\"dataset=='chickenpox' and mtype!='rand' and mtype!='block'\").groupby(['nof_filters','method'])['mse'].mean().reset_index(),\n         data.query(\"dataset=='chickenpox' and mtype!='rand' and mtype!='block'\").groupby(['nof_filters','method'])['mse'].std().reset_index(),\n         on=['method','nof_filters']).rename(columns={'mse_x':'mean','mse_y':'std'}).round(3).query(\"nof_filters==16\")\n\n\n\n\n\n\n\n\nnof_filters\nmethod\nmean\nstd\n\n\n\n\n0\n16\nIT-STGCN\n0.752\n0.013\n\n\n1\n16\nSTGCN\n0.752\n0.012"
  },
  {
    "objectID": "posts/2_research/2023-05-17-xx_GCONVGRU_simulation_table_reshape.html#random-1",
    "href": "posts/2_research/2023-05-17-xx_GCONVGRU_simulation_table_reshape.html#random-1",
    "title": "GConvGRU_Simulation Tables_reshape",
    "section": "Random",
    "text": "Random\n\npd.merge(data.query(\"dataset=='chickenpox' and mtype=='rand'\").groupby(['mrate','inter_method','nof_filters','method'])['mse'].mean().reset_index(),\n         data.query(\"dataset=='chickenpox' and mtype=='rand'\").groupby(['mrate','inter_method','nof_filters','method'])['mse'].std().reset_index(),\n         on=['method','inter_method','mrate','nof_filters']).rename(columns={'mse_x':'mean','mse_y':'std'}).round(3)\n\n\n\n\n\n\n\n\nmrate\ninter_method\nnof_filters\nmethod\nmean\nstd\n\n\n\n\n0\n0.3\nlinear\n16\nIT-STGCN\n0.851\n0.031\n\n\n1\n0.3\nlinear\n16\nSTGCN\n1.087\n0.046\n\n\n2\n0.8\nlinear\n16\nIT-STGCN\n1.586\n0.199\n\n\n3\n0.8\nlinear\n16\nSTGCN\n2.529\n0.292"
  },
  {
    "objectID": "posts/2_research/2023-05-17-xx_GCONVGRU_simulation_table_reshape.html#block-1",
    "href": "posts/2_research/2023-05-17-xx_GCONVGRU_simulation_table_reshape.html#block-1",
    "title": "GConvGRU_Simulation Tables_reshape",
    "section": "Block",
    "text": "Block\n\npd.merge(data.query(\"dataset=='chickenpox' and mtype=='block'\").groupby(['inter_method','mrate','nof_filters','method'])['mse'].mean().reset_index(),\n         data.query(\"dataset=='chickenpox' and mtype=='block'\").groupby(['inter_method','mrate','nof_filters','method'])['mse'].std().reset_index(),\n         on=['method','inter_method','mrate','nof_filters']).rename(columns={'mse_x':'mean','mse_y':'std'})\n\n\n\n\n\n\n\n\ninter_method\nmrate\nnof_filters\nmethod\nmean\nstd\n\n\n\n\n0\nlinear\n0.28777\n16\nIT-STGCN\n0.807041\n0.016362\n\n\n1\nlinear\n0.28777\n16\nSTGCN\n0.828224\n0.021919\n\n\n2\nnearest\n0.28777\n16\nIT-STGCN\n0.823756\n0.022918\n\n\n3\nnearest\n0.28777\n16\nSTGCN\n0.828498\n0.022007"
  },
  {
    "objectID": "posts/2_research/2023-05-17-xx_GCONVGRU_simulation_table_reshape.html#baseline-2",
    "href": "posts/2_research/2023-05-17-xx_GCONVGRU_simulation_table_reshape.html#baseline-2",
    "title": "GConvGRU_Simulation Tables_reshape",
    "section": "Baseline",
    "text": "Baseline\n\npd.merge(data.query(\"dataset=='pedalme' and mtype!='rand' and mtype!='block'\").groupby(['lags','nof_filters','method'])['mse'].mean().reset_index(),\n         data.query(\"dataset=='pedalme' and mtype!='rand' and mtype!='block'\").groupby(['lags','nof_filters','method'])['mse'].std().reset_index(),\n         on=['method','lags','nof_filters']).rename(columns={'mse_x':'mean','mse_y':'std'}).round(3).query(\"lags==4\")\n\n\n\n\n\n\n\n\nlags\nnof_filters\nmethod\nmean\nstd\n\n\n\n\n0\n4\n12\nIT-STGCN\n1.233\n0.115\n\n\n1\n4\n12\nSTGCN\n1.233\n0.099"
  },
  {
    "objectID": "posts/2_research/2023-05-17-xx_GCONVGRU_simulation_table_reshape.html#random-2",
    "href": "posts/2_research/2023-05-17-xx_GCONVGRU_simulation_table_reshape.html#random-2",
    "title": "GConvGRU_Simulation Tables_reshape",
    "section": "Random",
    "text": "Random\n\npd.merge(data.query(\"dataset=='pedalme' and mtype=='rand'\").groupby(['mrate','lags','inter_method','method'])['mse'].mean().reset_index(),\n         data.query(\"dataset=='pedalme' and mtype=='rand'\").groupby(['mrate','lags','inter_method','method'])['mse'].std().reset_index(),\n         on=['method','mrate','lags','inter_method']).rename(columns={'mse_x':'mean','mse_y':'std'}).round(3)\n\n\n\n\n\n\n\n\nmrate\nlags\ninter_method\nmethod\nmean\nstd\n\n\n\n\n0\n0.3\n4\nlinear\nIT-STGCN\n1.354\n0.134\n\n\n1\n0.3\n4\nlinear\nSTGCN\n1.575\n0.198\n\n\n2\n0.3\n4\nnearest\nIT-STGCN\n1.385\n0.173\n\n\n3\n0.3\n4\nnearest\nSTGCN\n1.527\n0.342\n\n\n4\n0.6\n4\nlinear\nIT-STGCN\n1.516\n0.211\n\n\n5\n0.6\n4\nlinear\nSTGCN\n1.655\n0.179\n\n\n6\n0.6\n4\nnearest\nIT-STGCN\n1.625\n0.324\n\n\n7\n0.6\n4\nnearest\nSTGCN\n1.851\n0.254"
  },
  {
    "objectID": "posts/2_research/2023-05-17-xx_GCONVGRU_simulation_table_reshape.html#block-2",
    "href": "posts/2_research/2023-05-17-xx_GCONVGRU_simulation_table_reshape.html#block-2",
    "title": "GConvGRU_Simulation Tables_reshape",
    "section": "Block",
    "text": "Block\n\npd.merge(data.query(\"dataset=='pedalme' and mtype=='block'\").groupby(['mrate','lags','inter_method','method'])['mse'].mean().reset_index(),\n         data.query(\"dataset=='pedalme' and mtype=='block'\").groupby(['mrate','lags','inter_method','method'])['mse'].std().reset_index(),\n         on=['method','mrate','lags','inter_method']).rename(columns={'mse_x':'mean','mse_y':'std'}).round(3).query(\"lags==4\")\n\n\n\n\n\n\n\n\nmrate\nlags\ninter_method\nmethod\nmean\nstd\n\n\n\n\n0\n0.286\n4\nlinear\nIT-STGCN\n1.329\n0.131\n\n\n1\n0.286\n4\nlinear\nSTGCN\n1.320\n0.111\n\n\n2\n0.286\n4\nnearest\nIT-STGCN\n1.289\n0.115\n\n\n3\n0.286\n4\nnearest\nSTGCN\n1.270\n0.114"
  },
  {
    "objectID": "posts/2_research/2023-05-17-xx_GCONVGRU_simulation_table_reshape.html#w_st",
    "href": "posts/2_research/2023-05-17-xx_GCONVGRU_simulation_table_reshape.html#w_st",
    "title": "GConvGRU_Simulation Tables_reshape",
    "section": "W_st",
    "text": "W_st\n\npd.merge(data_pedal2.query(\"mtype=='rand'\").groupby(['mrate','lags','inter_method','method'])['mse'].mean().reset_index(),\n         data_pedal2.query(\"mtype=='rand'\").groupby(['mrate','lags','inter_method','method'])['mse'].std().reset_index(),\n         on=['method','mrate','lags','inter_method']).rename(columns={'mse_x':'mean','mse_y':'std'}).round(3).query(\"lags==4\")\n\n\n\n\n\n\n\n\nmrate\nlags\ninter_method\nmethod\nmean\nstd\n\n\n\n\n0\n0.3\n4\nlinear\nIT-STGCN\n1.270\n0.163\n\n\n1\n0.3\n4\nlinear\nSTGCN\n1.556\n0.264\n\n\n2\n0.3\n4\nnearest\nIT-STGCN\n1.324\n0.163\n\n\n3\n0.3\n4\nnearest\nSTGCN\n1.520\n0.206\n\n\n4\n0.6\n4\nlinear\nIT-STGCN\n1.434\n0.222\n\n\n5\n0.6\n4\nlinear\nSTGCN\n1.678\n0.211\n\n\n6\n0.6\n4\nnearest\nIT-STGCN\n1.410\n0.208\n\n\n7\n0.6\n4\nnearest\nSTGCN\n1.771\n0.220\n\n\n\n\n\n\n\n\npd.merge(data_pedal2.query(\"mtype=='block'\").groupby(['mrate','lags','inter_method','method'])['mse'].mean().reset_index(),\n         data_pedal2.query(\"mtype=='block'\").groupby(['mrate','lags','inter_method','method'])['mse'].std().reset_index(),\n         on=['method','mrate','lags','inter_method']).rename(columns={'mse_x':'mean','mse_y':'std'}).round(3).query(\"lags==4\")\n\n\n\n\n\n\n\n\nmrate\nlags\ninter_method\nmethod\nmean\nstd\n\n\n\n\n0\n0.286\n4\nlinear\nIT-STGCN\n1.391\n0.151\n\n\n1\n0.286\n4\nlinear\nSTGCN\n1.420\n0.110\n\n\n2\n0.286\n4\nnearest\nIT-STGCN\n1.361\n0.114\n\n\n3\n0.286\n4\nnearest\nSTGCN\n1.430\n0.145"
  },
  {
    "objectID": "posts/2_research/2023-05-17-xx_GCONVGRU_simulation_table_reshape.html#baseline-3",
    "href": "posts/2_research/2023-05-17-xx_GCONVGRU_simulation_table_reshape.html#baseline-3",
    "title": "GConvGRU_Simulation Tables_reshape",
    "section": "Baseline",
    "text": "Baseline\n\npd.merge(data.query(\"dataset=='wikimath' and mrate==0\").groupby(['lags','nof_filters','method'])['mse'].mean().reset_index(),\n         data.query(\"dataset=='wikimath' and mrate==0\").groupby(['lags','nof_filters','method'])['mse'].std().reset_index(),\n         on=['lags','nof_filters','method']).rename(columns={'mse_x':'mean','mse_y':'std'}).round(3)\n\n\n\n\n\n\n\n\nlags\nnof_filters\nmethod\nmean\nstd\n\n\n\n\n0\n8\n12\nIT-STGCN\n0.529\n0.003\n\n\n1\n8\n12\nSTGCN\n0.528\n0.003"
  },
  {
    "objectID": "posts/2_research/2023-05-17-xx_GCONVGRU_simulation_table_reshape.html#random-3",
    "href": "posts/2_research/2023-05-17-xx_GCONVGRU_simulation_table_reshape.html#random-3",
    "title": "GConvGRU_Simulation Tables_reshape",
    "section": "Random",
    "text": "Random\n\npd.merge(data.query(\"dataset=='wikimath' and mtype=='rand'\").groupby(['mrate','lags','method'])['mse'].mean().reset_index(),\n         data.query(\"dataset=='wikimath' and mtype=='rand'\").groupby(['mrate','lags','method'])['mse'].std().reset_index(),\n         on=['method','mrate','lags']).rename(columns={'mse_x':'mean','mse_y':'std'}).round(3)\n\n\n\n\n\n\n\n\nmrate\nlags\nmethod\nmean\nstd\n\n\n\n\n0\n0.3\n8\nIT-STGCN\n0.518\n0.002\n\n\n1\n0.3\n8\nSTGCN\n0.570\n0.006\n\n\n2\n0.8\n8\nIT-STGCN\n0.687\n0.021\n\n\n3\n0.8\n8\nSTGCN\n0.932\n0.043"
  },
  {
    "objectID": "posts/2_research/2023-05-17-xx_GCONVGRU_simulation_table_reshape.html#block-3",
    "href": "posts/2_research/2023-05-17-xx_GCONVGRU_simulation_table_reshape.html#block-3",
    "title": "GConvGRU_Simulation Tables_reshape",
    "section": "Block",
    "text": "Block\n\npd.merge(data.query(\"dataset=='wikimath' and mtype=='block'\").groupby(['mrate','lags','method'])['mse'].mean().reset_index(),\n         data.query(\"dataset=='wikimath' and mtype=='block'\").groupby(['mrate','lags','method'])['mse'].std().reset_index(),\n         on=['method','mrate','lags']).rename(columns={'mse_x':'mean','mse_y':'std'})\n\n\n\n\n\n\n\n\nmrate\nlags\nmethod\nmean\nstd\n\n\n\n\n0\n0.003835\n8\nIT-STGCN\n0.528737\n0.002806\n\n\n1\n0.003835\n8\nSTGCN\n0.527871\n0.002606\n\n\n2\n0.095870\n8\nIT-STGCN\n0.529440\n0.003820\n\n\n3\n0.095870\n8\nSTGCN\n0.544176\n0.010772"
  },
  {
    "objectID": "posts/2_research/2023-05-17-xx_GCONVGRU_simulation_table_reshape.html#missing-values-on-the-same-nodes",
    "href": "posts/2_research/2023-05-17-xx_GCONVGRU_simulation_table_reshape.html#missing-values-on-the-same-nodes",
    "title": "GConvGRU_Simulation Tables_reshape",
    "section": "missing values on the same nodes",
    "text": "missing values on the same nodes\n\npd.merge(data_wiki_GSO.groupby(['mrate','lags','method'])['mse'].mean().reset_index(),\n        data_wiki_GSO.groupby(['mrate','lags','method'])['mse'].std().reset_index(),\n         on=['method','mrate','lags']).rename(columns={'mse_x':'mean','mse_y':'std'}).round(3)\n\n\n\n\n\n\n\n\nmrate\nlags\nmethod\nmean\nstd\n\n\n\n\n0\n0.512\n8\nIT-STGCN\n0.531\n0.002\n\n\n1\n0.512\n8\nSTGCN\n0.720\n0.013"
  },
  {
    "objectID": "posts/2_research/2023-05-17-xx_GCONVGRU_simulation_table_reshape.html#baseline-4",
    "href": "posts/2_research/2023-05-17-xx_GCONVGRU_simulation_table_reshape.html#baseline-4",
    "title": "GConvGRU_Simulation Tables_reshape",
    "section": "Baseline",
    "text": "Baseline\n\npd.merge(data.query(\"dataset=='windmillsmall' and mrate==0\").groupby(['lags','method'])['mse'].mean().reset_index(),\n         data.query(\"dataset=='windmillsmall' and mrate==0\").groupby(['lags','method'])['mse'].std().reset_index(),\n         on=['method','lags']).rename(columns={'mse_x':'mean','mse_y':'std'}).round(3)\n\n\n\n\n\n\n\n\nlags\nmethod\nmean\nstd\n\n\n\n\n0\n8\nIT-STGCN\n1.004\n0.004\n\n\n1\n8\nSTGCN\n1.003\n0.004"
  },
  {
    "objectID": "posts/2_research/2023-05-17-xx_GCONVGRU_simulation_table_reshape.html#random-4",
    "href": "posts/2_research/2023-05-17-xx_GCONVGRU_simulation_table_reshape.html#random-4",
    "title": "GConvGRU_Simulation Tables_reshape",
    "section": "Random",
    "text": "Random\n\npd.merge(data.query(\"dataset=='windmillsmall' and mtype=='rand'\").groupby(['mrate','lags','method'])['mse'].mean().reset_index(),\n         data.query(\"dataset=='windmillsmall' and mtype=='rand'\").groupby(['mrate','lags','method'])['mse'].std().reset_index(),\n         on=['method','mrate','lags']).rename(columns={'mse_x':'mean','mse_y':'std'}).round(3)\n\n\n\n\n\n\n\n\nmrate\nlags\nmethod\nmean\nstd\n\n\n\n\n0\n0.7\n8\nIT-STGCN\n1.193\n0.045\n\n\n1\n0.7\n8\nSTGCN\n1.661\n0.076"
  },
  {
    "objectID": "posts/2_research/2023-05-17-xx_GCONVGRU_simulation_table_reshape.html#block-4",
    "href": "posts/2_research/2023-05-17-xx_GCONVGRU_simulation_table_reshape.html#block-4",
    "title": "GConvGRU_Simulation Tables_reshape",
    "section": "Block",
    "text": "Block\n\npd.merge(data.query(\"dataset=='windmillsmall' and mtype=='block'\").groupby(['mrate','lags','method'])['mse'].mean().reset_index(),\n         data.query(\"dataset=='windmillsmall' and mtype=='block'\").groupby(['mrate','lags','method'])['mse'].std().reset_index(),\n         on=['method','mrate','lags']).rename(columns={'mse_x':'mean','mse_y':'std'}).round(3)\n\n\n\n\n\n\n\n\nmean\nmrate\nlags\nmethod\nstd"
  },
  {
    "objectID": "posts/2_research/2023-05-17-xx_GCONVGRU_simulation_table_reshape.html#baseline-5",
    "href": "posts/2_research/2023-05-17-xx_GCONVGRU_simulation_table_reshape.html#baseline-5",
    "title": "GConvGRU_Simulation Tables_reshape",
    "section": "Baseline",
    "text": "Baseline\n\npd.merge(data.query(\"dataset=='monte' and mrate==0\").groupby(['lags','method'])['mse'].mean().reset_index(),\n         data.query(\"dataset=='monte' and mrate==0\").groupby(['lags','method'])['mse'].std().reset_index(),\n         on=['method','lags']).rename(columns={'mse_x':'mean','mse_y':'std'}).round(3)\n\n\n\n\n\n\n\n\nlags\nmethod\nmean\nstd\n\n\n\n\n0\n4\nIT-STGCN\n0.931\n0.001\n\n\n1\n4\nSTGCN\n0.931\n0.002"
  },
  {
    "objectID": "posts/2_research/2023-05-17-xx_GCONVGRU_simulation_table_reshape.html#random-5",
    "href": "posts/2_research/2023-05-17-xx_GCONVGRU_simulation_table_reshape.html#random-5",
    "title": "GConvGRU_Simulation Tables_reshape",
    "section": "Random",
    "text": "Random\n\npd.merge(data.query(\"dataset=='monte' and mtype=='rand'\").groupby(['mrate','lags','inter_method','method'])['mse'].mean().reset_index(),\n         data.query(\"dataset=='monte' and mtype=='rand'\").groupby(['mrate','lags','inter_method','method'])['mse'].std().reset_index(),\n         on=['mrate','inter_method','method','mrate','lags']).rename(columns={'mse_x':'mean','mse_y':'std'})\n\n\n\n\n\n\n\n\nmrate\nlags\ninter_method\nmethod\nmean\nstd\n\n\n\n\n0\n0.8\n4\nnearest\nIT-STGCN\n1.09556\n0.018743\n\n\n1\n0.8\n4\nnearest\nSTGCN\n1.51600\n0.039793"
  },
  {
    "objectID": "posts/2_research/2023-05-17-xx_GCONVGRU_simulation_table_reshape.html#block-5",
    "href": "posts/2_research/2023-05-17-xx_GCONVGRU_simulation_table_reshape.html#block-5",
    "title": "GConvGRU_Simulation Tables_reshape",
    "section": "Block",
    "text": "Block\n\npd.merge(data.query(\"dataset=='monte' and mtype=='block'\").groupby(['mrate','lags','inter_method','method'])['mse'].mean().reset_index(),\n         data.query(\"dataset=='monte' and mtype=='block'\").groupby(['mrate','lags','inter_method','method'])['mse'].std().reset_index(),\n         on=['method','mrate','inter_method','lags']).rename(columns={'mse_x':'mean','mse_y':'std'})\n\n\n\n\n\n\n\n\nmrate\nlags\ninter_method\nmethod\nmean\nstd\n\n\n\n\n0\n0.149142\n4\ncubic\nIT-STGCN\n1.022866\n0.021048\n\n\n1\n0.149142\n4\ncubic\nSTGCN\n1.028363\n0.031275\n\n\n2\n0.149142\n4\nlinear\nIT-STGCN\n0.930156\n0.001956\n\n\n3\n0.149142\n4\nlinear\nSTGCN\n0.934719\n0.004724\n\n\n4\n0.149142\n4\nnearest\nIT-STGCN\n0.931785\n0.002158\n\n\n5\n0.149142\n4\nnearest\nSTGCN\n0.934596\n0.003562"
  },
  {
    "objectID": "posts/2_research/2023-05-30-GConvLSTM_simulation_table_reshape.html",
    "href": "posts/2_research/2023-05-30-GConvLSTM_simulation_table_reshape.html",
    "title": "GConvLSTM_Simulation Tables_reshape",
    "section": "",
    "text": "Simulation Tables"
  },
  {
    "objectID": "posts/2_research/2023-05-30-GConvLSTM_simulation_table_reshape.html#baseline",
    "href": "posts/2_research/2023-05-30-GConvLSTM_simulation_table_reshape.html#baseline",
    "title": "GConvLSTM_Simulation Tables_reshape",
    "section": "Baseline",
    "text": "Baseline\n\npd.merge(data.query(\"dataset=='fivenodes' and mtype!='rand' and mtype!='block'\").groupby(['nof_filters','method','lags'])['mse'].mean().reset_index(),\n         data.query(\"dataset=='fivenodes' and mtype!='rand' and mtype!='block'\").groupby(['nof_filters','method','lags'])['mse'].std().reset_index(),\n         on=['method','nof_filters','lags']).rename(columns={'mse_x':'mean','mse_y':'std'}).round(3)\n\n\n\n\n\n\n\n\nnof_filters\nmethod\nlags\nmean\nstd\n\n\n\n\n0\n12\nIT-STGCN\n2\n1.126\n0.034\n\n\n1\n12\nSTGCN\n2\n1.137\n0.047"
  },
  {
    "objectID": "posts/2_research/2023-05-30-GConvLSTM_simulation_table_reshape.html#random",
    "href": "posts/2_research/2023-05-30-GConvLSTM_simulation_table_reshape.html#random",
    "title": "GConvLSTM_Simulation Tables_reshape",
    "section": "Random",
    "text": "Random\n\npd.merge(data.query(\"dataset=='fivenodes' and mtype=='rand'\").groupby(['inter_method','mrate','nof_filters','method','lags'])['mse'].mean().reset_index(),\n         data.query(\"dataset=='fivenodes' and mtype=='rand'\").groupby(['inter_method','mrate','nof_filters','method','lags'])['mse'].std().reset_index(),\n         on=['method','inter_method','nof_filters','mrate','lags']).rename(columns={'mse_x':'mean','mse_y':'std'}).round(3).query(\"nof_filters==12\")\n\n\n\n\n\n\n\n\ninter_method\nmrate\nnof_filters\nmethod\nlags\nmean\nstd\n\n\n\n\n0\nlinear\n0.7\n12\nIT-STGCN\n2\n1.287\n0.075\n\n\n1\nlinear\n0.7\n12\nSTGCN\n2\n1.472\n0.125\n\n\n2\nlinear\n0.8\n12\nIT-STGCN\n2\n1.298\n0.060\n\n\n3\nlinear\n0.8\n12\nSTGCN\n2\n1.442\n0.111\n\n\n4\nnearest\n0.7\n12\nIT-STGCN\n2\n1.261\n0.077\n\n\n5\nnearest\n0.7\n12\nSTGCN\n2\n1.394\n0.085\n\n\n6\nnearest\n0.8\n12\nIT-STGCN\n2\n1.312\n0.065\n\n\n7\nnearest\n0.8\n12\nSTGCN\n2\n1.436\n0.098"
  },
  {
    "objectID": "posts/2_research/2023-05-30-GConvLSTM_simulation_table_reshape.html#block",
    "href": "posts/2_research/2023-05-30-GConvLSTM_simulation_table_reshape.html#block",
    "title": "GConvLSTM_Simulation Tables_reshape",
    "section": "Block",
    "text": "Block\n\npd.merge(data.query(\"dataset=='fivenodes' and mtype=='block'\").groupby(['inter_method','mrate','nof_filters','method'])['mse'].mean().reset_index(),\n         data.query(\"dataset=='fivenodes' and mtype=='block'\").groupby(['inter_method','mrate','nof_filters','method'])['mse'].std().reset_index(),\n         on=['method','inter_method','nof_filters','mrate']).rename(columns={'mse_x':'mean','mse_y':'std'}).round(3)\n\n\n\n\n\n\n\n\ninter_method\nmrate\nnof_filters\nmethod\nmean\nstd\n\n\n\n\n0\nlinear\n0.125\n12\nIT-STGCN\n1.140\n0.038\n\n\n1\nlinear\n0.125\n12\nSTGCN\n1.172\n0.055\n\n\n2\nnearest\n0.125\n12\nIT-STGCN\n1.121\n0.027\n\n\n3\nnearest\n0.125\n12\nSTGCN\n1.140\n0.058"
  },
  {
    "objectID": "posts/2_research/2023-05-30-GConvLSTM_simulation_table_reshape.html#baseline-1",
    "href": "posts/2_research/2023-05-30-GConvLSTM_simulation_table_reshape.html#baseline-1",
    "title": "GConvLSTM_Simulation Tables_reshape",
    "section": "Baseline",
    "text": "Baseline\n\npd.merge(data.query(\"dataset=='chickenpox' and mtype!='rand' and mtype!='block'\").groupby(['nof_filters','method'])['mse'].mean().reset_index(),\n         data.query(\"dataset=='chickenpox' and mtype!='rand' and mtype!='block'\").groupby(['nof_filters','method'])['mse'].std().reset_index(),\n         on=['method','nof_filters']).rename(columns={'mse_x':'mean','mse_y':'std'}).round(3).query(\"nof_filters==16\")\n\n\n\n\n\n\n\n\nnof_filters\nmethod\nmean\nstd"
  },
  {
    "objectID": "posts/2_research/2023-05-30-GConvLSTM_simulation_table_reshape.html#random-1",
    "href": "posts/2_research/2023-05-30-GConvLSTM_simulation_table_reshape.html#random-1",
    "title": "GConvLSTM_Simulation Tables_reshape",
    "section": "Random",
    "text": "Random\n\npd.merge(data.query(\"dataset=='chickenpox' and mtype=='rand'\").groupby(['inter_method','mrate','nof_filters','method','lags'])['mse'].std().reset_index(),\n         data.query(\"dataset=='chickenpox' and mtype=='rand'\").groupby(['inter_method','mrate','nof_filters','method','lags'])['mse'].std().reset_index(),\n         on=['method','inter_method','nof_filters','mrate','lags']).rename(columns={'mse_x':'mean','mse_y':'std'}).round(3)\n\n\n\n\n\n\n\n\ninter_method\nmrate\nnof_filters\nmethod\nlags\nmean\nstd\n\n\n\n\n0\nlinear\n0.3\n32\nIT-STGCN\n4\n0.035\n0.035\n\n\n1\nlinear\n0.3\n32\nSTGCN\n4\n0.057\n0.057\n\n\n2\nlinear\n0.8\n32\nIT-STGCN\n4\n0.080\n0.080\n\n\n3\nlinear\n0.8\n32\nSTGCN\n4\n0.111\n0.111"
  },
  {
    "objectID": "posts/2_research/2023-05-30-GConvLSTM_simulation_table_reshape.html#block-1",
    "href": "posts/2_research/2023-05-30-GConvLSTM_simulation_table_reshape.html#block-1",
    "title": "GConvLSTM_Simulation Tables_reshape",
    "section": "Block",
    "text": "Block\n\npd.merge(data.query(\"dataset=='chickenpox' and mtype=='block'\").groupby(['inter_method','mrate','nof_filters','method'])['mse'].mean().reset_index(),\n         data.query(\"dataset=='chickenpox' and mtype=='block'\").groupby(['inter_method','mrate','nof_filters','method'])['mse'].std().reset_index(),\n         on=['method','inter_method','mrate','nof_filters']).rename(columns={'mse_x':'mean','mse_y':'std'}).round(3)\n\n\n\n\n\n\n\n\ninter_method\nmrate\nnof_filters\nmethod\nmean\nstd\n\n\n\n\n0\nlinear\n0.288\n32\nIT-STGCN\n0.911\n0.069\n\n\n1\nlinear\n0.288\n32\nSTGCN\n0.900\n0.049\n\n\n2\nnearest\n0.288\n32\nIT-STGCN\n0.885\n0.040\n\n\n3\nnearest\n0.288\n32\nSTGCN\n0.896\n0.054"
  },
  {
    "objectID": "posts/2_research/2023-05-30-GConvLSTM_simulation_table_reshape.html#baseline-2",
    "href": "posts/2_research/2023-05-30-GConvLSTM_simulation_table_reshape.html#baseline-2",
    "title": "GConvLSTM_Simulation Tables_reshape",
    "section": "Baseline",
    "text": "Baseline\n\npd.merge(data.query(\"dataset=='pedalme' and mtype!='rand' and mtype!='block'\").groupby(['lags','nof_filters','method'])['mse'].mean().reset_index(),\n         data.query(\"dataset=='pedalme' and mtype!='rand' and mtype!='block'\").groupby(['lags','nof_filters','method'])['mse'].std().reset_index(),\n         on=['method','lags','nof_filters']).rename(columns={'mse_x':'mean','mse_y':'std'}).round(3).query(\"lags==4\")\n\n\n\n\n\n\n\n\nlags\nnof_filters\nmethod\nmean\nstd\n\n\n\n\n0\n4\n2\nIT-STGCN\n1.213\n0.050\n\n\n1\n4\n2\nSTGCN\n1.215\n0.059"
  },
  {
    "objectID": "posts/2_research/2023-05-30-GConvLSTM_simulation_table_reshape.html#random-2",
    "href": "posts/2_research/2023-05-30-GConvLSTM_simulation_table_reshape.html#random-2",
    "title": "GConvLSTM_Simulation Tables_reshape",
    "section": "Random",
    "text": "Random\n\npd.merge(data.query(\"dataset=='pedalme' and mtype=='rand'\").groupby(['mrate','lags','inter_method','method'])['mse'].mean().reset_index(),\n         data.query(\"dataset=='pedalme' and mtype=='rand'\").groupby(['mrate','lags','inter_method','method'])['mse'].std().reset_index(),\n         on=['method','mrate','lags','inter_method']).rename(columns={'mse_x':'mean','mse_y':'std'}).round(3)\n\n\n\n\n\n\n\n\nmrate\nlags\ninter_method\nmethod\nmean\nstd\n\n\n\n\n0\n0.3\n4\nlinear\nIT-STGCN\n1.227\n0.056\n\n\n1\n0.3\n4\nlinear\nSTGCN\n1.244\n0.041\n\n\n2\n0.3\n4\nnearest\nIT-STGCN\n1.224\n0.035\n\n\n3\n0.3\n4\nnearest\nSTGCN\n1.266\n0.068\n\n\n4\n0.6\n4\nlinear\nIT-STGCN\n1.255\n0.049\n\n\n5\n0.6\n4\nlinear\nSTGCN\n1.332\n0.089\n\n\n6\n0.6\n4\nnearest\nIT-STGCN\n1.248\n0.045\n\n\n7\n0.6\n4\nnearest\nSTGCN\n1.274\n0.078"
  },
  {
    "objectID": "posts/2_research/2023-05-30-GConvLSTM_simulation_table_reshape.html#block-2",
    "href": "posts/2_research/2023-05-30-GConvLSTM_simulation_table_reshape.html#block-2",
    "title": "GConvLSTM_Simulation Tables_reshape",
    "section": "Block",
    "text": "Block\n\npd.merge(data.query(\"dataset=='pedalme' and mtype=='block'\").groupby(['mrate','lags','inter_method','method'])['mse'].mean().reset_index(),\n         data.query(\"dataset=='pedalme' and mtype=='block'\").groupby(['mrate','lags','inter_method','method'])['mse'].std().reset_index(),\n         on=['method','mrate','lags','inter_method']).rename(columns={'mse_x':'mean','mse_y':'std'}).round(3).query(\"lags==4\")\n\n\n\n\n\n\n\n\nmrate\nlags\ninter_method\nmethod\nmean\nstd\n\n\n\n\n0\n0.286\n4\nlinear\nIT-STGCN\n1.241\n0.069\n\n\n1\n0.286\n4\nlinear\nSTGCN\n1.223\n0.042\n\n\n2\n0.286\n4\nnearest\nIT-STGCN\n1.222\n0.039\n\n\n3\n0.286\n4\nnearest\nSTGCN\n1.237\n0.046"
  },
  {
    "objectID": "posts/2_research/2023-05-30-GConvLSTM_simulation_table_reshape.html#w_st",
    "href": "posts/2_research/2023-05-30-GConvLSTM_simulation_table_reshape.html#w_st",
    "title": "GConvLSTM_Simulation Tables_reshape",
    "section": "W_st",
    "text": "W_st\n\npd.merge(data_pedal2.query(\"mtype=='rand'\").groupby(['mrate','lags','inter_method','method'])['mse'].mean().reset_index(),\n         data_pedal2.query(\"mtype=='rand'\").groupby(['mrate','lags','inter_method','method'])['mse'].std().reset_index(),\n         on=['method','mrate','lags','inter_method']).rename(columns={'mse_x':'mean','mse_y':'std'}).round(3).query(\"lags==4\")\n\n\n\n\n\n\n\n\nmrate\nlags\ninter_method\nmethod\nmean\nstd\n\n\n\n\n0\n0.3\n4\nlinear\nIT-STGCN\n1.340\n0.166\n\n\n1\n0.3\n4\nlinear\nSTGCN\n1.392\n0.109\n\n\n2\n0.3\n4\nnearest\nIT-STGCN\n1.368\n0.158\n\n\n3\n0.3\n4\nnearest\nSTGCN\n1.338\n0.118\n\n\n4\n0.6\n4\nlinear\nIT-STGCN\n1.312\n0.162\n\n\n5\n0.6\n4\nlinear\nSTGCN\n1.498\n0.083\n\n\n6\n0.6\n4\nnearest\nIT-STGCN\n1.313\n0.205\n\n\n7\n0.6\n4\nnearest\nSTGCN\n1.503\n0.101\n\n\n\n\n\n\n\n\npd.merge(data_pedal2.query(\"mtype=='block'\").groupby(['mrate','lags','inter_method','method'])['mse'].mean().reset_index(),\n         data_pedal2.query(\"mtype=='block'\").groupby(['mrate','lags','inter_method','method'])['mse'].std().reset_index(),\n         on=['method','mrate','lags','inter_method']).rename(columns={'mse_x':'mean','mse_y':'std'}).round(3).query(\"lags==4\")\n\n\n\n\n\n\n\n\nmrate\nlags\ninter_method\nmethod\nmean\nstd\n\n\n\n\n0\n0.286\n4\nlinear\nIT-STGCN\n1.329\n0.120\n\n\n1\n0.286\n4\nlinear\nSTGCN\n1.372\n0.199\n\n\n2\n0.286\n4\nnearest\nIT-STGCN\n1.310\n0.151\n\n\n3\n0.286\n4\nnearest\nSTGCN\n1.459\n0.153"
  },
  {
    "objectID": "posts/2_research/2023-05-30-GConvLSTM_simulation_table_reshape.html#baseline-3",
    "href": "posts/2_research/2023-05-30-GConvLSTM_simulation_table_reshape.html#baseline-3",
    "title": "GConvLSTM_Simulation Tables_reshape",
    "section": "Baseline",
    "text": "Baseline\n\npd.merge(data.query(\"dataset=='wikimath' and mrate==0\").groupby(['lags','nof_filters','method'])['mse'].mean().reset_index(),\n         data.query(\"dataset=='wikimath' and mrate==0\").groupby(['lags','nof_filters','method'])['mse'].std().reset_index(),\n         on=['lags','nof_filters','method']).rename(columns={'mse_x':'mean','mse_y':'std'}).round(3)\n\n\n\n\n\n\n\n\nlags\nnof_filters\nmethod\nmean\nstd\n\n\n\n\n0\n8\n64\nIT-STGCN\n0.626\n0.015\n\n\n1\n8\n64\nSTGCN\n0.640\n0.031"
  },
  {
    "objectID": "posts/2_research/2023-05-30-GConvLSTM_simulation_table_reshape.html#random-3",
    "href": "posts/2_research/2023-05-30-GConvLSTM_simulation_table_reshape.html#random-3",
    "title": "GConvLSTM_Simulation Tables_reshape",
    "section": "Random",
    "text": "Random\n\npd.merge(data.query(\"dataset=='wikimath' and mtype=='rand'\").groupby(['mrate','lags','method'])['mse'].mean().reset_index(),\n         data.query(\"dataset=='wikimath' and mtype=='rand'\").groupby(['mrate','lags','method'])['mse'].std().reset_index(),\n         on=['method','mrate','lags']).rename(columns={'mse_x':'mean','mse_y':'std'}).round(3)\n\n\n\n\n\n\n\n\nmrate\nlags\nmethod\nmean\nstd\n\n\n\n\n0\n0.3\n8\nIT-STGCN\n0.631\n0.019\n\n\n1\n0.3\n8\nSTGCN\n0.764\n0.057\n\n\n2\n0.8\n8\nIT-STGCN\n0.920\n0.069\n\n\n3\n0.8\n8\nSTGCN\n1.423\n0.121"
  },
  {
    "objectID": "posts/2_research/2023-05-30-GConvLSTM_simulation_table_reshape.html#block-3",
    "href": "posts/2_research/2023-05-30-GConvLSTM_simulation_table_reshape.html#block-3",
    "title": "GConvLSTM_Simulation Tables_reshape",
    "section": "Block",
    "text": "Block\n\npd.merge(data.query(\"dataset=='wikimath' and mtype=='block'\").groupby(['mrate','lags','method'])['mse'].mean().reset_index(),\n         data.query(\"dataset=='wikimath' and mtype=='block'\").groupby(['mrate','lags','method'])['mse'].std().reset_index(),\n         on=['method','mrate','lags']).rename(columns={'mse_x':'mean','mse_y':'std'})\n\n\n\n\n\n\n\n\nmrate\nlags\nmethod\nmean\nstd\n\n\n\n\n0\n0.119837\n8\nIT-STGCN\n0.627324\n0.013908\n\n\n1\n0.119837\n8\nSTGCN\n0.660386\n0.033577"
  },
  {
    "objectID": "posts/2_research/2023-05-30-GConvLSTM_simulation_table_reshape.html#missing-values-on-the-same-nodes",
    "href": "posts/2_research/2023-05-30-GConvLSTM_simulation_table_reshape.html#missing-values-on-the-same-nodes",
    "title": "GConvLSTM_Simulation Tables_reshape",
    "section": "missing values on the same nodes",
    "text": "missing values on the same nodes\n\npd.merge(data_wiki_GSO.groupby(['mrate','lags','method'])['mse'].mean().reset_index(),\n        data_wiki_GSO.groupby(['mrate','lags','method'])['mse'].std().reset_index(),\n         on=['method','mrate','lags']).rename(columns={'mse_x':'mean','mse_y':'std'}).round(3)\n\n\n\n\n\n\n\n\nmrate\nlags\nmethod\nmean\nstd\n\n\n\n\n0\n0.512\n8\nIT-STGCN\n0.653\n0.033\n\n\n1\n0.512\n8\nSTGCN\n0.963\n0.098"
  },
  {
    "objectID": "posts/2_research/2023-05-30-GConvLSTM_simulation_table_reshape.html#baseline-4",
    "href": "posts/2_research/2023-05-30-GConvLSTM_simulation_table_reshape.html#baseline-4",
    "title": "GConvLSTM_Simulation Tables_reshape",
    "section": "Baseline",
    "text": "Baseline\n\npd.merge(data.query(\"dataset=='windmillsmall' and mrate==0\").groupby(['lags','method'])['mse'].mean().reset_index(),\n         data.query(\"dataset=='windmillsmall' and mrate==0\").groupby(['lags','method'])['mse'].std().reset_index(),\n         on=['method','lags']).rename(columns={'mse_x':'mean','mse_y':'std'}).round(3)\n\n\n\n\n\n\n\n\nlags\nmethod\nmean\nstd\n\n\n\n\n0\n8\nIT-STGCN\n1.014\n0.031\n\n\n1\n8\nSTGCN\n1.023\n0.055"
  },
  {
    "objectID": "posts/2_research/2023-05-30-GConvLSTM_simulation_table_reshape.html#random-4",
    "href": "posts/2_research/2023-05-30-GConvLSTM_simulation_table_reshape.html#random-4",
    "title": "GConvLSTM_Simulation Tables_reshape",
    "section": "Random",
    "text": "Random\n\npd.merge(data.query(\"dataset=='windmillsmall' and mtype=='rand'\").groupby(['mrate','lags','method'])['mse'].mean().reset_index(),\n         data.query(\"dataset=='windmillsmall' and mtype=='rand'\").groupby(['mrate','lags','method'])['mse'].std().reset_index(),\n         on=['method','mrate','lags']).rename(columns={'mse_x':'mean','mse_y':'std'}).round(3)\n\n\n\n\n\n\n\n\nmrate\nlags\nmethod\nmean\nstd\n\n\n\n\n0\n0.7\n8\nIT-STGCN\n1.142\n0.021\n\n\n1\n0.7\n8\nSTGCN\n1.600\n0.056"
  },
  {
    "objectID": "posts/2_research/2023-05-30-GConvLSTM_simulation_table_reshape.html#block-4",
    "href": "posts/2_research/2023-05-30-GConvLSTM_simulation_table_reshape.html#block-4",
    "title": "GConvLSTM_Simulation Tables_reshape",
    "section": "Block",
    "text": "Block\n\npd.merge(data.query(\"dataset=='windmillsmall' and mtype=='block'\").groupby(['mrate','lags','method'])['mse'].mean().reset_index(),\n         data.query(\"dataset=='windmillsmall' and mtype=='block'\").groupby(['mrate','lags','method'])['mse'].std().reset_index(),\n         on=['method','mrate','lags']).rename(columns={'mse_x':'mean','mse_y':'std'}).round(3)\n\n\n\n\n\n\n\n\nmrate\nlags\nmethod\nmean\nstd\n\n\n\n\n0\n0.081\n8\nIT-STGCN\n0.997\n0.022\n\n\n1\n0.081\n8\nSTGCN\n0.989\n0.009"
  },
  {
    "objectID": "posts/2_research/2023-05-30-GConvLSTM_simulation_table_reshape.html#baseline-5",
    "href": "posts/2_research/2023-05-30-GConvLSTM_simulation_table_reshape.html#baseline-5",
    "title": "GConvLSTM_Simulation Tables_reshape",
    "section": "Baseline",
    "text": "Baseline\n\npd.merge(data.query(\"dataset=='monte' and mrate==0\").groupby(['lags','method'])['mse'].mean().reset_index(),\n         data.query(\"dataset=='monte' and mrate==0\").groupby(['lags','method'])['mse'].std().reset_index(),\n         on=['method','lags']).rename(columns={'mse_x':'mean','mse_y':'std'}).round(3)\n\n\n\n\n\n\n\n\nlags\nmethod\nmean\nstd\n\n\n\n\n0\n4\nIT-STGCN\n0.959\n0.012\n\n\n1\n4\nSTGCN\n0.960\n0.011"
  },
  {
    "objectID": "posts/2_research/2023-05-30-GConvLSTM_simulation_table_reshape.html#random-5",
    "href": "posts/2_research/2023-05-30-GConvLSTM_simulation_table_reshape.html#random-5",
    "title": "GConvLSTM_Simulation Tables_reshape",
    "section": "Random",
    "text": "Random\n\npd.merge(data.query(\"dataset=='monte' and mtype=='rand'\").groupby(['mrate','lags','inter_method','method'])['mse'].mean().reset_index(),\n         data.query(\"dataset=='monte' and mtype=='rand'\").groupby(['mrate','lags','inter_method','method'])['mse'].std().reset_index(),\n         on=['mrate','inter_method','method','mrate','lags']).rename(columns={'mse_x':'mean','mse_y':'std'})\n\n\n\n\n\n\n\n\nmrate\nlags\ninter_method\nmethod\nmean\nstd\n\n\n\n\n0\n0.8\n4\nnearest\nIT-STGCN\n1.156399\n0.061898\n\n\n1\n0.8\n4\nnearest\nSTGCN\n1.133692\n0.068590"
  },
  {
    "objectID": "posts/2_research/2023-05-30-GConvLSTM_simulation_table_reshape.html#block-5",
    "href": "posts/2_research/2023-05-30-GConvLSTM_simulation_table_reshape.html#block-5",
    "title": "GConvLSTM_Simulation Tables_reshape",
    "section": "Block",
    "text": "Block\n\npd.merge(data.query(\"dataset=='monte' and mtype=='block'\").groupby(['mrate','lags','inter_method','method'])['mse'].mean().reset_index(),\n         data.query(\"dataset=='monte' and mtype=='block'\").groupby(['mrate','lags','inter_method','method'])['mse'].std().reset_index(),\n         on=['method','mrate','inter_method','lags']).rename(columns={'mse_x':'mean','mse_y':'std'})\n\n\n\n\n\n\n\n\nmrate\nlags\ninter_method\nmethod\nmean\nstd\n\n\n\n\n0\n0.149142\n4\nnearest\nIT-STGCN\n0.949276\n0.007582\n\n\n1\n0.149142\n4\nnearest\nSTGCN\n0.949673\n0.005402"
  },
  {
    "objectID": "posts/2_research/2099-03-18-SimulationPlanner-Tutorial.html",
    "href": "posts/2_research/2099-03-18-SimulationPlanner-Tutorial.html",
    "title": "SimualtionPlanner-Tutorial",
    "section": "",
    "text": "table"
  },
  {
    "objectID": "posts/2_research/2099-03-18-SimulationPlanner-Tutorial.html#plnr_stgcn_rand",
    "href": "posts/2_research/2099-03-18-SimulationPlanner-Tutorial.html#plnr_stgcn_rand",
    "title": "SimualtionPlanner-Tutorial",
    "section": "PLNR_STGCN_RAND",
    "text": "PLNR_STGCN_RAND\n\nplans_stgcn_rand = {\n    'max_iteration': 30, \n    'method': ['STGCN', 'IT-STGCN'],\n    'mrate': [0,0.7,0.8],\n    'lags': [2], \n    'nof_filters': [12], \n    'inter_method': ['linear','nearest'],\n    'epoch': [50]\n}\n\n\nplnr = itstgcn.planner.PLNR_STGCN_RAND(plans_stgcn_rand,loader1,dataset_name='fivenodes')\nplnr.simulate()\n\n\nplans_stgcn_rand = {\n    'max_iteration': 30, \n    'method': ['STGCN', 'IT-STGCN'],\n    'mrate': [0.3,0.8],\n    'lags': [4], \n    'nof_filters': [16], \n    'inter_method': ['linear'],\n    'epoch': [50]\n}\n\n\nplnr = itstgcn.planner.PLNR_STGCN_RAND(plans_stgcn_rand,loader1,dataset_name='chickenpox')\nplnr.simulate()\n\n\nplans_stgcn_rand = {\n    'max_iteration': 30, \n    'method': ['STGCN', 'IT-STGCN'],\n    'mrate': [0],\n    'lags': [4], \n    'nof_filters': [16], \n    'inter_method': ['linear'],\n    'epoch': [50]\n}\n\n\nplnr = itstgcn.planner.PLNR_STGCN_RAND(plans_stgcn_rand,loader1,dataset_name='chickenpox')\nplnr.simulate()\n\n\nplans_stgcn_rand = {\n    'max_iteration': 1, \n    'method': ['STGCN', 'IT-STGCN'], \n    'mrate': [0],\n    'lags': [8], \n    'nof_filters': [12], \n    'inter_method': ['linear'],\n    'epoch': [50]\n}\n\n\nplnr = itstgcn.planner.PLNR_STGCN_RAND(plans_stgcn_rand,loader6,dataset_name='windmillsmall')\nplnr.simulate()\n\n\nplans_stgcn_rand = {\n    'max_iteration': 10, \n    'method': ['STGCN', 'IT-STGCN'], \n    'mrate': [0.3],\n    'lags': [2, 4], \n    'nof_filters': [12], \n    'inter_method': ['linear'],\n    'epoch': [50]\n}\n\n\nplnr = itstgcnadd.planner.PLNR_STGCN_RAND(plans_stgcn_rand,loader3,dataset_name='wikimath')\n\n\nplnr.simulate()\n\n\nplans_stgcn_rand = {\n    'max_iteration': 1, \n    'method': ['STGCN', 'IT-STGCN'], \n    'RecurrentGCN' : ['DCRNN'],\n    'mrate': [0],\n    'lags': [8], \n    'nof_filters': [12], \n    'inter_method': ['linear'],\n    'epoch': [50]\n}\n\n\nplnr = itstgcnadd.planner.PLNR_STGCN_RAND(plans_stgcn_rand,loader6,dataset_name='windmillsmall')\nplnr.simulate()\n\n\nmy_list = [[] for _ in range(11)] #windmilsmall\nanother_list = list(range(5000,7500)) # 17470*0.8 = 13976.0\nmy_list[1] = another_list\nmy_list[3] = another_list\nmy_list[5] = another_list\nmy_list[7] = another_list\nmy_list[9] = another_list\nmindex = my_list\n\n\n# mindex= [[],[],[],list(range(50,150)),[]]  # node 1\n# mindex= [list(range(10,100)),[],list(range(50,80)),[],[]] # node 2\n# mindex= [list(range(10,100)),[],list(range(50,80)),list(range(50,150)),[]] # node3\nplans_stgcn_block = {\n    'max_iteration': 1, \n    'method': ['STGCN', 'IT-STGCN'], \n    'mindex': [mindex],\n    'lags': [8], \n    'nof_filters': [12], \n    'inter_method': ['linear'],\n    'epoch': [50]\n}\n\n\nplnr = itstgcnEvolveGCNH.planner.PLNR_STGCN_MANUAL(plans_stgcn_block,loader6,dataset_name='windmillsmall')\nplnr.simulate(mindex=mindex,mtype='block')"
  },
  {
    "objectID": "posts/2_research/2099-03-18-SimulationPlanner-Tutorial.html#plnr_stgcn_manual",
    "href": "posts/2_research/2099-03-18-SimulationPlanner-Tutorial.html#plnr_stgcn_manual",
    "title": "SimualtionPlanner-Tutorial",
    "section": "PLNR_STGCN_MANUAL",
    "text": "PLNR_STGCN_MANUAL\n\nmy_list = [[] for _ in range(20)] #chickenpox\nanother_list = list(range(100,400))\nmy_list[1] = another_list\nmy_list[3] = another_list\nmy_list[5] = another_list\nmy_list[7] = another_list\nmy_list[9] = another_list\nmy_list[11] = another_list\nmy_list[13] = another_list\nmy_list[15] = another_list\nmindex = my_list\n\n\nmy_list = [[] for _ in range(15)] #pedalme\nanother_list = list(range(5,35))\nmy_list[2] = another_list\nmy_list[4] = another_list\nmy_list[7] = another_list\nmy_list[11] = another_list\nmindex = my_list\n\n\nimport random\nmy_list = [[] for _ in range(1068)] # wikimath\nanother_list = random.sample(range(570), 72)\n# my_list에서 250개 요소 무작위 선택\nselected_indexes = random.sample(range(len(my_list)), 250)\n# 선택된 요소에 해당하는 값들을 another_list에 할당\nfor index in selected_indexes:\n    my_list[index] = another_list\n\n\n# _data = itstgcn.load_data('./data/fivenodes.pkl')\n# _edges = torch.tensor(_data['edges']).nonzero().tolist()\n# _FX = _data['f'].tolist()\n# _node_ids = {'node1':0, 'node2':1, 'node3':2, 'node4':3, 'node5':4} \n# data_dict = {'edges':_edges, 'node_ids':_node_ids, 'FX':_FX}\n# loader = itstgcn.DatasetLoader(data_dict)\n# data_dict = itstgcn.load_data('./data/fivenodes.pkl')\n# loader = itstgcn.DatasetLoader(data_dict)\n\n\n# mindex= [[],[],[],list(range(50,150)),[]]\n# mindex= [list(range(50,150)),[],list(range(50,90)),list(range(50,150)),[]] # node 2\nplans_stgcn_block = {\n    'max_iteration': 10, \n    'method': ['STGCN', 'IT-STGCN'],\n    'RecurrentGCN' : ['GConvGRU','GConvLSTM'],\n    'mindex': [mindex],\n    'lags': [4], \n    'nof_filters': [16], \n    'inter_method': ['linear'],\n    'epoch': [50],\n    'lr': [0.01]\n}\n\n\nplnr = itstgcn.planner.PLNR_STGCN_MANUAL(plans_stgcn_block,loader1,dataset_name='chickenpox')\n\nplnr.simulate(mindex=mindex,mtype='block')\n\n\n# mindex= [[],[],[],list(range(50,150)),[]]\n# mindex= [list(range(50,150)),[],list(range(50,90)),list(range(50,150)),[]] # node 2\nplans_stgcn_block = {\n    'max_iteration': 10, \n    'method': ['STGCN', 'IT-STGCN'], \n    'RecurrentGCN' : ['GConvGRU','GConvLSTM'],\n    'mindex': [mindex],\n    'lags': [2,4], \n    'nof_filters': [12], \n    'inter_method': ['linear'],\n    'epoch': [50]\n}\n\n\nplnr = itstgcnadd.planner.PLNR_STGCN_MANUAL(plans_stgcn_block,loader3,dataset_name='wikimath')\n\n\nplnr.simulate(mindex=mindex,mtype='block')"
  },
  {
    "objectID": "posts/2_research/2099-03-18-SimulationPlanner-Tutorial.html#plnr_gnar_rand",
    "href": "posts/2_research/2099-03-18-SimulationPlanner-Tutorial.html#plnr_gnar_rand",
    "title": "SimualtionPlanner-Tutorial",
    "section": "PLNR_GNAR_RAND",
    "text": "PLNR_GNAR_RAND\n\n# _data = itstgcn.load_data('./data/fivenodes.pkl')\n# _edges = torch.tensor(_data['edges']).nonzero().tolist()\n# _FX = _data['f'].tolist()\n# _node_ids = {'node1':0, 'node2':1, 'node3':2, 'node4':3, 'node5':4} \n# data_dict = {'edges':_edges, 'node_ids':_node_ids, 'FX':_FX}\n# loader = itstgcn.DatasetLoader(data_dict)\n# data_dict=itstgcn.load_data('./data/fivenodes.pkl')\n# loader = itstgcn.DatasetLoader(data_dict)\n\n\nplans_gnar_rand = {\n    'max_iteration': 30, \n#    'method': ['GNAR'], \n    'mrate': [0.1],\n    'lags': [4], \n#    'nof_filters': [8,16], \n    'inter_method': ['linear','cubic'],\n#    'epoch': [1]\n}\n\n\nplnr = itstgcn.planner.PLNR_GNAR_RAND(plans_gnar_rand,loader2,dataset_name='pedalme')\nplnr.simulate()\n\n\nplans_gnar_rand = {\n    'max_iteration': 3, \n#    'method': ['GNAR'], \n    'mrate': [0],\n    'lags': [2,4], \n#    'nof_filters': [8,16], \n    'inter_method': ['linear'],\n#    'epoch': [1]\n}\n\n\nplnr = itstgcn.planner.PLNR_GNAR_RAND(plans_gnar_rand,loader3,dataset_name='wikimath')\nplnr.simulate()\n\n\nplans_gnar_rand = {\n    'max_iteration': 3, \n#    'method': ['GNAR'], \n    'mrate': [0,0.3],\n    'lags': [8], \n#    'nof_filters': [8,16], \n    'inter_method': ['linear'],\n#    'epoch': [1]\n}\n\n\nplnr = itstgcn.planner.PLNR_GNAR_RAND(plans_gnar_rand,loader5,dataset_name='windmillmedium')\nplnr.simulate()"
  },
  {
    "objectID": "posts/2_research/2099-03-18-SimulationPlanner-Tutorial.html#plnr_gnar_block",
    "href": "posts/2_research/2099-03-18-SimulationPlanner-Tutorial.html#plnr_gnar_block",
    "title": "SimualtionPlanner-Tutorial",
    "section": "PLNR_GNAR_BLOCK",
    "text": "PLNR_GNAR_BLOCK\n\n# _data = itstgcn.load_data('./data/fivenodes.pkl')\n# _edges = torch.tensor(_data['edges']).nonzero().tolist()\n# _FX = _data['f'].tolist()\n# _node_ids = {'node1':0, 'node2':1, 'node3':2, 'node4':3, 'node5':4} \n# data_dict = {'edges':_edges, 'node_ids':_node_ids, 'FX':_FX}\n# loader = itstgcn.DatasetLoader(data_dict)\n# loader = itstgcn.load_data('./data/fivenodes.pkl')\n\n\n Nodes : 26\n\nvertices represent 26 windmills\n-Edges : 676\n\nweighted edges describe the strength of relationships\n- Time : 17464\n\n\nmy_list = [[] for _ in range(26)] #medium\nanother_list = list(range(1000,2000))+list(range(4000,5000))+list(range(7000,8000)) #17464\n\nfor i in np.array(random.sample(range(0, 26), 15)):\n    my_list[i] = another_list\nmindex = my_list\n\n\nmy_list = [[] for _ in range(20)] #chickenpox\nanother_list = list(range(100,400))\nmy_list[2] = another_list\nmy_list[4] = another_list\nmy_list[6] = another_list\nmy_list[8] = another_list\nmy_list[10] = another_list\nmy_list[12] = another_list\nmy_list[14] = another_list\nmy_list[16] = another_list\nmindex = my_list\n\n\n# mindex= [[],[],[],list(range(50,150)),[]]\n# mindex= [list(range(50,150)),[],list(range(50,90)),list(range(50,150)),[]] # node 2\nplans_gnar_block = {\n    'max_iteration': 3, \n    'method': ['GNAR'], \n    'mindex': [mindex],\n    'lags': [4], \n    'inter_method': ['cubic','linear'],\n}\n\n\nplnr = itstgcn.planner.PLNR_GNAR_MANUAL(plans_gnar_block,loader1,dataset_name='chickenpox')\nplnr.simulate(mindex,mtype='block')\n\n\n# mindex= [[],[],[],list(range(50,150)),[]]\n# mindex= [list(range(50,150)),[],list(range(50,90)),list(range(50,150)),[]] # node 2\nplans_gnar_block = {\n    'max_iteration': 3, \n    'method': ['GNAR'], \n    'mindex': [mindex],\n    'lags': [2,4], \n    'inter_method': ['linear'],\n}"
  },
  {
    "objectID": "posts/2_research/2023-05-25-GCONVGRU_snd_Simulation_boxplot_reshape.html",
    "href": "posts/2_research/2023-05-25-GCONVGRU_snd_Simulation_boxplot_reshape.html",
    "title": "GConvGRU_Simulation Boxplot_reshape",
    "section": "",
    "text": "Simulation Study"
  },
  {
    "objectID": "posts/2_research/2023-05-25-GCONVGRU_snd_Simulation_boxplot_reshape.html#baseline",
    "href": "posts/2_research/2023-05-25-GCONVGRU_snd_Simulation_boxplot_reshape.html#baseline",
    "title": "GConvGRU_Simulation Boxplot_reshape",
    "section": "Baseline",
    "text": "Baseline\n\ndata.query(\"method!='GNAR' and mrate==0\").plot.box(backend='plotly',x='mrate',color='method',y='mse',facet_col='lags',facet_row='nof_filters',height=600)\n\n                                                \n\n\n\ndata.query(\"method!='GNAR' and mtype =='rand'\").plot.box(backend='plotly',x='mrate',color='method',y='mse',facet_col='mrate',facet_row='inter_method',height=600)\n\n                                                \n\n\n\ndata.query(\"method!='GNAR' and mtype =='block' \").plot.box(backend='plotly',x='mrate',color='method',y='mse',facet_col='inter_method',facet_row='lags',height=600)"
  },
  {
    "objectID": "posts/2_research/2023-05-25-GCONVGRU_snd_Simulation_boxplot_reshape.html#baseline-1",
    "href": "posts/2_research/2023-05-25-GCONVGRU_snd_Simulation_boxplot_reshape.html#baseline-1",
    "title": "GConvGRU_Simulation Boxplot_reshape",
    "section": "Baseline",
    "text": "Baseline\n\ndata.query(\"method!='GNAR' and mrate ==0 \").plot.box(backend='plotly',x='mrate',color='method',y='mse',facet_col='nof_filters',height=600)"
  },
  {
    "objectID": "posts/2_research/2023-05-25-GCONVGRU_snd_Simulation_boxplot_reshape.html#random",
    "href": "posts/2_research/2023-05-25-GCONVGRU_snd_Simulation_boxplot_reshape.html#random",
    "title": "GConvGRU_Simulation Boxplot_reshape",
    "section": "Random",
    "text": "Random\n\ndata.query(\"method!='GNAR' and mtype =='rand'\").plot.box(backend='plotly',x='mrate',color='method',y='mse',facet_col='nof_filters',facet_row='inter_method',height=800)"
  },
  {
    "objectID": "posts/2_research/2023-05-25-GCONVGRU_snd_Simulation_boxplot_reshape.html#block",
    "href": "posts/2_research/2023-05-25-GCONVGRU_snd_Simulation_boxplot_reshape.html#block",
    "title": "GConvGRU_Simulation Boxplot_reshape",
    "section": "Block",
    "text": "Block\n\ndata.query(\"method!='GNAR' and mtype =='block'\").plot.box(backend='plotly',x='mrate',color='method',y='mse',facet_col='inter_method',facet_row='lags',height=800)"
  },
  {
    "objectID": "posts/2_research/2023-05-25-GCONVGRU_snd_Simulation_boxplot_reshape.html#baseline-2",
    "href": "posts/2_research/2023-05-25-GCONVGRU_snd_Simulation_boxplot_reshape.html#baseline-2",
    "title": "GConvGRU_Simulation Boxplot_reshape",
    "section": "Baseline",
    "text": "Baseline\n\ndata.query(\"mrate ==0 \").plot.box(backend='plotly',x='epoch',color='method',y='mse',height=800)"
  },
  {
    "objectID": "posts/2_research/2023-05-25-GCONVGRU_snd_Simulation_boxplot_reshape.html#random-1",
    "href": "posts/2_research/2023-05-25-GCONVGRU_snd_Simulation_boxplot_reshape.html#random-1",
    "title": "GConvGRU_Simulation Boxplot_reshape",
    "section": "random",
    "text": "random\n\ndata.query(\"method!='GNAR' and mtype =='rand' \").plot.box(backend='plotly',x='mrate',color='method',y='mse',facet_col='lags',facet_row='inter_method',height=800)"
  },
  {
    "objectID": "posts/2_research/2023-05-25-GCONVGRU_snd_Simulation_boxplot_reshape.html#block-1",
    "href": "posts/2_research/2023-05-25-GCONVGRU_snd_Simulation_boxplot_reshape.html#block-1",
    "title": "GConvGRU_Simulation Boxplot_reshape",
    "section": "block",
    "text": "block\n\ndata.query(\"method!='GNAR' and mtype =='block' \").plot.box(backend='plotly',x='mrate',color='method',y='mse',facet_col='inter_method',facet_row='lags',height=600)"
  },
  {
    "objectID": "posts/2_research/2023-05-25-GCONVGRU_snd_Simulation_boxplot_reshape.html#weight-matrix-time-node-고려한-결과",
    "href": "posts/2_research/2023-05-25-GCONVGRU_snd_Simulation_boxplot_reshape.html#weight-matrix-time-node-고려한-결과",
    "title": "GConvGRU_Simulation Boxplot_reshape",
    "section": "weight matrix time, node 고려한 결과",
    "text": "weight matrix time, node 고려한 결과\n\ndf1 = pd.read_csv('./simulation_results/2023-05-28_10-40-44.csv')\ndf2 = pd.read_csv('./simulation_results/2023-05-28_11-09-06.csv')\n\n\ndata2 = pd.concat([df1,df2],axis=0)\n\n\ndata2.to_csv('./simulation_results/Real_simulation_reshape/pedalme_Simulation_itstgcnsnd.csv',index=False)\n\n\ndata2 = pd.read_csv('./simulation_results/Real_simulation_reshape/pedalme_Simulation_itstgcnsnd.csv')\n\n\ndata2.query(\"mtype=='rand'\").plot.box(backend='plotly',x='mrate',color='method',y='mse',facet_col='inter_method',facet_row='lags',height=1000)\n\n                                                \n\n\n\ndata2.query(\"mtype=='block'\").plot.box(backend='plotly',x='mrate',color='method',y='mse',facet_col='inter_method',facet_row='lags',height=1000)"
  },
  {
    "objectID": "posts/2_research/2023-05-25-GCONVGRU_snd_Simulation_boxplot_reshape.html#baseline-3",
    "href": "posts/2_research/2023-05-25-GCONVGRU_snd_Simulation_boxplot_reshape.html#baseline-3",
    "title": "GConvGRU_Simulation Boxplot_reshape",
    "section": "Baseline",
    "text": "Baseline\n\ndata.query(\"mrate==0 and method!='GNAR'\").plot.box(backend='plotly',x='mrate',color='method',y='mse',height=600)"
  },
  {
    "objectID": "posts/2_research/2023-05-25-GCONVGRU_snd_Simulation_boxplot_reshape.html#random-2",
    "href": "posts/2_research/2023-05-25-GCONVGRU_snd_Simulation_boxplot_reshape.html#random-2",
    "title": "GConvGRU_Simulation Boxplot_reshape",
    "section": "random",
    "text": "random\n\ndata.query(\"mtype=='rand' and mrate !=0 and method!='GNAR'\").plot.box(backend='plotly',x='mrate',color='method',y='mse',facet_col='lags',facet_row='nof_filters',height=800)"
  },
  {
    "objectID": "posts/2_research/2023-05-25-GCONVGRU_snd_Simulation_boxplot_reshape.html#block-2",
    "href": "posts/2_research/2023-05-25-GCONVGRU_snd_Simulation_boxplot_reshape.html#block-2",
    "title": "GConvGRU_Simulation Boxplot_reshape",
    "section": "block",
    "text": "block\n\ndata.query(\"mtype=='block' and method!='GNAR'\").plot.box(backend='plotly',x='mrate',color='method',y='mse',facet_col='lags',facet_row='nof_filters',height=1200)"
  },
  {
    "objectID": "posts/2_research/2023-05-25-GCONVGRU_snd_Simulation_boxplot_reshape.html#missing-values-on-the-same-nodes",
    "href": "posts/2_research/2023-05-25-GCONVGRU_snd_Simulation_boxplot_reshape.html#missing-values-on-the-same-nodes",
    "title": "GConvGRU_Simulation Boxplot_reshape",
    "section": "missing values on the same nodes",
    "text": "missing values on the same nodes\n\n# 10%\ndf1 = pd.read_csv('./simulation_results/2023-05-30_15-07-43.csv') # STGCN IT-STGCN block\ndf2 = pd.read_csv('./simulation_results/2023-05-31_01-58-40.csv') # STGCN IT-STGCN\n\n\ndata = pd.concat([df1,df2],axis=0)\n\n\ndata.to_csv('./simulation_results/Real_simulation_reshape/wikimath_GSO_st.csv',index=False)\n\n\ndata = pd.read_csv('./simulation_results/Real_simulation_reshape/wikimath_GSO_st.csv')\n\n\ndata.query(\"method!='GNAR' and mtype =='block' \").plot.box(backend='plotly',x='mrate',color='method',y='mse',facet_col='inter_method',facet_row='lags',height=800)"
  },
  {
    "objectID": "posts/2_research/2023-05-25-GCONVGRU_snd_Simulation_boxplot_reshape.html#baseline-4",
    "href": "posts/2_research/2023-05-25-GCONVGRU_snd_Simulation_boxplot_reshape.html#baseline-4",
    "title": "GConvGRU_Simulation Boxplot_reshape",
    "section": "Baseline",
    "text": "Baseline\n\ndata.query(\"method!='GNAR' and mrate ==0 \").plot.box(backend='plotly',x='mrate',color='method',y='mse',facet_col='lags',height=800)"
  },
  {
    "objectID": "posts/2_research/2023-05-25-GCONVGRU_snd_Simulation_boxplot_reshape.html#random-3",
    "href": "posts/2_research/2023-05-25-GCONVGRU_snd_Simulation_boxplot_reshape.html#random-3",
    "title": "GConvGRU_Simulation Boxplot_reshape",
    "section": "random",
    "text": "random\n\ndata.query(\"method!='GNAR' and mtype =='rand' \").plot.box(backend='plotly',x='mrate',color='method',y='mse',facet_col='lags',facet_row='inter_method',height=800)"
  },
  {
    "objectID": "posts/2_research/2023-05-25-GCONVGRU_snd_Simulation_boxplot_reshape.html#block-3",
    "href": "posts/2_research/2023-05-25-GCONVGRU_snd_Simulation_boxplot_reshape.html#block-3",
    "title": "GConvGRU_Simulation Boxplot_reshape",
    "section": "block",
    "text": "block\n\ndata.query(\"method!='GNAR' and mtype =='block' \").plot.box(backend='plotly',x='mrate',color='method',y='mse',facet_col='lags',facet_row='inter_method',height=800)"
  },
  {
    "objectID": "posts/2_research/2023-05-25-GCONVGRU_snd_Simulation_boxplot_reshape.html#baseline-5",
    "href": "posts/2_research/2023-05-25-GCONVGRU_snd_Simulation_boxplot_reshape.html#baseline-5",
    "title": "GConvGRU_Simulation Boxplot_reshape",
    "section": "Baseline",
    "text": "Baseline\n\ndata.query(\"mrate==0 and method!='GNAR'\").plot.box(backend='plotly',x='mrate',color='method',y='mse',facet_col='lags',facet_row='nof_filters',height=600)"
  },
  {
    "objectID": "posts/2_research/2023-05-25-GCONVGRU_snd_Simulation_boxplot_reshape.html#random-4",
    "href": "posts/2_research/2023-05-25-GCONVGRU_snd_Simulation_boxplot_reshape.html#random-4",
    "title": "GConvGRU_Simulation Boxplot_reshape",
    "section": "random",
    "text": "random\n\ndata.query(\"mtype=='rand' and mrate !=0 and method!='GNAR'\").plot.box(backend='plotly',x='mrate',color='method',y='mse',facet_col='lags',facet_row='inter_method',height=800)"
  },
  {
    "objectID": "posts/2_research/2023-05-25-GCONVGRU_snd_Simulation_boxplot_reshape.html#block-4",
    "href": "posts/2_research/2023-05-25-GCONVGRU_snd_Simulation_boxplot_reshape.html#block-4",
    "title": "GConvGRU_Simulation Boxplot_reshape",
    "section": "block",
    "text": "block\n\ndata.query(\"mtype=='block' and method!='GNAR'\").plot.box(backend='plotly',x='mrate',color='method',y='mse',facet_col='lags',facet_row='nof_filters',height=1200)"
  },
  {
    "objectID": "posts/2_research/2023-06-06-GCLSTM_Simulation_boxplot_reshape.html",
    "href": "posts/2_research/2023-06-06-GCLSTM_Simulation_boxplot_reshape.html",
    "title": "GCLSTM_Simulation_reshape",
    "section": "",
    "text": "Simulation Study"
  },
  {
    "objectID": "posts/2_research/2023-06-06-GCLSTM_Simulation_boxplot_reshape.html#baseline",
    "href": "posts/2_research/2023-06-06-GCLSTM_Simulation_boxplot_reshape.html#baseline",
    "title": "GCLSTM_Simulation_reshape",
    "section": "Baseline",
    "text": "Baseline\n\ndata.query(\"method!='GNAR' and mrate==0\").plot.box(backend='plotly',x='mrate',color='method',y='mse',facet_col='lags',facet_row='nof_filters',height=600)"
  },
  {
    "objectID": "posts/2_research/2023-06-06-GCLSTM_Simulation_boxplot_reshape.html#random",
    "href": "posts/2_research/2023-06-06-GCLSTM_Simulation_boxplot_reshape.html#random",
    "title": "GCLSTM_Simulation_reshape",
    "section": "Random",
    "text": "Random\n\ndata.query(\"method!='GNAR' and mtype =='rand'\").plot.box(backend='plotly',x='mrate',color='method',y='mse',facet_col='inter_method',facet_row='nof_filters',height=600)"
  },
  {
    "objectID": "posts/2_research/2023-06-06-GCLSTM_Simulation_boxplot_reshape.html#block",
    "href": "posts/2_research/2023-06-06-GCLSTM_Simulation_boxplot_reshape.html#block",
    "title": "GCLSTM_Simulation_reshape",
    "section": "Block",
    "text": "Block\n\ndata.query(\"method!='GNAR' and mtype =='block' \").plot.box(backend='plotly',x='mrate',color='method',y='mse',facet_col='inter_method',facet_row='nof_filters',height=600)"
  },
  {
    "objectID": "posts/2_research/2023-06-06-GCLSTM_Simulation_boxplot_reshape.html#baseline-1",
    "href": "posts/2_research/2023-06-06-GCLSTM_Simulation_boxplot_reshape.html#baseline-1",
    "title": "GCLSTM_Simulation_reshape",
    "section": "Baseline",
    "text": "Baseline\n\ndata.query(\"method!='GNAR' and mrate ==0 \").plot.box(backend='plotly',x='mrate',color='method',y='mse',facet_col='nof_filters',height=600)"
  },
  {
    "objectID": "posts/2_research/2023-06-06-GCLSTM_Simulation_boxplot_reshape.html#random-1",
    "href": "posts/2_research/2023-06-06-GCLSTM_Simulation_boxplot_reshape.html#random-1",
    "title": "GCLSTM_Simulation_reshape",
    "section": "Random",
    "text": "Random\n\ndata.query(\"method!='GNAR' and mtype =='rand'\").plot.box(backend='plotly',x='mrate',color='method',y='mse',facet_col='nof_filters',facet_row='inter_method',height=800)"
  },
  {
    "objectID": "posts/2_research/2023-06-06-GCLSTM_Simulation_boxplot_reshape.html#block-1",
    "href": "posts/2_research/2023-06-06-GCLSTM_Simulation_boxplot_reshape.html#block-1",
    "title": "GCLSTM_Simulation_reshape",
    "section": "Block",
    "text": "Block\n\ndata.query(\"method!='GNAR' and mtype =='block'\").plot.box(backend='plotly',x='mrate',color='method',y='mse',facet_col='inter_method',facet_row='lags',height=800)"
  },
  {
    "objectID": "posts/2_research/2023-06-06-GCLSTM_Simulation_boxplot_reshape.html#baseline-2",
    "href": "posts/2_research/2023-06-06-GCLSTM_Simulation_boxplot_reshape.html#baseline-2",
    "title": "GCLSTM_Simulation_reshape",
    "section": "Baseline",
    "text": "Baseline\n\ndata.query(\"mrate ==0 and lags!=2\").plot.box(backend='plotly',x='epoch',color='method',y='mse',facet_col='nof_filters',facet_row='lags',height=400)"
  },
  {
    "objectID": "posts/2_research/2023-06-06-GCLSTM_Simulation_boxplot_reshape.html#random-2",
    "href": "posts/2_research/2023-06-06-GCLSTM_Simulation_boxplot_reshape.html#random-2",
    "title": "GCLSTM_Simulation_reshape",
    "section": "random",
    "text": "random\n\ndata.query(\"method!='GNAR' and mtype =='rand' and lags!=2\").plot.box(backend='plotly',x='mrate',color='method',y='mse',facet_col='nof_filters',facet_row='inter_method',height=800)"
  },
  {
    "objectID": "posts/2_research/2023-06-06-GCLSTM_Simulation_boxplot_reshape.html#block-2",
    "href": "posts/2_research/2023-06-06-GCLSTM_Simulation_boxplot_reshape.html#block-2",
    "title": "GCLSTM_Simulation_reshape",
    "section": "block",
    "text": "block\n\ndata.query(\"method!='GNAR' and mtype =='block' and lags!=2 \").plot.box(backend='plotly',x='mrate',color='method',y='mse',facet_col='nof_filters',facet_row='inter_method',height=800)"
  },
  {
    "objectID": "posts/2_research/2023-06-06-GCLSTM_Simulation_boxplot_reshape.html#weight-matrix-time-node-고려한-결과",
    "href": "posts/2_research/2023-06-06-GCLSTM_Simulation_boxplot_reshape.html#weight-matrix-time-node-고려한-결과",
    "title": "GCLSTM_Simulation_reshape",
    "section": "weight matrix time, node 고려한 결과",
    "text": "weight matrix time, node 고려한 결과\n\n# df1 = pd.read_csv('./simulation_results/2023-06-13_18-41-53.csv') # gclm 잘못 돌림\ndf1 = pd.read_csv('./simulation_results/2023-06-13_18-14-13.csv')\ndf2 = pd.read_csv('./simulation_results/2023-06-16_20-25-15.csv')\n\n\ndata2 = pd.concat([df1,df2],axis=0)\n\n\ndata2.to_csv('./simulation_results/Real_simulation_reshape/GCLSTM_pedalme_Simulation_itstgcnsnd.csv',index=False)\n\n\ndata2 = pd.read_csv('./simulation_results/Real_simulation_reshape/GCLSTM_pedalme_Simulation_itstgcnsnd.csv')\n\n\ndata2.query(\"mtype=='rand'\").plot.box(backend='plotly',x='mrate',color='method',y='mse',facet_col='inter_method',facet_row='nof_filters',height=1000)\n\n                                                \n\n\n\ndata2.query(\"mtype=='block'\").plot.box(backend='plotly',x='mrate',color='method',y='mse',facet_col='inter_method',facet_row='nof_filters',height=1000)"
  },
  {
    "objectID": "posts/2_research/2023-06-06-GCLSTM_Simulation_boxplot_reshape.html#baseline-3",
    "href": "posts/2_research/2023-06-06-GCLSTM_Simulation_boxplot_reshape.html#baseline-3",
    "title": "GCLSTM_Simulation_reshape",
    "section": "Baseline",
    "text": "Baseline\n\ndata.query(\"mrate==0 and method!='GNAR'\").plot.box(backend='plotly',x='mrate',color='method',y='mse',height=600)"
  },
  {
    "objectID": "posts/2_research/2023-06-06-GCLSTM_Simulation_boxplot_reshape.html#random-3",
    "href": "posts/2_research/2023-06-06-GCLSTM_Simulation_boxplot_reshape.html#random-3",
    "title": "GCLSTM_Simulation_reshape",
    "section": "random",
    "text": "random\n\ndata.query(\"mtype=='rand' and mrate !=0 and method!='GNAR'\").plot.box(backend='plotly',x='mrate',color='method',y='mse',facet_col='lags',facet_row='nof_filters',height=800)"
  },
  {
    "objectID": "posts/2_research/2023-06-06-GCLSTM_Simulation_boxplot_reshape.html#block-3",
    "href": "posts/2_research/2023-06-06-GCLSTM_Simulation_boxplot_reshape.html#block-3",
    "title": "GCLSTM_Simulation_reshape",
    "section": "block",
    "text": "block\n\ndata.query(\"mtype=='block' and method!='GNAR'\").plot.box(backend='plotly',x='mrate',color='method',y='mse',facet_col='lags',facet_row='nof_filters',height=1200)"
  },
  {
    "objectID": "posts/2_research/2023-06-06-GCLSTM_Simulation_boxplot_reshape.html#missing-values-on-the-same-nodes",
    "href": "posts/2_research/2023-06-06-GCLSTM_Simulation_boxplot_reshape.html#missing-values-on-the-same-nodes",
    "title": "GCLSTM_Simulation_reshape",
    "section": "missing values on the same nodes",
    "text": "missing values on the same nodes\n\ndf1 = pd.read_csv('./simulation_results/2023-06-17_01-42-41.csv') # STGCN IT-STGCN block\n# df2 = pd.read_csv('./simulation_results/2023-06-17_03-35-20.csv') # STGCN IT-STGCN\ndf3 = pd.read_csv('./simulation_results/2023-06-17_10-06-34.csv') \n\ndf4 = pd.read_csv('./simulation_results/2023-07-10_05-37-15.csv') \ndf5 = pd.read_csv('./simulation_results/2023-07-10_11-00-23.csv') \ndf6 = pd.read_csv('./simulation_results/2023-07-10_16-03-18.csv') \n\n\ndata = pd.concat([df1,df3,df4,df5,df6],axis=0)\n\n\ndata.to_csv('./simulation_results/Real_simulation_reshape/GCLSTM_wikimath_GSO_st.csv',index=False)\n\n\ndata = pd.read_csv('./simulation_results/Real_simulation_reshape/GCLSTM_wikimath_GSO_st.csv')\n\n\ndata.query(\"method!='GNAR' and mtype =='block' \").plot.box(backend='plotly',x='mrate',color='method',y='mse',facet_col='inter_method',facet_row='lags',height=800)"
  },
  {
    "objectID": "posts/2_research/2023-06-06-GCLSTM_Simulation_boxplot_reshape.html#baseline-4",
    "href": "posts/2_research/2023-06-06-GCLSTM_Simulation_boxplot_reshape.html#baseline-4",
    "title": "GCLSTM_Simulation_reshape",
    "section": "Baseline",
    "text": "Baseline\n\ndata.query(\"method!='GNAR' and mrate ==0 \").plot.box(backend='plotly',x='mrate',color='method',y='mse',facet_col='lags',height=800)"
  },
  {
    "objectID": "posts/2_research/2023-06-06-GCLSTM_Simulation_boxplot_reshape.html#random-4",
    "href": "posts/2_research/2023-06-06-GCLSTM_Simulation_boxplot_reshape.html#random-4",
    "title": "GCLSTM_Simulation_reshape",
    "section": "random",
    "text": "random\n\ndata.query(\"method!='GNAR' and mtype =='rand' \").plot.box(backend='plotly',x='mrate',color='method',y='mse',facet_col='lags',facet_row='inter_method',height=800)"
  },
  {
    "objectID": "posts/2_research/2023-06-06-GCLSTM_Simulation_boxplot_reshape.html#block-4",
    "href": "posts/2_research/2023-06-06-GCLSTM_Simulation_boxplot_reshape.html#block-4",
    "title": "GCLSTM_Simulation_reshape",
    "section": "block",
    "text": "block\n\ndata.query(\"method!='GNAR' and mtype =='block' \").plot.box(backend='plotly',x='mrate',color='method',y='mse',facet_col='lags',facet_row='inter_method',height=800)"
  },
  {
    "objectID": "posts/2_research/2023-06-06-GCLSTM_Simulation_boxplot_reshape.html#baseline-5",
    "href": "posts/2_research/2023-06-06-GCLSTM_Simulation_boxplot_reshape.html#baseline-5",
    "title": "GCLSTM_Simulation_reshape",
    "section": "Baseline",
    "text": "Baseline\n\ndata.query(\"mrate==0 and method!='GNAR'\").plot.box(backend='plotly',x='mrate',color='method',y='mse',facet_col='lags',facet_row='nof_filters',height=600)"
  },
  {
    "objectID": "posts/2_research/2023-06-06-GCLSTM_Simulation_boxplot_reshape.html#random-5",
    "href": "posts/2_research/2023-06-06-GCLSTM_Simulation_boxplot_reshape.html#random-5",
    "title": "GCLSTM_Simulation_reshape",
    "section": "random",
    "text": "random\n\ndata.query(\"mtype=='rand' and mrate !=0 and method!='GNAR'\").plot.box(backend='plotly',x='mrate',color='method',y='mse',facet_col='lags',facet_row='inter_method',height=800)"
  },
  {
    "objectID": "posts/2_research/2023-06-06-GCLSTM_Simulation_boxplot_reshape.html#block-5",
    "href": "posts/2_research/2023-06-06-GCLSTM_Simulation_boxplot_reshape.html#block-5",
    "title": "GCLSTM_Simulation_reshape",
    "section": "block",
    "text": "block\n\ndata.query(\"mtype=='block' and method!='GNAR'\").plot.box(backend='plotly',x='mrate',color='method',y='mse',facet_col='lags',facet_row='nof_filters',height=1200)"
  },
  {
    "objectID": "posts/2_research/2023-02-07-ESTGCN_WIKI_DATA_2.html",
    "href": "posts/2_research/2023-02-07-ESTGCN_WIKI_DATA_2.html",
    "title": "Class of Method(WikiMath) lag 1",
    "section": "",
    "text": "ST-GCN Dataset WikiMathsDatasetLoader"
  },
  {
    "objectID": "posts/2_research/2023-02-07-ESTGCN_WIKI_DATA_2.html#train",
    "href": "posts/2_research/2023-02-07-ESTGCN_WIKI_DATA_2.html#train",
    "title": "Class of Method(WikiMath) lag 1",
    "section": "Train",
    "text": "Train\n\ndata_train=[]\nfor time, snapshot in enumerate(train_dataset):\n    data_train.append([time,snapshot])\n\n\ndata_train[0][1].x.shape,data_train[0][1].y.shape,data_train[0][1].edge_index.shape,data_train[0][1].edge_attr.shape\n\n(torch.Size([1068, 1]),\n torch.Size([1068]),\n torch.Size([2, 27079]),\n torch.Size([27079]))\n\n\n\ntime\n\n583\n\n\n\nT_train = time\nN = len(data_train[0][1].x)\n\n\nedge_index = data_train[0][1].edge_index\nedge_attr = data_train[0][1].edge_attr\n\n\nx_train = []\nfor i in range(time):\n    x_train.append(data_train[i][1].x)\n\n\ndata_tensor = torch.Tensor()\n# Iterate over the data points of the dataset\nfor i in x_train:\n    # Concatenate the data point to the tensor\n    data_tensor = torch.cat((data_tensor, i), dim=0)\nx_train = data_tensor.reshape(time,1068,-1)\nx_train.shape\n\ntorch.Size([583, 1068, 1])\n\n\n\ny_train = []\nfor i in range(time):\n    y_train.append(data_train[i][1].y)\n\n\ndata_tensor = torch.Tensor()\n# Iterate over the data points of the dataset\nfor i in y_train:\n    # Concatenate the data point to the tensor\n    data_tensor = torch.cat((data_tensor, i), dim=0)\ny_train = data_tensor.reshape(time,1068)\ny_train.shape\n\ntorch.Size([583, 1068])\n\n\n\nx_train.shape, y_train.shape\n\n(torch.Size([583, 1068, 1]), torch.Size([583, 1068]))"
  },
  {
    "objectID": "posts/2_research/2023-02-07-ESTGCN_WIKI_DATA_2.html#test",
    "href": "posts/2_research/2023-02-07-ESTGCN_WIKI_DATA_2.html#test",
    "title": "Class of Method(WikiMath) lag 1",
    "section": "Test",
    "text": "Test\n\ndata_test=[]\nfor time, snapshot in enumerate(test_dataset):\n    data_test.append([time,snapshot])\n\n\ndata_test[0][1].x.shape,data_test[0][1].y.shape,data_test[0][1].edge_index.shape,data_test[0][1].edge_attr.shape\n\n(torch.Size([1068, 1]),\n torch.Size([1068]),\n torch.Size([2, 27079]),\n torch.Size([27079]))\n\n\n\ntime\n\n145\n\n\n\nT_test = time\n\n\nx_test = []\nfor i in range(time):\n    x_test.append(data_test[i][1].x)\n\n\ndata_tensor = torch.Tensor()\n# Iterate over the data points of the dataset\nfor i in x_test:\n    # Concatenate the data point to the tensor\n    data_tensor = torch.cat((data_tensor, i), dim=0)\nx_test = data_tensor.reshape(time,1068,-1)\nx_test.shape\n\ntorch.Size([145, 1068, 1])\n\n\n\ny_test = []\nfor i in range(time):\n    y_test.append(data_test[i][1].y)\n\n\ndata_tensor = torch.Tensor()\n# Iterate over the data points of the dataset\nfor i in y_test:\n    # Concatenate the data point to the tensor\n    data_tensor = torch.cat((data_tensor, i), dim=0)\ny_test = data_tensor.reshape(time,1068)\ny_test.shape\n\ntorch.Size([145, 1068])\n\n\n\nx_test.shape, y_test.shape\n\n(torch.Size([145, 1068, 1]), torch.Size([145, 1068]))"
  },
  {
    "objectID": "posts/2_research/2023-02-07-ESTGCN_WIKI_DATA_2.html#시나리오1-baseline",
    "href": "posts/2_research/2023-02-07-ESTGCN_WIKI_DATA_2.html#시나리오1-baseline",
    "title": "Class of Method(WikiMath) lag 1",
    "section": "시나리오1 (Baseline)",
    "text": "시나리오1 (Baseline)\n시나리오1\n\nmissing rate: 0%\n보간방법: None\n\n\nSTGCN 으로 적합 + 예측\n\nX = x_train.float()[:-1,:,:]\ny = y_train.reshape(T_train,N,1).float()[1:,:,:]\n\n\nXX = x_test[:-1,:,:].float()\nyy = y_test.reshape(T_test,N,1)[1:,:,:].float()\n\n\nnet = RecurrentGCN(node_features=1, filters=4)\noptimizer = torch.optim.Adam(net.parameters(), lr=0.01)\nnet.train()\nfor epoch in tqdm(range(50)):\n    for time, (xt,yt) in enumerate(zip(X,y)):\n        yt_hat = net(xt, edge_index, edge_attr)\n        cost = torch.mean((yt_hat-yt)**2)\n        cost.backward()\n        optimizer.step()\n        optimizer.zero_grad()\n\n100%|██████████| 50/50 [05:31&lt;00:00,  6.62s/it]\n\n\n\nyhat = torch.stack([net(xt, edge_index, edge_attr) for xt in X]).detach().numpy()\nyyhat = torch.stack([net(xt, edge_index, edge_attr) for xt in XX]).detach().numpy()\n\n\nstgcn_train = yhat.squeeze() # stgcn은 stgcn에 의한 적합결과를 의미함\nstgcn_test = yyhat.squeeze() \n\n\ntrain_mse_eachnode_stgcn = (((y-yhat).squeeze())**2).mean(axis=0)\ntrain_mse_total_stgcn = (((y-yhat).squeeze())**2).mean()\ntest_mse_eachnode_stgcn = (((yy-yyhat).squeeze())**2).mean(axis=0)\ntest_mse_total_stgcn = (((yy-yyhat).squeeze())**2).mean()\n\n\n\nGNAR 으로 적합 + 예측\n-\n\n%load_ext rpy2.ipython\n\n\n%%R\nlibrary(GNAR)\nlibrary(igraph)\n\n\nEdge = np.array(edge_index)\nX_gnar = np.array(x_train.squeeze())\n\n\nw=np.zeros((1068,1068))\n\n\nfor k in range(27079):\n    w[edge_index[0][k],edge_index[1][k]] = 1\n\n\n%R -i X_gnar\n%R -i w\n%R -i T_test\n\n\n%%R\nwikiNet &lt;- matrixtoGNAR(w)\n\n\n%%R\nanswer &lt;- GNARfit(vts = X_gnar, net = wikiNet, alphaOrder = 1, betaOrder = c(1))\nprediction &lt;- predict(answer,n.ahead=T_test)\n\n\n%%R\ngnar_train &lt;- residuals(answer)\ngnar_test &lt;- prediction\n\n\n%R -o gnar_train\n%R -o gnar_test\n\n\ntrain_mse_eachnode_gnar = (gnar_train**2).mean(axis=0)\ntrain_mse_total_gnar = (gnar_train**2).mean()\ntest_mse_eachnode_gnar = ((yy.squeeze()-gnar_test[1:])**2).mean(axis=0)\ntest_mse_total_gnar = ((yy.squeeze()-gnar_test[1:])**2).mean()\n\n\n\n결과시각화\n\nfig = plot(x_train.squeeze()[:,:5],'--o',color='gray',label='complete data',alpha=0.2)\nfig = plot_add(fig,x_test.squeeze()[:,:5],'--o',color='gray',label='complete data',alpha=0.2,t=range(581,726))\nax = fig.get_axes()\nfor i,a in enumerate(ax):     \n    a.set_title('node{0} \\n STGCN: mse(train) = {1:.2f}, mse(test) = {2:.2f} \\n GNAR: mse(train) = {3:.2f}, mse(test) = {4:.2f}'.format(i,train_mse_eachnode_stgcn[i],test_mse_eachnode_stgcn[i],train_mse_eachnode_gnar[i],test_mse_eachnode_gnar[i]))\n    a.plot(range(1,583),stgcn_train[:,i],label='STCGCN (train)',color='C0')\n    a.plot(range(584,728),stgcn_test[:,i],label='STCGCN (test)',color='C0')\n    a.plot(range(1,583),gnar_train[:,i],label='GNAR (train)',color='C1')\n    a.plot(range(583,728),gnar_test[:,i],label='GNAR (test)',color='C1')\n    a.legend()\nfig.set_figwidth(14)\nfig.suptitle(\"Scenario1: STGCN \\n missing=0% \\n interpolation=None \\n\\n STGCN: mse(train) = {0:.2f}, mse(test) = {1:.2f} \\n GNAR: mse(train) = {2:.2f}, mse(test) = {3:.2f} \\n\".format(train_mse_total_stgcn,test_mse_total_stgcn,train_mse_total_gnar,test_mse_total_gnar),size=15)\nfig.tight_layout()\nfig"
  },
  {
    "objectID": "posts/2_research/2023-02-07-ESTGCN_WIKI_DATA_2.html#시나리오2",
    "href": "posts/2_research/2023-02-07-ESTGCN_WIKI_DATA_2.html#시나리오2",
    "title": "Class of Method(WikiMath) lag 1",
    "section": "시나리오2",
    "text": "시나리오2\n시나리오2\n\nmissing rate: 50%\n보간방법: linear\n\n- 결측치생성 + 보간\n\n_zero = Missing(x_train.squeeze())\n_zero.miss(percent = 0.5)\n_zero.second_linear()\n\n\nmissing_index = _zero.number\ninterpolated_signal = _zero.train_linear\n\n\nfig = plot(x_train.squeeze()[:,:5],'--o',color='gray',label='complete data',alpha=0.2)\nfig = plot_add(fig,x_test.squeeze()[:,:5],'--o',color='gray',label='complete data',alpha=0.2,t=range(581,726))\nax = fig.get_axes()\nfor i,a in enumerate(ax):     \n    a.plot(missing_index[i],x_train[:,:,0][:,i][missing_index[i]],'xk',label='missing')\n    a.plot(interpolated_signal[:,i],'-',color='gray',label='linear interpolation')\n    a.legend()\nfig.set_figwidth(15)\nfig\n\n\n\n\n\n\n\n\n\nSTGCN 으로 적합 + 예측\n\nX = x_train.float()[:-1,:,:]\ny = y_train.reshape(T_train,N,1).float()[1:,:,:]\n\n\nXX = x_test[:-1,:,:].float()\nyy = y_test.reshape(T_test,N,1)[1:,:,:].float()\n\n\nnet = RecurrentGCN(node_features=1, filters=4)\noptimizer = torch.optim.Adam(net.parameters(), lr=0.01)\nnet.train()\nfor epoch in tqdm(range(50)):\n    for time, (xt,yt) in enumerate(zip(X,y)):\n        yt_hat = net(xt, edge_index, edge_attr)\n        cost = torch.mean((yt_hat-yt)**2)\n        cost.backward()\n        optimizer.step()\n        optimizer.zero_grad()\n\n100%|██████████| 50/50 [05:34&lt;00:00,  6.68s/it]\n\n\n\nyhat = torch.stack([net(xt, edge_index, edge_attr) for xt in X]).detach().numpy()\nyyhat = torch.stack([net(xt, edge_index, edge_attr) for xt in XX]).detach().numpy()\n\n\nstgcn_train = yhat.squeeze() # stgcn은 stgcn에 의한 적합결과를 의미함\nstgcn_test = yyhat.squeeze() \n\n\ntrain_mse_eachnode_stgcn = (((y-yhat).squeeze())**2).mean(axis=0)\ntrain_mse_total_stgcn = (((y-yhat).squeeze())**2).mean()\ntest_mse_eachnode_stgcn = (((yy-yyhat).squeeze())**2).mean(axis=0)\ntest_mse_total_stgcn = (((yy-yyhat).squeeze())**2).mean()\n\n\n\nESTGCN 으로 적합 + 예측\n\nX = x_train.float()[:-1,:,:]\ny = y_train.reshape(T_train,N,1).float()[1:,:,:]\n\n\nXX = x_test[:-1,:,:].float()\nyy = y_test.reshape(T_test,N,1)[1:,:,:].float()\n\n- ESTGCN\n\nnet = RecurrentGCN(node_features=1, filters=4)\noptimizer = torch.optim.Adam(net.parameters(), lr=0.01)\nnet.train()\nsignal = interpolated_signal.copy()\nfor epoch in tqdm(range(50)):\n    signal = update_from_freq_domain(signal,missing_index)\n    X = torch.tensor(signal).reshape(T_train,N,1).float()[:-1,:,:]\n    y = torch.tensor(signal).reshape(T_train,N,1).float()[1:,:,:]\n    for time, (xt,yt) in enumerate(zip(X,y)):        \n        yt_hat = net(xt, edge_index, edge_attr)\n        cost = torch.mean((yt_hat-yt)**2)\n        cost.backward()\n        optimizer.step()\n        optimizer.zero_grad()\n    signal = torch.concat([X.squeeze(),yt_hat.detach().squeeze().reshape(1,-1)])        \n\n100%|██████████| 50/50 [06:56&lt;00:00,  8.33s/it]\n\n\n- ESTGCN\n\nyhat = torch.stack([net(xt, edge_index, edge_attr) for xt in X]).detach().numpy()\nyyhat = torch.stack([net(xt, edge_index, edge_attr) for xt in XX]).detach().numpy()\n\n\nx_test.shape,y_test.shape\n\n(torch.Size([145, 1068, 1]), torch.Size([145, 1068]))\n\n\n\nreal_y = y_train.reshape(T_train,N,1).float()[1:,:,:]\n\ntrain_mse_eachnode_estgcn = (((real_y-yhat).squeeze())**2).mean(axis=0)\ntrain_mse_total_estgcn = (((real_y-yhat).squeeze())**2).mean()\ntest_mse_eachnode_estgcn = (((yy-yyhat).squeeze())**2).mean(axis=0)\ntest_mse_total_estgcn = (((yy-yyhat).squeeze())**2).mean()\n\n\nestgcn_train = yhat.squeeze() # stgcn은 stgcn에 의한 적합결과를 의미함\nestgcn_test = yyhat.squeeze() \n\n\n\nGNAR 으로 적합 + 예측\n-\n\nEdge = np.array(edge_index)\nX_gnar = np.array(x_train.squeeze())\n\n\nw=np.zeros((1068,1068))\n\n\nfor k in range(27079):\n    w[edge_index[0][k],edge_index[1][k]] = 1\n\n\n%R -i X_gnar\n%R -i w\n%R -i T_test\n\n\n%%R\nwikiNet &lt;- matrixtoGNAR(w)\n\n\n%%R\nanswer &lt;- GNARfit(vts = X_gnar, net = wikiNet, alphaOrder = 1, betaOrder = c(1))\nprediction &lt;- predict(answer,n.ahead=T_test)\n\n\n%%R\ngnar_train &lt;- residuals(answer)\ngnar_test &lt;- prediction\n\n\n%R -o gnar_train\n%R -o gnar_test\n\n\ntrain_mse_eachnode_gnar = (gnar_train**2).mean(axis=0)\ntrain_mse_total_gnar = (gnar_train**2).mean()\ntest_mse_eachnode_gnar = ((yy.squeeze()-gnar_test[1:])**2).mean(axis=0)\ntest_mse_total_gnar = ((yy.squeeze()-gnar_test[1:])**2).mean()\n\n\n\n결과시각화\n\nfig = plot(x_train.squeeze()[:,:5],'--o',color='gray',label='complete data',alpha=0.2)\nfig = plot_add(fig,x_test.squeeze()[:,:5],'--o',color='gray',label='complete data',alpha=0.2,t=range(581,726))\nax = fig.get_axes()\nfor i,a in enumerate(ax):     \n    a.set_title('node{0} \\n STGCN: mse(train) = {1:.2f}, mse(test) = {2:.2f} \\n ESTGCN: mse(train) = {3:.2f}, mse(test) = {4:.2f}\\n GNAR: mse(train) = {5:.2f}, mse(test) = {6:.2f}'.format(i,train_mse_eachnode_stgcn[i],test_mse_eachnode_stgcn[i],train_mse_eachnode_estgcn[i],test_mse_eachnode_estgcn[i],train_mse_eachnode_gnar[i],test_mse_eachnode_gnar[i]))\n    a.plot(missing_index[i],x_train[missing_index[i],i,0],'xk',label='missing')\n    a.plot(interpolated_signal[:,i],'-',color='gray',label='linear interpolation')\n    a.plot(range(1,583),stgcn_train.squeeze()[:,i],'--.',label='STCGCN (train)',color='C0')\n    a.plot(range(584,728),stgcn_test.squeeze()[:,i],'--.',label='STCGCN (test)',color='C0')\n    a.plot(range(1,583),estgcn_train.squeeze()[:,i],label='ESTCGCN (train)',color='C1')\n    a.plot(range(584,728),estgcn_test.squeeze()[:,i],label='ESTCGCN (test)',color='C1')\n    a.plot(range(1,583),gnar_train.squeeze()[:,i],label='GNAR (train)',color='C2')\n    a.plot(range(584,729),gnar_test.squeeze()[:,i],label='GNAR (test)',color='C2')\n    \n    a.legend()\nfig.set_figwidth(14)\nfig.suptitle(\"Scenario2: \\n missing=50% \\n interpolation=linear \\n\\n STGCN: mse(train) = {0:.2f}, mse(test) = {1:.2f} \\n ESTGCN: mse(train) = {2:.2f}, mse(test) = {3:.2f} \\n GNAR: mse(train) = {4:.2f}, mse(test) = {5:.2f} \\n\".format(train_mse_total_stgcn,test_mse_total_stgcn,train_mse_total_estgcn,test_mse_total_estgcn,train_mse_total_gnar,test_mse_total_gnar),size=15)\nfig.tight_layout()\nfig"
  },
  {
    "objectID": "posts/2_research/2023-02-07-ESTGCN_WIKI_DATA_2.html#시나리오3",
    "href": "posts/2_research/2023-02-07-ESTGCN_WIKI_DATA_2.html#시나리오3",
    "title": "Class of Method(WikiMath) lag 1",
    "section": "시나리오3",
    "text": "시나리오3\n시나리오3\n\nmissing rate: 80%\n보간방법: linear\n\n- 결측치생성 + 보간\n\n_zero = Missing(x_train.squeeze())\n_zero.miss(percent = 0.8)\n_zero.second_linear()\n\n\nmissing_index = _zero.number\ninterpolated_signal = _zero.train_linear\n\n\nfig = plot(x_train.squeeze()[:,:5],'--o',color='gray',label='complete data',alpha=0.2)\nfig = plot_add(fig,x_test.squeeze()[:,:5],'--o',color='gray',label='complete data',alpha=0.2,t=range(581,726))\nax = fig.get_axes()\nfor i,a in enumerate(ax):     \n    a.plot(missing_index[i],x_train.squeeze()[:,i][missing_index[i]],'xk',label='missing')\n    a.plot(interpolated_signal[:,i],'-',color='gray',label='linear interpolation')\n    a.legend()\nfig.set_figwidth(15)\nfig\n\n\n\n\n\n\n\n\n\nSTGCN 으로 적합 + 예측\n\nX = x_train.float()[:-1,:,:]\ny = y_train.reshape(T_train,N,1).float()[1:,:,:]\n\n\nXX = x_test[:-1,:,:].float()\nyy = y_test.reshape(T_test,N,1)[1:,:,:].float()\n\n\nnet = RecurrentGCN(node_features=1, filters=4)\noptimizer = torch.optim.Adam(net.parameters(), lr=0.01)\nnet.train()\nfor epoch in tqdm(range(50)):\n    for time, (xt,yt) in enumerate(zip(X,y)):\n        yt_hat = net(xt, edge_index, edge_attr)\n        cost = torch.mean((yt_hat-yt)**2)\n        cost.backward()\n        optimizer.step()\n        optimizer.zero_grad()\n\n100%|██████████| 50/50 [05:51&lt;00:00,  7.04s/it]\n\n\n\nyhat = torch.stack([net(xt, edge_index, edge_attr) for xt in X]).detach().numpy()\nyyhat = torch.stack([net(xt, edge_index, edge_attr) for xt in XX]).detach().numpy()\n\n\nstgcn_train = yhat.squeeze() # stgcn은 stgcn에 의한 적합결과를 의미함\nstgcn_test = yyhat.squeeze() \n\n\ntrain_mse_eachnode_stgcn = (((y-yhat).squeeze())**2).mean(axis=0)\ntrain_mse_total_stgcn = (((y-yhat).squeeze())**2).mean()\ntest_mse_eachnode_stgcn = (((yy-yyhat).squeeze())**2).mean(axis=0)\ntest_mse_total_stgcn = (((yy-yyhat).squeeze())**2).mean()\n\n\n\nESTGCN 으로 적합 + 예측\n\nX = x_train.float()[:-1,:,:]\ny = y_train.reshape(T_train,N,1).float()[1:,:,:]\n\n\nXX = x_test[:-1,:,:].float()\nyy = y_test.reshape(T_test,N,1)[1:,:,:].float()\n\n- ESTGCN\n\nnet = RecurrentGCN(node_features=1, filters=4)\noptimizer = torch.optim.Adam(net.parameters(), lr=0.01)\nnet.train()\nsignal = interpolated_signal.copy()\nfor epoch in tqdm(range(50)):\n    signal = update_from_freq_domain(signal,missing_index)\n    X = torch.tensor(signal).reshape(T_train,N,1).float()[:-1,:,:]\n    y = torch.tensor(signal).reshape(T_train,N,1).float()[1:,:,:]\n    for time, (xt,yt) in enumerate(zip(X,y)):        \n        yt_hat = net(xt, edge_index, edge_attr)\n        cost = torch.mean((yt_hat-yt)**2)\n        cost.backward()\n        optimizer.step()\n        optimizer.zero_grad()\n    signal = torch.concat([X.squeeze(),yt_hat.detach().squeeze().reshape(1,-1)])        \n\n100%|██████████| 50/50 [07:00&lt;00:00,  8.40s/it]\n\n\n- ESTGCN\n\nyhat = torch.stack([net(xt, edge_index, edge_attr) for xt in X]).detach().numpy()\nyyhat = torch.stack([net(xt, edge_index, edge_attr) for xt in XX]).detach().numpy()\n\n\nreal_y = y_train.reshape(T_train,N,1).float()[1:,:,:]\n\ntrain_mse_eachnode_estgcn = (((real_y-yhat).squeeze())**2).mean(axis=0)\ntrain_mse_total_estgcn = (((real_y-yhat).squeeze())**2).mean()\ntest_mse_eachnode_estgcn = (((yy-yyhat).squeeze())**2).mean(axis=0)\ntest_mse_total_estgcn = (((yy-yyhat).squeeze())**2).mean()\n\n\nestgcn_train = yhat.squeeze() # stgcn은 stgcn에 의한 적합결과를 의미함\nestgcn_test = yyhat.squeeze() \n\n\n\nGNAR 으로 적합 + 예측\n-\n\nEdge = np.array(edge_index)\nX_gnar = np.array(x_train.squeeze())\n\n\nw=np.zeros((1068,1068))\n\n\nfor k in range(27079):\n    w[edge_index[0][k],edge_index[1][k]] = 1\n\n\n%R -i X_gnar\n%R -i w\n%R -i T_test\n\n\n%%R\nwikiNet &lt;- matrixtoGNAR(w)\n\n\n%%R\nanswer &lt;- GNARfit(vts = X_gnar, net = wikiNet, alphaOrder = 1, betaOrder = c(1))\nprediction &lt;- predict(answer,n.ahead=T_test)\n\n\n%%R\ngnar_train &lt;- residuals(answer)\ngnar_test &lt;- prediction\n\n\n%R -o gnar_train\n%R -o gnar_test\n\n\ntrain_mse_eachnode_gnar = (gnar_train**2).mean(axis=0)\ntrain_mse_total_gnar = (gnar_train**2).mean()\ntest_mse_eachnode_gnar = ((yy.squeeze()-gnar_test[1:])**2).mean(axis=0)\ntest_mse_total_gnar = ((yy.squeeze()-gnar_test[1:])**2).mean()\n\n\n\n결과시각화\n\nfig = plot(x_train.squeeze()[:,:5],'--o',color='gray',label='complete data',alpha=0.2)\nfig = plot_add(fig,x_test.squeeze()[:,:5],'--o',color='gray',label='complete data',alpha=0.2,t=range(581,726))\nax = fig.get_axes()\nfor i,a in enumerate(ax):     \n    a.set_title('node{0} \\n STGCN: mse(train) = {1:.2f}, mse(test) = {2:.2f} \\n ESTGCN: mse(train) = {3:.2f}, mse(test) = {4:.2f}\\n GNAR: mse(train) = {5:.2f}, mse(test) = {6:.2f}'.format(i,train_mse_eachnode_stgcn[i],test_mse_eachnode_stgcn[i],train_mse_eachnode_estgcn[i],test_mse_eachnode_estgcn[i],train_mse_eachnode_gnar[i],test_mse_eachnode_gnar[i]))\n    a.plot(missing_index[i],x_train[missing_index[i],i,0],'xk',label='missing')\n    a.plot(interpolated_signal[:,i],'-',color='gray',label='linear interpolation')\n    a.plot(range(1,583),stgcn_train.squeeze()[:,i],'--.',label='STCGCN (train)',color='C0')\n    a.plot(range(584,728),stgcn_test.squeeze()[:,i],'--.',label='STCGCN (test)',color='C0')\n    a.plot(range(1,583),estgcn_train.squeeze()[:,i],label='ESTCGCN (train)',color='C1')\n    a.plot(range(584,728),estgcn_test.squeeze()[:,i],label='ESTCGCN (test)',color='C1')\n    a.plot(range(1,583),gnar_train.squeeze()[:,i],label='GNAR (train)',color='C2')\n    a.plot(range(584,729),gnar_test.squeeze()[:,i],label='GNAR (test)',color='C2')\n    \n    a.legend()\nfig.set_figwidth(14)\nfig.suptitle(\"Scenario2: \\n missing=50% \\n interpolation=linear \\n\\n STGCN: mse(train) = {0:.2f}, mse(test) = {1:.2f} \\n ESTGCN: mse(train) = {2:.2f}, mse(test) = {3:.2f} \\n GNAR: mse(train) = {4:.2f}, mse(test) = {5:.2f} \\n\".format(train_mse_total_stgcn,test_mse_total_stgcn,train_mse_total_estgcn,test_mse_total_estgcn,train_mse_total_gnar,test_mse_total_gnar),size=15)\nfig.tight_layout()\nfig"
  },
  {
    "objectID": "posts/2_research/2023-02-07-ESTGCN_WIKI_DATA_2.html#시나리오4",
    "href": "posts/2_research/2023-02-07-ESTGCN_WIKI_DATA_2.html#시나리오4",
    "title": "Class of Method(WikiMath) lag 1",
    "section": "시나리오4",
    "text": "시나리오4\n시나리오4\n\nmissing rate: 30%\n보간방법: linear\n\n- 결측치생성 + 보간\n\n_zero = Missing(x_train.squeeze())\n_zero.miss(percent = 0.3)\n_zero.second_linear()\n\n\nmissing_index = _zero.number\ninterpolated_signal = _zero.train_linear\n\n\nfig = plot(x_train.squeeze()[:,:5],'--o',color='gray',label='complete data',alpha=0.2)\nfig = plot_add(fig,x_test.squeeze()[:,:5],'--o',color='gray',label='complete data',alpha=0.2,t=range(581,726))\nax = fig.get_axes()\nfor i,a in enumerate(ax):     \n    a.plot(missing_index[i],x_train.squeeze()[:,i][missing_index[i]],'xk',label='missing')\n    a.plot(interpolated_signal[:,i],'-',color='gray',label='linear interpolation')\n    a.legend()\nfig.set_figwidth(15)\nfig\n\n\n\n\n\n\n\n\n\nSTGCN 으로 적합 + 예측\n\nX = x_train.float()[:-1,:,:]\ny = y_train.reshape(T_train,N,1).float()[1:,:,:]\n\n\nXX = x_test[:-1,:,:].float()\nyy = y_test.reshape(T_test,N,1)[1:,:,:].float()\n\n\nnet = RecurrentGCN(node_features=1, filters=4)\noptimizer = torch.optim.Adam(net.parameters(), lr=0.01)\nnet.train()\nfor epoch in tqdm(range(50)):\n    for time, (xt,yt) in enumerate(zip(X,y)):\n        yt_hat = net(xt, edge_index, edge_attr)\n        cost = torch.mean((yt_hat-yt)**2)\n        cost.backward()\n        optimizer.step()\n        optimizer.zero_grad()\n\n100%|██████████| 50/50 [05:54&lt;00:00,  7.09s/it]\n\n\n\nyhat = torch.stack([net(xt, edge_index, edge_attr) for xt in X]).detach().numpy()\nyyhat = torch.stack([net(xt, edge_index, edge_attr) for xt in XX]).detach().numpy()\n\n\nstgcn_train = yhat.squeeze() # stgcn은 stgcn에 의한 적합결과를 의미함\nstgcn_test = yyhat.squeeze() \n\n\ntrain_mse_eachnode_stgcn = (((y-yhat).squeeze())**2).mean(axis=0)\ntrain_mse_total_stgcn = (((y-yhat).squeeze())**2).mean()\ntest_mse_eachnode_stgcn = (((yy-yyhat).squeeze())**2).mean(axis=0)\ntest_mse_total_stgcn = (((yy-yyhat).squeeze())**2).mean()\n\n\n\nESTGCN 으로 적합 + 예측\n\nX = x_train.float()[:-1,:,:]\ny = y_train.reshape(T_train,N,1).float()[1:,:,:]\n\n\nXX = x_test[:-1,:,:].float()\nyy = y_test.reshape(T_test,N,1)[1:,:,:].float()\n\n- ESTGCN\n\nnet = RecurrentGCN(node_features=1, filters=4)\noptimizer = torch.optim.Adam(net.parameters(), lr=0.01)\nnet.train()\nsignal = interpolated_signal.copy()\nfor epoch in tqdm(range(50)):\n    signal = update_from_freq_domain(signal,missing_index)\n    X = torch.tensor(signal).reshape(T_train,N,1).float()[:-1,:,:]\n    y = torch.tensor(signal).reshape(T_train,N,1).float()[1:,:,:]\n    for time, (xt,yt) in enumerate(zip(X,y)):        \n        yt_hat = net(xt, edge_index, edge_attr)\n        cost = torch.mean((yt_hat-yt)**2)\n        cost.backward()\n        optimizer.step()\n        optimizer.zero_grad()\n    signal = torch.concat([X.squeeze(),yt_hat.detach().squeeze().reshape(1,-1)])        \n\n100%|██████████| 50/50 [07:00&lt;00:00,  8.40s/it]\n\n\n- ESTGCN\n\nyhat = torch.stack([net(xt, edge_index, edge_attr) for xt in X]).detach().numpy()\nyyhat = torch.stack([net(xt, edge_index, edge_attr) for xt in XX]).detach().numpy()\n\n\nreal_y = y_train.reshape(T_train,N,1).float()[1:,:,:]\n\ntrain_mse_eachnode_estgcn = (((real_y-yhat).squeeze())**2).mean(axis=0)\ntrain_mse_total_estgcn = (((real_y-yhat).squeeze())**2).mean()\ntest_mse_eachnode_estgcn = (((yy-yyhat).squeeze())**2).mean(axis=0)\ntest_mse_total_estgcn = (((yy-yyhat).squeeze())**2).mean()\n\n\nestgcn_train = yhat.squeeze() # stgcn은 stgcn에 의한 적합결과를 의미함\nestgcn_test = yyhat.squeeze() \n\n\n\nGNAR 으로 적합 + 예측\n-\n\nEdge = np.array(edge_index)\nX_gnar = np.array(x_train.squeeze())\n\n\nw=np.zeros((1068,1068))\n\n\nfor k in range(27079):\n    w[edge_index[0][k],edge_index[1][k]] = 1\n\n\n%R -i X_gnar\n%R -i w\n%R -i T_test\n\n\n%%R\nwikiNet &lt;- matrixtoGNAR(w)\n\n\n%%R\nanswer &lt;- GNARfit(vts = X_gnar, net = wikiNet, alphaOrder = 1, betaOrder = c(1))\nprediction &lt;- predict(answer,n.ahead=T_test)\n\n\n%%R\ngnar_train &lt;- residuals(answer)\ngnar_test &lt;- prediction\n\n\n%R -o gnar_train\n%R -o gnar_test\n\n\ntrain_mse_eachnode_gnar = (gnar_train**2).mean(axis=0)\ntrain_mse_total_gnar = (gnar_train**2).mean()\ntest_mse_eachnode_gnar = ((yy.squeeze()-gnar_test[1:])**2).mean(axis=0)\ntest_mse_total_gnar = ((yy.squeeze()-gnar_test[1:])**2).mean()\n\n\n\n결과시각화\n\nfig = plot(x_train.squeeze()[:,:5],'--o',color='gray',label='complete data',alpha=0.2)\nfig = plot_add(fig,x_test.squeeze()[:,:5],'--o',color='gray',label='complete data',alpha=0.2,t=range(581,726))\nax = fig.get_axes()\nfor i,a in enumerate(ax):     \n    a.set_title('node{0} \\n STGCN: mse(train) = {1:.2f}, mse(test) = {2:.2f} \\n ESTGCN: mse(train) = {3:.2f}, mse(test) = {4:.2f}\\n GNAR: mse(train) = {5:.2f}, mse(test) = {6:.2f}'.format(i,train_mse_eachnode_stgcn[i],test_mse_eachnode_stgcn[i],train_mse_eachnode_estgcn[i],test_mse_eachnode_estgcn[i],train_mse_eachnode_gnar[i],test_mse_eachnode_gnar[i]))\n    a.plot(missing_index[i],x_train[missing_index[i],i,0],'xk',label='missing')\n    a.plot(interpolated_signal[:,i],'-',color='gray',label='linear interpolation')\n    a.plot(range(1,583),stgcn_train.squeeze()[:,i],'--.',label='STCGCN (train)',color='C0')\n    a.plot(range(584,728),stgcn_test.squeeze()[:,i],'--.',label='STCGCN (test)',color='C0')\n    a.plot(range(1,583),estgcn_train.squeeze()[:,i],label='ESTCGCN (train)',color='C1')\n    a.plot(range(584,728),estgcn_test.squeeze()[:,i],label='ESTCGCN (test)',color='C1')\n    a.plot(range(1,583),gnar_train.squeeze()[:,i],label='GNAR (train)',color='C2')\n    a.plot(range(584,729),gnar_test.squeeze()[:,i],label='GNAR (test)',color='C2')\n    \n    a.legend()\nfig.set_figwidth(14)\nfig.suptitle(\"Scenario2: \\n missing=50% \\n interpolation=linear \\n\\n STGCN: mse(train) = {0:.2f}, mse(test) = {1:.2f} \\n ESTGCN: mse(train) = {2:.2f}, mse(test) = {3:.2f} \\n GNAR: mse(train) = {4:.2f}, mse(test) = {5:.2f} \\n\".format(train_mse_total_stgcn,test_mse_total_stgcn,train_mse_total_estgcn,test_mse_total_estgcn,train_mse_total_gnar,test_mse_total_gnar),size=15)\nfig.tight_layout()\nfig"
  },
  {
    "objectID": "posts/2_research/2023-06-28-DYGRENCODER_Simulation_boxplot_reshape (SEOYEON CHOI's conflicted copy 2023-09-27).html",
    "href": "posts/2_research/2023-06-28-DYGRENCODER_Simulation_boxplot_reshape (SEOYEON CHOI's conflicted copy 2023-09-27).html",
    "title": "DYGRENCODER_Simulation_reshape",
    "section": "",
    "text": "Simulation Study"
  },
  {
    "objectID": "posts/2_research/2023-06-28-DYGRENCODER_Simulation_boxplot_reshape (SEOYEON CHOI's conflicted copy 2023-09-27).html#baseline",
    "href": "posts/2_research/2023-06-28-DYGRENCODER_Simulation_boxplot_reshape (SEOYEON CHOI's conflicted copy 2023-09-27).html#baseline",
    "title": "DYGRENCODER_Simulation_reshape",
    "section": "Baseline",
    "text": "Baseline\n\ndata.query(\"method!='GNAR' and mrate==0\").plot.box(backend='plotly',x='mrate',color='method',y='mse',facet_col='lags',facet_row='nof_filters',height=600)"
  },
  {
    "objectID": "posts/2_research/2023-06-28-DYGRENCODER_Simulation_boxplot_reshape (SEOYEON CHOI's conflicted copy 2023-09-27).html#random",
    "href": "posts/2_research/2023-06-28-DYGRENCODER_Simulation_boxplot_reshape (SEOYEON CHOI's conflicted copy 2023-09-27).html#random",
    "title": "DYGRENCODER_Simulation_reshape",
    "section": "Random",
    "text": "Random\n\ndata.query(\"method!='GNAR' and mtype =='rand'\").plot.box(backend='plotly',x='mrate',color='method',y='mse',facet_col='inter_method',facet_row='nof_filters',height=600)"
  },
  {
    "objectID": "posts/2_research/2023-06-28-DYGRENCODER_Simulation_boxplot_reshape (SEOYEON CHOI's conflicted copy 2023-09-27).html#block",
    "href": "posts/2_research/2023-06-28-DYGRENCODER_Simulation_boxplot_reshape (SEOYEON CHOI's conflicted copy 2023-09-27).html#block",
    "title": "DYGRENCODER_Simulation_reshape",
    "section": "Block",
    "text": "Block\n\ndata.query(\"method!='GNAR' and mtype =='block' \").plot.box(backend='plotly',x='mrate',color='method',y='mse',facet_col='inter_method',facet_row='nof_filters',height=600)"
  },
  {
    "objectID": "posts/2_research/2023-06-28-DYGRENCODER_Simulation_boxplot_reshape (SEOYEON CHOI's conflicted copy 2023-09-27).html#baseline-1",
    "href": "posts/2_research/2023-06-28-DYGRENCODER_Simulation_boxplot_reshape (SEOYEON CHOI's conflicted copy 2023-09-27).html#baseline-1",
    "title": "DYGRENCODER_Simulation_reshape",
    "section": "Baseline",
    "text": "Baseline\n\ndata.query(\"method!='GNAR' and mrate ==0 \").plot.box(backend='plotly',x='mrate',color='method',y='mse',facet_col='nof_filters',height=600)"
  },
  {
    "objectID": "posts/2_research/2023-06-28-DYGRENCODER_Simulation_boxplot_reshape (SEOYEON CHOI's conflicted copy 2023-09-27).html#random-1",
    "href": "posts/2_research/2023-06-28-DYGRENCODER_Simulation_boxplot_reshape (SEOYEON CHOI's conflicted copy 2023-09-27).html#random-1",
    "title": "DYGRENCODER_Simulation_reshape",
    "section": "Random",
    "text": "Random\n\ndata.query(\"method!='GNAR' and mtype =='rand'\").plot.box(backend='plotly',x='mrate',color='method',y='mse',facet_col='nof_filters',facet_row='inter_method',height=800)"
  },
  {
    "objectID": "posts/2_research/2023-06-28-DYGRENCODER_Simulation_boxplot_reshape (SEOYEON CHOI's conflicted copy 2023-09-27).html#block-1",
    "href": "posts/2_research/2023-06-28-DYGRENCODER_Simulation_boxplot_reshape (SEOYEON CHOI's conflicted copy 2023-09-27).html#block-1",
    "title": "DYGRENCODER_Simulation_reshape",
    "section": "Block",
    "text": "Block\n\ndata.query(\"method!='GNAR' and mtype =='block'\").plot.box(backend='plotly',x='mrate',color='method',y='mse',facet_col='inter_method',facet_row='lags',height=800)"
  },
  {
    "objectID": "posts/2_research/2023-06-28-DYGRENCODER_Simulation_boxplot_reshape (SEOYEON CHOI's conflicted copy 2023-09-27).html#baseline-2",
    "href": "posts/2_research/2023-06-28-DYGRENCODER_Simulation_boxplot_reshape (SEOYEON CHOI's conflicted copy 2023-09-27).html#baseline-2",
    "title": "DYGRENCODER_Simulation_reshape",
    "section": "Baseline",
    "text": "Baseline\n\ndata.query(\"mrate ==0 and lags!=2\").plot.box(backend='plotly',x='epoch',color='method',y='mse',facet_col='nof_filters',facet_row='lags',height=400)"
  },
  {
    "objectID": "posts/2_research/2023-06-28-DYGRENCODER_Simulation_boxplot_reshape (SEOYEON CHOI's conflicted copy 2023-09-27).html#random-2",
    "href": "posts/2_research/2023-06-28-DYGRENCODER_Simulation_boxplot_reshape (SEOYEON CHOI's conflicted copy 2023-09-27).html#random-2",
    "title": "DYGRENCODER_Simulation_reshape",
    "section": "random",
    "text": "random\n\ndata.query(\"method!='GNAR' and mtype =='rand' and lags!=2\").plot.box(backend='plotly',x='mrate',color='method',y='mse',facet_col='nof_filters',facet_row='inter_method',height=800)"
  },
  {
    "objectID": "posts/2_research/2023-06-28-DYGRENCODER_Simulation_boxplot_reshape (SEOYEON CHOI's conflicted copy 2023-09-27).html#block-2",
    "href": "posts/2_research/2023-06-28-DYGRENCODER_Simulation_boxplot_reshape (SEOYEON CHOI's conflicted copy 2023-09-27).html#block-2",
    "title": "DYGRENCODER_Simulation_reshape",
    "section": "block",
    "text": "block\n\ndata.query(\"method!='GNAR' and mtype =='block' and lags!=2 \").plot.box(backend='plotly',x='mrate',color='method',y='mse',facet_col='nof_filters',facet_row='inter_method',height=800)"
  },
  {
    "objectID": "posts/2_research/2023-06-28-DYGRENCODER_Simulation_boxplot_reshape (SEOYEON CHOI's conflicted copy 2023-09-27).html#weight-matrix-time-node-고려한-결과",
    "href": "posts/2_research/2023-06-28-DYGRENCODER_Simulation_boxplot_reshape (SEOYEON CHOI's conflicted copy 2023-09-27).html#weight-matrix-time-node-고려한-결과",
    "title": "DYGRENCODER_Simulation_reshape",
    "section": "weight matrix time, node 고려한 결과",
    "text": "weight matrix time, node 고려한 결과\n\ndf1 = pd.read_csv('./simulation_results/2023-06-30_13-25-56.csv')\ndf2 = pd.read_csv('./simulation_results/2023-06-30_14-00-19.csv')\n\n\ndata2 = pd.concat([df1,df2],axis=0)\n\n\ndata2.to_csv('./simulation_results/Real_simulation_reshape/DYGRENCODER_pedalme_Simulation_itstgcnsnd.csv',index=False)\n\n\ndata2 = pd.read_csv('./simulation_results/Real_simulation_reshape/DYGRENCODER_pedalme_Simulation_itstgcnsnd.csv')\n\n\ndata2.query(\"mtype=='rand'\").plot.box(backend='plotly',x='mrate',color='method',y='mse',facet_col='inter_method',facet_row='nof_filters',height=1000)\n\n                                                \n\n\n\ndata2.query(\"mtype=='block'\").plot.box(backend='plotly',x='mrate',color='method',y='mse',facet_col='inter_method',facet_row='nof_filters',height=1000)"
  },
  {
    "objectID": "posts/2_research/2023-06-28-DYGRENCODER_Simulation_boxplot_reshape (SEOYEON CHOI's conflicted copy 2023-09-27).html#baseline-3",
    "href": "posts/2_research/2023-06-28-DYGRENCODER_Simulation_boxplot_reshape (SEOYEON CHOI's conflicted copy 2023-09-27).html#baseline-3",
    "title": "DYGRENCODER_Simulation_reshape",
    "section": "Baseline",
    "text": "Baseline\n\ndata.query(\"mrate==0 and method!='GNAR'\").plot.box(backend='plotly',x='mrate',color='method',y='mse',height=600)"
  },
  {
    "objectID": "posts/2_research/2023-06-28-DYGRENCODER_Simulation_boxplot_reshape (SEOYEON CHOI's conflicted copy 2023-09-27).html#random-3",
    "href": "posts/2_research/2023-06-28-DYGRENCODER_Simulation_boxplot_reshape (SEOYEON CHOI's conflicted copy 2023-09-27).html#random-3",
    "title": "DYGRENCODER_Simulation_reshape",
    "section": "random",
    "text": "random\n\ndata.query(\"mtype=='rand' and mrate !=0 and method!='GNAR'\").plot.box(backend='plotly',x='mrate',color='method',y='mse',facet_col='lags',facet_row='nof_filters',height=800)"
  },
  {
    "objectID": "posts/2_research/2023-06-28-DYGRENCODER_Simulation_boxplot_reshape (SEOYEON CHOI's conflicted copy 2023-09-27).html#block-3",
    "href": "posts/2_research/2023-06-28-DYGRENCODER_Simulation_boxplot_reshape (SEOYEON CHOI's conflicted copy 2023-09-27).html#block-3",
    "title": "DYGRENCODER_Simulation_reshape",
    "section": "block",
    "text": "block\n\ndata.query(\"mtype=='block' and method!='GNAR'\").plot.box(backend='plotly',x='mrate',color='method',y='mse',facet_col='lags',facet_row='nof_filters',height=1200)"
  },
  {
    "objectID": "posts/2_research/2023-06-28-DYGRENCODER_Simulation_boxplot_reshape (SEOYEON CHOI's conflicted copy 2023-09-27).html#missing-values-on-the-same-nodes",
    "href": "posts/2_research/2023-06-28-DYGRENCODER_Simulation_boxplot_reshape (SEOYEON CHOI's conflicted copy 2023-09-27).html#missing-values-on-the-same-nodes",
    "title": "DYGRENCODER_Simulation_reshape",
    "section": "missing values on the same nodes",
    "text": "missing values on the same nodes\n\ndf1 = pd.read_csv('./simulation_results/2023-07-01_17-41-40.csv') # STGCN IT-STGCN block\ndf2 = pd.read_csv('./simulation_results/2023-07-01_21-00-26.csv') # STGCN IT-STGCN\ndf3 = pd.read_csv('./simulation_results/2023-07-02_00-17-30.csv') \n\n\ndata = pd.concat([df1,df2,df3],axis=0)\n\n\ndata.to_csv('./simulation_results/Real_simulation_reshape/DYGRENCODER_wikimath_GSO_st.csv',index=False)\n\n\ndata = pd.read_csv('./simulation_results/Real_simulation_reshape/DYGRENCODER_wikimath_GSO_st.csv')\n\n\ndata.query(\"method!='GNAR' and mtype =='block' \").plot.box(backend='plotly',x='mrate',color='method',y='mse',facet_col='inter_method',facet_row='lags',height=800)"
  },
  {
    "objectID": "posts/2_research/2023-06-28-DYGRENCODER_Simulation_boxplot_reshape (SEOYEON CHOI's conflicted copy 2023-09-27).html#baseline-4",
    "href": "posts/2_research/2023-06-28-DYGRENCODER_Simulation_boxplot_reshape (SEOYEON CHOI's conflicted copy 2023-09-27).html#baseline-4",
    "title": "DYGRENCODER_Simulation_reshape",
    "section": "Baseline",
    "text": "Baseline\n\ndata.query(\"method!='GNAR' and mrate ==0 \").plot.box(backend='plotly',x='mrate',color='method',y='mse',facet_col='lags',height=800)"
  },
  {
    "objectID": "posts/2_research/2023-06-28-DYGRENCODER_Simulation_boxplot_reshape (SEOYEON CHOI's conflicted copy 2023-09-27).html#random-4",
    "href": "posts/2_research/2023-06-28-DYGRENCODER_Simulation_boxplot_reshape (SEOYEON CHOI's conflicted copy 2023-09-27).html#random-4",
    "title": "DYGRENCODER_Simulation_reshape",
    "section": "random",
    "text": "random\n\ndata.query(\"method!='GNAR' and mtype =='rand' \").plot.box(backend='plotly',x='mrate',color='method',y='mse',facet_col='lags',facet_row='inter_method',height=800)"
  },
  {
    "objectID": "posts/2_research/2023-06-28-DYGRENCODER_Simulation_boxplot_reshape (SEOYEON CHOI's conflicted copy 2023-09-27).html#block-4",
    "href": "posts/2_research/2023-06-28-DYGRENCODER_Simulation_boxplot_reshape (SEOYEON CHOI's conflicted copy 2023-09-27).html#block-4",
    "title": "DYGRENCODER_Simulation_reshape",
    "section": "block",
    "text": "block\n\ndata.query(\"method!='GNAR' and mtype =='block' \").plot.box(backend='plotly',x='mrate',color='method',y='mse',facet_col='lags',facet_row='inter_method',height=800)"
  },
  {
    "objectID": "posts/2_research/2023-06-28-DYGRENCODER_Simulation_boxplot_reshape (SEOYEON CHOI's conflicted copy 2023-09-27).html#baseline-5",
    "href": "posts/2_research/2023-06-28-DYGRENCODER_Simulation_boxplot_reshape (SEOYEON CHOI's conflicted copy 2023-09-27).html#baseline-5",
    "title": "DYGRENCODER_Simulation_reshape",
    "section": "Baseline",
    "text": "Baseline\n\ndata.query(\"mrate==0 and method!='GNAR'\").plot.box(backend='plotly',x='mrate',color='method',y='mse',facet_col='lags',facet_row='nof_filters',height=600)"
  },
  {
    "objectID": "posts/2_research/2023-06-28-DYGRENCODER_Simulation_boxplot_reshape (SEOYEON CHOI's conflicted copy 2023-09-27).html#random-5",
    "href": "posts/2_research/2023-06-28-DYGRENCODER_Simulation_boxplot_reshape (SEOYEON CHOI's conflicted copy 2023-09-27).html#random-5",
    "title": "DYGRENCODER_Simulation_reshape",
    "section": "random",
    "text": "random\n\ndata.query(\"mtype=='rand' and mrate !=0 and method!='GNAR'\").plot.box(backend='plotly',x='mrate',color='method',y='mse',facet_col='lags',facet_row='inter_method',height=800)"
  },
  {
    "objectID": "posts/2_research/2023-06-28-DYGRENCODER_Simulation_boxplot_reshape (SEOYEON CHOI's conflicted copy 2023-09-27).html#block-5",
    "href": "posts/2_research/2023-06-28-DYGRENCODER_Simulation_boxplot_reshape (SEOYEON CHOI's conflicted copy 2023-09-27).html#block-5",
    "title": "DYGRENCODER_Simulation_reshape",
    "section": "block",
    "text": "block\n\ndata.query(\"mtype=='block' and method!='GNAR'\").plot.box(backend='plotly',x='mrate',color='method',y='mse',facet_col='lags',facet_row='nof_filters',height=1200)"
  },
  {
    "objectID": "posts/2_research/2023-03-20-data load, data save as pickle.html",
    "href": "posts/2_research/2023-03-20-data load, data save as pickle.html",
    "title": "data load, data save as pickle",
    "section": "",
    "text": "https://pytorch-geometric-temporal.readthedocs.io/en/latest/_modules/index.html\n1st\n2nd\n3rd\n4rd\n5th - TwitterTennisDatasetLoader(원핫 인코딩 함수도 별도로 있음) - get_dataset(self) -&gt; DynamicGraphTemporalSignal: - dataset = DynamicGraphTemporalSignal(self.edges, self.edge_weights, self.features, self.target"
  },
  {
    "objectID": "posts/2_research/2023-03-20-data load, data save as pickle.html#windmill-비교",
    "href": "posts/2_research/2023-03-20-data load, data save as pickle.html#windmill-비교",
    "title": "data load, data save as pickle",
    "section": "Windmill 비교",
    "text": "Windmill 비교\n차이점\n\n관측소 수 차이\n\n\n하지만 어떤 노드가 겹치는지 알 수 없음\n\n\nfig,ax = plt.subplots(3, 1, figsize=(30, 20))\nax[0].plot(np.array(loader4.targets).reshape(319,-1)[0][:],label='observed') # laarge\nax[1].plot(np.array(loader5.targets).reshape(26,-1)[0][:],label='observed') # medium\nax[2].plot(np.array(loader6.targets).reshape(11,-1)[0][:],label='observed') # small"
  },
  {
    "objectID": "posts/2_research/2024-02-01-GNAR-R.html",
    "href": "posts/2_research/2024-02-01-GNAR-R.html",
    "title": "GNAR-R",
    "section": "",
    "text": "library(GNAR)\n\n\nlibrary(jsonlite)"
  },
  {
    "objectID": "posts/2_research/2024-02-01-GNAR-R.html#random",
    "href": "posts/2_research/2024-02-01-GNAR-R.html#random",
    "title": "GNAR-R",
    "section": "random",
    "text": "random\n\nfixed_data &lt;- fiveVTS\nfixed_net_input &lt;- fiveNet\nfixed_lags &lt;- 2\n\nresults_df &lt;- data.frame(mrate = numeric(),\n                         mse = numeric())\n\nmissing_rates &lt;- c(0,0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8) \n\nfor (rate in missing_rates) {\n  result &lt;- cal_mse(fixed_data, fixed_net_input, fixed_lags, rate)\n  results_df &lt;- rbind(results_df, data.frame(mrate = rate, mse = result[2]))\n}\nresults_df\n\n\nA data.frame: 9 × 2\n\n\nmrate\ntotal\n\n\n&lt;dbl&gt;\n&lt;dbl&gt;\n\n\n\n\n0.0\n1.257729\n\n\n0.1\n1.252735\n\n\n0.2\n1.265524\n\n\n0.3\n1.250937\n\n\n0.4\n1.271402\n\n\n0.5\n1.278707\n\n\n0.6\n1.301504\n\n\n0.7\n1.301504\n\n\n0.8\n1.301504\n\n\n\n\n\n\nresults_df['dataset'] = 'FiveVTS'\nresults_df['method'] = 'GNAR'\nresults_df['mtype'] = 'rand'\nresults_df['lags'] = fixed_lags\nresults_df['nof_filters'] = NA\nresults_df['inter_method'] = NA\nresults_df['epoch'] = NA\nresults_df['calculation_time'] = NA\nresults_df['model'] = 'GNAR'\ncolnames(results_df)[2] &lt;- \"mse\"\nFiveVTS_results &lt;- results_df"
  },
  {
    "objectID": "posts/2_research/2024-02-01-GNAR-R.html#block",
    "href": "posts/2_research/2024-02-01-GNAR-R.html#block",
    "title": "GNAR-R",
    "section": "block",
    "text": "block\n\nFiveVTS_mindex &lt;- fromJSON(\"FiveVTS_mindex.json\")\n\n\nfixed_data &lt;- fiveVTS\nfixed_net_input &lt;- fiveNet\nfixed_lags &lt;- 2\nfixed_mindex &lt;- FiveVTS_mindex\n\nresults_df &lt;- data.frame(mrate = numeric(),\n                         mse = numeric())\n\ntotal_length &lt;- 0\n\nfor (i in seq_along(fixed_mindex)) {\n  total_length &lt;- total_length + length(fixed_mindex[[i]])\n}\n\nrate = total_length/(length(fixed_data[,])*0.8)\n\nresults_df &lt;- data.frame(mrate = rate,\n                         mse = block_cal_mse(fixed_data, fixed_net_input, fixed_lags, fixed_mindex)[2])\nresults_df\n\n\nA data.frame: 1 × 2\n\n\nmrate\ntotal\n\n\n&lt;dbl&gt;\n&lt;dbl&gt;\n\n\n\n\n0.125\n1.272369\n\n\n\n\n\n\nresults_df['dataset'] = 'FiveVTS'\nresults_df['method'] = 'GNAR'\nresults_df['mtype'] = 'block'\nresults_df['lags'] = fixed_lags\nresults_df['nof_filters'] = NA\nresults_df['inter_method'] = NA\nresults_df['epoch'] = NA\nresults_df['calculation_time'] = NA\nresults_df['model'] = 'GNAR'\ncolnames(results_df)[2] &lt;- \"mse\"\nFiveVTS_block_results &lt;- results_df"
  },
  {
    "objectID": "posts/2_research/2024-02-01-GNAR-R.html#random-1",
    "href": "posts/2_research/2024-02-01-GNAR-R.html#random-1",
    "title": "GNAR-R",
    "section": "random",
    "text": "random\n\nChickenpox&lt;-read.csv('./data/Chickenpox.csv',header = TRUE)\n\n\nChickenpox_w&lt;-read.csv('./data/Chickenpox_w.csv',header = FALSE)[2:21,2:21]\n\n\nfixed_data &lt;- as.matrix(Chickenpox[,2:ncol(Chickenpox)])\nfixed_net_input &lt;- GNAR::matrixtoGNAR(as.matrix(Chickenpox_w))\nfixed_lags &lt;- 4\n\nresults_df &lt;- data.frame(mrate = numeric(),\n                         mse = numeric())\n\nmissing_rates &lt;- c(0,0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8)\n\nfor (rate in missing_rates) {\n  result &lt;- cal_mse(fixed_data, fixed_net_input, fixed_lags, rate)\n  results_df &lt;- rbind(results_df, data.frame(mrate = rate, mse = result[2]))\n}\nresults_df\n\nWARNING: diagonal entries present in original matrix, these will be removed\n\n\n\nA data.frame: 9 × 2\n\n\nmrate\ntotal\n\n\n&lt;dbl&gt;\n&lt;dbl&gt;\n\n\n\n\n0.0\n1.016342\n\n\n0.1\n1.016460\n\n\n0.2\n1.015845\n\n\n0.3\n1.018481\n\n\n0.4\n1.020843\n\n\n0.5\n1.019158\n\n\n0.6\n1.021600\n\n\n0.7\n1.021411\n\n\n0.8\n1.020084\n\n\n\n\n\n\nresults_df['dataset'] = 'Chickenpox'\nresults_df['method'] = 'GNAR'\nresults_df['mtype'] = 'rand'\nresults_df['lags'] = fixed_lags\nresults_df['nof_filters'] = NA\nresults_df['inter_method'] = NA\nresults_df['epoch'] = NA\nresults_df['calculation_time'] = NA\nresults_df['model'] = 'GNAR'\ncolnames(results_df)[2] &lt;- \"mse\"\nChickenpox_results &lt;- results_df"
  },
  {
    "objectID": "posts/2_research/2024-02-01-GNAR-R.html#block-1",
    "href": "posts/2_research/2024-02-01-GNAR-R.html#block-1",
    "title": "GNAR-R",
    "section": "block",
    "text": "block\n\nChickenpox_mindex &lt;- fromJSON(\"Chickenpox_mindex.json\")\n\n\nfixed_data &lt;- as.matrix(Chickenpox[,2:ncol(Chickenpox)])\nfixed_net_input &lt;- GNAR::matrixtoGNAR(as.matrix(Chickenpox_w))\nfixed_lags &lt;- 4\nfixed_mindex &lt;- Chickenpox_mindex\n\ntotal_length &lt;- 0\n\nfor (i in seq_along(fixed_mindex)) {\n  total_length &lt;- total_length + length(fixed_mindex[[i]])\n}\n\nrate = total_length/(length(fixed_data[,])*0.8)\n\nresults_df &lt;- data.frame(mrate = rate,\n                         mse = block_cal_mse(fixed_data, fixed_net_input, fixed_lags, fixed_mindex)[2])\nresults_df\n\nWARNING: diagonal entries present in original matrix, these will be removed\n\n\n\nA data.frame: 1 × 2\n\n\nmrate\ntotal\n\n\n&lt;dbl&gt;\n&lt;dbl&gt;\n\n\n\n\n0.2884615\n1.017119\n\n\n\n\n\n\nresults_df['dataset'] = 'Chickenpox'\nresults_df['method'] = 'GNAR'\nresults_df['mtype'] = 'block'\nresults_df['lags'] = fixed_lags\nresults_df['nof_filters'] = NA\nresults_df['inter_method'] = NA\nresults_df['epoch'] = NA\nresults_df['calculation_time'] = NA\nresults_df['model'] = 'GNAR'\ncolnames(results_df)[2] &lt;- \"mse\"\nChickenpox_block_results &lt;- results_df"
  },
  {
    "objectID": "posts/2_research/2024-02-01-GNAR-R.html#random-2",
    "href": "posts/2_research/2024-02-01-GNAR-R.html#random-2",
    "title": "GNAR-R",
    "section": "random",
    "text": "random\n\nPedalMe&lt;-read.csv('./data/PedalMe.csv',header = TRUE)\n\n\nPedalMe_w&lt;-read.csv('./data/PedalMe_w.csv',header = FALSE)[2:16,2:16]\n\n\nfixed_data &lt;- as.matrix(PedalMe[,2:ncol(PedalMe)])\nfixed_net_input &lt;- GNAR::matrixtoGNAR(as.matrix(PedalMe_w))\nfixed_lags &lt;- 4\n\nresults_df &lt;- data.frame(mrate = numeric(),\n                         mse = numeric())\n\nmissing_rates &lt;- c(0,0.1, 0.2, 0.3, 0.4, 0.5, 0.6,0.7, 0.8) \n\nfor (rate in missing_rates) {\n  result &lt;- cal_mse(fixed_data, fixed_net_input, fixed_lags, rate)\n  results_df &lt;- rbind(results_df, data.frame(mrate = rate, mse = result[2]))\n}\nresults_df\n\nWARNING: diagonal entries present in original matrix, these will be removed\n\n\n\nA data.frame: 9 × 2\n\n\nmrate\ntotal\n\n\n&lt;dbl&gt;\n&lt;dbl&gt;\n\n\n\n\n0.0\n0.5016028\n\n\n0.1\n0.4971018\n\n\n0.2\n0.5027820\n\n\n0.3\n0.5148579\n\n\n0.4\n0.5226205\n\n\n0.5\n0.5917800\n\n\n0.6\n0.4917371\n\n\n0.7\n0.4917371\n\n\n0.8\n0.4917371\n\n\n\n\n\n\nresults_df['dataset'] = 'Pedalme'\nresults_df['method'] = 'GNAR'\nresults_df['mtype'] = 'rand'\nresults_df['lags'] = fixed_lags\nresults_df['nof_filters'] = NA\nresults_df['inter_method'] = NA\nresults_df['epoch'] = NA\nresults_df['calculation_time'] = NA\nresults_df['model'] = 'GNAR'\ncolnames(results_df)[2] &lt;- \"mse\"\nPedalme_results &lt;- results_df"
  },
  {
    "objectID": "posts/2_research/2024-02-01-GNAR-R.html#block-2",
    "href": "posts/2_research/2024-02-01-GNAR-R.html#block-2",
    "title": "GNAR-R",
    "section": "block",
    "text": "block\n\nPedalme_mindex &lt;- fromJSON(\"Pedalme_mindex.json\")\n\n\nfixed_data &lt;- as.matrix(PedalMe[,2:ncol(PedalMe)])\nfixed_net_input &lt;- GNAR::matrixtoGNAR(as.matrix(PedalMe_w))\nfixed_lags &lt;- 4\nfixed_mindex &lt;- Pedalme_mindex\n\ntotal_length &lt;- 0\n\nfor (i in seq_along(fixed_mindex)) {\n  total_length &lt;- total_length + length(fixed_mindex[[i]])\n}\n\nrate = total_length/(length(fixed_data[,])*0.8)\n\nresults_df &lt;- data.frame(mrate = rate,\n                         mse = block_cal_mse(fixed_data, fixed_net_input, fixed_lags, fixed_mindex)[2])\nresults_df\n\nWARNING: diagonal entries present in original matrix, these will be removed\n\n\n\nA data.frame: 1 × 2\n\n\nmrate\ntotal\n\n\n&lt;dbl&gt;\n&lt;dbl&gt;\n\n\n\n\n0.2941176\n0.4916068\n\n\n\n\n\n\nresults_df['dataset'] = 'Pedalme'\nresults_df['method'] = 'GNAR'\nresults_df['mtype'] = 'block'\nresults_df['lags'] = fixed_lags\nresults_df['nof_filters'] = NA\nresults_df['inter_method'] = NA\nresults_df['epoch'] = NA\nresults_df['calculation_time'] = NA\nresults_df['model'] = 'GNAR'\ncolnames(results_df)[2] &lt;- \"mse\"\nPedalme_block_results &lt;- results_df"
  },
  {
    "objectID": "posts/2_research/2024-02-01-GNAR-R.html#random-3",
    "href": "posts/2_research/2024-02-01-GNAR-R.html#random-3",
    "title": "GNAR-R",
    "section": "random",
    "text": "random\n\nWikiMath &lt;- read.csv('./data/WikiMath.csv',header = TRUE)\n\n\nWikiMath_w&lt;-read.csv('./data/WikiMath_w.csv',header = FALSE)[2:1069,2:1069]\n\n\nfixed_data &lt;- as.matrix(WikiMath[,2:ncol(WikiMath)])\nfixed_net_input &lt;- GNAR::matrixtoGNAR(as.matrix(WikiMath_w))\nfixed_lags &lt;- 8\n\nresults_df &lt;- data.frame(mrate = numeric(),\n                         mse = numeric())\n\nmissing_rates &lt;- c(0, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7,0.8) \n\nfor (rate in missing_rates) {\n  result &lt;- cal_mse(fixed_data, fixed_net_input, fixed_lags, rate)\n  results_df &lt;- rbind(results_df, data.frame(mrate = rate, mse = result[2]))\n}\nresults_df\n\n\nA data.frame: 9 × 2\n\n\nmrate\ntotal\n\n\n&lt;dbl&gt;\n&lt;dbl&gt;\n\n\n\n\n0.0\n0.9022260\n\n\n0.1\n0.9015769\n\n\n0.2\n0.9015677\n\n\n0.3\n0.9112567\n\n\n0.4\n0.9576448\n\n\n0.5\n0.9042646\n\n\n0.6\n0.9465203\n\n\n0.7\n0.9607108\n\n\n0.8\n2.0823862\n\n\n\n\n\n\ncal_mse(fixed_data, fixed_net_input, fixed_lags, 0.8)\n\n\n    $each\n        \n0.7166091987134660.336095876138031.530378350956560.5718807466420370.6790402795040070.867709447789410.942112270186731.230953527508050.2555627669914661.410217889331641.119021551942910.1207776889929210.3516869172759311.035472478257730.3876370999810851.672862808103980.6456884800114861.02399008686320.4705151110971221.398306648190960.5274748071327370.87359632820323.916394054493720.8097487100597261.523529698098720.9548099692493180.594271558025431.073841033579511.160500995940770.8123788428111240.08436704694920110.5828864111658270.6109411217030981.775016109023240.8321884811207730.2855478814694510.8567938020228230.7861799715705121.082856145398231.276737586530781.121100594237440.6480716639937750.6739798277829250.7127155026277981.872622358683750.1242003021212320.5559500493420050.4351666491239540.7696179182734430.8819596356228350.409655687369420.2588211421097950.6543693463222351.040902792579221.10618126487650.5808181223432550.9016489862296633.866634635869410.5303371627412440.9721100047688480.3167860965081650.5578535022716720.6093460173184180.9571857342793320.8589519868649472.054415246698080.6081488897869410.4305811703955331.018489698904880.980912312327290.1480076712863011.01409833187781.055904005387790.5267440304809171.322747155122240.8000763995910260.7804193294927460.8379199115306720.5102691422560270.3804256622200731.33540741811941.003536094883910.6347462823378370.755383731538571.082677476221750.7213531297709732.298333412481740.8406878628253810.531979555581880.8595973701629250.1409115589110280.7759180156249290.9341275951274674.816594414452430.8492706037085810.895184626473680.5137477638858612.932686401216240.6896005184597221.197166332391580.7647062640912880.8689739871465790.0478038782773930.8820960007305320.3233491644911150.5717884186954140.6588094150815020.7521182042841130.8753702765881891.473306278246860.6027642325702740.2334493817063280.7805110745091380.8110335604475620.6745028138466620.70352705795521.655612774288530.4456940024322670.5762805713544294.860899609911132.919536257620610.4824726851957180.759377686366450.5326848116654471.438773904312982.169687978314560.9845458712821850.913006522086271.065068286873960.8558207065575024.714095775797481.001577503281450.1016309601910991.248096711795141.404041402374281.430014252035691.263458380516791.389009448209281.078593692349211.176800028499531.379563553230530.9194157658222812.166353020068850.9246478602875930.951604129948110.9618906146833681.025263908988120.2088568714498111.326374127319650.5781109448374910.3908153874192060.9908280803664050.650090110387830.9456136814211530.2262577809832310.4814195853933210.8504131196546580.727407643558230.7319428463939611.924669702541380.3824867462047280.763152342070820.07185971664395530.6898403022206080.7252619062531082.198284575806810.7130322224757871.060660098358450.9501221835545270.8325138102908471.069020816707561.075620444390961.105845011145951.224214447673781.138428873757021.139732995496230.2303399523143961.111150134702410.461958681372911.108300540694931.298173131222511.481890619779680.807518565586241.216264458416250.7411531339984660.2031259118464640.1764419561346481.571006449048870.9664370577639411.143195564972670.426831594602630.06475265724003680.7992704618682290.5991042070417840.4582844630569470.7601881559333321.255209960516450.5834489648294830.8130623843052370.754284899404217⋯0.6979558593181061.536696914585042.154270269212491.052154785758020.7559963335897390.7523913440453490.9529992441348160.650222906310621.531306933825651.127486546286391.201013559631270.03099603063461160.9445258841102961.107617664328270.751590123436070.8961901353274010.9944018837833271.042727942685771.136591428801850.06602175558388820.6995057878285070.9928291568781621.006442595334140.7515055932773021.552251720149160.6934805921731250.9408302795483290.9882403759399090.8773396540414161.82352511542981.086500446171592.160991317701011.348555309564811.158912219232223.336676893116051.083647905782250.5870252369237111.151264925450061.10062860523051.021704719896760.9575846952938541.129174630814461.434514298188662.297683285176381.264956542347581.127004281276221.648701327717760.989704209284810.5719267398803711.626112288914973.233415132503610.6685067673491591.071654809822271.119664259972020.8899838393237121.241657510283320.9669987410924921.162784065099051.140978360960581.165221443461491.08603001903810.2472581536306180.5575650015534010.6235332369417541.123424181701171.56625304695593.796129369038750.8909229818527611.217098263594012.973770890527350.1210588885067320.8187124137691.557609692962280.9835118683708581.428342534657130.9217333157772111.434752566210523.055214555310981.623119277395870.9502835621362220.8684423778870970.9760654232752051.08259755535361.185080812630651.45982593817720.9799551865204810.5387015528634070.5603028463173760.8522561964255930.03005158504784660.4880450125378060.7512307954073380.647736694346841.002602602159920.7716230876938460.7487272778453212.349712838795180.9956653436192860.5885509816729930.8044538994991231.65050095705322.286190055425571.776787909565931.814240632902791.401947980429941.248607567892291.171718730159531.617648965277831.967871829489581.493407695899980.3399727133946441.082634895649751.044564090527060.6775657519402860.6636541382664681.177617220295570.6452359325406681.97065786763180.6788634505624510.2340079358457620.5264795533525020.06189250720645460.6571648076544590.9707646896168771.822765529910081.18178410042511.234609186366330.2080771089559540.7693326254477581.195196999042180.6300580528632720.2133025943989920.7334022644853521.232734906390810.8295766647245671.387213436791620.998303047014132.321220574062620.9229490618204020.8797680281025880.8020604209801540.7626404733547711.002609026735120.8806416587934710.7304504667042340.9212905210069031.020460081375910.8692496566299140.3974693101111130.992562606366371.346231523721771.30240122242661.112183179706220.5129840371049191.145240416702861.93580480545511.046381498354170.7020307119963530.09927500968478771.791968560231240.3383032653688291.511590965587511.197101362732711.778191461415372.01895155772581.708349122642880.7550206177213111.721130355920571.459494523515230.9694835315812150.1578001591860261.603267464875880.8926755105551040.915893539770681.021583472823651.542134297444892.101798027572744.143015820464514.544087794336771.086288676844160.530566020369531.487441645517791.011366088363872.052279957378290.9012463123160620.9105330443232782.622453796746371.686136321050511.829871700713031.457876118888561.317853736800451.519405593590390.7666349058686250.9809039513667061.609919612772370.148803694151021.220301227631430.7611996755908811.295442857641651.32105800531045\n\n    $total\n        1.07726145079173\n\n\n\n\nresults_df['dataset'] = 'Wikimath'\nresults_df['method'] = 'GNAR'\nresults_df['mtype'] = 'rand'\nresults_df['lags'] = fixed_lags\nresults_df['nof_filters'] = NA\nresults_df['inter_method'] = NA\nresults_df['epoch'] = NA\nresults_df['calculation_time'] = NA\nresults_df['model'] = 'GNAR'\ncolnames(results_df)[2] &lt;- \"mse\"\nWikimath_results &lt;- results_df"
  },
  {
    "objectID": "posts/2_research/2024-02-01-GNAR-R.html#block-3",
    "href": "posts/2_research/2024-02-01-GNAR-R.html#block-3",
    "title": "GNAR-R",
    "section": "block",
    "text": "block\n\nWikimath_mindex &lt;- fromJSON(\"./GNAR_weight_matrix/Wikimath_mindex.json\")\n\n\nfixed_data &lt;- as.matrix(WikiMath[,2:ncol(WikiMath)])\nfixed_net_input &lt;- GNAR::matrixtoGNAR(as.matrix(WikiMath_w))\nfixed_lags &lt;- 8\nfixed_mindex &lt;- Wikimath_mindex\n\ntotal_length &lt;- 0\n\nfor (i in seq_along(fixed_mindex)) {\n  total_length &lt;- total_length + length(fixed_mindex[[i]])\n}\n\nrate = total_length/(length(fixed_data[,])*0.8)\n\nresults_df &lt;- data.frame(mrate = rate,\n                         mse = block_cal_mse(fixed_data, fixed_net_input, fixed_lags, fixed_mindex)[2])\nresults_df\n\n\nA data.frame: 1 × 2\n\n\nmrate\ntotal\n\n\n&lt;dbl&gt;\n&lt;dbl&gt;\n\n\n\n\n0.6002771\n0.9083379\n\n\n\n\n\n\nresults_df['dataset'] = 'Wikimath'\nresults_df['method'] = 'GNAR'\nresults_df['mtype'] = 'block'\nresults_df['lags'] = fixed_lags\nresults_df['nof_filters'] = NA\nresults_df['inter_method'] = NA\nresults_df['epoch'] = NA\nresults_df['calculation_time'] = NA\nresults_df['model'] = 'GNAR'\ncolnames(results_df)[2] &lt;- \"mse\"\nWikimath_block_results &lt;- results_df"
  },
  {
    "objectID": "posts/2_research/2024-02-01-GNAR-R.html#random-4",
    "href": "posts/2_research/2024-02-01-GNAR-R.html#random-4",
    "title": "GNAR-R",
    "section": "random",
    "text": "random\n\nWindmillsmall&lt;-read.csv('./data/Windmillsmall.csv',header = TRUE)\n\n\nWindmillsmall_w&lt;-read.csv('./data/Windmillsmall_w.csv',header = FALSE)[2:12,2:12]\n\n계속 오류남;;\n\n# fixed_data &lt;- as.matrix(Windmillsmall[,2:ncol(Windmillsmall)])\n# fixed_net_input &lt;- GNAR::matrixtoGNAR(as.matrix(Windmillsmall_w))\n# fixed_lags &lt;- 8\n\n# results_df &lt;- data.frame(mrate = numeric(),\n#                          mse = numeric())\n\n# missing_rates &lt;- c(0,0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7,0.8) \n\n# for (rate in missing_rates) {\n#   result &lt;- cal_mse(fixed_data, fixed_net_input, fixed_lags, rate)\n#   results_df &lt;- rbind(results_df, data.frame(mrate = rate, mse = result))\n# }\n# results_df\n\n\nresults_df &lt;- data.frame(\n  mrate = c(0.8, 0.7, 0.6, 0.5, 0.4, 0.3, 0.2, 0.1, 0.0),\n  mse = c(0.980465007853614,0.980465007853614,0.980742615896619,0.980465007853614, 0.980529157673243, 0.980548927794843,0.980486712335988,0.980471934178215, 0.980486581012802)\n)\n\n\nresults_df['dataset'] = 'Windmillsmall'\nresults_df['method'] = 'GNAR'\nresults_df['mtype'] = 'rand'\nresults_df['lags'] = fixed_lags\nresults_df['nof_filters'] = NA\nresults_df['inter_method'] = NA\nresults_df['epoch'] = NA\nresults_df['calculation_time'] = NA\nresults_df['model'] = 'GNAR'\ncolnames(results_df)[2] &lt;- \"mse\"\nWindmillsmall_results &lt;- results_df\n\n\nfixed_data &lt;- as.matrix(Windmillsmall[,2:ncol(Windmillsmall)])\nfixed_net_input &lt;- GNAR::matrixtoGNAR(as.matrix(Windmillsmall_w))\nfixed_lags &lt;- 8\n\nWARNING: diagonal entries present in original matrix, these will be removed\n\n\n\ncal_mse(fixed_data, fixed_net_input, fixed_lags, 0.8)[2]\n\n$total = 0.980465007853614\n\n\n\ncal_mse(fixed_data, fixed_net_input, fixed_lags, 0.7)[2]\n\n$total = 0.980465007853614\n\n\n\ncal_mse(fixed_data, fixed_net_input, fixed_lags, 0.6)[2]\n\n$total = 0.980742615896619\n\n\n\ncal_mse(fixed_data, fixed_net_input, fixed_lags, 0.5)[2]\n\n$total = 0.980465007853614\n\n\n\ncal_mse(fixed_data, fixed_net_input, fixed_lags, 0.4)[2]\n\n$total = 0.980529157673243\n\n\n\ncal_mse(fixed_data, fixed_net_input, fixed_lags, 0.3)[2]\n\n$total = 0.980548927794843\n\n\n\ncal_mse(fixed_data, fixed_net_input, fixed_lags, 0.2)[2]\n\n$total = 0.980486712335988\n\n\n\ncal_mse(fixed_data, fixed_net_input, fixed_lags, 0.1)[2]\n\n$total = 0.980471934178215\n\n\n\ncal_mse(fixed_data, fixed_net_input, fixed_lags, 0)[2]\n\n$total = 0.980486581012802"
  },
  {
    "objectID": "posts/2_research/2024-02-01-GNAR-R.html#block-4",
    "href": "posts/2_research/2024-02-01-GNAR-R.html#block-4",
    "title": "GNAR-R",
    "section": "block",
    "text": "block\n\nWindmillsmall_mindex &lt;- fromJSON(\"Windmillsmall_mindex.json\")\n\n\nfixed_data &lt;- as.matrix(Windmillsmall[,2:ncol(Windmillsmall)])\nfixed_net_input &lt;- GNAR::matrixtoGNAR(as.matrix(Windmillsmall_w))\nfixed_lags &lt;- 8\nfixed_mindex &lt;- Windmillsmall_mindex\n\ntotal_length &lt;- 0\n\nfor (i in seq_along(fixed_mindex)) {\n  total_length &lt;- total_length + length(fixed_mindex[[i]])\n}\n\nrate = total_length/(length(fixed_data[,])*0.8)\n\nresults_df &lt;- data.frame(mrate = rate,\n                         mse = block_cal_mse(fixed_data, fixed_net_input, fixed_lags, fixed_mindex)[2])\nresults_df\n\nWARNING: diagonal entries present in original matrix, these will be removed\n\n\n\nA data.frame: 1 × 2\n\n\nmrate\ntotal\n\n\n&lt;dbl&gt;\n&lt;dbl&gt;\n\n\n\n\n0.2861885\n0.9804698\n\n\n\n\n\n\nresults_df['dataset'] = 'Windmillsmall'\nresults_df['method'] = 'GNAR'\nresults_df['mtype'] = 'block'\nresults_df['lags'] = fixed_lags\nresults_df['nof_filters'] = NA\nresults_df['inter_method'] = NA\nresults_df['epoch'] = NA\nresults_df['calculation_time'] = NA\nresults_df['model'] = 'GNAR'\ncolnames(results_df)[2] &lt;- \"mse\"\nWindmillsmall_block_results &lt;- results_df"
  },
  {
    "objectID": "posts/2_research/2024-02-01-GNAR-R.html#random-5",
    "href": "posts/2_research/2024-02-01-GNAR-R.html#random-5",
    "title": "GNAR-R",
    "section": "random",
    "text": "random\n\nMontevideoBus&lt;-read.csv('./data/MontevideoBus.csv',header = TRUE)\n\n\nMontevideoBus_w&lt;-read.csv('./data/MontevideoBus_w.csv',header = FALSE)[2:676,2:676]\n\n\nfixed_data &lt;- as.matrix(MontevideoBus[,2:ncol(MontevideoBus)])\nfixed_net_input &lt;- GNAR::matrixtoGNAR(as.matrix(MontevideoBus_w))\nfixed_lags &lt;- 8\n\nresults_df &lt;- data.frame(mrate = numeric(),\n                         mse = numeric())\n\nmissing_rates &lt;- c(0,0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7,0.8) \n\nfor (rate in missing_rates) {\n  result &lt;- cal_mse(fixed_data, fixed_net_input, fixed_lags, rate)\n  results_df &lt;- rbind(results_df, data.frame(mrate = rate, mse = result[2]))\n}\nresults_df\n\n\nA data.frame: 9 × 2\n\n\nmrate\ntotal\n\n\n&lt;dbl&gt;\n&lt;dbl&gt;\n\n\n\n\n0.0\n1.002868\n\n\n0.1\n1.002744\n\n\n0.2\n1.002750\n\n\n0.3\n1.002771\n\n\n0.4\n1.001533\n\n\n0.5\n1.002278\n\n\n0.6\n1.002135\n\n\n0.7\n1.003126\n\n\n0.8\n1.003196\n\n\n\n\n\n\nresults_df['dataset'] = 'MontevideoBus'\nresults_df['method'] = 'GNAR'\nresults_df['mtype'] = 'rand'\nresults_df['lags'] = fixed_lags\nresults_df['nof_filters'] = NA\nresults_df['inter_method'] = NA\nresults_df['epoch'] = NA\nresults_df['calculation_time'] = NA\nresults_df['model'] = 'GNAR'\ncolnames(results_df)[2] &lt;- \"mse\"\nMontevideoBus_results &lt;- results_df"
  },
  {
    "objectID": "posts/2_research/2024-02-01-GNAR-R.html#block-5",
    "href": "posts/2_research/2024-02-01-GNAR-R.html#block-5",
    "title": "GNAR-R",
    "section": "block",
    "text": "block\n\nMontevideoBus_mindex &lt;- fromJSON(\"MontevideoBus_mindex.json\")\n\n\nfixed_data &lt;- as.matrix(MontevideoBus[,2:ncol(MontevideoBus)])\nfixed_net_input &lt;- GNAR::matrixtoGNAR(as.matrix(MontevideoBus_w))\nfixed_lags &lt;- 8\nfixed_mindex &lt;- MontevideoBus_mindex\n\ntotal_length &lt;- 0\n\nfor (i in seq_along(fixed_mindex)) {\n  total_length &lt;- total_length + length(fixed_mindex[[i]])\n}\n\nrate = total_length/(length(fixed_data[,])*0.8)\n\nresults_df &lt;- data.frame(mrate = rate,\n                         mse = block_cal_mse(fixed_data, fixed_net_input, fixed_lags, fixed_mindex)[2])\nresults_df\n\n\nA data.frame: 1 × 2\n\n\nmrate\ntotal\n\n\n&lt;dbl&gt;\n&lt;dbl&gt;\n\n\n\n\n0.1495439\n1.002894\n\n\n\n\n\n\nresults_df['dataset'] = 'MontevideoBus'\nresults_df['method'] = 'GNAR'\nresults_df['mtype'] = 'block'\nresults_df['lags'] = fixed_lags\nresults_df['nof_filters'] = NA\nresults_df['inter_method'] = NA\nresults_df['epoch'] = NA\nresults_df['calculation_time'] = NA\nresults_df['model'] = 'GNAR'\ncolnames(results_df)[2] &lt;- \"mse\"\nMontevideoBus_block_results &lt;- results_df"
  },
  {
    "objectID": "posts/2_research/2023-05-30-GConvLSTM_Simulation_boxplot_reshape.html",
    "href": "posts/2_research/2023-05-30-GConvLSTM_Simulation_boxplot_reshape.html",
    "title": "GConvLSTM_Simulation_reshape",
    "section": "",
    "text": "Simulation Study"
  },
  {
    "objectID": "posts/2_research/2023-05-30-GConvLSTM_Simulation_boxplot_reshape.html#baseline",
    "href": "posts/2_research/2023-05-30-GConvLSTM_Simulation_boxplot_reshape.html#baseline",
    "title": "GConvLSTM_Simulation_reshape",
    "section": "Baseline",
    "text": "Baseline\n\ndata.query(\"method!='GNAR' and mrate==0\").plot.box(backend='plotly',x='mrate',color='method',y='mse',facet_col='lags',facet_row='nof_filters',height=600)"
  },
  {
    "objectID": "posts/2_research/2023-05-30-GConvLSTM_Simulation_boxplot_reshape.html#random",
    "href": "posts/2_research/2023-05-30-GConvLSTM_Simulation_boxplot_reshape.html#random",
    "title": "GConvLSTM_Simulation_reshape",
    "section": "Random",
    "text": "Random\n\ndata.query(\"method!='GNAR' and mtype =='rand'\").plot.box(backend='plotly',x='mrate',color='method',y='mse',facet_col='inter_method',facet_row='nof_filters',height=600)"
  },
  {
    "objectID": "posts/2_research/2023-05-30-GConvLSTM_Simulation_boxplot_reshape.html#block",
    "href": "posts/2_research/2023-05-30-GConvLSTM_Simulation_boxplot_reshape.html#block",
    "title": "GConvLSTM_Simulation_reshape",
    "section": "Block",
    "text": "Block\n\ndata.query(\"method!='GNAR' and mtype =='block' \").plot.box(backend='plotly',x='mrate',color='method',y='mse',facet_col='inter_method',facet_row='nof_filters',height=600)"
  },
  {
    "objectID": "posts/2_research/2023-05-30-GConvLSTM_Simulation_boxplot_reshape.html#baseline-1",
    "href": "posts/2_research/2023-05-30-GConvLSTM_Simulation_boxplot_reshape.html#baseline-1",
    "title": "GConvLSTM_Simulation_reshape",
    "section": "Baseline",
    "text": "Baseline\n\ndata.query(\"method!='GNAR' and mrate ==0 \").plot.box(backend='plotly',x='mrate',color='method',y='mse',facet_col='nof_filters',height=600)"
  },
  {
    "objectID": "posts/2_research/2023-05-30-GConvLSTM_Simulation_boxplot_reshape.html#random-1",
    "href": "posts/2_research/2023-05-30-GConvLSTM_Simulation_boxplot_reshape.html#random-1",
    "title": "GConvLSTM_Simulation_reshape",
    "section": "Random",
    "text": "Random\n\ndata.query(\"method!='GNAR' and mtype =='rand'\").plot.box(backend='plotly',x='mrate',color='method',y='mse',facet_col='nof_filters',facet_row='inter_method',height=800)"
  },
  {
    "objectID": "posts/2_research/2023-05-30-GConvLSTM_Simulation_boxplot_reshape.html#block-1",
    "href": "posts/2_research/2023-05-30-GConvLSTM_Simulation_boxplot_reshape.html#block-1",
    "title": "GConvLSTM_Simulation_reshape",
    "section": "Block",
    "text": "Block\n\ndata.query(\"method!='GNAR' and mtype =='block'\").plot.box(backend='plotly',x='mrate',color='method',y='mse',facet_col='inter_method',facet_row='lags',height=800)"
  },
  {
    "objectID": "posts/2_research/2023-05-30-GConvLSTM_Simulation_boxplot_reshape.html#baseline-2",
    "href": "posts/2_research/2023-05-30-GConvLSTM_Simulation_boxplot_reshape.html#baseline-2",
    "title": "GConvLSTM_Simulation_reshape",
    "section": "Baseline",
    "text": "Baseline\n\ndata.query(\"mrate ==0 and lags!=2\").plot.box(backend='plotly',x='epoch',color='method',y='mse',facet_col='nof_filters',facet_row='lags',height=400)"
  },
  {
    "objectID": "posts/2_research/2023-05-30-GConvLSTM_Simulation_boxplot_reshape.html#random-2",
    "href": "posts/2_research/2023-05-30-GConvLSTM_Simulation_boxplot_reshape.html#random-2",
    "title": "GConvLSTM_Simulation_reshape",
    "section": "random",
    "text": "random\n\ndata.query(\"method!='GNAR' and mtype =='rand' and lags!=2\").plot.box(backend='plotly',x='mrate',color='method',y='mse',facet_col='nof_filters',facet_row='inter_method',height=800)"
  },
  {
    "objectID": "posts/2_research/2023-05-30-GConvLSTM_Simulation_boxplot_reshape.html#block-2",
    "href": "posts/2_research/2023-05-30-GConvLSTM_Simulation_boxplot_reshape.html#block-2",
    "title": "GConvLSTM_Simulation_reshape",
    "section": "block",
    "text": "block\n\ndata.query(\"method!='GNAR' and mtype =='block' and lags!=2 \").plot.box(backend='plotly',x='mrate',color='method',y='mse',facet_col='nof_filters',facet_row='inter_method',height=800)"
  },
  {
    "objectID": "posts/2_research/2023-05-30-GConvLSTM_Simulation_boxplot_reshape.html#weight-matrix-time-node-고려한-결과",
    "href": "posts/2_research/2023-05-30-GConvLSTM_Simulation_boxplot_reshape.html#weight-matrix-time-node-고려한-결과",
    "title": "GConvLSTM_Simulation_reshape",
    "section": "weight matrix time, node 고려한 결과",
    "text": "weight matrix time, node 고려한 결과\n\ndf1 = pd.read_csv('./simulation_results/2023-06-10_15-33-00.csv')\ndf2 = pd.read_csv('./simulation_results/2023-06-10_16-06-20.csv')\n\n\ndata2 = pd.concat([df1,df2],axis=0)\n\n\ndata2.to_csv('./simulation_results/Real_simulation_reshape/GConvLSTM_pedalme_Simulation_itstgcnsnd.csv',index=False)\n\n\ndata2 = pd.read_csv('./simulation_results/Real_simulation_reshape/GConvLSTM_pedalme_Simulation_itstgcnsnd.csv')\n\n\ndata2.query(\"mtype=='rand'\").plot.box(backend='plotly',x='mrate',color='method',y='mse',facet_col='inter_method',facet_row='nof_filters',height=1000)\n\n                                                \n\n\n\ndata2.query(\"mtype=='block'\").plot.box(backend='plotly',x='mrate',color='method',y='mse',facet_col='inter_method',facet_row='nof_filters',height=1000)"
  },
  {
    "objectID": "posts/2_research/2023-05-30-GConvLSTM_Simulation_boxplot_reshape.html#baseline-3",
    "href": "posts/2_research/2023-05-30-GConvLSTM_Simulation_boxplot_reshape.html#baseline-3",
    "title": "GConvLSTM_Simulation_reshape",
    "section": "Baseline",
    "text": "Baseline\n\ndata.query(\"mrate==0 \").plot.box(backend='plotly',x='mrate',color='method',y='mse',height=600)"
  },
  {
    "objectID": "posts/2_research/2023-05-30-GConvLSTM_Simulation_boxplot_reshape.html#random-3",
    "href": "posts/2_research/2023-05-30-GConvLSTM_Simulation_boxplot_reshape.html#random-3",
    "title": "GConvLSTM_Simulation_reshape",
    "section": "random",
    "text": "random\n\ndata.query(\"mtype=='rand' and mrate !=0 and method!='GNAR'\").plot.box(backend='plotly',x='mrate',color='method',y='mse',facet_col='lags',facet_row='nof_filters',height=800)"
  },
  {
    "objectID": "posts/2_research/2023-05-30-GConvLSTM_Simulation_boxplot_reshape.html#block-3",
    "href": "posts/2_research/2023-05-30-GConvLSTM_Simulation_boxplot_reshape.html#block-3",
    "title": "GConvLSTM_Simulation_reshape",
    "section": "block",
    "text": "block\n\ndata.query(\"mtype=='block' \").plot.box(backend='plotly',x='mrate',color='method',y='mse',facet_col='lags',facet_row='nof_filters',height=1200)"
  },
  {
    "objectID": "posts/2_research/2023-05-30-GConvLSTM_Simulation_boxplot_reshape.html#missing-values-on-the-same-nodes",
    "href": "posts/2_research/2023-05-30-GConvLSTM_Simulation_boxplot_reshape.html#missing-values-on-the-same-nodes",
    "title": "GConvLSTM_Simulation_reshape",
    "section": "missing values on the same nodes",
    "text": "missing values on the same nodes\n\ndf1 = pd.read_csv('./simulation_results/2023-06-14_19-23-44.csv') # STGCN IT-STGCN block\ndf2 = pd.read_csv('./simulation_results/2023-06-15_15-51-38.csv')\ndf3 = pd.read_csv('./simulation_results/2023-06-16_04-32-51.csv')\n\n\ndata = pd.concat([df1,df2,df3],axis=0)\n\n\ndata.to_csv('./simulation_results/Real_simulation_reshape/GConvLSTM_wikimath_GSO_st.csv',index=False)\n\n\ndata = pd.read_csv('./simulation_results/Real_simulation_reshape/GConvLSTM_wikimath_GSO_st.csv')\n\n\ndata.query(\"method!='GNAR' and mtype =='block' \").plot.box(backend='plotly',x='mrate',color='method',y='mse',facet_col='inter_method',facet_row='lags',height=800)"
  },
  {
    "objectID": "posts/2_research/2023-05-30-GConvLSTM_Simulation_boxplot_reshape.html#baseline-4",
    "href": "posts/2_research/2023-05-30-GConvLSTM_Simulation_boxplot_reshape.html#baseline-4",
    "title": "GConvLSTM_Simulation_reshape",
    "section": "Baseline",
    "text": "Baseline\n\ndata.query(\"method!='GNAR' and mrate ==0 \").plot.box(backend='plotly',x='mrate',color='method',y='mse',facet_col='lags',height=800)"
  },
  {
    "objectID": "posts/2_research/2023-05-30-GConvLSTM_Simulation_boxplot_reshape.html#random-4",
    "href": "posts/2_research/2023-05-30-GConvLSTM_Simulation_boxplot_reshape.html#random-4",
    "title": "GConvLSTM_Simulation_reshape",
    "section": "random",
    "text": "random\n\ndata.query(\"method!='GNAR' and mtype =='rand' \").plot.box(backend='plotly',x='mrate',color='method',y='mse',facet_col='lags',facet_row='inter_method',height=800)"
  },
  {
    "objectID": "posts/2_research/2023-05-30-GConvLSTM_Simulation_boxplot_reshape.html#block-4",
    "href": "posts/2_research/2023-05-30-GConvLSTM_Simulation_boxplot_reshape.html#block-4",
    "title": "GConvLSTM_Simulation_reshape",
    "section": "block",
    "text": "block\n\ndata.query(\"method!='GNAR' and mtype =='block' \").plot.box(backend='plotly',x='mrate',color='method',y='mse',facet_col='lags',facet_row='inter_method',height=800)"
  },
  {
    "objectID": "posts/2_research/2023-05-30-GConvLSTM_Simulation_boxplot_reshape.html#baseline-5",
    "href": "posts/2_research/2023-05-30-GConvLSTM_Simulation_boxplot_reshape.html#baseline-5",
    "title": "GConvLSTM_Simulation_reshape",
    "section": "Baseline",
    "text": "Baseline\n\ndata.query(\"mrate==0 and method!='GNAR'\").plot.box(backend='plotly',x='mrate',color='method',y='mse',facet_col='lags',facet_row='nof_filters',height=600)"
  },
  {
    "objectID": "posts/2_research/2023-05-30-GConvLSTM_Simulation_boxplot_reshape.html#random-5",
    "href": "posts/2_research/2023-05-30-GConvLSTM_Simulation_boxplot_reshape.html#random-5",
    "title": "GConvLSTM_Simulation_reshape",
    "section": "random",
    "text": "random\n\ndata.query(\"mtype=='rand' and mrate !=0 and method!='GNAR'\").plot.box(backend='plotly',x='mrate',color='method',y='mse',facet_col='lags',facet_row='inter_method',height=800)"
  },
  {
    "objectID": "posts/2_research/2023-05-30-GConvLSTM_Simulation_boxplot_reshape.html#block-5",
    "href": "posts/2_research/2023-05-30-GConvLSTM_Simulation_boxplot_reshape.html#block-5",
    "title": "GConvLSTM_Simulation_reshape",
    "section": "block",
    "text": "block\n\ndata.query(\"mtype=='block' and method!='GNAR'\").plot.box(backend='plotly',x='mrate',color='method',y='mse',facet_col='lags',facet_row='nof_filters',height=1200)"
  },
  {
    "objectID": "posts/2_research/2023-08-25-batch.html",
    "href": "posts/2_research/2023-08-25-batch.html",
    "title": "Batch",
    "section": "",
    "text": "import itstgcn_batch\nimport torch\nimport itstgcn_batch.planner \nimport pandas as pd\n\nimport numpy as np\nimport random\n\n/home/csy/anaconda3/envs/temp_csy/lib/python3.8/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n  from .autonotebook import tqdm as notebook_tqdm\nloader6 = itstgcn_batch.load_data('./data/Windmillsmall.pkl')\nplans_stgcn_rand = {\n    'max_iteration': 30, \n    'method': ['STGCN', 'IT-STGCN'],\n    'mrate': [0.7],\n    'lags': [8], \n    'nof_filters': [12], \n    'batch_size':[64],\n    'inter_method': ['linear'],\n    'epoch': [5]\n}\nplnr = itstgcn_batch.planner.PLNR_STGCN_RAND(plans_stgcn_rand,loader6,dataset_name='0829tstwindmillsmall')\nplnr.simulate()\n\nepoch=2  loss=17.999483498264125241088867       1/1\npd.read_csv('./simulation_results/2023-08-27_13-29-25.csv') # STGCN IT-STGCN\ndata.query(\"dataset=='windmillsmall' and mtype=='rand' and mrate==0.7 and method == 'STGCN'\").sort_values('mse')[:5]\n\n\n\n\n\n\n\n\ndataset\nmethod\nmrate\nmtype\nlags\nnof_filters\ninter_method\nepoch\nmse\ncalculation_time\n\n\n\n\n20\nwindmillsmall\nSTGCN\n0.7\nrand\n8\n12\nlinear\n50\n1.491985\n4189.720611\n\n\n24\nwindmillsmall\nSTGCN\n0.7\nrand\n8\n12\nlinear\n50\n1.526696\n4184.767751\n\n\n96\nwindmillsmall\nSTGCN\n0.7\nrand\n8\n12\nlinear\n50\n1.548946\n4182.209651\n\n\n82\nwindmillsmall\nSTGCN\n0.7\nrand\n8\n12\nlinear\n50\n1.574623\n3112.238717\n\n\n108\nwindmillsmall\nSTGCN\n0.7\nrand\n8\n12\nlinear\n50\n1.577491\n4354.155174\ndata.query(\"dataset=='windmillsmall' and mtype=='rand' and mrate==0.7 and method != 'STGCN'\").sort_values('mse')[:5]\n\n\n\n\n\n\n\n\ndataset\nmethod\nmrate\nmtype\nlags\nnof_filters\ninter_method\nepoch\nmse\ncalculation_time\n\n\n\n\n35\nwindmillsmall\nIT-STGCN\n0.7\nrand\n8\n12\nlinear\n50\n1.118357\n25106.537400\n\n\n13\nwindmillsmall\nIT-STGCN\n0.7\nrand\n8\n12\nlinear\n50\n1.133768\n33006.879024\n\n\n109\nwindmillsmall\nIT-STGCN\n0.7\nrand\n8\n12\nlinear\n50\n1.134867\n43213.207462\n\n\n105\nwindmillsmall\nIT-STGCN\n0.7\nrand\n8\n12\nlinear\n50\n1.148162\n39068.090318\n\n\n91\nwindmillsmall\nIT-STGCN\n0.7\nrand\n8\n12\nlinear\n50\n1.155443\n37083.352598\ndata_dict = itstgcn_batch.load_data('./data/fivenodes.pkl')\nloader = itstgcn_batch.DatasetLoader(data_dict)\nplans_stgcn_rand = {\n    'max_iteration': 1, \n    'method': ['STGCN', 'IT-STGCN'],\n    'mrate': [0.7],\n    'lags': [8], \n    'nof_filters': [12], \n    'batch_size':[16],\n    'inter_method': ['linear'],\n    'epoch': [5]\n}\nplnr = itstgcn_batch.planner.PLNR_STGCN_RAND(plans_stgcn_rand,loader,dataset_name='0829fivenodes')\nplnr.simulate()\n\nepoch=6  loss=5.8522477626800535831665      1/52/53/54/55/5\nepoch=6  loss=0.85238128900527950052795 1/52/53/54/55/5\n1/1 is done\nAll results are stored in ./simulation_results/2023-09-01_17-11-14.csv\npd.read_csv('./simulation_results/2023-09-01_17-09-10.csv') # STGCN IT-STGCN\n\n\n\n\n\n\n\n\ndataset\nmethod\nmrate\nmtype\nlags\nnof_filters\nbatch_size\ninter_method\nepoch\nmse\ncalculation_time\n\n\n\n\n0\n0829fivenodes\nSTGCN\n0.7\nrand\n8\n12\n16\nlinear\n2\n1.252692\n2.234628\n\n\n1\n0829fivenodes\nIT-STGCN\n0.7\nrand\n8\n12\n16\nlinear\n2\n1.639029\n20.325224\npd.read_csv('./simulation_results/2023-08-29_18-04-15.csv') # STGCN IT-STGCN\n\n\n\n\n\n\n\n\ndataset\nmethod\nmrate\nmtype\nlags\nnof_filters\nbatch_size\ninter_method\nepoch\nmse\ncalculation_time\n\n\n\n\n0\n0829fivenodes\nSTGCN\n0.7\nrand\n8\n12\n64\nlinear\n50\n1.515168\n36.609239\n\n\n1\n0829fivenodes\nIT-STGCN\n0.7\nrand\n8\n12\n64\nlinear\n50\n1.485295\n135.137178\npd.read_csv('./simulation_results/2023-08-29_18-00-15.csv') # STGCN IT-STGCN\n\n\n\n\n\n\n\n\ndataset\nmethod\nmrate\nmtype\nlags\nnof_filters\nbatch_size\ninter_method\nepoch\nmse\ncalculation_time\n\n\n\n\n0\n0829fivenodes\nSTGCN\n0.7\nrand\n8\n12\n32\nlinear\n50\n1.869130\n61.931024\n\n\n1\n0829fivenodes\nIT-STGCN\n0.7\nrand\n8\n12\n32\nlinear\n50\n1.320484\n239.835408\npd.read_csv('./simulation_results/2023-08-29_18-07-28.csv') # STGCN IT-STGCN\n\n\n\n\n\n\n\n\ndataset\nmethod\nmrate\nmtype\nlags\nnof_filters\nbatch_size\ninter_method\nepoch\nmse\ncalculation_time\n\n\n\n\n0\n0829fivenodes\nSTGCN\n0.7\nrand\n8\n12\n64\nlinear\n30\n1.354607\n32.734412\n\n\n1\n0829fivenodes\nIT-STGCN\n0.7\nrand\n8\n12\n64\nlinear\n30\n1.372238\n88.575825\npd.read_csv('./simulation_results/2023-08-29_18-13-12.csv') # STGCN IT-STGCN\n\n\n\n\n\n\n\n\ndataset\nmethod\nmrate\nmtype\nlags\nnof_filters\nbatch_size\ninter_method\nepoch\nmse\ncalculation_time\n\n\n\n\n0\n0829fivenodes\nSTGCN\n0.7\nrand\n8\n12\n16\nlinear\n30\n1.595359\n30.866718\n\n\n1\n0829fivenodes\nIT-STGCN\n0.7\nrand\n8\n12\n16\nlinear\n30\n1.214464\n261.216966\npd.read_csv('./simulation_results/2023-08-29_17-47-21.csv') # STGCN IT-STGCN\n\n\n\n\n\n\n\n\ndataset\nmethod\nmrate\nmtype\nlags\nnof_filters\nbatch_size\ninter_method\nepoch\nmse\ncalculation_time\n\n\n\n\n0\n0829fivenodes\nSTGCN\n0.7\nrand\n8\n12\n32\nlinear\n20\n1.432889\n14.750201\n\n\n1\n0829fivenodes\nIT-STGCN\n0.7\nrand\n8\n12\n32\nlinear\n20\n1.285243\n95.778697\npd.read_csv('./simulation_results/2023-08-29_17-45-17.csv') # STGCN IT-STGCN\n\n\n\n\n\n\n\n\ndataset\nmethod\nmrate\nmtype\nlags\nnof_filters\nbatch_size\ninter_method\nepoch\nmse\ncalculation_time\n\n\n\n\n0\n0829fivenodes\nSTGCN\n0.7\nrand\n8\n12\n32\nlinear\n10\n1.290018\n10.178871\n\n\n1\n0829fivenodes\nIT-STGCN\n0.7\nrand\n8\n12\n32\nlinear\n10\n1.299491\n47.223043\npd.read_csv('./simulation_results/2023-08-29_17-41-40.csv') # STGCN IT-STGCN\n\n\n\n\n\n\n\n\ndataset\nmethod\nmrate\nmtype\nlags\nnof_filters\nbatch_size\ninter_method\nepoch\nmse\ncalculation_time\n\n\n\n\n0\n0829fivenodes\nSTGCN\n0.7\nrand\n8\n12\n16\nlinear\n10\n1.428137\n10.698743\n\n\n1\n0829fivenodes\nIT-STGCN\n0.7\nrand\n8\n12\n16\nlinear\n10\n1.553111\n97.193523\nfrom torch_geometric_temporal.dataset import ChickenpoxDatasetLoader\nloader1 = ChickenpoxDatasetLoader()\nplans_stgcn_rand = {\n    'max_iteration': 1, \n    'method': ['STGCN', 'IT-STGCN'],\n    'mrate': [0.7],\n    'lags': [8], \n    'nof_filters': [16], \n    'batch_size':[128],\n    'inter_method': ['linear'],\n    'epoch': [50]\n}\nplnr = itstgcn_batch.planner.PLNR_STGCN_RAND(plans_stgcn_rand,loader1,dataset_name='testchickenpox')\nplnr.simulate()\n\nepoch=51     loss=29.052431225776672952057      1/502/503/504/505/506/507/508/509/5010/5011/5012/5013/5014/5015/5016/5017/5018/5019/5020/5021/5022/5023/5024/5025/5026/5027/5028/5029/5030/5031/5032/5033/5034/5035/5036/5037/5038/5039/5040/5041/5042/5043/5044/5045/5046/5047/5048/5049/5050/50\nbatch=2  t=250   loss=0.6726564764976501    1/502/503/504/505/506/507/508/509/5010/5011/5012/5013/5014/5015/5016/5017/5018/5019/5020/5021/5022/5023/5024/5025/5026/5027/5028/5029/5030/5031/5032/5033/5034/5035/5036/5037/5038/5039/5040/5041/5042/5043/5044/5045/50\npd.read_csv('./simulation_results/2023-08-30_18-31-28.csv') # STGCN IT-STGCN\n\n\n\n\n\n\n\n\ndataset\nmethod\nmrate\nmtype\nlags\nnof_filters\nbatch_size\ninter_method\nepoch\nmse\ncalculation_time\n\n\n\n\n0\ntestchickenpox\nSTGCN\n0.7\nrand\n8\n16\n64\nlinear\n50\n2.069710\n266.923153\n\n\n1\ntestchickenpox\nIT-STGCN\n0.7\nrand\n8\n16\n64\nlinear\n50\n1.167303\n1718.838966\npd.read_csv('./simulation_results/2023-08-29_19-14-32.csv') # STGCN IT-STGCN\n\n\n\n\n\n\n\n\ndataset\nmethod\nmrate\nmtype\nlags\nnof_filters\nbatch_size\ninter_method\nepoch\nmse\ncalculation_time\n\n\n\n\n0\ntestchickenpox\nSTGCN\n0.7\nrand\n8\n16\n32\nlinear\n50\n2.145854\n200.711365\n\n\n1\ntestchickenpox\nIT-STGCN\n0.7\nrand\n8\n16\n32\nlinear\n50\n1.078493\n3340.726743\nfrom torch_geometric_temporal.dataset import PedalMeDatasetLoader\nloader2 = PedalMeDatasetLoader()\nplans_stgcn_rand = {\n    'max_iteration': 1, \n    'method': ['STGCN', 'IT-STGCN'],\n    'mrate': [0.7],\n    'lags': [8], \n    'nof_filters': [12], \n    'batch_size':[8],\n    'inter_method': ['linear'],\n    'epoch': [2]\n}\nplnr = itstgcn_batch.planner.PLNR_STGCN_RAND(plans_stgcn_rand,loader2,dataset_name='0829testpedal')\nplnr.simulate()\n\nepoch=3  loss=0.9055710236231486826279  1/22/2\nepoch=3  loss=0.1323363482952118952118      1/22/2\n1/1 is done\nAll results are stored in ./simulation_results/2023-09-01_17-07-32.csv\npd.read_csv('./simulation_results/2023-09-01_17-07-32.csv') # STGCN IT-STGCN\n\n\n\n\n\n\n\n\ndataset\nmethod\nmrate\nmtype\nlags\nnof_filters\nbatch_size\ninter_method\nepoch\nmse\ncalculation_time\n\n\n\n\n0\n0829testpedal\nSTGCN\n0.7\nrand\n8\n12\n8\nlinear\n2\n1.371031\n0.362818\n\n\n1\n0829testpedal\nIT-STGCN\n0.7\nrand\n8\n12\n8\nlinear\n2\n1.281301\n1.038512\npd.read_csv('./simulation_results/2023-08-29_18-14-16.csv') # STGCN IT-STGCN\n\n\n\n\n\n\n\n\ndataset\nmethod\nmrate\nmtype\nlags\nnof_filters\nbatch_size\ninter_method\nepoch\nmse\ncalculation_time\n\n\n\n\n0\n0829testpedal\nSTGCN\n0.7\nrand\n8\n12\n8\nlinear\n50\n1.989184\n8.535240\n\n\n1\n0829testpedal\nIT-STGCN\n0.7\nrand\n8\n12\n8\nlinear\n50\n1.307249\n22.340453\npd.read_csv('./simulation_results/2023-08-29_18-14-16.csv') # STGCN IT-STGCN\n\n\n\n\n\n\n\n\ndataset\nmethod\nmrate\nmtype\nlags\nnof_filters\nbatch_size\ninter_method\nepoch\nmse\ncalculation_time\n\n\n\n\n0\n0829testpedal\nSTGCN\n0.7\nrand\n8\n12\n8\nlinear\n50\n1.989184\n8.535240\n\n\n1\n0829testpedal\nIT-STGCN\n0.7\nrand\n8\n12\n8\nlinear\n50\n1.307249\n22.340453\npd.read_csv('./simulation_results/2023-08-28_12-49-19_x.csv') # STGCN IT-STGCN\n\n\n\n\n\n\n\n\ndataset\nmethod\nmrate\nmtype\nlags\nnof_filters\nbatch_size\ninter_method\nepoch\nmse\ncalculation_time\n\n\n\n\n0\ntestpedal\nSTGCN\n0.7\nrand\n8\n12\n10\nlinear\n50\n3.081795\n8.496341\n\n\n1\ntestpedal\nIT-STGCN\n0.7\nrand\n8\n12\n10\nlinear\n50\n2.982537\n27.241297\nfrom torch_geometric_temporal.dataset import WikiMathsDatasetLoader\nloader3 = WikiMathsDatasetLoader()\nfrom torch_geometric_temporal.dataset import MontevideoBusDatasetLoader\nloader10 = MontevideoBusDatasetLoader()"
  },
  {
    "objectID": "posts/2_research/2023-08-25-batch.html#load-data",
    "href": "posts/2_research/2023-08-25-batch.html#load-data",
    "title": "Batch",
    "section": "Load data",
    "text": "Load data\n\n# read dataframe \ndf = pd.read_csv('data_eng_230710.csv')\n# make y, y_upper, y_period, time, regions \ny = df.loc[:,'Bukchoncheon':'Gyeongju-si'].to_numpy()\nyU = df.loc[:,'Bukchoncheon_Upper':'Gyeongju-si_Upper'].to_numpy()\nyP = np.divide(y, yU+1e-10)"
  },
  {
    "objectID": "posts/2_research/2023-08-25-batch.html#sample_codes",
    "href": "posts/2_research/2023-08-25-batch.html#sample_codes",
    "title": "Batch",
    "section": "sample_codes",
    "text": "sample_codes\n- 예시1: y를 학습\n학습~\n\n# step1: create lrnr object\nlrnr = RGCN_Learner()\n# step2: load data \nlrnr.lags = 4 \nlrnr.train_ratio = 0.8\nlrnr.load(y) \n# step3: construct networks \nlrnr.nof_filters = 16\nlrnr.model = RecurrentGCN(node_features=lrnr.lags, filters=lrnr.nof_filters).to(device)\nlrnr.optimizer = torch.optim.Adam(lrnr.model.parameters(),lr=10/1000)\n# step4: learn \nlrnr.get_batches(batch_size=24)\nfor e in range(5):    \n    lrnr.learn()\n\nepoch=1  loss=0.623498441781415453993988            \nepoch=2  loss=0.097667323512046848847961        \nepoch=3  loss=0.083123757352316121851349            \nepoch=4  loss=0.081149838734851325569305            \nepoch=5  loss=0.080320493751313838172531            \n\n\n\nplt.plot(y[:100,0])\n\n에폭별 적합결과(\\(\\hat{y}\\)) 시각화\n\nlrnr.figs[0]\n\n\n\n\n\n\n\n\n\nlrnr.figs[1]\n\n\n\n\n\n\n\n\n\nlrnr.figs[-1]\n\n\n\n\n\n\n\n\n- 예시2: yU 학습\n\n# step1: create lrnr object\nlrnr = RGCN_Learner()\n# step2: load data \nlrnr.lags = 4 \nlrnr.train_ratio = 0.8\nlrnr.load(yU) \n# step3: construct networks \nlrnr.nof_filters = 16\nlrnr.model = RecurrentGCN(node_features=lrnr.lags, filters=lrnr.nof_filters).to(device)\nlrnr.optimizer = torch.optim.Adam(lrnr.model.parameters(),lr=10/1000)\n# step4: learn \nlrnr.get_batches(batch_size=24)\nfor e in range(5):    \n    lrnr.learn()\n\nepoch=1  loss=1.98579962707536175294647     \nepoch=2  loss=0.202098417204133351100159        \nepoch=3  loss=0.052548440736393594218872        \nepoch=4  loss=0.0272646296531135271772957   \nepoch=5  loss=0.0191931016526596582006645   \n\n\n\nlrnr.figs[-1]\n\n\n\n\n\n\n\n\n5번정도 더 돌려보자\n\nfor e in range(5):    \n    lrnr.learn()\n\nepoch=6  loss=0.0169690931215882312923717       \nepoch=7  loss=0.0141667058247379891905975       \nepoch=8  loss=0.0124263072353896015870743       \nepoch=9  loss=0.0103476242200189951082373       \nepoch=10     loss=0.009508996026937005688799        \n\n\n\nlrnr.figs[-1]\n\n\n\n\n\n\n\n\n- 예시3: yP 학습\n\n# step1: create lrnr object\nlrnr = RGCN_Learner()\n# step2: load data \nlrnr.lags = 4 \nlrnr.train_ratio = 0.8\nlrnr.load(yP) \n# step3: construct networks \nlrnr.nof_filters = 16\nlrnr.model = RecurrentGCN(node_features=lrnr.lags, filters=lrnr.nof_filters).to(device)\nlrnr.optimizer = torch.optim.Adam(lrnr.model.parameters(),lr=10/1000)\n# step4: learn \nlrnr.get_batches(batch_size=24)\nfor e in range(5):    \n    lrnr.learn()\n\nepoch=1  loss=0.030719049366970748393135            \nepoch=2  loss=0.0119630681123411254793358       \nepoch=3  loss=0.0109235838672882638420868       \nepoch=4  loss=0.0107088599754714682600975       \nepoch=5  loss=0.0105961727974719788335514       \n\n\n\nlrnr.figs[-1]"
  },
  {
    "objectID": "posts/2_research/2022-12-07-torchgcn.html",
    "href": "posts/2_research/2022-12-07-torchgcn.html",
    "title": "TORCH_GEOMETRIC.NN",
    "section": "",
    "text": "221207\nhttps://pytorch-geometric.readthedocs.io/en/latest/modules/nn.html\n\nimport torch\nfrom torch_geometric.data import Data\n\n\nedge_index = torch.tensor([[0, 1, 1, 2],\n                           [1, 0, 2, 1]], dtype=torch.long)\nx = torch.tensor([[-1], [0], [1]], dtype=torch.float)\ndata = Data(x=x, edge_index=edge_index)\n\n\ndata\n\nData(x=[3, 1], edge_index=[2, 4])\n\n\n\nimport networkx as nx\nimport matplotlib.pyplot as plt\n\n\nG=nx.Graph()\nG.add_node('0')\nG.add_node('1')\nG.add_node('2')\nG.add_edge('0','1')\nG.add_edge('1','2')\npos = {}\npos['0'] = (0,0)\npos['1'] = (1,1)\npos['2'] = (2,0)\nnx.draw(G,pos,with_labels=True)\nplt.show()\n\n\n\n\n\n\n\n\n\nfrom torch.nn import Linear, ReLU\nfrom torch_geometric.nn import Sequential, GCNConv\n\nex\nmodel = Sequential('x, edge_index', [\n    (GCNConv(in_channels, 64), 'x, edge_index -&gt; x'),\n    ReLU(inplace=True),\n    (GCNConv(64, 64), 'x, edge_index -&gt; x'),\n    ReLU(inplace=True),\n    Linear(64, out_channels),\n])\n\nmodel = Sequential('x, edge_index', [\n    (GCNConv(3, 64), 'x, edge_index -&gt; x'),\n    ReLU(inplace=True),\n    (GCNConv(64, 64), 'x, edge_index -&gt; x'),\n    ReLU(inplace=True),\n    Linear(64, 3),\n])\n\n\nmodel(x,edge_index)\n\n\nfrom torch.nn import Linear, ReLU, Dropout\nfrom torch_geometric.nn import Sequential, GCNConv, JumpingKnowledge\nfrom torch_geometric.nn import global_mean_pool\n\nmodel = Sequential('x, edge_index, batch', [\n    (Dropout(p=0.5), 'x -&gt; x'),\n    (GCNConv(dataset.num_features, 64), 'x, edge_index -&gt; x1'),\n    ReLU(inplace=True),\n    (GCNConv(64, 64), 'x1, edge_index -&gt; x2'),\n    ReLU(inplace=True),\n    (lambda x1, x2: [x1, x2], 'x1, x2 -&gt; xs'),\n    (JumpingKnowledge(\"cat\", 64, num_layers=2), 'xs -&gt; x'),\n    (global_mean_pool, 'x, batch -&gt; x'),\n    Linear(2 * 64, dataset.num_classes),\n])\n\nmodel = Sequential('x, edge_index, batch', [\n    (Dropout(p=0.5), 'x -&gt; x'),\n    (GCNConv(dataset.num_features, 64), 'x, edge_index -&gt; x1'),\n    ReLU(inplace=True),\n    (GCNConv(64, 64), 'x1, edge_index -&gt; x2'),\n    ReLU(inplace=True),\n    (lambda x1, x2: [x1, x2], 'x1, x2 -&gt; xs'),\n    (JumpingKnowledge(\"cat\", 64, num_layers=2), 'xs -&gt; x'),\n    (global_mean_pool, 'x, batch -&gt; x'),\n    Linear(2 * 64, dataset.num_classes),\n])\n\n\ntorch_geometric.nn.Linear()"
  },
  {
    "objectID": "posts/2_research/2023-05-25-GCONVGRU_simulation_table_reshape.html",
    "href": "posts/2_research/2023-05-25-GCONVGRU_simulation_table_reshape.html",
    "title": "GConvGRU and GNAR_Simulation Tables_reshape",
    "section": "",
    "text": "Simulation Tables"
  },
  {
    "objectID": "posts/2_research/2023-05-25-GCONVGRU_simulation_table_reshape.html#baseline",
    "href": "posts/2_research/2023-05-25-GCONVGRU_simulation_table_reshape.html#baseline",
    "title": "GConvGRU and GNAR_Simulation Tables_reshape",
    "section": "Baseline",
    "text": "Baseline\n\npd.merge(data.query(\"dataset=='fivenodes' and mtype!='rand' and mtype!='block'\").groupby(['nof_filters','method','lags'])['mse'].mean().reset_index(),\n         data.query(\"dataset=='fivenodes' and mtype!='rand' and mtype!='block'\").groupby(['nof_filters','method','lags'])['mse'].std().reset_index(),\n         on=['method','nof_filters','lags']).rename(columns={'mse_x':'mean','mse_y':'std'}).round(3)\n\n\n\n\n\n\n\n\nnof_filters\nmethod\nlags\nmean\nstd\n\n\n\n\n0\n12\nIT-STGCN\n2\n0.732\n0.005\n\n\n1\n12\nSTGCN\n2\n0.732\n0.005"
  },
  {
    "objectID": "posts/2_research/2023-05-25-GCONVGRU_simulation_table_reshape.html#random",
    "href": "posts/2_research/2023-05-25-GCONVGRU_simulation_table_reshape.html#random",
    "title": "GConvGRU and GNAR_Simulation Tables_reshape",
    "section": "Random",
    "text": "Random\n\npd.merge(data.query(\"dataset=='fivenodes' and mtype=='rand'\").groupby(['mrate','nof_filters','method','lags'])['mse'].mean().reset_index(),\n         data.query(\"dataset=='fivenodes' and mtype=='rand'\").groupby(['mrate','nof_filters','method','lags'])['mse'].std().reset_index(),\n         on=['method','nof_filters','mrate','lags']).rename(columns={'mse_x':'mean','mse_y':'std'}).round(3).query(\"nof_filters==12\")\n\n\n\n\n\n\n\n\nmrate\nnof_filters\nmethod\nlags\nmean\nstd\n\n\n\n\n0\n0.3\n12\nIT-STGCN\n2\n1.186\n0.051\n\n\n1\n0.3\n12\nSTGCN\n2\n1.208\n0.051\n\n\n2\n0.5\n12\nIT-STGCN\n2\n1.242\n0.061\n\n\n3\n0.5\n12\nSTGCN\n2\n1.330\n0.073\n\n\n4\n0.6\n12\nIT-STGCN\n2\n1.251\n0.055\n\n\n5\n0.6\n12\nSTGCN\n2\n1.422\n0.086\n\n\n6\n0.7\n12\nIT-STGCN\n2\n1.167\n0.059\n\n\n7\n0.7\n12\nSTGCN\n2\n2.077\n0.252\n\n\n8\n0.8\n12\nIT-STGCN\n2\n1.371\n0.097\n\n\n9\n0.8\n12\nSTGCN\n2\n2.432\n0.263"
  },
  {
    "objectID": "posts/2_research/2023-05-25-GCONVGRU_simulation_table_reshape.html#block",
    "href": "posts/2_research/2023-05-25-GCONVGRU_simulation_table_reshape.html#block",
    "title": "GConvGRU and GNAR_Simulation Tables_reshape",
    "section": "Block",
    "text": "Block\n\npd.merge(data.query(\"dataset=='fivenodes' and mtype=='block'\").groupby(['mrate','nof_filters','method'])['mse'].mean().reset_index(),\n         data.query(\"dataset=='fivenodes' and mtype=='block'\").groupby(['mrate','nof_filters','method'])['mse'].std().reset_index(),\n         on=['method','nof_filters','mrate']).rename(columns={'mse_x':'mean','mse_y':'std'}).round(3)\n\n\n\n\n\n\n\n\nmrate\nnof_filters\nmethod\nmean\nstd\n\n\n\n\n0\n0.125\n12\nIT-STGCN\n1.160\n0.042\n\n\n1\n0.125\n12\nSTGCN\n1.215\n0.036"
  },
  {
    "objectID": "posts/2_research/2023-05-25-GCONVGRU_simulation_table_reshape.html#baseline-1",
    "href": "posts/2_research/2023-05-25-GCONVGRU_simulation_table_reshape.html#baseline-1",
    "title": "GConvGRU and GNAR_Simulation Tables_reshape",
    "section": "Baseline",
    "text": "Baseline\n\npd.merge(data.query(\"dataset=='chickenpox' and mtype!='rand' and mtype!='block'\").groupby(['nof_filters','method'])['mse'].mean().reset_index(),\n         data.query(\"dataset=='chickenpox' and mtype!='rand' and mtype!='block'\").groupby(['nof_filters','method'])['mse'].std().reset_index(),\n         on=['method','nof_filters']).rename(columns={'mse_x':'mean','mse_y':'std'}).round(3).query(\"nof_filters==16\")\n\n\n\n\n\n\n\n\nnof_filters\nmethod\nmean\nstd\n\n\n\n\n0\n16\nIT-STGCN\n0.752\n0.013\n\n\n1\n16\nSTGCN\n0.752\n0.012"
  },
  {
    "objectID": "posts/2_research/2023-05-25-GCONVGRU_simulation_table_reshape.html#random-1",
    "href": "posts/2_research/2023-05-25-GCONVGRU_simulation_table_reshape.html#random-1",
    "title": "GConvGRU and GNAR_Simulation Tables_reshape",
    "section": "Random",
    "text": "Random\n\npd.merge(data.query(\"dataset=='chickenpox' and mtype=='rand'\").groupby(['mrate','inter_method','nof_filters','method'])['mse'].mean().reset_index(),\n         data.query(\"dataset=='chickenpox' and mtype=='rand'\").groupby(['mrate','inter_method','nof_filters','method'])['mse'].std().reset_index(),\n         on=['method','inter_method','mrate','nof_filters']).rename(columns={'mse_x':'mean','mse_y':'std'}).round(3)\n\n\n\n\n\n\n\n\nmrate\ninter_method\nnof_filters\nmethod\nmean\nstd\n\n\n\n\n0\n0.3\nlinear\n16\nIT-STGCN\n0.851\n0.031\n\n\n1\n0.3\nlinear\n16\nSTGCN\n1.087\n0.046\n\n\n2\n0.5\nlinear\n16\nIT-STGCN\n0.958\n0.072\n\n\n3\n0.5\nlinear\n16\nSTGCN\n1.530\n0.106\n\n\n4\n0.6\nlinear\n16\nIT-STGCN\n1.120\n0.072\n\n\n5\n0.6\nlinear\n16\nSTGCN\n1.753\n0.181\n\n\n6\n0.8\nlinear\n16\nIT-STGCN\n1.586\n0.199\n\n\n7\n0.8\nlinear\n16\nSTGCN\n2.529\n0.292"
  },
  {
    "objectID": "posts/2_research/2023-05-25-GCONVGRU_simulation_table_reshape.html#block-1",
    "href": "posts/2_research/2023-05-25-GCONVGRU_simulation_table_reshape.html#block-1",
    "title": "GConvGRU and GNAR_Simulation Tables_reshape",
    "section": "Block",
    "text": "Block\n\npd.merge(data.query(\"dataset=='chickenpox' and mtype=='block'\").groupby(['inter_method','mrate','nof_filters','method'])['mse'].mean().reset_index(),\n         data.query(\"dataset=='chickenpox' and mtype=='block'\").groupby(['inter_method','mrate','nof_filters','method'])['mse'].std().reset_index(),\n         on=['method','inter_method','mrate','nof_filters']).rename(columns={'mse_x':'mean','mse_y':'std'})\n\n\n\n\n\n\n\n\ninter_method\nmrate\nnof_filters\nmethod\nmean\nstd\n\n\n\n\n0\nlinear\n0.28777\n16\nIT-STGCN\n0.807041\n0.016362\n\n\n1\nlinear\n0.28777\n16\nSTGCN\n0.828224\n0.021919\n\n\n2\nnearest\n0.28777\n16\nIT-STGCN\n0.823756\n0.022918\n\n\n3\nnearest\n0.28777\n16\nSTGCN\n0.828498\n0.022007"
  },
  {
    "objectID": "posts/2_research/2023-05-25-GCONVGRU_simulation_table_reshape.html#baseline-2",
    "href": "posts/2_research/2023-05-25-GCONVGRU_simulation_table_reshape.html#baseline-2",
    "title": "GConvGRU and GNAR_Simulation Tables_reshape",
    "section": "Baseline",
    "text": "Baseline\n\npd.merge(data.query(\"dataset=='pedalme' and mtype!='rand' and mtype!='block'\").groupby(['lags','nof_filters','method'])['mse'].mean().reset_index(),\n         data.query(\"dataset=='pedalme' and mtype!='rand' and mtype!='block'\").groupby(['lags','nof_filters','method'])['mse'].std().reset_index(),\n         on=['method','lags','nof_filters']).rename(columns={'mse_x':'mean','mse_y':'std'}).round(3).query(\"lags==4\")\n\n\n\n\n\n\n\n\nlags\nnof_filters\nmethod\nmean\nstd\n\n\n\n\n0\n4\n12\nIT-STGCN\n1.233\n0.115\n\n\n1\n4\n12\nSTGCN\n1.233\n0.099"
  },
  {
    "objectID": "posts/2_research/2023-05-25-GCONVGRU_simulation_table_reshape.html#random-2",
    "href": "posts/2_research/2023-05-25-GCONVGRU_simulation_table_reshape.html#random-2",
    "title": "GConvGRU and GNAR_Simulation Tables_reshape",
    "section": "Random",
    "text": "Random\n\npd.merge(data.query(\"dataset=='pedalme' and mtype=='rand'\").groupby(['mrate','lags','inter_method','method'])['mse'].mean().reset_index(),\n         data.query(\"dataset=='pedalme' and mtype=='rand'\").groupby(['mrate','lags','inter_method','method'])['mse'].std().reset_index(),\n         on=['method','mrate','lags','inter_method']).rename(columns={'mse_x':'mean','mse_y':'std'}).round(3)\n\n\n\n\n\n\n\n\nmrate\nlags\ninter_method\nmethod\nmean\nstd\n\n\n\n\n0\n0.3\n4\nlinear\nIT-STGCN\n1.354\n0.134\n\n\n1\n0.3\n4\nlinear\nSTGCN\n1.575\n0.198\n\n\n2\n0.3\n4\nnearest\nIT-STGCN\n1.385\n0.173\n\n\n3\n0.3\n4\nnearest\nSTGCN\n1.527\n0.342\n\n\n4\n0.5\n4\nlinear\nIT-STGCN\n1.528\n0.190\n\n\n5\n0.5\n4\nlinear\nSTGCN\n1.593\n0.195\n\n\n6\n0.5\n4\nnearest\nIT-STGCN\n1.507\n0.235\n\n\n7\n0.5\n4\nnearest\nSTGCN\n1.673\n0.223\n\n\n8\n0.6\n4\nlinear\nIT-STGCN\n1.516\n0.211\n\n\n9\n0.6\n4\nlinear\nSTGCN\n1.655\n0.179\n\n\n10\n0.6\n4\nnearest\nIT-STGCN\n1.625\n0.324\n\n\n11\n0.6\n4\nnearest\nSTGCN\n1.851\n0.254\n\n\n12\n0.8\n4\nlinear\nIT-STGCN\n1.753\n0.306\n\n\n13\n0.8\n4\nlinear\nSTGCN\n1.753\n0.148\n\n\n14\n0.8\n4\nnearest\nIT-STGCN\n1.608\n0.243\n\n\n15\n0.8\n4\nnearest\nSTGCN\n1.871\n0.214"
  },
  {
    "objectID": "posts/2_research/2023-05-25-GCONVGRU_simulation_table_reshape.html#block-2",
    "href": "posts/2_research/2023-05-25-GCONVGRU_simulation_table_reshape.html#block-2",
    "title": "GConvGRU and GNAR_Simulation Tables_reshape",
    "section": "Block",
    "text": "Block\n\npd.merge(data.query(\"dataset=='pedalme' and mtype=='block'\").groupby(['mrate','lags','inter_method','method'])['mse'].mean().reset_index(),\n         data.query(\"dataset=='pedalme' and mtype=='block'\").groupby(['mrate','lags','inter_method','method'])['mse'].std().reset_index(),\n         on=['method','mrate','lags','inter_method']).rename(columns={'mse_x':'mean','mse_y':'std'}).round(3).query(\"lags==4\")\n\n\n\n\n\n\n\n\nmrate\nlags\ninter_method\nmethod\nmean\nstd\n\n\n\n\n0\n0.286\n4\nlinear\nIT-STGCN\n1.329\n0.131\n\n\n1\n0.286\n4\nlinear\nSTGCN\n1.320\n0.111\n\n\n2\n0.286\n4\nnearest\nIT-STGCN\n1.289\n0.115\n\n\n3\n0.286\n4\nnearest\nSTGCN\n1.270\n0.114"
  },
  {
    "objectID": "posts/2_research/2023-05-25-GCONVGRU_simulation_table_reshape.html#w_st",
    "href": "posts/2_research/2023-05-25-GCONVGRU_simulation_table_reshape.html#w_st",
    "title": "GConvGRU and GNAR_Simulation Tables_reshape",
    "section": "W_st",
    "text": "W_st\n\npd.merge(data_pedal2.query(\"mtype=='rand'\").groupby(['mrate','lags','inter_method','method'])['mse'].mean().reset_index(),\n         data_pedal2.query(\"mtype=='rand'\").groupby(['mrate','lags','inter_method','method'])['mse'].std().reset_index(),\n         on=['method','mrate','lags','inter_method']).rename(columns={'mse_x':'mean','mse_y':'std'}).round(3).query(\"lags==4\")\n\n\n\n\n\n\n\n\nmrate\nlags\ninter_method\nmethod\nmean\nstd\n\n\n\n\n0\n0.3\n4\nlinear\nIT-STGCN\n1.270\n0.163\n\n\n1\n0.3\n4\nlinear\nSTGCN\n1.556\n0.264\n\n\n2\n0.3\n4\nnearest\nIT-STGCN\n1.324\n0.163\n\n\n3\n0.3\n4\nnearest\nSTGCN\n1.520\n0.206\n\n\n4\n0.6\n4\nlinear\nIT-STGCN\n1.434\n0.222\n\n\n5\n0.6\n4\nlinear\nSTGCN\n1.678\n0.211\n\n\n6\n0.6\n4\nnearest\nIT-STGCN\n1.410\n0.208\n\n\n7\n0.6\n4\nnearest\nSTGCN\n1.771\n0.220\n\n\n\n\n\n\n\n\npd.merge(data_pedal2.query(\"mtype=='block'\").groupby(['mrate','lags','inter_method','method'])['mse'].mean().reset_index(),\n         data_pedal2.query(\"mtype=='block'\").groupby(['mrate','lags','inter_method','method'])['mse'].std().reset_index(),\n         on=['method','mrate','lags','inter_method']).rename(columns={'mse_x':'mean','mse_y':'std'}).round(3).query(\"lags==4\")\n\n\n\n\n\n\n\n\nmrate\nlags\ninter_method\nmethod\nmean\nstd\n\n\n\n\n0\n0.286\n4\nlinear\nIT-STGCN\n1.391\n0.151\n\n\n1\n0.286\n4\nlinear\nSTGCN\n1.420\n0.110\n\n\n2\n0.286\n4\nnearest\nIT-STGCN\n1.361\n0.114\n\n\n3\n0.286\n4\nnearest\nSTGCN\n1.430\n0.145"
  },
  {
    "objectID": "posts/2_research/2023-05-25-GCONVGRU_simulation_table_reshape.html#baseline-3",
    "href": "posts/2_research/2023-05-25-GCONVGRU_simulation_table_reshape.html#baseline-3",
    "title": "GConvGRU and GNAR_Simulation Tables_reshape",
    "section": "Baseline",
    "text": "Baseline\n\npd.merge(data.query(\"dataset=='wikimath' and mrate==0\").groupby(['lags','nof_filters','method'])['mse'].mean().reset_index(),\n         data.query(\"dataset=='wikimath' and mrate==0\").groupby(['lags','nof_filters','method'])['mse'].std().reset_index(),\n         on=['lags','nof_filters','method']).rename(columns={'mse_x':'mean','mse_y':'std'}).round(3)\n\n\n\n\n\n\n\n\nlags\nnof_filters\nmethod\nmean\nstd\n\n\n\n\n0\n8\n12\nIT-STGCN\n0.529\n0.003\n\n\n1\n8\n12\nSTGCN\n0.528\n0.003"
  },
  {
    "objectID": "posts/2_research/2023-05-25-GCONVGRU_simulation_table_reshape.html#random-3",
    "href": "posts/2_research/2023-05-25-GCONVGRU_simulation_table_reshape.html#random-3",
    "title": "GConvGRU and GNAR_Simulation Tables_reshape",
    "section": "Random",
    "text": "Random\n\npd.merge(data.query(\"dataset=='wikimath' and mtype=='rand'\").groupby(['mrate','lags','method'])['mse'].mean().reset_index(),\n         data.query(\"dataset=='wikimath' and mtype=='rand'\").groupby(['mrate','lags','method'])['mse'].std().reset_index(),\n         on=['method','mrate','lags']).rename(columns={'mse_x':'mean','mse_y':'std'}).round(3)\n\n\n\n\n\n\n\n\nmrate\nlags\nmethod\nmean\nstd\n\n\n\n\n0\n0.3\n8\nIT-STGCN\n0.518\n0.002\n\n\n1\n0.3\n8\nSTGCN\n0.570\n0.006\n\n\n2\n0.5\n8\nIT-STGCN\n0.524\n0.003\n\n\n3\n0.5\n8\nSTGCN\n0.658\n0.010\n\n\n4\n0.6\n8\nIT-STGCN\n0.539\n0.004\n\n\n5\n0.6\n8\nSTGCN\n0.731\n0.015\n\n\n6\n0.8\n8\nIT-STGCN\n0.687\n0.021\n\n\n7\n0.8\n8\nSTGCN\n0.932\n0.043"
  },
  {
    "objectID": "posts/2_research/2023-05-25-GCONVGRU_simulation_table_reshape.html#block-3",
    "href": "posts/2_research/2023-05-25-GCONVGRU_simulation_table_reshape.html#block-3",
    "title": "GConvGRU and GNAR_Simulation Tables_reshape",
    "section": "Block",
    "text": "Block\n\npd.merge(data.query(\"dataset=='wikimath' and mtype=='block'\").groupby(['mrate','lags','method'])['mse'].mean().reset_index(),\n         data.query(\"dataset=='wikimath' and mtype=='block'\").groupby(['mrate','lags','method'])['mse'].std().reset_index(),\n         on=['method','mrate','lags']).rename(columns={'mse_x':'mean','mse_y':'std'})\n\n\n\n\n\n\n\n\nmrate\nlags\nmethod\nmean\nstd\n\n\n\n\n0\n0.003835\n8\nIT-STGCN\n0.528737\n0.002806\n\n\n1\n0.003835\n8\nSTGCN\n0.527871\n0.002606\n\n\n2\n0.095870\n8\nIT-STGCN\n0.529440\n0.003820\n\n\n3\n0.095870\n8\nSTGCN\n0.544176\n0.010772\n\n\n4\n0.119837\n8\nIT-STGCN\n0.522825\n0.002422\n\n\n5\n0.119837\n8\nSTGCN\n0.531188\n0.002295"
  },
  {
    "objectID": "posts/2_research/2023-05-25-GCONVGRU_simulation_table_reshape.html#missing-values-on-the-same-nodes",
    "href": "posts/2_research/2023-05-25-GCONVGRU_simulation_table_reshape.html#missing-values-on-the-same-nodes",
    "title": "GConvGRU and GNAR_Simulation Tables_reshape",
    "section": "missing values on the same nodes",
    "text": "missing values on the same nodes\n\npd.merge(data_wiki_GSO.groupby(['mrate','lags','method'])['mse'].mean().reset_index(),\n        data_wiki_GSO.groupby(['mrate','lags','method'])['mse'].std().reset_index(),\n         on=['method','mrate','lags']).rename(columns={'mse_x':'mean','mse_y':'std'}).round(3)\n\n\n\n\n\n\n\n\nmrate\nlags\nmethod\nmean\nstd\n\n\n\n\n0\n0.512\n8\nIT-STGCN\n0.533\n0.003\n\n\n1\n0.512\n8\nSTGCN\n0.726\n0.015"
  },
  {
    "objectID": "posts/2_research/2023-05-25-GCONVGRU_simulation_table_reshape.html#baseline-4",
    "href": "posts/2_research/2023-05-25-GCONVGRU_simulation_table_reshape.html#baseline-4",
    "title": "GConvGRU and GNAR_Simulation Tables_reshape",
    "section": "Baseline",
    "text": "Baseline\n\npd.merge(data.query(\"dataset=='windmillsmall' and mrate==0\").groupby(['lags','method'])['mse'].mean().reset_index(),\n         data.query(\"dataset=='windmillsmall' and mrate==0\").groupby(['lags','method'])['mse'].std().reset_index(),\n         on=['method','lags']).rename(columns={'mse_x':'mean','mse_y':'std'}).round(3)\n\n\n\n\n\n\n\n\nlags\nmethod\nmean\nstd\n\n\n\n\n0\n8\nIT-STGCN\n1.004\n0.004\n\n\n1\n8\nSTGCN\n1.003\n0.004"
  },
  {
    "objectID": "posts/2_research/2023-05-25-GCONVGRU_simulation_table_reshape.html#random-4",
    "href": "posts/2_research/2023-05-25-GCONVGRU_simulation_table_reshape.html#random-4",
    "title": "GConvGRU and GNAR_Simulation Tables_reshape",
    "section": "Random",
    "text": "Random\n\npd.merge(data.query(\"dataset=='windmillsmall' and mtype=='rand'\").groupby(['mrate','lags','method'])['mse'].mean().reset_index(),\n         data.query(\"dataset=='windmillsmall' and mtype=='rand'\").groupby(['mrate','lags','method'])['mse'].std().reset_index(),\n         on=['method','mrate','lags']).rename(columns={'mse_x':'mean','mse_y':'std'}).round(3)\n\n\n\n\n\n\n\n\nmrate\nlags\nmethod\nmean\nstd\n\n\n\n\n0\n0.7\n8\nIT-STGCN\n1.194\n0.042\n\n\n1\n0.7\n8\nSTGCN\n1.662\n0.073\n\n\n\n\n\n\n\n\ndata.query(\"dataset=='windmillsmall' and mtype=='rand' and mrate==0.7 and method == 'STGCN'\").sort_values('mse')[:5]\n\n\n\n\n\n\n\n\ndataset\nmethod\nmrate\nmtype\nlags\nnof_filters\ninter_method\nepoch\nmse\ncalculation_time\n\n\n\n\n20\nwindmillsmall\nSTGCN\n0.7\nrand\n8\n12\nlinear\n50\n1.491985\n4189.720611\n\n\n24\nwindmillsmall\nSTGCN\n0.7\nrand\n8\n12\nlinear\n50\n1.526696\n4184.767751\n\n\n96\nwindmillsmall\nSTGCN\n0.7\nrand\n8\n12\nlinear\n50\n1.548946\n4182.209651\n\n\n82\nwindmillsmall\nSTGCN\n0.7\nrand\n8\n12\nlinear\n50\n1.574623\n3112.238717\n\n\n108\nwindmillsmall\nSTGCN\n0.7\nrand\n8\n12\nlinear\n50\n1.577491\n4354.155174\n\n\n\n\n\n\n\n\ndata.query(\"dataset=='windmillsmall' and mtype=='rand' and mrate==0.7 and method != 'STGCN'\").sort_values('mse')[:5]\n\n\n\n\n\n\n\n\ndataset\nmethod\nmrate\nmtype\nlags\nnof_filters\ninter_method\nepoch\nmse\ncalculation_time\n\n\n\n\n35\nwindmillsmall\nIT-STGCN\n0.7\nrand\n8\n12\nlinear\n50\n1.118357\n25106.537400\n\n\n13\nwindmillsmall\nIT-STGCN\n0.7\nrand\n8\n12\nlinear\n50\n1.133768\n33006.879024\n\n\n109\nwindmillsmall\nIT-STGCN\n0.7\nrand\n8\n12\nlinear\n50\n1.134867\n43213.207462\n\n\n105\nwindmillsmall\nIT-STGCN\n0.7\nrand\n8\n12\nlinear\n50\n1.148162\n39068.090318\n\n\n91\nwindmillsmall\nIT-STGCN\n0.7\nrand\n8\n12\nlinear\n50\n1.155443\n37083.352598"
  },
  {
    "objectID": "posts/2_research/2023-05-25-GCONVGRU_simulation_table_reshape.html#block-4",
    "href": "posts/2_research/2023-05-25-GCONVGRU_simulation_table_reshape.html#block-4",
    "title": "GConvGRU and GNAR_Simulation Tables_reshape",
    "section": "Block",
    "text": "Block\n\npd.merge(data.query(\"dataset=='windmillsmall' and mtype=='block'\").groupby(['mrate','lags','method'])['mse'].mean().reset_index(),\n         data.query(\"dataset=='windmillsmall' and mtype=='block'\").groupby(['mrate','lags','method'])['mse'].std().reset_index(),\n         on=['method','mrate','lags']).rename(columns={'mse_x':'mean','mse_y':'std'}).round(3)\n\n\n\n\n\n\n\n\nmrate\nlags\nmethod\nmean\nstd\n\n\n\n\n0\n0.081\n8\nIT-STGCN\n1.007\n0.005\n\n\n1\n0.081\n8\nSTGCN\n1.008\n0.006"
  },
  {
    "objectID": "posts/2_research/2023-05-25-GCONVGRU_simulation_table_reshape.html#baseline-5",
    "href": "posts/2_research/2023-05-25-GCONVGRU_simulation_table_reshape.html#baseline-5",
    "title": "GConvGRU and GNAR_Simulation Tables_reshape",
    "section": "Baseline",
    "text": "Baseline\n\npd.merge(data.query(\"dataset=='monte' and mrate==0\").groupby(['lags','method'])['mse'].mean().reset_index(),\n         data.query(\"dataset=='monte' and mrate==0\").groupby(['lags','method'])['mse'].std().reset_index(),\n         on=['method','lags']).rename(columns={'mse_x':'mean','mse_y':'std'}).round(3)\n\n\n\n\n\n\n\n\nlags\nmethod\nmean\nstd\n\n\n\n\n0\n4\nIT-STGCN\n0.931\n0.001\n\n\n1\n4\nSTGCN\n0.931\n0.002"
  },
  {
    "objectID": "posts/2_research/2023-05-25-GCONVGRU_simulation_table_reshape.html#random-5",
    "href": "posts/2_research/2023-05-25-GCONVGRU_simulation_table_reshape.html#random-5",
    "title": "GConvGRU and GNAR_Simulation Tables_reshape",
    "section": "Random",
    "text": "Random\n\npd.merge(data.query(\"dataset=='monte' and mtype=='rand'\").groupby(['mrate','lags','inter_method','method'])['mse'].mean().reset_index(),\n         data.query(\"dataset=='monte' and mtype=='rand'\").groupby(['mrate','lags','inter_method','method'])['mse'].std().reset_index(),\n         on=['mrate','inter_method','method','mrate','lags']).rename(columns={'mse_x':'mean','mse_y':'std'})\n\n\n\n\n\n\n\n\nmrate\nlags\ninter_method\nmethod\nmean\nstd\n\n\n\n\n0\n0.3\n4\nnearest\nIT-STGCN\n0.936185\n0.001825\n\n\n1\n0.3\n4\nnearest\nSTGCN\n0.991390\n0.007285\n\n\n2\n0.5\n4\nnearest\nIT-STGCN\n0.942045\n0.002642\n\n\n3\n0.5\n4\nnearest\nSTGCN\n1.149221\n0.017820\n\n\n4\n0.7\n4\nnearest\nIT-STGCN\n1.015221\n0.012403\n\n\n5\n0.7\n4\nnearest\nSTGCN\n1.393108\n0.027555\n\n\n6\n0.8\n4\nnearest\nIT-STGCN\n1.095560\n0.018743\n\n\n7\n0.8\n4\nnearest\nSTGCN\n1.516000\n0.039793"
  },
  {
    "objectID": "posts/2_research/2023-05-25-GCONVGRU_simulation_table_reshape.html#block-5",
    "href": "posts/2_research/2023-05-25-GCONVGRU_simulation_table_reshape.html#block-5",
    "title": "GConvGRU and GNAR_Simulation Tables_reshape",
    "section": "Block",
    "text": "Block\n\npd.merge(data.query(\"dataset=='monte' and mtype=='block'\").groupby(['mrate','lags','inter_method','method'])['mse'].mean().reset_index(),\n         data.query(\"dataset=='monte' and mtype=='block'\").groupby(['mrate','lags','inter_method','method'])['mse'].std().reset_index(),\n         on=['method','mrate','inter_method','lags']).rename(columns={'mse_x':'mean','mse_y':'std'})\n\n\n\n\n\n\n\n\nmrate\nlags\ninter_method\nmethod\nmean\nstd\n\n\n\n\n0\n0.149142\n4\ncubic\nIT-STGCN\n1.022866\n0.021048\n\n\n1\n0.149142\n4\ncubic\nSTGCN\n1.028363\n0.031275\n\n\n2\n0.149142\n4\nlinear\nIT-STGCN\n0.930156\n0.001956\n\n\n3\n0.149142\n4\nlinear\nSTGCN\n0.934719\n0.004724\n\n\n4\n0.149142\n4\nnearest\nIT-STGCN\n0.931785\n0.002158\n\n\n5\n0.149142\n4\nnearest\nSTGCN\n0.934596\n0.003562"
  },
  {
    "objectID": "posts/2_research/2023-01-26-ESTGCN_WIKI_DATA.html",
    "href": "posts/2_research/2023-01-26-ESTGCN_WIKI_DATA.html",
    "title": "Class of Method(WikiMath) lag 4",
    "section": "",
    "text": "ST-GCN Dataset WikiMathsDatasetLoader"
  },
  {
    "objectID": "posts/2_research/2023-01-26-ESTGCN_WIKI_DATA.html#train",
    "href": "posts/2_research/2023-01-26-ESTGCN_WIKI_DATA.html#train",
    "title": "Class of Method(WikiMath) lag 4",
    "section": "Train",
    "text": "Train\n\ndata_train=[]\nfor time, snapshot in enumerate(train_dataset):\n    data_train.append([time,snapshot])\n\n\ndata_train[0][1].x.shape,data_train[0][1].y.shape,data_train[0][1].edge_index.shape,data_train[0][1].edge_attr.shape\n\n(torch.Size([1068, 4]),\n torch.Size([1068]),\n torch.Size([2, 27079]),\n torch.Size([27079]))\n\n\n\ntime\n\n580\n\n\n\nT_train = time\nN = len(data_train[0][1].x)\n\n\nedge_index = data_train[0][1].edge_index\nedge_attr = data_train[0][1].edge_attr\n\n\nx_train = []\nfor i in range(time):\n    x_train.append(data_train[i][1].x)\n\n\ndata_tensor = torch.Tensor()\n# Iterate over the data points of the dataset\nfor i in x_train:\n    # Concatenate the data point to the tensor\n    data_tensor = torch.cat((data_tensor, i), dim=0)\nx_train = data_tensor.reshape(time,1068,-1)\nx_train.shape\n\ntorch.Size([580, 1068, 4])\n\n\n\ny_train = []\nfor i in range(time):\n    y_train.append(data_train[i][1].y)\n\n\ndata_tensor = torch.Tensor()\n# Iterate over the data points of the dataset\nfor i in y_train:\n    # Concatenate the data point to the tensor\n    data_tensor = torch.cat((data_tensor, i), dim=0)\ny_train = data_tensor.reshape(time,1068)\ny_train.shape\n\ntorch.Size([580, 1068])\n\n\n\nx_train.shape, y_train.shape\n\n(torch.Size([580, 1068, 4]), torch.Size([580, 1068]))"
  },
  {
    "objectID": "posts/2_research/2023-01-26-ESTGCN_WIKI_DATA.html#test",
    "href": "posts/2_research/2023-01-26-ESTGCN_WIKI_DATA.html#test",
    "title": "Class of Method(WikiMath) lag 4",
    "section": "Test",
    "text": "Test\n\ndata_test=[]\nfor time, snapshot in enumerate(test_dataset):\n    data_test.append([time,snapshot])\n\n\ndata_test[0][1].x.shape,data_test[0][1].y.shape,data_test[0][1].edge_index.shape,data_test[0][1].edge_attr.shape\n\n(torch.Size([1068, 4]),\n torch.Size([1068]),\n torch.Size([2, 27079]),\n torch.Size([27079]))\n\n\n\ntime\n\n145\n\n\n\nT_test = time\n\n\nx_test = []\nfor i in range(time):\n    x_test.append(data_test[i][1].x)\n\n\ndata_tensor = torch.Tensor()\n# Iterate over the data points of the dataset\nfor i in x_test:\n    # Concatenate the data point to the tensor\n    data_tensor = torch.cat((data_tensor, i), dim=0)\nx_test = data_tensor.reshape(time,1068,-1)\nx_test.shape\n\ntorch.Size([145, 1068, 4])\n\n\n\ny_test = []\nfor i in range(time):\n    y_test.append(data_test[i][1].y)\n\n\ndata_tensor = torch.Tensor()\n# Iterate over the data points of the dataset\nfor i in y_test:\n    # Concatenate the data point to the tensor\n    data_tensor = torch.cat((data_tensor, i), dim=0)\ny_test = data_tensor.reshape(time,1068)\ny_test.shape\n\ntorch.Size([145, 1068])\n\n\n\nx_test.shape, y_test.shape\n\n(torch.Size([145, 1068, 4]), torch.Size([145, 1068]))"
  },
  {
    "objectID": "posts/2_research/2023-01-26-ESTGCN_WIKI_DATA.html#시나리오1-baseline",
    "href": "posts/2_research/2023-01-26-ESTGCN_WIKI_DATA.html#시나리오1-baseline",
    "title": "Class of Method(WikiMath) lag 4",
    "section": "시나리오1 (Baseline)",
    "text": "시나리오1 (Baseline)\n시나리오1\n\nmissing rate: 0%\n보간방법: None\n\n\nSTGCN 으로 적합 + 예측\n\nX = torch.tensor(np.stack([x_train_f[i:(T_train-4+i),:] for i in range(4)],axis = -1)).float()\ny = x_train_f[4:T_train,:].reshape(T_train-4,N,-1).float()\n\n\nXX = x_test\nyy = y_test\n\n\nnet = RecurrentGCN(node_features=4, filters=4)\noptimizer = torch.optim.Adam(net.parameters(), lr=0.01)\nnet.train()\nfor epoch in tqdm(range(50)):\n    for time, (xt,yt) in enumerate(zip(X,y)):\n        yt_hat = net(xt, edge_index, edge_attr)\n        cost = torch.mean((yt_hat-yt)**2)\n        cost.backward()\n        optimizer.step()\n        optimizer.zero_grad()\n\n100%|██████████| 50/50 [05:47&lt;00:00,  6.96s/it]\n\n\n\nyhat = torch.stack([net(xt, edge_index, edge_attr) for xt in X]).detach().numpy()\nyyhat = torch.stack([net(xt, edge_index, edge_attr) for xt in XX]).detach().numpy()\n\n\nstgcn_train = yhat.squeeze() # stgcn은 stgcn에 의한 적합결과를 의미함\nstgcn_test = yyhat.squeeze() \n\n\ntrain_mse_eachnode_stgcn = (((y.squeeze()-yhat.squeeze()).squeeze())**2).mean(axis=0)\ntrain_mse_total_stgcn = (((y.squeeze()-yhat.squeeze()).squeeze())**2).mean()\ntest_mse_eachnode_stgcn = (((yy-yyhat.squeeze()).squeeze())**2).mean(axis=0)\ntest_mse_total_stgcn = (((yy-yyhat.squeeze()).squeeze())**2).mean()\n\n\n\nGNAR 으로 적합 + 예측\n-\n\n%load_ext rpy2.ipython\n\n\n%%R\nlibrary(GNAR)\nlibrary(igraph)\n\nR[write to console]: Loading required package: igraph\n\nR[write to console]: \nAttaching package: ‘igraph’\n\n\nR[write to console]: The following objects are masked from ‘package:stats’:\n\n    decompose, spectrum\n\n\nR[write to console]: The following object is masked from ‘package:base’:\n\n    union\n\n\nR[write to console]: Loading required package: wordcloud\n\nR[write to console]: Loading required package: RColorBrewer\n\n\n\n\nEdge = np.array(edge_index)\nX_gnar = np.array(x_train_f)\n\n\nw=np.zeros((1068,1068))\n\n\nfor k in range(27079):\n    w[edge_index[0][k],edge_index[1][k]] = 1\n\n\n%R -i X_gnar\n%R -i w\n%R -i T_test\n\n\n%%R\nwikiNet &lt;- matrixtoGNAR(w)\n\n\n%%R\nanswer &lt;- GNARfit(vts = X_gnar, net = wikiNet, alphaOrder = 4, betaOrder = c(1,1,1,1))\nprediction &lt;- predict(answer,n.ahead=T_test)\n\n\n%%R\ngnar_train &lt;- residuals(answer)\ngnar_test &lt;- prediction\n\n\n%R -o gnar_train\n%R -o gnar_test\n\n\ntrain_mse_eachnode_gnar = (gnar_train**2).mean(axis=0)\ntrain_mse_total_gnar = (gnar_train**2).mean()\ntest_mse_eachnode_gnar = ((yy-gnar_test)**2).mean(axis=0)\ntest_mse_total_gnar = ((yy-gnar_test)**2).mean()\n\n\n\n결과시각화\n\nfig = plot(x_train_f[:,:5],'--o',color='gray',label='complete data',alpha=0.2,h=5)\nfig = plot_add(fig,x_test_f[:,:5],'--o',color='gray',label='complete data',alpha=0.2,t=range(581,729))\nax = fig.get_axes()\nfor i,a in enumerate(ax):     \n    a.set_title('node{0} \\n STGCN: mse(train) = {1:.2f}, mse(test) = {2:.2f} \\n GNAR: mse(train) = {3:.2f}, mse(test) = {4:.2f}'.format(i,train_mse_eachnode_stgcn[i],test_mse_eachnode_stgcn[i],train_mse_eachnode_gnar[i],test_mse_eachnode_gnar[i]))\n    a.plot(range(4,580),stgcn_train[:,i],label='STCGCN (train)',color='C0')\n    a.plot(range(583,728),stgcn_test[:,i],label='STCGCN (test)',color='C0')\n    a.plot(range(4,583),gnar_train[:,i],label='GNAR (train)',color='C1')\n    a.plot(range(583,728),gnar_test[:,i],label='GNAR (test)',color='C1')\n    a.legend()\nfig.set_figwidth(14)\nfig.suptitle(\"Scenario1: STGCN \\n missing=0% \\n interpolation=None \\n\\n STGCN: mse(train) = {0:.2f}, mse(test) = {1:.2f} \\n GNAR: mse(train) = {2:.2f}, mse(test) = {3:.2f} \\n\".format(train_mse_total_stgcn,test_mse_total_stgcn,train_mse_total_gnar,test_mse_total_gnar),size=15)\nfig.tight_layout()\nfig"
  },
  {
    "objectID": "posts/2_research/2023-01-26-ESTGCN_WIKI_DATA.html#시나리오2",
    "href": "posts/2_research/2023-01-26-ESTGCN_WIKI_DATA.html#시나리오2",
    "title": "Class of Method(WikiMath) lag 4",
    "section": "시나리오2",
    "text": "시나리오2\n시나리오2\n\nmissing rate: 50%\n보간방법: linear\n\n- 결측치생성 + 보간\n\n_zero = Missing(x_train_f)\n_zero.miss(percent = 0.5)\n_zero.second_linear()\n\n\nmissing_index = _zero.number\ninterpolated_signal = _zero.train_linear\n\n\nfig = plot(x_train_f[:,:5],'--o',color='gray',label='complete data',alpha=0.2)\nfig = plot_add(fig,x_test_f[:,:5],'--o',color='gray',label='complete data',alpha=0.2,t=range(581,729))\nax = fig.get_axes()\nfor i,a in enumerate(ax):     \n    a.plot(missing_index[i],x_train[:,:,0][:,i][missing_index[i]],'xk',label='missing')\n    a.plot(interpolated_signal[:,i],'-',color='gray',label='linear interpolation')\n    a.legend()\nfig.set_figwidth(15)\nfig\n\n\n\n\n\n\n\n\n\nSTGCN 으로 적합 + 예측\nimport numpy as np\n\nT= 100\nN= 5\nlag =4 \n\nsignal=np.arange(T*N).reshape(T,N)\n\nX= np.stack([signal[i:(T-lag+i),:] for i in range(lag)],axis=-1)\nX.shape\n\ny=signal[lag:].reshape(T-lag,N,1)\ny.shape\n\nX = torch.tensor(np.stack([interpolated_signal[i:(T_train-4+i),:] for i in range(4)],axis = -1)).float()\ny = torch.tensor(interpolated_signal[4:,:]).float()\n\n\nXX = x_test\nyy = y_test\n\n\nnet = RecurrentGCN(node_features=4, filters=4)\noptimizer = torch.optim.Adam(net.parameters(), lr=0.01)\nnet.train()\nfor epoch in tqdm(range(50)):\n    for time, (xt,yt) in enumerate(zip(X,y)):\n        yt_hat = net(xt, edge_index, edge_attr)\n        cost = torch.mean((yt_hat-yt)**2)\n        cost.backward()\n        optimizer.step()\n        optimizer.zero_grad()\n\n100%|██████████| 50/50 [06:23&lt;00:00,  7.67s/it]\n\n\n\nyhat = torch.stack([net(xt, edge_index, edge_attr) for xt in X]).detach().numpy()\nyyhat = torch.stack([net(xt, edge_index, edge_attr) for xt in XX]).detach().numpy()\n\n\nreal_y = y_train[4:,:]\n\ntrain_mse_eachnode_stgcn = (((real_y-yhat.squeeze()).squeeze())**2).mean(axis=0)\ntrain_mse_total_stgcn = (((real_y-yhat.squeeze()).squeeze())**2).mean()\ntest_mse_eachnode_stgcn = (((yy-yyhat.squeeze()).squeeze())**2).mean(axis=0)\ntest_mse_total_stgcn = (((yy-yyhat.squeeze()).squeeze())**2).mean()\n\n\nstgcn_train = yhat.squeeze() # stgcn은 stgcn에 의한 적합결과를 의미함\nstgcn_test = yyhat.squeeze() \n\n\n\nESTGCN 으로 적합 + 예측\n\nX = torch.tensor(np.stack([interpolated_signal[i:(T_train-4+i),:] for i in range(4)],axis = -1)).float()\ny = torch.tensor(interpolated_signal[4:,:]).float()\n\n\nXX = x_test\nyy = y_test\n\n- ESTGCN\n\nnet = RecurrentGCN(node_features=4, filters=4)\noptimizer = torch.optim.Adam(net.parameters(), lr=0.01)\nnet.train()\nsignal = interpolated_signal.copy()\nfor epoch in tqdm(range(50)):\n    signal = update_from_freq_domain(signal,missing_index)\n    X = torch.tensor(np.stack([signal[i:(T_train+i),:] for i in range(4)],axis = -1)).reshape(T_train,N,4).float()\n    y = torch.tensor(signal).reshape(-1,N,1).float()[4:,:,:]\n    for time, (xt,yt) in enumerate(zip(X,y)):        \n        yt_hat = net(xt, edge_index, edge_attr)\n        cost = torch.mean((yt_hat-yt)**2)\n        cost.backward()\n        optimizer.step()\n        optimizer.zero_grad()\n    signal = torch.concat([X[:-1,:,0], X[-1,:,:].T, yt_hat.detach().reshape(1,-1)]).squeeze()\n\n100%|██████████| 50/50 [07:11&lt;00:00,  8.63s/it]\n\n\n- ESTGCN\n\nyhat = torch.stack([net(xt, edge_index, edge_attr) for xt in X]).detach().numpy()\nyyhat = torch.stack([net(xt, edge_index, edge_attr) for xt in XX]).detach().numpy()\n\n\ny_train.shape,T_train\n\n(torch.Size([580, 1068]), 580)\n\n\n\nreal_y = y_train\n\ntrain_mse_eachnode_estgcn = (((real_y-yhat.squeeze()).squeeze())**2).mean(axis=0)\ntrain_mse_total_estgcn = (((real_y-yhat.squeeze()).squeeze())**2).mean()\ntest_mse_eachnode_estgcn = (((yy-yyhat.squeeze()).squeeze())**2).mean(axis=0)\ntest_mse_total_estgcn = (((yy-yyhat.squeeze()).squeeze())**2).mean()\n\n\nestgcn_train = yhat.squeeze() # stgcn은 stgcn에 의한 적합결과를 의미함\nestgcn_test = yyhat.squeeze() \n\n\n\nGNAR 으로 적합 + 예측\n-\n\nX_gnar = np.array(torch.concat([X[:-1,:,0], x_train[-1,:,:].T]))\nEdge = np.array(edge_index)\n\n\nw=np.zeros((1068,1068))\n\n\nfor k in range(27079):\n    w[edge_index[0][k],edge_index[1][k]] = 1\n\n\n%R -i X_gnar\n%R -i w\n%R -i T_test\n\n\n%%R\nwikiNet &lt;- matrixtoGNAR(w)\n\n\n%%R\nanswer &lt;- GNARfit(vts = X_gnar, net = wikiNet, alphaOrder = 4, betaOrder = c(1,1,1,1))\nprediction &lt;- predict(answer,n.ahead=T_test)\n\n\n%%R\ngnar_train &lt;- residuals(answer)\ngnar_test &lt;- prediction\n\n\n%R -o gnar_train\n%R -o gnar_test\n\n\ntrain_mse_eachnode_gnar = (gnar_train**2).mean(axis=0)\ntrain_mse_total_gnar = (gnar_train**2).mean()\ntest_mse_eachnode_gnar = ((y_test-gnar_test)**2).mean(axis=0)\ntest_mse_total_gnar = ((y_test-gnar_test)**2).mean()\n\n\n\n결과시각화\n\nfig = plot(x_train_f[:,:5],'--o',color='gray',label='complete data',alpha=0.2,h=5)\nfig = plot_add(fig,x_test_f[:,:5],'--o',color='gray',label='complete data',alpha=0.2,t=range(581,729))\nax = fig.get_axes()\nfor i,a in enumerate(ax):     \n    a.set_title('node{0} \\n STGCN: mse(train) = {1:.2f}, mse(test) = {2:.2f} \\n ESTGCN: mse(train) = {3:.2f}, mse(test) = {4:.2f}\\n GNAR: mse(train) = {5:.2f}, mse(test) = {6:.2f}'.format(i,train_mse_eachnode_stgcn[i],test_mse_eachnode_stgcn[i],train_mse_eachnode_estgcn[i],test_mse_eachnode_estgcn[i],train_mse_eachnode_gnar[i],test_mse_eachnode_gnar[i]))\n    a.plot(missing_index[i],x_train[missing_index[i],i,0],'xk',label='missing')\n    a.plot(interpolated_signal[:,i],'-',color='gray',label='linear interpolation')\n    a.plot(range(4,580),stgcn_train.squeeze()[:,i],'--.',label='STCGCN (train)',color='C0')\n    a.plot(range(583,728),stgcn_test.squeeze()[:,i],'--.',label='STCGCN (test)',color='C0')\n    a.plot(range(4,584),estgcn_train.squeeze()[:,i],label='ESTCGCN (train)',color='C1')\n    a.plot(range(582,727),estgcn_test.squeeze()[:,i],label='ESTCGCN (test)',color='C1')\n    a.plot(range(4,583),gnar_train.squeeze()[:,i],label='GNAR (train)',color='C2')\n    a.plot(range(583,728),gnar_test.squeeze()[:,i],label='GNAR (test)',color='C2')\n    \n    a.legend()\nfig.set_figwidth(14)\nfig.suptitle(\"Scenario2: \\n missing=50% \\n interpolation=linear \\n\\n STGCN: mse(train) = {0:.2f}, mse(test) = {1:.2f} \\n ESTGCN: mse(train) = {2:.2f}, mse(test) = {3:.2f} \\n GNAR: mse(train) = {4:.2f}, mse(test) = {5:.2f} \\n\".format(train_mse_total_stgcn,test_mse_total_stgcn,train_mse_total_estgcn,test_mse_total_estgcn,train_mse_total_gnar,test_mse_total_gnar),size=15)\nfig.tight_layout()\nfig"
  },
  {
    "objectID": "posts/2_research/2023-01-26-ESTGCN_WIKI_DATA.html#시나리오3",
    "href": "posts/2_research/2023-01-26-ESTGCN_WIKI_DATA.html#시나리오3",
    "title": "Class of Method(WikiMath) lag 4",
    "section": "시나리오3",
    "text": "시나리오3\n시나리오3\n\nmissing rate: 80%\n보간방법: linear\n\n- 결측치생성 + 보간\n\n_zero = Missing(x_train_f)\n_zero.miss(percent = 0.8)\n_zero.second_linear()\n\n\nmissing_index = _zero.number\ninterpolated_signal = _zero.train_linear\n\n\nfig = plot(x_train_f[:,:5],'--o',color='gray',label='complete data',alpha=0.2,h=5)\nfig = plot_add(fig,x_test_f[:,:5],'--o',color='gray',label='complete data',alpha=0.2,t=range(581,729))\nax = fig.get_axes()\nfor i,a in enumerate(ax):     \n    a.plot(missing_index[i],x_train_f[:,i][missing_index[i]],'xk',label='missing')\n    a.plot(interpolated_signal[:,i],'-',color='gray',label='linear interpolation')\n    a.legend()\nfig.set_figwidth(15)\nfig\n\n\n\n\n\n\n\n\n\nSTGCN 으로 적합 + 예측\n\nX = torch.tensor(np.stack([interpolated_signal[i:(T_train-4+i),:] for i in range(4)],axis = -1)).float()\ny = torch.tensor(interpolated_signal[4:,:]).float()\n\n\nXX = x_test\nyy = y_test\n\n\nnet = RecurrentGCN(node_features=4, filters=4)\noptimizer = torch.optim.Adam(net.parameters(), lr=0.01)\nnet.train()\nfor epoch in tqdm(range(50)):\n    for time, (xt,yt) in enumerate(zip(X,y)):\n        yt_hat = net(xt, edge_index, edge_attr)\n        cost = torch.mean((yt_hat-yt)**2)\n        cost.backward()\n        optimizer.step()\n        optimizer.zero_grad()\n\n100%|██████████| 50/50 [06:40&lt;00:00,  8.01s/it]\n\n\n\nyhat = torch.stack([net(xt, edge_index, edge_attr) for xt in X]).detach().numpy()\nyyhat = torch.stack([net(xt, edge_index, edge_attr) for xt in XX]).detach().numpy()\n\n\nreal_y = y_train[4:,:]\n\ntrain_mse_eachnode_stgcn = (((real_y-yhat.squeeze()).squeeze())**2).mean(axis=0)\ntrain_mse_total_stgcn = (((real_y-yhat.squeeze()).squeeze())**2).mean()\ntest_mse_eachnode_stgcn = (((yy-yyhat.squeeze()).squeeze())**2).mean(axis=0)\ntest_mse_total_stgcn = (((yy-yyhat.squeeze()).squeeze())**2).mean()\n\n\nstgcn_train = yhat.squeeze() # stgcn은 stgcn에 의한 적합결과를 의미함\nstgcn_test = yyhat.squeeze() \n\n\n\nESTGCN 으로 적합 + 예측\n\nX = torch.tensor(np.stack([interpolated_signal[i:(T_train-4+i),:] for i in range(4)],axis = -1)).float()\ny = torch.tensor(interpolated_signal[4:,:]).float()\n\n\nXX = x_test\nyy = y_test\n\n- ESTGCN\n\nnet = RecurrentGCN(node_features=4, filters=4)\noptimizer = torch.optim.Adam(net.parameters(), lr=0.01)\nnet.train()\nsignal = interpolated_signal.copy()\nfor epoch in tqdm(range(50)):\n    signal = update_from_freq_domain(signal,missing_index)\n    X = torch.tensor(np.stack([signal[i:(T_train+i),:] for i in range(4)],axis = -1)).reshape(T_train,N,4).float()\n    y = torch.tensor(signal).reshape(-1,N,1).float()[4:,:,:]\n    for time, (xt,yt) in enumerate(zip(X,y)):        \n        yt_hat = net(xt, edge_index, edge_attr)\n        cost = torch.mean((yt_hat-yt)**2)\n        cost.backward()\n        optimizer.step()\n        optimizer.zero_grad()\n    signal = torch.concat([X[:-1,:,0], X[-1,:,:].T, yt_hat.detach().reshape(1,-1)]).squeeze()\n\n100%|██████████| 50/50 [07:18&lt;00:00,  8.77s/it]\n\n\n\nyhat = torch.stack([net(xt, edge_index, edge_attr) for xt in X]).detach().numpy()\nyyhat = torch.stack([net(xt, edge_index, edge_attr) for xt in XX]).detach().numpy()\n\n\nreal_y = y_train\n\ntrain_mse_eachnode_estgcn = (((real_y-yhat.squeeze()).squeeze())**2).mean(axis=0)\ntrain_mse_total_estgcn = (((real_y-yhat.squeeze()).squeeze())**2).mean()\ntest_mse_eachnode_estgcn = (((yy-yyhat.squeeze()).squeeze())**2).mean(axis=0)\ntest_mse_total_estgcn = (((yy-yyhat.squeeze()).squeeze())**2).mean()\n\n\nestgcn_train = yhat.squeeze() # stgcn은 stgcn에 의한 적합결과를 의미함\nestgcn_test = yyhat.squeeze() \n\n\n\nGNAR 으로 적합 + 예측\n-\n\nX_gnar = np.array(torch.concat([X[:-1,:,0], x_train[-1,:,:].T]))\nEdge = np.array(edge_index)\n\n\nw=np.zeros((1068,1068))\n\n\nfor k in range(27079):\n    w[edge_index[0][k],edge_index[1][k]] = 1\n\n\n%R -i X_gnar\n%R -i w\n%R -i T_test\n\n\n%%R\nwikiNet &lt;- matrixtoGNAR(w)\n\n\n%%R\nanswer &lt;- GNARfit(vts = X_gnar, net = wikiNet, alphaOrder = 4, betaOrder = c(1,1,1,1))\nprediction &lt;- predict(answer,n.ahead=T_test)\n\n\n%%R\ngnar_train &lt;- residuals(answer)\ngnar_test &lt;- prediction\n\n\n%R -o gnar_train\n%R -o gnar_test\n\n\ntrain_mse_eachnode_gnar = (gnar_train**2).mean(axis=0)\ntrain_mse_total_gnar = (gnar_train**2).mean()\ntest_mse_eachnode_gnar = ((y_test-gnar_test)**2).mean(axis=0)\ntest_mse_total_gnar = ((y_test-gnar_test)**2).mean()\n\n\n\n결과시각화\n\nfig = plot(x_train_f[:,:5],'--o',color='gray',label='complete data',alpha=0.2,h=5)\nfig = plot_add(fig,x_test_f[:,:5],'--o',color='gray',label='complete data',alpha=0.2,t=range(581,729))\nax = fig.get_axes()\nfor i,a in enumerate(ax):     \n    a.set_title('node{0} \\n STGCN: mse(train) = {1:.2f}, mse(test) = {2:.2f} \\n ESTGCN: mse(train) = {3:.2f}, mse(test) = {4:.2f}\\n GNAR: mse(train) = {5:.2f}, mse(test) = {6:.2f}'.format(i,train_mse_eachnode_stgcn[i],test_mse_eachnode_stgcn[i],train_mse_eachnode_estgcn[i],test_mse_eachnode_estgcn[i],train_mse_eachnode_gnar[i],test_mse_eachnode_gnar[i]))\n    a.plot(missing_index[i],x_train[missing_index[i],i,0],'xk',label='missing')\n    a.plot(interpolated_signal[:,i],'-',color='gray',label='linear interpolation')\n    a.plot(range(4,580),stgcn_train.squeeze()[:,i],'--.',label='STCGCN (train)',color='C0')\n    a.plot(range(583,728),stgcn_test.squeeze()[:,i],'--.',label='STCGCN (test)',color='C0')\n    a.plot(range(4,584),estgcn_train.squeeze()[:,i],label='ESTCGCN (train)',color='C1')\n    a.plot(range(582,727),estgcn_test.squeeze()[:,i],label='ESTCGCN (test)',color='C1')\n    a.plot(range(4,583),gnar_train.squeeze()[:,i],label='GNAR (train)',color='C2')\n    a.plot(range(583,728),gnar_test.squeeze()[:,i],label='GNAR (test)',color='C2')\n    \n    a.legend()\nfig.set_figwidth(14)\nfig.suptitle(\"Scenario2: \\n missing=50% \\n interpolation=linear \\n\\n STGCN: mse(train) = {0:.2f}, mse(test) = {1:.2f} \\n ESTGCN: mse(train) = {2:.2f}, mse(test) = {3:.2f} \\n GNAR: mse(train) = {4:.2f}, mse(test) = {5:.2f} \\n\".format(train_mse_total_stgcn,test_mse_total_stgcn,train_mse_total_estgcn,test_mse_total_estgcn,train_mse_total_gnar,test_mse_total_gnar),size=15)\nfig.tight_layout()\nfig"
  },
  {
    "objectID": "posts/2_research/2023-01-26-ESTGCN_WIKI_DATA.html#시나리오4",
    "href": "posts/2_research/2023-01-26-ESTGCN_WIKI_DATA.html#시나리오4",
    "title": "Class of Method(WikiMath) lag 4",
    "section": "시나리오4",
    "text": "시나리오4\n시나리오4\n\nmissing rate: 30%\n보간방법: linear\n\n- 결측치생성 + 보간\n\n_zero = Missing(x_train_f)\n_zero.miss(percent = 0.3)\n_zero.second_linear()\n\n\nmissing_index = _zero.number\ninterpolated_signal = _zero.train_linear\n\n\nfig = plot(x_train_f[:,:5],'--o',color='gray',label='complete data',alpha=0.2,h=5)\nfig = plot_add(fig,x_test_f[:,:5],'--o',color='gray',label='complete data',alpha=0.2,t=range(581,729))\nax = fig.get_axes()\nfor i,a in enumerate(ax):     \n    a.plot(missing_index[i],x_train_f[:,i][missing_index[i]],'xk',label='missing')\n    a.plot(interpolated_signal[:,i],'-',color='gray',label='linear interpolation')\n    a.legend()\nfig.set_figwidth(15)\nfig\n\n\n\n\n\n\n\n\n\nSTGCN 으로 적합 + 예측\n\nX = torch.tensor(np.stack([interpolated_signal[i:(T_train-4+i),:] for i in range(4)],axis = -1)).float()\ny = torch.tensor(interpolated_signal[4:,:]).float()\n\n\nXX = x_test\nyy = y_test\n\n\nnet = RecurrentGCN(node_features=4, filters=4)\noptimizer = torch.optim.Adam(net.parameters(), lr=0.01)\nnet.train()\nfor epoch in tqdm(range(50)):\n    for time, (xt,yt) in enumerate(zip(X,y)):\n        yt_hat = net(xt, edge_index, edge_attr)\n        cost = torch.mean((yt_hat-yt)**2)\n        cost.backward()\n        optimizer.step()\n        optimizer.zero_grad()\n\n100%|██████████| 50/50 [06:32&lt;00:00,  7.86s/it]\n\n\n\nyhat = torch.stack([net(xt, edge_index, edge_attr) for xt in X]).detach().numpy()\nyyhat = torch.stack([net(xt, edge_index, edge_attr) for xt in XX]).detach().numpy()\n\n\nreal_y = y_train[4:,:]\n\ntrain_mse_eachnode_stgcn = (((real_y-yhat.squeeze()).squeeze())**2).mean(axis=0)\ntrain_mse_total_stgcn = (((real_y-yhat.squeeze()).squeeze())**2).mean()\ntest_mse_eachnode_stgcn = (((yy-yyhat.squeeze()).squeeze())**2).mean(axis=0)\ntest_mse_total_stgcn = (((yy-yyhat.squeeze()).squeeze())**2).mean()\n\n\nstgcn_train = yhat.squeeze() # stgcn은 stgcn에 의한 적합결과를 의미함\nstgcn_test = yyhat.squeeze() \n\n\n\nESTGCN 으로 적합 + 예측\n\nX = torch.tensor(np.stack([interpolated_signal[i:(T_train-4+i),:] for i in range(4)],axis = -1)).float()\ny = torch.tensor(interpolated_signal[4:,:]).float()\n\n\nXX = x_test\nyy = y_test\n\n- ESTGCN\n\nnet = RecurrentGCN(node_features=4, filters=4)\noptimizer = torch.optim.Adam(net.parameters(), lr=0.01)\nnet.train()\nsignal = interpolated_signal.copy()\nfor epoch in tqdm(range(50)):\n    signal = update_from_freq_domain(signal,missing_index)\n    X = torch.tensor(np.stack([signal[i:(T_train+i),:] for i in range(4)],axis = -1)).reshape(T_train,N,4).float()\n    y = torch.tensor(signal).reshape(-1,N,1).float()[4:,:,:]\n    for time, (xt,yt) in enumerate(zip(X,y)):        \n        yt_hat = net(xt, edge_index, edge_attr)\n        cost = torch.mean((yt_hat-yt)**2)\n        cost.backward()\n        optimizer.step()\n        optimizer.zero_grad()\n    signal = torch.concat([X[:-1,:,0], X[-1,:,:].T, yt_hat.detach().reshape(1,-1)]).squeeze()\n\n100%|██████████| 50/50 [07:13&lt;00:00,  8.66s/it]\n\n\n\nyhat = torch.stack([net(xt, edge_index, edge_attr) for xt in X]).detach().numpy()\nyyhat = torch.stack([net(xt, edge_index, edge_attr) for xt in XX]).detach().numpy()\n\n\nreal_y = y_train\n\ntrain_mse_eachnode_estgcn = (((real_y-yhat.squeeze()).squeeze())**2).mean(axis=0)\ntrain_mse_total_estgcn = (((real_y-yhat.squeeze()).squeeze())**2).mean()\ntest_mse_eachnode_estgcn = (((yy-yyhat.squeeze()).squeeze())**2).mean(axis=0)\ntest_mse_total_estgcn = (((yy-yyhat.squeeze()).squeeze())**2).mean()\n\n\nestgcn_train = yhat.squeeze() # stgcn은 stgcn에 의한 적합결과를 의미함\nestgcn_test = yyhat.squeeze() \n\n\n\nGNAR 으로 적합 + 예측\n-\n\nX_gnar = np.array(torch.concat([X[:-1,:,0], x_train[-1,:,:].T]))\nEdge = np.array(edge_index)\n\n\nw=np.zeros((1068,1068))\n\n\nfor k in range(27079):\n    w[edge_index[0][k],edge_index[1][k]] = 1\n\n\n%R -i X_gnar\n%R -i w\n%R -i T_test\n\n\n%%R\nwikiNet &lt;- matrixtoGNAR(w)\n\n\n%%R\nanswer &lt;- GNARfit(vts = X_gnar, net = wikiNet, alphaOrder = 4, betaOrder = c(1,1,1,1))\nprediction &lt;- predict(answer,n.ahead=T_test)\n\n\n%%R\ngnar_train &lt;- residuals(answer)\ngnar_test &lt;- prediction\n\n\n%R -o gnar_train\n%R -o gnar_test\n\n\ntrain_mse_eachnode_gnar = (gnar_train**2).mean(axis=0)\ntrain_mse_total_gnar = (gnar_train**2).mean()\ntest_mse_eachnode_gnar = ((y_test-gnar_test)**2).mean(axis=0)\ntest_mse_total_gnar = ((y_test-gnar_test)**2).mean()\n\n\n\n결과시각화\n\nfig = plot(x_train_f[:,:5],'--o',color='gray',label='complete data',alpha=0.2,h=5)\nfig = plot_add(fig,x_test_f[:,:5],'--o',color='gray',label='complete data',alpha=0.2,t=range(581,729))\nax = fig.get_axes()\nfor i,a in enumerate(ax):     \n    a.set_title('node{0} \\n STGCN: mse(train) = {1:.2f}, mse(test) = {2:.2f} \\n ESTGCN: mse(train) = {3:.2f}, mse(test) = {4:.2f}\\n GNAR: mse(train) = {5:.2f}, mse(test) = {6:.2f}'.format(i,train_mse_eachnode_stgcn[i],test_mse_eachnode_stgcn[i],train_mse_eachnode_estgcn[i],test_mse_eachnode_estgcn[i],train_mse_eachnode_gnar[i],test_mse_eachnode_gnar[i]))\n    a.plot(missing_index[i],x_train[missing_index[i],i,0],'xk',label='missing')\n    a.plot(interpolated_signal[:,i],'-',color='gray',label='linear interpolation')\n    a.plot(range(4,580),stgcn_train.squeeze()[:,i],'--.',label='STCGCN (train)',color='C0')\n    a.plot(range(583,728),stgcn_test.squeeze()[:,i],'--.',label='STCGCN (test)',color='C0')\n    a.plot(range(4,584),estgcn_train.squeeze()[:,i],label='ESTCGCN (train)',color='C1')\n    a.plot(range(582,727),estgcn_test.squeeze()[:,i],label='ESTCGCN (test)',color='C1')\n    a.plot(range(4,583),gnar_train.squeeze()[:,i],label='GNAR (train)',color='C2')\n    a.plot(range(583,728),gnar_test.squeeze()[:,i],label='GNAR (test)',color='C2')\n    \n    a.legend()\nfig.set_figwidth(14)\nfig.suptitle(\"Scenario4: \\n missing=30% \\n interpolation=linear \\n\\n STGCN: mse(train) = {0:.2f}, mse(test) = {1:.2f} \\n ESTGCN: mse(train) = {2:.2f}, mse(test) = {3:.2f} \\n GNAR: mse(train) = {4:.2f}, mse(test) = {5:.2f} \\n\".format(train_mse_total_stgcn,test_mse_total_stgcn,train_mse_total_estgcn,test_mse_total_estgcn,train_mse_total_gnar,test_mse_total_gnar),size=15)\nfig.tight_layout()\nfig"
  },
  {
    "objectID": "posts/2_research/2023-06-06-GCLSTM_simulation_table_reshape.html",
    "href": "posts/2_research/2023-06-06-GCLSTM_simulation_table_reshape.html",
    "title": "GCLSTM_Simulation Tables_reshape",
    "section": "",
    "text": "Simulation Tables"
  },
  {
    "objectID": "posts/2_research/2023-06-06-GCLSTM_simulation_table_reshape.html#baseline",
    "href": "posts/2_research/2023-06-06-GCLSTM_simulation_table_reshape.html#baseline",
    "title": "GCLSTM_Simulation Tables_reshape",
    "section": "Baseline",
    "text": "Baseline\n\npd.merge(data.query(\"dataset=='fivenodes' and mtype!='rand' and mtype!='block'\").groupby(['nof_filters','method','lags'])['mse'].mean().reset_index(),\n         data.query(\"dataset=='fivenodes' and mtype!='rand' and mtype!='block'\").groupby(['nof_filters','method','lags'])['mse'].std().reset_index(),\n         on=['method','nof_filters','lags']).rename(columns={'mse_x':'mean','mse_y':'std'}).round(3)\n\n\n\n\n\n\n\n\nnof_filters\nmethod\nlags\nmean\nstd\n\n\n\n\n0\n4\nIT-STGCN\n2\n1.212\n0.026\n\n\n1\n4\nSTGCN\n2\n1.206\n0.020"
  },
  {
    "objectID": "posts/2_research/2023-06-06-GCLSTM_simulation_table_reshape.html#random",
    "href": "posts/2_research/2023-06-06-GCLSTM_simulation_table_reshape.html#random",
    "title": "GCLSTM_Simulation Tables_reshape",
    "section": "Random",
    "text": "Random\n\npd.merge(data.query(\"dataset=='fivenodes' and mtype=='rand'\").groupby(['mrate','nof_filters','method','lags'])['mse'].mean().reset_index(),\n         data.query(\"dataset=='fivenodes' and mtype=='rand'\").groupby(['mrate','nof_filters','method','lags'])['mse'].std().reset_index(),\n         on=['method','nof_filters','mrate','lags']).rename(columns={'mse_x':'mean','mse_y':'std'}).round(3)\n\n\n\n\n\n\n\n\nmrate\nnof_filters\nmethod\nlags\nmean\nstd\n\n\n\n\n0\n0.3\n4\nIT-STGCN\n2\n1.219\n0.022\n\n\n1\n0.3\n4\nSTGCN\n2\n1.221\n0.029\n\n\n2\n0.5\n4\nIT-STGCN\n2\n1.220\n0.027\n\n\n3\n0.5\n4\nSTGCN\n2\n1.236\n0.035\n\n\n4\n0.6\n4\nIT-STGCN\n2\n1.215\n0.024\n\n\n5\n0.6\n4\nSTGCN\n2\n1.242\n0.041\n\n\n6\n0.7\n4\nIT-STGCN\n2\n1.226\n0.033\n\n\n7\n0.7\n4\nSTGCN\n2\n1.245\n0.052\n\n\n8\n0.8\n4\nIT-STGCN\n2\n1.236\n0.028\n\n\n9\n0.8\n4\nSTGCN\n2\n1.261\n0.048"
  },
  {
    "objectID": "posts/2_research/2023-06-06-GCLSTM_simulation_table_reshape.html#block",
    "href": "posts/2_research/2023-06-06-GCLSTM_simulation_table_reshape.html#block",
    "title": "GCLSTM_Simulation Tables_reshape",
    "section": "Block",
    "text": "Block\n\npd.merge(data.query(\"dataset=='fivenodes' and mtype=='block'\").groupby(['mrate','nof_filters','method'])['mse'].mean().reset_index(),\n         data.query(\"dataset=='fivenodes' and mtype=='block'\").groupby(['mrate','nof_filters','method'])['mse'].std().reset_index(),\n         on=['method','nof_filters','mrate']).rename(columns={'mse_x':'mean','mse_y':'std'}).round(3)\n\n\n\n\n\n\n\n\nmrate\nnof_filters\nmethod\nmean\nstd\n\n\n\n\n0\n0.125\n4\nIT-STGCN\n1.217\n0.023\n\n\n1\n0.125\n4\nSTGCN\n1.246\n0.036"
  },
  {
    "objectID": "posts/2_research/2023-06-06-GCLSTM_simulation_table_reshape.html#baseline-1",
    "href": "posts/2_research/2023-06-06-GCLSTM_simulation_table_reshape.html#baseline-1",
    "title": "GCLSTM_Simulation Tables_reshape",
    "section": "Baseline",
    "text": "Baseline\n\npd.merge(data.query(\"dataset=='chickenpox' and mtype!='rand' and mtype!='block'\").groupby(['nof_filters','method'])['mse'].mean().reset_index(),\n         data.query(\"dataset=='chickenpox' and mtype!='rand' and mtype!='block'\").groupby(['nof_filters','method'])['mse'].std().reset_index(),\n         on=['method','nof_filters']).rename(columns={'mse_x':'mean','mse_y':'std'}).round(3).query(\"nof_filters==16\")\n\n\n\n\n\n\n\n\nnof_filters\nmethod\nmean\nstd\n\n\n\n\n0\n16\nIT-STGCN\n0.878\n0.047\n\n\n1\n16\nSTGCN\n0.892\n0.054"
  },
  {
    "objectID": "posts/2_research/2023-06-06-GCLSTM_simulation_table_reshape.html#random-1",
    "href": "posts/2_research/2023-06-06-GCLSTM_simulation_table_reshape.html#random-1",
    "title": "GCLSTM_Simulation Tables_reshape",
    "section": "Random",
    "text": "Random\n\npd.merge(data.query(\"dataset=='chickenpox' and mtype=='rand'\").groupby(['mrate','inter_method','nof_filters','method'])['mse'].mean().reset_index(),\n         data.query(\"dataset=='chickenpox' and mtype=='rand'\").groupby(['mrate','inter_method','nof_filters','method'])['mse'].std().reset_index(),\n         on=['method','inter_method','mrate','nof_filters']).rename(columns={'mse_x':'mean','mse_y':'std'}).round(3)\n\n\n\n\n\n\n\n\nmrate\ninter_method\nnof_filters\nmethod\nmean\nstd\n\n\n\n\n0\n0.3\nlinear\n16\nIT-STGCN\n0.850\n0.022\n\n\n1\n0.3\nlinear\n16\nSTGCN\n1.050\n0.036\n\n\n2\n0.5\nlinear\n16\nIT-STGCN\n0.899\n0.023\n\n\n3\n0.5\nlinear\n16\nSTGCN\n1.514\n0.050\n\n\n4\n0.6\nlinear\n16\nIT-STGCN\n0.997\n0.030\n\n\n5\n0.6\nlinear\n16\nSTGCN\n1.807\n0.064\n\n\n6\n0.8\nlinear\n16\nIT-STGCN\n1.371\n0.072\n\n\n7\n0.8\nlinear\n16\nSTGCN\n2.172\n0.186"
  },
  {
    "objectID": "posts/2_research/2023-06-06-GCLSTM_simulation_table_reshape.html#block-1",
    "href": "posts/2_research/2023-06-06-GCLSTM_simulation_table_reshape.html#block-1",
    "title": "GCLSTM_Simulation Tables_reshape",
    "section": "Block",
    "text": "Block\n\npd.merge(data.query(\"dataset=='chickenpox' and mtype=='block'\").groupby(['inter_method','mrate','nof_filters','method'])['mse'].mean().reset_index(),\n         data.query(\"dataset=='chickenpox' and mtype=='block'\").groupby(['inter_method','mrate','nof_filters','method'])['mse'].std().reset_index(),\n         on=['method','inter_method','mrate','nof_filters']).rename(columns={'mse_x':'mean','mse_y':'std'})\n\n\n\n\n\n\n\n\ninter_method\nmrate\nnof_filters\nmethod\nmean\nstd\n\n\n\n\n0\nlinear\n0.28777\n16\nIT-STGCN\n0.883365\n0.045500\n\n\n1\nlinear\n0.28777\n16\nSTGCN\n0.889922\n0.033144\n\n\n2\nnearest\n0.28777\n16\nIT-STGCN\n0.901308\n0.054389\n\n\n3\nnearest\n0.28777\n16\nSTGCN\n0.884887\n0.041756"
  },
  {
    "objectID": "posts/2_research/2023-06-06-GCLSTM_simulation_table_reshape.html#baseline-2",
    "href": "posts/2_research/2023-06-06-GCLSTM_simulation_table_reshape.html#baseline-2",
    "title": "GCLSTM_Simulation Tables_reshape",
    "section": "Baseline",
    "text": "Baseline\n\npd.merge(data.query(\"dataset=='pedalme' and mtype!='rand' and mtype!='block'\").groupby(['lags','nof_filters','method'])['mse'].mean().reset_index(),\n         data.query(\"dataset=='pedalme' and mtype!='rand' and mtype!='block'\").groupby(['lags','nof_filters','method'])['mse'].std().reset_index(),\n         on=['method','lags','nof_filters']).rename(columns={'mse_x':'mean','mse_y':'std'}).round(3).query(\"lags==4\")\n\n\n\n\n\n\n\n\nlags\nnof_filters\nmethod\nmean\nstd\n\n\n\n\n0\n4\n4\nIT-STGCN\n1.170\n0.040\n\n\n1\n4\n4\nSTGCN\n1.191\n0.036"
  },
  {
    "objectID": "posts/2_research/2023-06-06-GCLSTM_simulation_table_reshape.html#random-2",
    "href": "posts/2_research/2023-06-06-GCLSTM_simulation_table_reshape.html#random-2",
    "title": "GCLSTM_Simulation Tables_reshape",
    "section": "Random",
    "text": "Random\n\npd.merge(data.query(\"dataset=='pedalme' and mtype=='rand'\").groupby(['mrate','lags','inter_method','method'])['mse'].mean().reset_index(),\n         data.query(\"dataset=='pedalme' and mtype=='rand'\").groupby(['mrate','lags','inter_method','method'])['mse'].std().reset_index(),\n         on=['method','mrate','lags','inter_method']).rename(columns={'mse_x':'mean','mse_y':'std'}).round(3)\n\n\n\n\n\n\n\n\nmrate\nlags\ninter_method\nmethod\nmean\nstd\n\n\n\n\n0\n0.3\n4\nlinear\nIT-STGCN\n1.202\n0.029\n\n\n1\n0.3\n4\nlinear\nSTGCN\n1.267\n0.041\n\n\n2\n0.3\n4\nnearest\nIT-STGCN\n1.211\n0.039\n\n\n3\n0.3\n4\nnearest\nSTGCN\n1.256\n0.038\n\n\n4\n0.6\n4\nlinear\nIT-STGCN\n1.278\n0.040\n\n\n5\n0.6\n4\nlinear\nSTGCN\n1.364\n0.068\n\n\n6\n0.6\n4\nnearest\nIT-STGCN\n1.259\n0.042\n\n\n7\n0.6\n4\nnearest\nSTGCN\n1.365\n0.064"
  },
  {
    "objectID": "posts/2_research/2023-06-06-GCLSTM_simulation_table_reshape.html#block-2",
    "href": "posts/2_research/2023-06-06-GCLSTM_simulation_table_reshape.html#block-2",
    "title": "GCLSTM_Simulation Tables_reshape",
    "section": "Block",
    "text": "Block\n\npd.merge(data.query(\"dataset=='pedalme' and mtype=='block'\").groupby(['mrate','lags','inter_method','method'])['mse'].mean().reset_index(),\n         data.query(\"dataset=='pedalme' and mtype=='block'\").groupby(['mrate','lags','inter_method','method'])['mse'].std().reset_index(),\n         on=['method','mrate','lags','inter_method']).rename(columns={'mse_x':'mean','mse_y':'std'}).round(3).query(\"lags==4\")\n\n\n\n\n\n\n\n\nmrate\nlags\ninter_method\nmethod\nmean\nstd\n\n\n\n\n0\n0.286\n4\nlinear\nIT-STGCN\n1.182\n0.031\n\n\n1\n0.286\n4\nlinear\nSTGCN\n1.211\n0.023\n\n\n2\n0.286\n4\nnearest\nIT-STGCN\n1.195\n0.029\n\n\n3\n0.286\n4\nnearest\nSTGCN\n1.248\n0.019"
  },
  {
    "objectID": "posts/2_research/2023-06-06-GCLSTM_simulation_table_reshape.html#w_st",
    "href": "posts/2_research/2023-06-06-GCLSTM_simulation_table_reshape.html#w_st",
    "title": "GCLSTM_Simulation Tables_reshape",
    "section": "W_st",
    "text": "W_st\n\npd.merge(data_pedal2.query(\"mtype=='rand'\").groupby(['mrate','lags','inter_method','method'])['mse'].mean().reset_index(),\n         data_pedal2.query(\"mtype=='rand'\").groupby(['mrate','lags','inter_method','method'])['mse'].std().reset_index(),\n         on=['method','mrate','lags','inter_method']).rename(columns={'mse_x':'mean','mse_y':'std'}).round(3).query(\"lags==4\")\n\n\n\n\n\n\n\n\nmrate\nlags\ninter_method\nmethod\nmean\nstd\n\n\n\n\n0\n0.3\n4\nlinear\nIT-STGCN\n1.191\n0.041\n\n\n1\n0.3\n4\nlinear\nSTGCN\n1.264\n0.041\n\n\n2\n0.3\n4\nnearest\nIT-STGCN\n1.193\n0.033\n\n\n3\n0.3\n4\nnearest\nSTGCN\n1.250\n0.049\n\n\n4\n0.6\n4\nlinear\nIT-STGCN\n1.260\n0.084\n\n\n5\n0.6\n4\nlinear\nSTGCN\n1.340\n0.059\n\n\n6\n0.6\n4\nnearest\nIT-STGCN\n1.231\n0.044\n\n\n7\n0.6\n4\nnearest\nSTGCN\n1.355\n0.068\n\n\n\n\n\n\n\n\npd.merge(data_pedal2.query(\"mtype=='block'\").groupby(['mrate','lags','inter_method','method'])['mse'].mean().reset_index(),\n         data_pedal2.query(\"mtype=='block'\").groupby(['mrate','lags','inter_method','method'])['mse'].std().reset_index(),\n         on=['method','mrate','lags','inter_method']).rename(columns={'mse_x':'mean','mse_y':'std'}).round(3).query(\"lags==4\")\n\n\n\n\n\n\n\n\nmrate\nlags\ninter_method\nmethod\nmean\nstd\n\n\n\n\n0\n0.286\n4\nlinear\nIT-STGCN\n1.182\n0.045\n\n\n1\n0.286\n4\nlinear\nSTGCN\n1.225\n0.030\n\n\n2\n0.286\n4\nnearest\nIT-STGCN\n1.185\n0.035\n\n\n3\n0.286\n4\nnearest\nSTGCN\n1.249\n0.027"
  },
  {
    "objectID": "posts/2_research/2023-06-06-GCLSTM_simulation_table_reshape.html#baseline-3",
    "href": "posts/2_research/2023-06-06-GCLSTM_simulation_table_reshape.html#baseline-3",
    "title": "GCLSTM_Simulation Tables_reshape",
    "section": "Baseline",
    "text": "Baseline\n\npd.merge(data.query(\"dataset=='wikimath' and mrate==0\").groupby(['lags','nof_filters','method'])['mse'].mean().reset_index(),\n         data.query(\"dataset=='wikimath' and mrate==0\").groupby(['lags','nof_filters','method'])['mse'].std().reset_index(),\n         on=['lags','nof_filters','method']).rename(columns={'mse_x':'mean','mse_y':'std'}).round(3)\n\n\n\n\n\n\n\n\nlags\nnof_filters\nmethod\nmean\nstd\n\n\n\n\n0\n8\n64\nIT-STGCN\n0.643\n0.024\n\n\n1\n8\n64\nSTGCN\n0.645\n0.018"
  },
  {
    "objectID": "posts/2_research/2023-06-06-GCLSTM_simulation_table_reshape.html#random-3",
    "href": "posts/2_research/2023-06-06-GCLSTM_simulation_table_reshape.html#random-3",
    "title": "GCLSTM_Simulation Tables_reshape",
    "section": "Random",
    "text": "Random\n\npd.merge(data.query(\"dataset=='wikimath' and mtype=='rand'\").groupby(['mrate','lags','method'])['mse'].mean().reset_index(),\n         data.query(\"dataset=='wikimath' and mtype=='rand'\").groupby(['mrate','lags','method'])['mse'].std().reset_index(),\n         on=['method','mrate','lags']).rename(columns={'mse_x':'mean','mse_y':'std'}).round(3)\n\n\n\n\n\n\n\n\nmrate\nlags\nmethod\nmean\nstd\n\n\n\n\n0\n0.3\n8\nIT-STGCN\n0.628\n0.020\n\n\n1\n0.3\n8\nSTGCN\n0.674\n0.020\n\n\n2\n0.8\n8\nIT-STGCN\n0.815\n0.058\n\n\n3\n0.8\n8\nSTGCN\n1.407\n0.117"
  },
  {
    "objectID": "posts/2_research/2023-06-06-GCLSTM_simulation_table_reshape.html#block-3",
    "href": "posts/2_research/2023-06-06-GCLSTM_simulation_table_reshape.html#block-3",
    "title": "GCLSTM_Simulation Tables_reshape",
    "section": "Block",
    "text": "Block\n\npd.merge(data.query(\"dataset=='wikimath' and mtype=='block'\").groupby(['mrate','lags','method'])['mse'].mean().reset_index(),\n         data.query(\"dataset=='wikimath' and mtype=='block'\").groupby(['mrate','lags','method'])['mse'].std().reset_index(),\n         on=['method','mrate','lags']).rename(columns={'mse_x':'mean','mse_y':'std'})\n\n\n\n\n\n\n\n\nmrate\nlags\nmethod\nmean\nstd\n\n\n\n\n0\n0.119837\n8\nIT-STGCN\n0.640461\n0.019198\n\n\n1\n0.119837\n8\nSTGCN\n0.637772\n0.012983"
  },
  {
    "objectID": "posts/2_research/2023-06-06-GCLSTM_simulation_table_reshape.html#missing-values-on-the-same-nodes",
    "href": "posts/2_research/2023-06-06-GCLSTM_simulation_table_reshape.html#missing-values-on-the-same-nodes",
    "title": "GCLSTM_Simulation Tables_reshape",
    "section": "missing values on the same nodes",
    "text": "missing values on the same nodes\n\npd.merge(data_wiki_GSO.groupby(['mrate','lags','method'])['mse'].mean().reset_index(),\n        data_wiki_GSO.groupby(['mrate','lags','method'])['mse'].std().reset_index(),\n         on=['method','mrate','lags']).rename(columns={'mse_x':'mean','mse_y':'std'}).round(3)\n\n\n\n\n\n\n\n\nmrate\nlags\nmethod\nmean\nstd\n\n\n\n\n0\n0.512\n8\nIT-STGCN\n0.617\n0.011\n\n\n1\n0.512\n8\nSTGCN\n0.823\n0.048"
  },
  {
    "objectID": "posts/2_research/2023-06-06-GCLSTM_simulation_table_reshape.html#baseline-4",
    "href": "posts/2_research/2023-06-06-GCLSTM_simulation_table_reshape.html#baseline-4",
    "title": "GCLSTM_Simulation Tables_reshape",
    "section": "Baseline",
    "text": "Baseline\n\npd.merge(data.query(\"dataset=='windmillsmall' and mrate==0\").groupby(['lags','method'])['mse'].mean().reset_index(),\n         data.query(\"dataset=='windmillsmall' and mrate==0\").groupby(['lags','method'])['mse'].std().reset_index(),\n         on=['method','lags']).rename(columns={'mse_x':'mean','mse_y':'std'}).round(3)\n\n\n\n\n\n\n\n\nlags\nmethod\nmean\nstd\n\n\n\n\n0\n8\nIT-STGCN\n0.992\n0.011\n\n\n1\n8\nSTGCN\n0.992\n0.009"
  },
  {
    "objectID": "posts/2_research/2023-06-06-GCLSTM_simulation_table_reshape.html#random-4",
    "href": "posts/2_research/2023-06-06-GCLSTM_simulation_table_reshape.html#random-4",
    "title": "GCLSTM_Simulation Tables_reshape",
    "section": "Random",
    "text": "Random\n\npd.merge(data.query(\"dataset=='windmillsmall' and mtype=='rand'\").groupby(['mrate','lags','method'])['mse'].mean().reset_index(),\n         data.query(\"dataset=='windmillsmall' and mtype=='rand'\").groupby(['mrate','lags','method'])['mse'].std().reset_index(),\n         on=['method','mrate','lags']).rename(columns={'mse_x':'mean','mse_y':'std'}).round(3)\n\n\n\n\n\n\n\n\nmrate\nlags\nmethod\nmean\nstd\n\n\n\n\n0\n0.7\n8\nIT-STGCN\n1.116\n0.021\n\n\n1\n0.7\n8\nSTGCN\n1.573\n0.105"
  },
  {
    "objectID": "posts/2_research/2023-06-06-GCLSTM_simulation_table_reshape.html#block-4",
    "href": "posts/2_research/2023-06-06-GCLSTM_simulation_table_reshape.html#block-4",
    "title": "GCLSTM_Simulation Tables_reshape",
    "section": "Block",
    "text": "Block\n\npd.merge(data.query(\"dataset=='windmillsmall' and mtype=='block'\").groupby(['mrate','lags','method'])['mse'].mean().reset_index(),\n         data.query(\"dataset=='windmillsmall' and mtype=='block'\").groupby(['mrate','lags','method'])['mse'].std().reset_index(),\n         on=['method','mrate','lags']).rename(columns={'mse_x':'mean','mse_y':'std'}).round(3)\n\n\n\n\n\n\n\n\nmrate\nlags\nmethod\nmean\nstd\n\n\n\n\n0\n0.081\n8\nIT-STGCN\n0.985\n0.003\n\n\n1\n0.081\n8\nSTGCN\n0.985\n0.002"
  },
  {
    "objectID": "posts/2_research/2023-06-06-GCLSTM_simulation_table_reshape.html#baseline-5",
    "href": "posts/2_research/2023-06-06-GCLSTM_simulation_table_reshape.html#baseline-5",
    "title": "GCLSTM_Simulation Tables_reshape",
    "section": "Baseline",
    "text": "Baseline\n\npd.merge(data.query(\"dataset=='monte' and mrate==0\").groupby(['lags','method'])['mse'].mean().reset_index(),\n         data.query(\"dataset=='monte' and mrate==0\").groupby(['lags','method'])['mse'].std().reset_index(),\n         on=['method','lags']).rename(columns={'mse_x':'mean','mse_y':'std'}).round(3)\n\n\n\n\n\n\n\n\nlags\nmethod\nmean\nstd\n\n\n\n\n0\n4\nIT-STGCN\n0.969\n0.012\n\n\n1\n4\nSTGCN\n0.970\n0.011"
  },
  {
    "objectID": "posts/2_research/2023-06-06-GCLSTM_simulation_table_reshape.html#random-5",
    "href": "posts/2_research/2023-06-06-GCLSTM_simulation_table_reshape.html#random-5",
    "title": "GCLSTM_Simulation Tables_reshape",
    "section": "Random",
    "text": "Random\n\npd.merge(data.query(\"dataset=='monte' and mtype=='rand'\").groupby(['mrate','lags','inter_method','method'])['mse'].mean().reset_index(),\n         data.query(\"dataset=='monte' and mtype=='rand'\").groupby(['mrate','lags','inter_method','method'])['mse'].std().reset_index(),\n         on=['mrate','inter_method','method','mrate','lags']).rename(columns={'mse_x':'mean','mse_y':'std'})\n\n\n\n\n\n\n\n\nmrate\nlags\ninter_method\nmethod\nmean\nstd\n\n\n\n\n0\n0.8\n4\nnearest\nIT-STGCN\n1.031532\n0.028135\n\n\n1\n0.8\n4\nnearest\nSTGCN\n1.140193\n0.061301"
  },
  {
    "objectID": "posts/2_research/2023-06-06-GCLSTM_simulation_table_reshape.html#block-5",
    "href": "posts/2_research/2023-06-06-GCLSTM_simulation_table_reshape.html#block-5",
    "title": "GCLSTM_Simulation Tables_reshape",
    "section": "Block",
    "text": "Block\n\npd.merge(data.query(\"dataset=='monte' and mtype=='block'\").groupby(['mrate','lags','inter_method','method'])['mse'].mean().reset_index(),\n         data.query(\"dataset=='monte' and mtype=='block'\").groupby(['mrate','lags','inter_method','method'])['mse'].std().reset_index(),\n         on=['method','mrate','inter_method','lags']).rename(columns={'mse_x':'mean','mse_y':'std'})\n\n\n\n\n\n\n\n\nmrate\nlags\ninter_method\nmethod\nmean\nstd\n\n\n\n\n0\n0.149142\n4\nnearest\nIT-STGCN\n0.958620\n0.007766\n\n\n1\n0.149142\n4\nnearest\nSTGCN\n0.955762\n0.005461"
  },
  {
    "objectID": "posts/2_research/2023-04-25-note_matrix.html",
    "href": "posts/2_research/2023-04-25-note_matrix.html",
    "title": "Note_weight amatrix",
    "section": "",
    "text": "weight matrix\nimport numpy as np\n- 하이퍼파라메터\nT = 5 # time\nN = 3 # number of nodes\n- wt,ws,f\nwt = np.array([abs(i-j) &lt; 2 for i in range(T) for j in range(T)]).reshape(T,T)*1.0 - np.eye(T)\nwt\n\narray([[0., 1., 0., 0., 0.],\n       [1., 0., 1., 0., 0.],\n       [0., 1., 0., 1., 0.],\n       [0., 0., 1., 0., 1.],\n       [0., 0., 0., 1., 0.]])\nws = np.array([[0,1,1],[1,0,0],[1,0,0]]) + np.eye(N)\nws\n\narray([[1., 1., 1.],\n       [1., 1., 0.],\n       [1., 0., 1.]])\nf = np.random.randn(T,N)\nf\n\narray([[-0.73176413, -0.95155223, -0.00206147],\n       [ 1.39680239,  0.84786031,  0.0152849 ],\n       [ 0.06111198, -1.37704116, -0.28557273],\n       [-0.88306776, -0.65056859,  0.05926914],\n       [ 0.3802191 , -0.76405896,  0.90468637]])\n- f를 펼침\nf_flatten = f.reshape(-1,1)\nf_flatten\n\narray([[-0.73176413],\n       [-0.95155223],\n       [-0.00206147],\n       [ 1.39680239],\n       [ 0.84786031],\n       [ 0.0152849 ],\n       [ 0.06111198],\n       [-1.37704116],\n       [-0.28557273],\n       [-0.88306776],\n       [-0.65056859],\n       [ 0.05926914],\n       [ 0.3802191 ],\n       [-0.76405896],\n       [ 0.90468637]])\n- 펼쳐진 f에 대응하는 W 생성\ndef flatten_weight(ws,wt):\n  N = len(ws)\n  T = len(wt)\n  Is = np.eye(N,N)\n  lst = [[0]*T for t in range(T)]\n  for i in range(T):\n    for j in range(T):\n      if i==j: \n        lst[i][j] = ws \n      elif abs(i-j)==1:\n        lst[i][j] = Is\n      else:\n        lst[i][j] = Is*0\n  return np.concatenate([np.concatenate(l,axis=1) for l in lst],axis=0)\nW_flatten = flatten_weight(ws,wt)\nW_flatten\n\narray([[1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n       [1., 1., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n       [1., 0., 1., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n       [1., 0., 0., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0.],\n       [0., 1., 0., 1., 1., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.],\n       [0., 0., 1., 1., 0., 1., 0., 0., 1., 0., 0., 0., 0., 0., 0.],\n       [0., 0., 0., 1., 0., 0., 1., 1., 1., 1., 0., 0., 0., 0., 0.],\n       [0., 0., 0., 0., 1., 0., 1., 1., 0., 0., 1., 0., 0., 0., 0.],\n       [0., 0., 0., 0., 0., 1., 1., 0., 1., 0., 0., 1., 0., 0., 0.],\n       [0., 0., 0., 0., 0., 0., 1., 0., 0., 1., 1., 1., 1., 0., 0.],\n       [0., 0., 0., 0., 0., 0., 0., 1., 0., 1., 1., 0., 0., 1., 0.],\n       [0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 0., 1., 0., 0., 1.],\n       [0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 1., 1., 1.],\n       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 1., 1., 0.],\n       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 0., 1.]])\n- trim\n임의로 ftrimed_flatten이 f_flatten과 같다고 생각하자.\nftrimed_flatten = f_flatten\nftrimed_flatten\n\narray([[-0.73176413],\n       [-0.95155223],\n       [-0.00206147],\n       [ 1.39680239],\n       [ 0.84786031],\n       [ 0.0152849 ],\n       [ 0.06111198],\n       [-1.37704116],\n       [-0.28557273],\n       [-0.88306776],\n       [-0.65056859],\n       [ 0.05926914],\n       [ 0.3802191 ],\n       [-0.76405896],\n       [ 0.90468637]])\n- ftrimed\nftrimed = ftrimed_flatten.reshape(T,N) \nftrimed\n\narray([[-0.73176413, -0.95155223, -0.00206147],\n       [ 1.39680239,  0.84786031,  0.0152849 ],\n       [ 0.06111198, -1.37704116, -0.28557273],\n       [-0.88306776, -0.65056859,  0.05926914],\n       [ 0.3802191 , -0.76405896,  0.90468637]])\nf # 와 비교.. 잘 reconstructiond 되어씀 \n\narray([[-0.73176413, -0.95155223, -0.00206147],\n       [ 1.39680239,  0.84786031,  0.0152849 ],\n       [ 0.06111198, -1.37704116, -0.28557273],\n       [-0.88306776, -0.65056859,  0.05926914],\n       [ 0.3802191 , -0.76405896,  0.90468637]])"
  },
  {
    "objectID": "posts/2_research/2023-04-25-note_matrix.html#chickenpox",
    "href": "posts/2_research/2023-04-25-note_matrix.html#chickenpox",
    "title": "Note_weight amatrix",
    "section": "Chickenpox",
    "text": "Chickenpox\n\nfrom torch_geometric_temporal.dataset import ChickenpoxDatasetLoader\nloader1 = ChickenpoxDatasetLoader()\n\n\na = loader1.get_dataset(lags=1)\n\ntime,number of nodes\n\nT,N,_ = np.array(a.features).shape\n\n- wt,ws,f\n\nwt = np.zeros((T,T))\nfor i in range(T):\n    for j in range(T):\n        if i==j :\n            wt[i,j] = 0\n        elif np.abs(i-j) &lt;= 1 : \n            wt[i,j] = 1\n\n\nmtr = a.edge_index\nmtr2 = a.edge_weight\n\n\nws = np.zeros((N,N))\nfor i in range(N):\n    for j in range(mtr2.shape[0]):\n        if mtr[0][j] == i :\n            ws[i,mtr[1][j]] = mtr2[j]\n\n\nnp.array(ws).shape\n\n(20, 20)\n\n\n\nf = np.array(a.features).reshape(T,N)\n\n- f를 펼침\n\nf_flatten = f.reshape(-1,1)\nf_flatten\n\narray([[-1.08135724e-03],\n       [-7.11136085e-01],\n       [-3.22808515e+00],\n       ...,\n       [ 4.71099041e-02],\n       [ 2.45684924e+00],\n       [-3.44296107e-01]])\n\n\n- 펼쳐진 f에 대응하는 W 생성\n\ndef flatten_weight(ws,wt):\n  N = len(ws)\n  T = len(wt)\n  Is = np.eye(N,N)\n  lst = [[0]*T for t in range(T)]\n  for i in range(T):\n    for j in range(T):\n      if i==j: \n        lst[i][j] = ws \n      elif abs(i-j)==1:\n        lst[i][j] = Is\n      else:\n        lst[i][j] = Is*0\n  return np.concatenate([np.concatenate(l,axis=1) for l in lst],axis=0)\n\n\nW_flatten = flatten_weight(ws,wt)\nW_flatten\n\narray([[1., 1., 0., ..., 0., 0., 0.],\n       [1., 1., 0., ..., 0., 0., 0.],\n       [0., 0., 1., ..., 0., 0., 0.],\n       ...,\n       [0., 0., 0., ..., 1., 1., 1.],\n       [0., 0., 0., ..., 1., 1., 1.],\n       [0., 0., 0., ..., 1., 1., 1.]])\n\n\n\nnp.save('./weight_st/W_chickenpox.npy', W_flatten)\n\n\nnp.load('./weight_st/W_chickenpox.npy')\n\narray([[1., 1., 0., ..., 0., 0., 0.],\n       [1., 1., 0., ..., 0., 0., 0.],\n       [0., 0., 1., ..., 0., 0., 0.],\n       ...,\n       [0., 0., 0., ..., 1., 1., 1.],\n       [0., 0., 0., ..., 1., 1., 1.],\n       [0., 0., 0., ..., 1., 1., 1.]])\n\n\n\nd = np.array(W_flatten.sum(axis=1))\n\n\nD = np.diag(d)\n\n\nL = np.array(np.diag(1/np.sqrt(d)) @ (D-W_flatten) @ np.diag(1/np.sqrt(d)))\n\n\nlamb, Psi = np.linalg.eigh(L)\n\n\nnp.save('./weight_st/Psi_chickenpox.npy', Psi)\n\n\nnp.load('./weight_st/Psi_chickenpox.npy')\n\narray([[ 1.04115841e-02, -1.47242159e-02,  1.47242532e-02, ...,\n         6.41186713e-04, -4.27546925e-04,  2.13800213e-04],\n       [ 8.23107997e-03, -1.16404883e-02,  1.16404382e-02, ...,\n        -4.77715858e-04,  3.18549731e-04, -1.59296626e-04],\n       [ 8.23107997e-03, -1.16404502e-02,  1.16402861e-02, ...,\n         2.85275946e-04, -1.90235274e-04,  9.51330388e-05],\n       ...,\n       [ 8.23107997e-03,  1.16404565e-02,  1.16403112e-02, ...,\n        -6.65738542e-04, -4.43946869e-04, -2.22009806e-04],\n       [ 1.04115841e-02,  1.47242028e-02,  1.47242009e-02, ...,\n         1.35543431e-04,  9.03781585e-05,  4.51938438e-05],\n       [ 8.23107997e-03,  1.16404680e-02,  1.16403570e-02, ...,\n         5.65995233e-04,  3.77431091e-04,  1.88745843e-04]])\n\n\n- trim\nftrimed_flatten = trim(f_flatten,W_flatten)"
  },
  {
    "objectID": "posts/2_research/2023-04-25-note_matrix.html#pedalme",
    "href": "posts/2_research/2023-04-25-note_matrix.html#pedalme",
    "title": "Note_weight amatrix",
    "section": "Pedalme",
    "text": "Pedalme\n\nfrom torch_geometric_temporal.dataset import PedalMeDatasetLoader\nloader2 = PedalMeDatasetLoader()\n\n\na = loader2.get_dataset(lags=1)\n\ntime,number of nodes\n\nT,N,_ = np.array(a.features).shape\n\n- wt,ws,f\n\nwt = np.zeros((T,T))\nfor i in range(T):\n    for j in range(T):\n        if i==j :\n            wt[i,j] = 0\n        elif np.abs(i-j) &lt;= 1 : \n            wt[i,j] = 1\n\n\nmtr = a.edge_index\nmtr2 = a.edge_weight\n\n\nws = np.zeros((N,N))\nfor i in range(N):\n    for j in range(mtr2.shape[0]):\n        if mtr[0][j] == i :\n            ws[i,mtr[1][j]] = mtr2[j]\n\n\nnp.array(wt).shape\n\n(34, 34)\n\n\n\nnp.array(ws).shape\n\n(15, 15)\n\n\n\n34*15\n\n510\n\n\n\nf = np.array(a.features).reshape(T,N)\n\n- f를 펼침\n\nf_flatten = f.reshape(-1,1)\n# f_flatten\n\n- 펼쳐진 f에 대응하는 W 생성\n\ndef flatten_weight(ws,wt):\n  N = len(ws)\n  T = len(wt)\n  Is = np.eye(N,N)\n  lst = [[0]*T for t in range(T)]\n  for i in range(T):\n    for j in range(T):\n      if i==j: \n        lst[i][j] = ws \n      elif abs(i-j)==1:\n        lst[i][j] = Is\n      else:\n        lst[i][j] = Is*0\n  return np.concatenate([np.concatenate(l,axis=1) for l in lst],axis=0)\n\n\nW_flatten = flatten_weight(ws,wt)\nW_flatten\n\narray([[1.        , 0.42545896, 0.15735536, ..., 0.        , 0.        ,\n        0.        ],\n       [0.42545896, 1.        , 0.06751402, ..., 0.        , 0.        ,\n        0.        ],\n       [0.15735536, 0.06751402, 1.        , ..., 0.        , 0.        ,\n        0.        ],\n       ...,\n       [0.        , 0.        , 0.        , ..., 1.        , 0.07069877,\n        0.06899971],\n       [0.        , 0.        , 0.        , ..., 0.07069877, 1.        ,\n        0.32983841],\n       [0.        , 0.        , 0.        , ..., 0.06899971, 0.32983841,\n        1.        ]])\n\n\n- trim\nftrimed_flatten = trim(f_flatten,W_flatten)\n\nnp.save('./weight_st/W_pedalme.npy', W_flatten)\n\n\nnp.load('./weight_st/W_pedalme.npy')\n\narray([[1.        , 0.42545896, 0.15735536, ..., 0.        , 0.        ,\n        0.        ],\n       [0.42545896, 1.        , 0.06751402, ..., 0.        , 0.        ,\n        0.        ],\n       [0.15735536, 0.06751402, 1.        , ..., 0.        , 0.        ,\n        0.        ],\n       ...,\n       [0.        , 0.        , 0.        , ..., 1.        , 0.07069877,\n        0.06899971],\n       [0.        , 0.        , 0.        , ..., 0.07069877, 1.        ,\n        0.32983841],\n       [0.        , 0.        , 0.        , ..., 0.06899971, 0.32983841,\n        1.        ]])\n\n\n\nd = np.array(W_flatten.sum(axis=1))\n\n\nD = np.diag(d)\n\n\nL = np.array(np.diag(1/np.sqrt(d)) @ (D-W_flatten) @ np.diag(1/np.sqrt(d)))\n\n\nlamb, Psi = np.linalg.eigh(L)\n\n\nPsi.T.shape\n\n(510, 510)\n\n\n\nPsi.shape\n\n(510, 510)\n\n\n\nnp.save('./weight_st/Psi_pedalme.npy', Psi)\n\n\nnp.load('./weight_st/Psi_pedalme.npy')\n\narray([[ 4.61219573e-02, -6.52404719e-02,  6.52791904e-02, ...,\n        -4.33862149e-04, -2.17206920e-04,  8.97548015e-05],\n       [ 4.44297451e-02, -6.28451968e-02,  6.28773329e-02, ...,\n        -2.56832039e-05, -1.49624707e-05,  7.00873447e-06],\n       [ 3.51291880e-02, -4.95907787e-02,  4.93238061e-02, ...,\n        -2.71803598e-02, -1.77647577e-02,  8.79413561e-03],\n       ...,\n       [ 3.75563137e-02,  5.30576275e-02,  5.28917644e-02, ...,\n         5.85830960e-03, -4.16888716e-03, -2.15565030e-03],\n       [ 3.87057680e-02,  5.47153694e-02,  5.46437177e-02, ...,\n         1.08555123e-03, -6.10976014e-04, -2.76608545e-04],\n       [ 4.03127107e-02,  5.70025038e-02,  5.69740769e-02, ...,\n        -2.05662084e-04,  9.91534370e-05,  4.05213281e-05]])"
  },
  {
    "objectID": "posts/2_research/2023-04-25-note_matrix.html#wikimath",
    "href": "posts/2_research/2023-04-25-note_matrix.html#wikimath",
    "title": "Note_weight amatrix",
    "section": "Wikimath",
    "text": "Wikimath\n\nfrom torch_geometric_temporal.dataset import WikiMathsDatasetLoader\nloader3 = WikiMathsDatasetLoader()\n\n\na = loader3.get_dataset(lags=1)\n\ntime,number of nodes\n\nT,N,_ = np.array(a.features).shape\n\n- wt,ws,f\n\nwt = np.zeros((T,T))\nfor i in range(T):\n    for j in range(T):\n        if i==j :\n            wt[i,j] = 0\n        elif np.abs(i-j) &lt;= 1 : \n            wt[i,j] = 1\n\n\nmtr = a.edge_index\nmtr2 = a.edge_weight\n\n\nnp.array(mtr).shape\n\n(2, 27079)\n\n\n\nnp.array(mtr2).shape\n\n(27079,)\n\n\n\nmtr\n\narray([[   0,    0,    0, ..., 1056, 1063, 1065],\n       [   1,    2,    3, ..., 1059, 1064, 1066]])\n\n\n\npd.DataFrame(mtr2).iloc[:,0].unique()\n\narray([ 1,  4,  2,  5,  3,  6,  7,  9,  8, 12, 10, 13, 16, 11])\n\n\n\nws = np.zeros((N,N))\nfor i in range(N):\n    for j in range(mtr2.shape[0]):\n        if mtr[0][j] == i :\n            ws[i,mtr[1][j]] = mtr2[j]\n\n\nnp.array(ws).shape\n\n(1068, 1068)\n\n\n\nf = np.array(a.features).reshape(T,N)\n\n- f를 펼침\n\nf_flatten = f.reshape(-1,1)\n# f_flatten\n\n- 펼쳐진 f에 대응하는 W 생성\n\nN = len(ws)\nT = len(wt)\nIs = np.eye(N,N)\nlst = [[0]*T for t in range(T)]\n\n\ndef flatten_weight(ws,wt):\n  N = len(ws)\n  T = len(wt)\n  Is = np.eye(N,N)\n  lst = [[0]*T for t in range(T)]\n  for i in range(T):\n    for j in range(T):\n      if i==j: \n        lst[i][j] = ws \n      elif abs(i-j)==1:\n        lst[i][j] = Is\n      else:\n        lst[i][j] = Is*0\n  return np.concatenate([np.concatenate(l,axis=1) for l in lst],axis=0)\n\n\nW_flatten = flatten_weight(ws,wt)\nW_flatten\n\n- trim\nftrimed_flatten = trim(f_flatten,W_flatten)\n\nnp.save('./weight_st/W_wikimath.npy', W_flatten)\n\n\nnp.load('./weight_st/W_wikimath.npy')"
  },
  {
    "objectID": "posts/2_research/2023-04-25-note_matrix.html#windmillsmall",
    "href": "posts/2_research/2023-04-25-note_matrix.html#windmillsmall",
    "title": "Note_weight amatrix",
    "section": "Windmillsmall",
    "text": "Windmillsmall\n\nfrom torch_geometric_temporal.dataset import WindmillOutputSmallDatasetLoader\nloader6 = WindmillOutputSmallDatasetLoader()\n\n\na = loader6.get_dataset(lags=1)\n\ntime,number of nodes\n\nT,N,_ = np.array(a.features).shape\n\n- wt,ws,f\n\nwt = np.zeros((T,T))\nfor i in range(T):\n    for j in range(T):\n        if i==j :\n            wt[i,j] = 0\n        elif np.abs(i-j) &lt;= 1 : \n            wt[i,j] = 1\n\n\nmtr = a.edge_index\nmtr2 = a.edge_weight\n\n\nws = np.zeros((N,N))\nfor i in range(N):\n    for j in range(mtr2.shape[0]):\n        if mtr[0][j] == i :\n            ws[i,mtr[1][j]] = mtr2[j]\n\n\nnp.array(ws).shape\n\n(11, 11)\n\n\n\nf = np.array(a.features).reshape(T,N)\n\n- f를 펼침\n\nf_flatten = f.reshape(-1,1)\n# f_flatten\n\n- 펼쳐진 f에 대응하는 W 생성\n\ndef flatten_weight(ws,wt):\n  N = len(ws)\n  T = len(wt)\n  Is = np.eye(N,N)\n  lst = [[0]*T for t in range(T)]\n  for i in range(T):\n    for j in range(T):\n      if i==j: \n        lst[i][j] = ws \n      elif abs(i-j)==1:\n        lst[i][j] = Is\n      else:\n        lst[i][j] = Is*0\n  return np.concatenate([np.concatenate(l,axis=1) for l in lst],axis=0)\n\n\nW_flatten = flatten_weight(ws,wt)\nW_flatten\n\n- trim\nftrimed_flatten = trim(f_flatten,W_flatten)\n\nnp.save('./weight_st/W_windmillsmall.npy', W_flatten)\n\n\nnp.load('./weight_st/W_windmillsmall.npy')"
  },
  {
    "objectID": "posts/2_research/2023-04-05-Simulation_boxplot.html",
    "href": "posts/2_research/2023-04-05-Simulation_boxplot.html",
    "title": "Simulation",
    "section": "",
    "text": "Simulation Study"
  },
  {
    "objectID": "posts/2_research/2023-04-05-Simulation_boxplot.html#baseline",
    "href": "posts/2_research/2023-04-05-Simulation_boxplot.html#baseline",
    "title": "Simulation",
    "section": "Baseline",
    "text": "Baseline\n\ndata.query(\"method!='GNAR' and mrate ==0 \").plot.box(backend='plotly',x='epoch',color='method',y='mse',facet_col='lags',height=800)"
  },
  {
    "objectID": "posts/2_research/2023-04-05-Simulation_boxplot.html#random",
    "href": "posts/2_research/2023-04-05-Simulation_boxplot.html#random",
    "title": "Simulation",
    "section": "random",
    "text": "random\n\ndata.query(\"method!='GNAR' and mtype =='rand' \").plot.box(backend='plotly',x='mrate',color='method',y='mse',facet_col='lags',facet_row='inter_method',height=800)"
  },
  {
    "objectID": "posts/2_research/2023-04-05-Simulation_boxplot.html#block",
    "href": "posts/2_research/2023-04-05-Simulation_boxplot.html#block",
    "title": "Simulation",
    "section": "block",
    "text": "block\n\ndata.query(\"method!='GNAR' and mtype =='block' and inter_method=='cubic' \").plot.box(backend='plotly',x='mrate',color='method',y='mse',facet_col='lags',facet_row='inter_method',height=600)\n\n                                                \n\n\n\ndata.query(\"method!='GNAR' and mtype =='block' and inter_method!='cubic' \").plot.box(backend='plotly',x='mrate',color='method',y='mse',facet_col='lags',facet_row='inter_method',height=600)"
  },
  {
    "objectID": "posts/2_research/2023-04-05-Simulation_boxplot.html#weight-matrix-time-node-고려한-결과",
    "href": "posts/2_research/2023-04-05-Simulation_boxplot.html#weight-matrix-time-node-고려한-결과",
    "title": "Simulation",
    "section": "weight matrix time, node 고려한 결과",
    "text": "weight matrix time, node 고려한 결과\n\ndf1 = pd.read_csv('./simulation_results/2023-04-30_13-00-12.csv')\ndf2 = pd.read_csv('./simulation_results/2023-04-30_13-31-32.csv')\ndf3 = pd.read_csv('./simulation_results/2023-04-30_14-01-49.csv')\ndf4 = pd.read_csv('./simulation_results/2023-04-30_14-31-56.csv')\ndf5 = pd.read_csv('./simulation_results/2023-04-30_15-02-23.csv')\ndf6 = pd.read_csv('./simulation_results/2023-04-30_15-33-03.csv')\ndf7 = pd.read_csv('./simulation_results/2023-04-30_16-07-43.csv')\ndf8 = pd.read_csv('./simulation_results/2023-04-30_16-41-35.csv')\ndf9 = pd.read_csv('./simulation_results/2023-04-30_17-14-51.csv')\ndf10 = pd.read_csv('./simulation_results/2023-04-30_17-49-34.csv')\ndf11 = pd.read_csv('./simulation_results/2023-04-30_18-21-29.csv')\ndf12 = pd.read_csv('./simulation_results/2023-04-30_18-50-24.csv')\ndf13 = pd.read_csv('./simulation_results/2023-04-30_20-33-28.csv')\ndf14 = pd.read_csv('./simulation_results/2023-05-04_16-40-05.csv')\ndf15 = pd.read_csv('./simulation_results/2023-05-04_17-34-00.csv')\n\n\ndata2 = pd.concat([df1,df2,df3,df4,df5,df6,df7,df8,df9,df10,df11,df12,df13,df14,df15],axis=0)\n\n\ndata2.to_csv('./simulation_results/Real_simulation/pedalme_Simulation_itstgcnsnd.csv',index=False)\n\n\ndata2 = pd.read_csv('./simulation_results/Real_simulation/pedalme_Simulation_itstgcnsnd.csv')\n\n\ndata2.query(\"mtype!='block'\").plot.box(backend='plotly',x='mrate',color='method',y='mse',facet_col='lags',facet_row='inter_method',height=1000)\n\n                                                \n\n\n\ndata2.query(\"mtype=='block'\").plot.box(backend='plotly',x='mrate',color='method',y='mse',facet_col='lags',facet_row='inter_method',height=1200)"
  },
  {
    "objectID": "posts/2_research/2023-04-05-Simulation_boxplot.html#baseline-1",
    "href": "posts/2_research/2023-04-05-Simulation_boxplot.html#baseline-1",
    "title": "Simulation",
    "section": "Baseline",
    "text": "Baseline\n\ndata.query(\"mrate==0 and method!='GNAR'\").plot.box(backend='plotly',x='mrate',color='method',y='mse',facet_col='lags',facet_row='nof_filters',height=600)"
  },
  {
    "objectID": "posts/2_research/2023-04-05-Simulation_boxplot.html#random-1",
    "href": "posts/2_research/2023-04-05-Simulation_boxplot.html#random-1",
    "title": "Simulation",
    "section": "random",
    "text": "random\n\ndata.query(\"method=='GNAR'\").groupby('mrate')['mse'].unique()\n\nmrate\n0.0    [1.2959295511245728, 1.2547194957733154]\n0.3    [1.2959295511245728, 1.2547194957733154]\n0.5    [1.2959295511245728, 1.2547194957733154]\nName: mse, dtype: object\n\n\n\ndata.query(\"mtype=='rand' and mrate !=0 and method!='GNAR'\").plot.box(backend='plotly',x='mrate',color='method',y='mse',facet_col='lags',facet_row='nof_filters',height=1200)"
  },
  {
    "objectID": "posts/2_research/2023-04-05-Simulation_boxplot.html#block-1",
    "href": "posts/2_research/2023-04-05-Simulation_boxplot.html#block-1",
    "title": "Simulation",
    "section": "block",
    "text": "block\n\ndf1 = pd.read_csv('./simulation_results/2023-04-27_07-50-11.csv')\ndf2 = pd.read_csv('./simulation_results/2023-04-27_22-09-07.csv')\ndf3 = pd.read_csv('./simulation_results/2023-04-28_14-40-59.csv')\ndf4 = pd.read_csv('./simulation_results/2023-05-14_19-46-46.csv')\n# df5 = pd.read_csv('./simulation_results/2023-05-14_19-46-46.csv')\n\n\ndata = pd.concat([df1,df2,df3,df4],axis=0)\n\n\ndata.to_csv('./simulation_results/Real_simulation/wikimath_block.csv',index=False)\n\n\ndata = pd.read_csv('./simulation_results/Real_simulation/wikimath_block.csv')\n\n\ndata.query(\"mtype=='block' and method!='GNAR'\").plot.box(backend='plotly',x='mrate',color='method',y='mse',facet_col='lags',facet_row='nof_filters',height=1200)"
  },
  {
    "objectID": "posts/2_research/2023-04-05-Simulation_boxplot.html#missing-values-on-the-same-nodes",
    "href": "posts/2_research/2023-04-05-Simulation_boxplot.html#missing-values-on-the-same-nodes",
    "title": "Simulation",
    "section": "missing values on the same nodes",
    "text": "missing values on the same nodes\n\n# 10%\ndf1 = pd.read_csv('./simulation_results/2023-04-29_03-57-07.csv') # STGCN IT-STGCN block\ndf2 = pd.read_csv('./simulation_results/2023-04-29_20-15-46.csv') # STGCN IT-STGCN\ndf3 = pd.read_csv('./simulation_results/2023-04-30_16-19-58.csv') # STGCN IT-STGCN\n# 60% 확인하고 다시 돌리기\ndf4 = pd.read_csv('./simulation_results/2023-05-05_04-21-57.csv') # STGCN IT-STGCN 60%\ndf5 = pd.read_csv('./simulation_results/2023-05-06_11-34-46.csv') # STGCN IT-STGCN\ndf6 = pd.read_csv('./simulation_results/2023-05-06_23-43-35.csv') # STGCN IT-STGCN\ndf7 = pd.read_csv('./simulation_results/2023-05-07_14-06-44.csv') # STGCN IT-STGCN\n\n\ndata = pd.concat([df1,df2,df3,df4,df5,df6,df7],axis=0)\n\n\ndata.to_csv('./simulation_results/Real_simulation/wikimath_GSO_st.csv',index=False)\n\n\ndata = pd.read_csv('./simulation_results/Real_simulation/wikimath_GSO_st.csv')\n\n\ndata.query(\"method=='GNAR'\")['mse'].unique()\n\narray([], dtype=float64)\n\n\n\ndata.query(\"method!='GNAR' and mtype =='block' \").plot.box(backend='plotly',x='mrate',color='method',y='mse',facet_col='inter_method',facet_row='lags',height=800)"
  },
  {
    "objectID": "posts/2_research/2023-04-05-Simulation_boxplot.html#baseline-2",
    "href": "posts/2_research/2023-04-05-Simulation_boxplot.html#baseline-2",
    "title": "Simulation",
    "section": "Baseline",
    "text": "Baseline\n\ndf1 = pd.read_csv('./simulation_results/2023-04-17_06-05-37.csv') # STGCN IT-STGCN 70%\ndf2 = pd.read_csv('./simulation_results/2023-04-17_08-05-26.csv') # STGCN IT-STGCN\ndf3 = pd.read_csv('./simulation_results/2023-04-17_13-41-19.csv') # STGCN IT-STGCN\ndf4 = pd.read_csv('./simulation_results/2023-04-17_15-44-21.csv') # STGCN IT-STGCN\ndf5 = pd.read_csv('./simulation_results/2023-04-17_21-27-38.csv') # STGCN IT-STGCN\n# df6 = pd.read_csv('./simulation_results/2023-04-15_15-00-32.csv') # GNAR 30%, 50%, 70% # 뭔가 일단 필요없어서 데이터셋에서 뺌\ndf7 = pd.read_csv('./simulation_results/2023-04-18_05-01-55.csv') # STGCN IT-STGCN\ndf8 = pd.read_csv('./simulation_results/2023-04-18_06-14-06.csv') # STGCN IT-STGCN\ndf9 = pd.read_csv('./simulation_results/2023-04-18_17-32-30.csv') # STGCN IT-STGCN\ndf10 = pd.read_csv('./simulation_results/2023-04-19_01-52-24.csv') # STGCN IT-STGCN\ndf11 = pd.read_csv('./simulation_results/2023-04-19_07-50-52.csv') # STGCN IT-STGCN\ndf12 = pd.read_csv('./simulation_results/2023-04-19_09-30-25.csv') # STGCN IT-STGCN\ndf13 = pd.read_csv('./simulation_results/2023-04-19_15-32-55.csv') # STGCN IT-STGCN\ndf14 = pd.read_csv('./simulation_results/2023-04-19_17-12-06.csv') # STGCN IT-STGCN\ndf15 = pd.read_csv('./simulation_results/2023-04-19_23-07-36.csv') # STGCN IT-STGCN\ndf16 = pd.read_csv('./simulation_results/2023-04-20_00-46-43.csv') # STGCN IT-STGCN\ndf17 = pd.read_csv('./simulation_results/2023-04-20_06-51-34.csv') # STGCN IT-STGCN\ndf18 = pd.read_csv('./simulation_results/2023-04-20_08-30-27.csv') # STGCN IT-STGCN\ndf19 = pd.read_csv('./simulation_results/2023-04-20_14-28-35.csv') # STGCN IT-STGCN\ndf20 = pd.read_csv('./simulation_results/2023-04-20_16-08-39.csv') # STGCN IT-STGCN\ndf21 = pd.read_csv('./simulation_results/2023-04-20_22-09-37.csv') # STGCN IT-STGCN\ndf22 = pd.read_csv('./simulation_results/2023-04-20_23-48-26.csv') # STGCN IT-STGCN\ndf23 = pd.read_csv('./simulation_results/2023-04-21_05-36-47.csv') # STGCN IT-STGCN\ndf24 = pd.read_csv('./simulation_results/2023-04-21_15-26-00.csv') # STGCN IT-STGCN\ndf25 = pd.read_csv('./simulation_results/2023-04-21_23-27-11.csv') # STGCN IT-STGCN\ndf26 = pd.read_csv('./simulation_results/2023-04-22_07-46-08.csv') # STGCN IT-STGCN\ndf27 = pd.read_csv('./simulation_results/2023-04-22_15-45-20.csv') # STGCN IT-STGCN\ndf28 = pd.read_csv('./simulation_results/2023-04-22_22-57-31.csv') # STGCN IT-STGCN\ndf29 = pd.read_csv('./simulation_results/2023-04-23_07-00-15.csv') # STGCN IT-STGCN\ndf30 = pd.read_csv('./simulation_results/2023-04-23_15-18-02.csv') # STGCN IT-STGCN\ndf31 = pd.read_csv('./simulation_results/2023-04-23_15-22-36.csv') # GNAR 70%\n# baseline\ndf32 = pd.read_csv('./simulation_results/2023-04-29_06-54-40.csv') # GNAR \ndf33 = pd.read_csv('./simulation_results/2023-04-30_18-55-12.csv')\ndf34 = pd.read_csv('./simulation_results/2023-05-01_02-55-33.csv')\ndf35 = pd.read_csv('./simulation_results/2023-05-01_10-21-15.csv')\ndf36 = pd.read_csv('./simulation_results/2023-05-01_19-23-57.csv')\ndf37 = pd.read_csv('./simulation_results/2023-05-02_01-10-53.csv')\ndf38 = pd.read_csv('./simulation_results/2023-05-02_08-26-53.csv')\ndf39 = pd.read_csv('./simulation_results/2023-05-02_16-00-40.csv')\ndf40 = pd.read_csv('./simulation_results/2023-05-03_00-34-09.csv')\ndf41 = pd.read_csv('./simulation_results/2023-05-03_08-04-42.csv')\ndf42 = pd.read_csv('./simulation_results/2023-05-03_15-50-50.csv')\ndf43 = pd.read_csv('./simulation_results/2023-05-03_23-46-56.csv')\ndf44 = pd.read_csv('./simulation_results/2023-05-04_05-22-59.csv')\ndf45 = pd.read_csv('./simulation_results/2023-05-04_09-22-37.csv')\ndf46 = pd.read_csv('./simulation_results/2023-05-04_15-00-57.csv')\ndf47 = pd.read_csv('./simulation_results/2023-05-04_23-41-21.csv')\ndf48 = pd.read_csv('./simulation_results/2023-05-05_07-23-04.csv')\ndf49 = pd.read_csv('./simulation_results/2023-05-05_15-03-17.csv')\ndf50 = pd.read_csv('./simulation_results/2023-05-06_05-18-07.csv')\ndf51 = pd.read_csv('./simulation_results/2023-05-06_12-57-14.csv')\ndf52 = pd.read_csv('./simulation_results/2023-05-06_19-10-23.csv')\ndf53 = pd.read_csv('./simulation_results/2023-05-07_03-20-10.csv')\ndf54 = pd.read_csv('./simulation_results/2023-05-07_11-26-24.csv')\ndf55 = pd.read_csv('./simulation_results/2023-05-08_00-04-56.csv')\ndf56 = pd.read_csv('./simulation_results/2023-05-08_04-27-01.csv')\ndf57 = pd.read_csv('./simulation_results/2023-05-08_10-06-55.csv')\ndf58 = pd.read_csv('./simulation_results/2023-05-08_17-50-36.csv')\ndf59 = pd.read_csv('./simulation_results/2023-05-09_03-28-08.csv')\ndf60 = pd.read_csv('./simulation_results/2023-05-09_11-08-10.csv')\ndf61 = pd.read_csv('./simulation_results/2023-05-09_20-11-45.csv')\n\n\ndata = pd.concat([df1,df2,df3,df4,df5,df7,df8,df9,df10,df11,df12,df13,df14,df15,df16,df17,df18,\n                 df19,df20,df21,df22,df23,df24,df25,df26,df27,df28,df29,df30,df31,df32,df33,df34,\n                 df35,df36,df37,df38,df39,df40,df41,df42,df43,df44,df45,df46,df47,df48,df49,df50,\n                 df51,df52,df53,df54,df55,df56,df57,df58,df59,df60,df61],axis=0)\n\n\ndata.to_csv('./simulation_results/Real_simulation/windmillsmall.csv',index=False)\n\n\ndata = pd.read_csv('./simulation_results/Real_simulation/windmillsmall.csv')\n\n\ndata.query(\"method=='GNAR' and mrate ==0\")['mse'].unique()\n\narray([1.64923024])\n\n\n\ndata.query(\"method!='GNAR' and mrate ==0 \").plot.box(backend='plotly',x='mrate',color='method',y='mse',facet_col='lags',height=800)"
  },
  {
    "objectID": "posts/2_research/2023-04-05-Simulation_boxplot.html#random-2",
    "href": "posts/2_research/2023-04-05-Simulation_boxplot.html#random-2",
    "title": "Simulation",
    "section": "random",
    "text": "random\n\ndata.query(\"method=='GNAR' and mrate !=0\")['mse'].unique()\n\narray([1.64923024])\n\n\n\ndata.query(\"method!='GNAR' and mtype =='rand' \").plot.box(backend='plotly',x='mrate',color='method',y='mse',facet_col='lags',facet_row='inter_method',height=800)"
  },
  {
    "objectID": "posts/2_research/2023-04-05-Simulation_boxplot.html#block-2",
    "href": "posts/2_research/2023-04-05-Simulation_boxplot.html#block-2",
    "title": "Simulation",
    "section": "block",
    "text": "block\n\ndf1 = pd.read_csv('./simulation_results/2023-04-24_02-48-08.csv') # STGCN IT-STGCN block\ndf2 = pd.read_csv('./simulation_results/2023-04-24_10-57-10.csv') # STGCN IT-STGCN\ndf3 = pd.read_csv('./simulation_results/2023-04-24_18-53-34.csv') # STGCN IT-STGCN\ndf4 = pd.read_csv('./simulation_results/2023-04-25_02-30-27.csv') # STGCN IT-STGCN\ndf5 = pd.read_csv('./simulation_results/2023-04-25_10-48-46.csv') # STGCN IT-STGCN\ndf6 = pd.read_csv('./simulation_results/2023-04-25_10-53-14.csv') # GNAR \ndf7 = pd.read_csv('./simulation_results/2023-04-25_18-40-53.csv') # STGCN IT-STGCN\ndf8 = pd.read_csv('./simulation_results/2023-04-25_23-30-08.csv') # STGCN IT-STGCN\ndf9 = pd.read_csv('./simulation_results/2023-04-26_04-15-00.csv') # STGCN IT-STGCN\ndf10 = pd.read_csv('./simulation_results/2023-04-27_07-59-36.csv') # STGCN IT-STGCN\ndf11 = pd.read_csv('./simulation_results/2023-04-27_15-29-00.csv') # STGCN IT-STGCN\ndf12 = pd.read_csv('./simulation_results/2023-04-27_23-37-18.csv') # STGCN IT-STGCN\ndf13 = pd.read_csv('./simulation_results/2023-04-28_08-21-54.csv') # STGCN IT-STGCN\ndf14 = pd.read_csv('./simulation_results/2023-04-28_16-06-55.csv') # STGCN IT-STGCN\ndf15 = pd.read_csv('./simulation_results/2023-04-28_21-19-37.csv') # STGCN IT-STGCN\ndf16 = pd.read_csv('./simulation_results/2023-04-29_03-07-03.csv') # STGCN IT-STGCN\ndf17 = pd.read_csv('./simulation_results/2023-04-29_09-00-42.csv') # STGCN IT-STGCN\ndf18 = pd.read_csv('./simulation_results/2023-04-29_19-07-49.csv') # STGCN IT-STGCN\ndf19 = pd.read_csv('./simulation_results/2023-04-30_05-14-07.csv') # STGCN IT-STGCN\ndf20 = pd.read_csv('./simulation_results/2023-04-30_15-23-16.csv') # STGCN IT-STGCN\ndf21 = pd.read_csv('./simulation_results/2023-05-01_00-16-37.csv') # STGCN IT-STGCN\ndf22 = pd.read_csv('./simulation_results/2023-05-01_07-41-52.csv') # STGCN IT-STGCN\ndf23 = pd.read_csv('./simulation_results/2023-05-01_16-21-41.csv') # STGCN IT-STGCN\ndf24 = pd.read_csv('./simulation_results/2023-05-01_23-38-23.csv') # STGCN IT-STGCN\ndf25 = pd.read_csv('./simulation_results/2023-05-02_13-51-13.csv') # STGCN IT-STGCN\ndf26 = pd.read_csv('./simulation_results/2023-05-02_21-43-26.csv') # STGCN IT-STGCN\ndf27 = pd.read_csv('./simulation_results/2023-05-03_06-04-32.csv') # STGCN IT-STGCN\ndf28 = pd.read_csv('./simulation_results/2023-05-03_13-43-11.csv') # STGCN IT-STGCN\ndf29 = pd.read_csv('./simulation_results/2023-05-03_21-58-04.csv') # STGCN IT-STGCN\ndf30 = pd.read_csv('./simulation_results/2023-05-04_04-39-00.csv') # STGCN IT-STGCN\n\n\ndata = pd.concat([df1,df2,df3,df4,df5,df6,df7,df8,df9,df10,df11,df12,df13,df14,df15,df16,\n                 df17,df18,df19,df20,df21,df22,df23,df24,df25,df26,df27,df28,df29,df30],axis=0)\n\n\ndata.to_csv('./simulation_results/Real_simulation/windmillsmall_block.csv',index=False)\n\n\ndata = pd.read_csv('./simulation_results/Real_simulation/windmillsmall_block.csv')\n\n\ndata.query(\"method=='GNAR'\")['mse'].unique()\n\narray([1.64923024])\n\n\n\ndata.query(\"method!='GNAR' and mtype =='block' \").plot.box(backend='plotly',x='mrate',color='method',y='mse',facet_col='lags',facet_row='inter_method',height=800)"
  },
  {
    "objectID": "posts/2_research/2023-04-05-Simulation_boxplot.html#baseline-3",
    "href": "posts/2_research/2023-04-05-Simulation_boxplot.html#baseline-3",
    "title": "Simulation",
    "section": "Baseline",
    "text": "Baseline\n\ndata.query(\"mrate==0 and method!='GNAR'\").plot.box(backend='plotly',x='mrate',color='method',y='mse',facet_col='lags',facet_row='nof_filters',height=600)"
  },
  {
    "objectID": "posts/2_research/2023-04-05-Simulation_boxplot.html#random-3",
    "href": "posts/2_research/2023-04-05-Simulation_boxplot.html#random-3",
    "title": "Simulation",
    "section": "random",
    "text": "random\n\ndata.query(\"method=='GNAR'\").groupby('mrate')['mse'].unique()\n\nmrate\n0.0    [1.0619367361068726, 1.068463921546936]\n0.3    [1.0619367361068726, 1.068463921546936]\n0.4    [1.0619367361068726, 1.068463921546936]\n0.8    [1.0619367361068726, 1.068463921546936]\n0.9    [1.0619367361068726, 1.068463921546936]\nName: mse, dtype: object\n\n\n\ndata.query(\"mtype=='rand' and mrate !=0 and method!='GNAR' and mrate!=0.8 and mrate!=0.9\").sort_values('lags').plot.box(backend='plotly',x='mrate',color='method',y='mse',facet_col='lags',facet_row='inter_method',height=800)\n\n                                                \n\n\n\ndata.query(\"mtype=='rand' and mrate !=0 and method!='GNAR' and mrate!=0.3 and mrate!=0.4\").plot.box(backend='plotly',x='mrate',color='method',y='mse',facet_col='lags',facet_row='inter_method',height=800)"
  },
  {
    "objectID": "posts/2_research/2023-04-05-Simulation_boxplot.html#block-3",
    "href": "posts/2_research/2023-04-05-Simulation_boxplot.html#block-3",
    "title": "Simulation",
    "section": "block",
    "text": "block\n\ndf1 = pd.read_csv('./simulation_results/2023-05-04_21-03-21.csv')\ndf2 = pd.read_csv('./simulation_results/2023-05-05_12-10-44.csv')\ndf3 = pd.read_csv('./simulation_results/2023-05-06_12-42-22.csv')\ndf4 = pd.read_csv('./simulation_results/2023-05-06_15-40-47.csv')\n\n\ndata = pd.concat([df1,df2,df3,df4],axis=0)\n\n\ndata.to_csv('./simulation_results/Real_simulation/monte_block.csv',index=False)\n\n\ndata = pd.read_csv('./simulation_results/Real_simulation/monte_block.csv')\n\n\ndata.query(\"mtype=='block' and method=='GNAR'\")['mse'].mean()\n\n1.0652003288269043\n\n\n\ndata.query(\"mtype=='block' and method!='GNAR'\").plot.box(backend='plotly',x='mrate',color='method',y='mse',facet_col='lags',facet_row='nof_filters',height=1200)"
  },
  {
    "objectID": "posts/2_research/2023-04-29-pedalme_GSO_st.html",
    "href": "posts/2_research/2023-04-29-pedalme_GSO_st.html",
    "title": "Padalme GSO_st",
    "section": "",
    "text": "edit\nimport itstgcnsnd\nimport torch\nimport itstgcnsnd.planner\nfrom torch_geometric_temporal.dataset import PedalMeDatasetLoader\nloader2 = PedalMeDatasetLoader()\nimport numpy as np\nfrom torch_geometric_temporal.nn.recurrent import GConvGRU\nimport copy\nimport torch_geometric_temporal\nimport torch.nn.functional as F\nfrom rpy2.robjects.vectors import FloatVector\nimport rpy2.robjects as robjects\nfrom rpy2.robjects.packages import importr\nimport rpy2.robjects.numpy2ri as rpyn\nGNAR = importr('GNAR') # import GNAR \n#igraph = importr('igraph') # import igraph \nebayesthresh = importr('EbayesThresh').ebayesthresh\ndef flatten_weight(T,N,ws,wt):\n    Is = np.eye(N,N)\n    lst = [[0]*T for t in range(T)]\n    for i in range(T):\n        for j in range(T):\n            if i==j: \n                lst[i][j] = ws \n            elif abs(i-j)==1:\n                lst[i][j] = Is\n            else:\n                lst[i][j] = Is*0\n    return np.concatenate([np.concatenate(l,axis=1) for l in lst],axis=0) # TN*TN matrix\n\ndef make_Psi(T,N,edge_index,edge_weight):\n    wt = np.zeros((T,T))\n    for i in range(T):\n        for j in range(T):\n            if i==j :\n                wt[i,j] = 0\n            elif np.abs(i-j) &lt;= 1 : \n                wt[i,j] = 1\n    ws = np.zeros((N,N))\n    for i in range(N):\n        for j in range(edge_weight.shape[0]):\n            if edge_index[0][j] == i :\n                ws[i,edge_index[1][j]] = edge_weight[j]\n    W = flatten_weight(T,N,ws,wt) # TN*TN matrix\n    d = np.array(W.sum(axis=1))\n    D = np.diag(d)\n    L = np.array(np.diag(1/np.sqrt(d)) @ (D-W) @ np.diag(1/np.sqrt(d)))\n    lamb, Psi = np.linalg.eigh(L)\n    return Psi # TN*TN matrix\n\ndef trim(f,edge_index,edge_weight):\n    f = np.array(f)\n    if len(f.shape)==1: f = f.reshape(-1,1)\n    T,N = f.shape # f = T*N matrix\n    Psi = make_Psi(T,N,edge_index,edge_weight) # TN*TN matrix\n    fbar = Psi.T @ f.reshape(-1,1) # TN*TN X TN*1 matrix = TN*1 matrix\n    fbar_threshed = np.stack([ebayesthresh(FloatVector(fbar.reshape(-1,N)[:,i])) for i in range(N)],axis=1)\n    fhat_flatten = Psi @ fbar_threshed.reshape(-1,1) # inverse dft \n    fhat = fhat_flatten.reshape(-1,N)\n    return fhat\n\ndef update_from_freq_domain(signal, missing_index,edge_index,edge_weight):\n    signal = np.array(signal)\n    T,N = signal.shape \n    signal_trimed = trim(signal,edge_index,edge_weight)\n    for i in range(N):\n        try: \n            signal[missing_index[i],i] = signal_trimed[missing_index[i],i]\n        except: \n            pass \n    return signal\n\nclass StgcnLearner:\n    def __init__(self,train_dataset,dataset_name = None):\n        self.train_dataset = train_dataset\n        self.lags = torch.tensor(train_dataset.features).shape[-1]\n        self.dataset_name = str(train_dataset) if dataset_name is None else dataset_name\n        self.mindex= getattr(self.train_dataset,'mindex',None)\n        self.mrate_eachnode = getattr(self.train_dataset,'mrate_eachnode',0)\n        self.mrate_total = getattr(self.train_dataset,'mrate_total',0)\n        self.mtype = getattr(self.train_dataset,'mtype',None)\n        self.interpolation_method = getattr(self.train_dataset,'interpolation_method',None)\n        self.method = 'STGCN'\n    def learn(self,filters=32,epoch=50):\n        self.model = RecurrentGCN(node_features=self.lags, filters=filters)\n        self.optimizer = torch.optim.Adam(self.model.parameters(), lr=0.01)\n        self.model.train()\n        for e in range(epoch):\n            for t, snapshot in enumerate(self.train_dataset):\n                yt_hat = self.model(snapshot.x, snapshot.edge_index, snapshot.edge_attr)\n                cost = torch.mean((yt_hat-snapshot.y)**2)\n                cost.backward()\n                self.optimizer.step()\n                self.optimizer.zero_grad()\n            print('{}/{}'.format(e+1,epoch),end='\\r')\n        # recording HP\n        self.nof_filters = filters\n        self.epochs = epoch+1\n    def __call__(self,dataset):\n        X = torch.tensor(dataset.features).float()\n        y = torch.tensor(dataset.targets).float()\n        yhat = torch.stack([self.model(snapshot.x, snapshot.edge_index, snapshot.edge_attr) for snapshot in dataset]).detach().squeeze().float()\n        return {'X':X, 'y':y, 'yhat':yhat} \n\nclass ITStgcnLearner(StgcnLearner):\n    def __init__(self,train_dataset,dataset_name = None):\n        super().__init__(train_dataset)\n        self.method = 'IT-STGCN'\n    def learn(self,filters=32,epoch=50):\n        self.model = RecurrentGCN(node_features=self.lags, filters=filters)\n        self.optimizer = torch.optim.Adam(self.model.parameters(), lr=0.01)\n        self.model.train()\n        train_dataset_temp = copy.copy(self.train_dataset)\n        for e in range(epoch):\n            f,lags = convert_train_dataset(train_dataset_temp)\n            f = update_from_freq_domain(f,self.mindex,self.train_dataset.edge_index,self.train_dataset.edge_weight)\n            T,N = f.shape \n            data_dict_temp = {\n                'edges':self.train_dataset.edge_index.T.tolist(), \n                'node_ids':{'node'+str(i):i for i in range(N)}, \n                'FX':f\n            }\n            train_dataset_temp = DatasetLoader(data_dict_temp).get_dataset(lags=self.lags)  \n            for t, snapshot in enumerate(train_dataset_temp):\n                yt_hat = self.model(snapshot.x, snapshot.edge_index, snapshot.edge_attr)\n                cost = torch.mean((yt_hat-snapshot.y)**2)\n                cost.backward()\n                self.optimizer.step()\n                self.optimizer.zero_grad()\n            print('{}/{}'.format(e+1,epoch),end='\\r')\n        # record\n        self.nof_filters = filters\n        self.epochs = epoch+1\n        \n        \ndef convert_train_dataset(train_dataset):\n    lags = torch.tensor(train_dataset.features).shape[-1]\n    f = torch.concat([train_dataset[0].x.T,torch.tensor(train_dataset.targets)],axis=0).numpy()\n    return f,lags \n\n\nclass RecurrentGCN(torch.nn.Module):\n    def __init__(self, node_features, filters):\n        super(RecurrentGCN, self).__init__()\n        self.recurrent = GConvGRU(node_features, filters, 2)\n        self.linear = torch.nn.Linear(filters, 1)\n\n    def forward(self, x, edge_index, edge_weight):\n        h = self.recurrent(x, edge_index, edge_weight)\n        h = F.relu(h)\n        h = self.linear(h)\n        return h\n    \nclass DatasetLoader(object):\n    def __init__(self,data_dict):\n        self._dataset = data_dict \n    def _get_edges(self):\n        self._edges = np.array(self._dataset[\"edges\"]).T\n\n    def _get_edge_weights(self):\n        self._edge_weights = np.ones(self._edges.shape[1])\n\n    def _get_targets_and_features(self):\n        stacked_target = np.array(self._dataset[\"FX\"])\n        self.features = [\n            stacked_target[i : i + self.lags, :].T\n            for i in range(stacked_target.shape[0] - self.lags)\n        ]\n        self.targets = [\n            stacked_target[i + self.lags, :].T\n            for i in range(stacked_target.shape[0] - self.lags)\n        ]\n\n    def get_dataset(self, lags: int = 4) -&gt; torch_geometric_temporal.signal.StaticGraphTemporalSignal:\n        \"\"\"Returning the Chickenpox Hungary data iterator.\n\n        Args types:\n            * **lags** *(int)* - The number of time lags.\n        Return types:\n            * **dataset** *(torch_geometric_temporal.signal.StaticGraphTemporalSignal)* - The Chickenpox Hungary dataset.\n        \"\"\"\n        self.lags = lags\n        self._get_edges()\n        self._get_edge_weights()\n        self._get_targets_and_features()\n        dataset = torch_geometric_temporal.signal.StaticGraphTemporalSignal(\n            self._edges, self._edge_weights, self.features, self.targets\n        )\n        return dataset\n    \n    \n    \n    \n\nclass Evaluator:\n    def __init__(self,learner,train_dataset,test_dataset):\n        self.learner = learner\n        # self.learner.model.eval()\n        try:self.learner.model.eval()\n        except:pass\n        self.train_dataset = train_dataset\n        self.test_dataset = test_dataset\n        self.lags = self.learner.lags\n        rslt_tr = self.learner(self.train_dataset) \n        rslt_test = self.learner(self.test_dataset)\n        self.X_tr = rslt_tr['X']\n        self.y_tr = rslt_tr['y']\n        self.f_tr = torch.concat([self.train_dataset[0].x.T,self.y_tr],axis=0).float()\n        self.yhat_tr = rslt_tr['yhat']\n        self.fhat_tr = torch.concat([self.train_dataset[0].x.T,self.yhat_tr],axis=0).float()\n        self.X_test = rslt_test['X']\n        self.y_test = rslt_test['y']\n        self.f_test = self.y_test \n        self.yhat_test = rslt_test['yhat']\n        self.fhat_test = self.yhat_test\n        self.f = torch.concat([self.f_tr,self.f_test],axis=0)\n        self.fhat = torch.concat([self.fhat_tr,self.fhat_test],axis=0)\n    def calculate_mse(self):\n        test_base_mse_eachnode = ((self.y_test - self.y_test.mean(axis=0).reshape(-1,self.y_test.shape[-1]))**2).mean(axis=0).tolist()\n        test_base_mse_total = ((self.y_test - self.y_test.mean(axis=0).reshape(-1,self.y_test.shape[-1]))**2).mean().item()\n        train_mse_eachnode = ((self.y_tr-self.yhat_tr)**2).mean(axis=0).tolist()\n        train_mse_total = ((self.y_tr-self.yhat_tr)**2).mean().item()\n        test_mse_eachnode = ((self.y_test-self.yhat_test)**2).mean(axis=0).tolist()\n        test_mse_total = ((self.y_test-self.yhat_test)**2).mean().item()\n        self.mse = {'train': {'each_node': train_mse_eachnode, 'total': train_mse_total},\n                    'test': {'each_node': test_mse_eachnode, 'total': test_mse_total},\n                    'test(base)': {'each_node': test_base_mse_eachnode, 'total': test_base_mse_total},\n                   }\n    def _plot(self,*args,t=None,h=2.5,max_node=5,**kwargs):\n        T,N = self.f.shape\n        if t is None: t = range(T)\n        fig = plt.figure()\n        nof_axs = max(min(N,max_node),2)\n        if min(N,max_node)&lt;2: \n            print('max_node should be &gt;=2')\n        ax = fig.subplots(nof_axs ,1)\n        for n in range(nof_axs):\n            ax[n].plot(t,self.f[:,n],color='gray',*args,**kwargs)\n            ax[n].set_title('node='+str(n))\n        fig.set_figheight(nof_axs*h)\n        fig.tight_layout()\n        plt.close()\n        return fig\n    def plot(self,*args,t=None,h=2.5,**kwargs):\n        self.calculate_mse()\n        fig = self._plot(*args,t=None,h=2.5,**kwargs)\n        ax = fig.get_axes()\n        for i,a in enumerate(ax):\n            _mse1= self.mse['train']['each_node'][i]\n            _mse2= self.mse['test']['each_node'][i]\n            _mse3= self.mse['test(base)']['each_node'][i]\n            _mrate = self.learner.mrate_eachnode if set(dir(self.learner.mrate_eachnode)) & {'__getitem__'} == set() else self.learner.mrate_eachnode[i]\n            _title = 'node{0}, mrate: {1:.2f}% \\n mse(train) = {2:.2f}, mse(test) = {3:.2f}, mse(test_base) = {4:.2f}'.format(i,_mrate*100,_mse1,_mse2,_mse3)\n            a.set_title(_title)\n            _t1 = self.lags\n            _t2 = self.yhat_tr.shape[0]+self.lags\n            _t3 = len(self.f)\n            a.plot(range(_t1,_t2),self.yhat_tr[:,i],label='fitted (train)',color='C0')\n            a.plot(range(_t2,_t3),self.yhat_test[:,i],label='fitted (test)',color='C1')\n            a.legend()\n        _mse1= self.mse['train']['total']\n        _mse2= self.mse['test']['total']\n        _mse3= self.mse['test(base)']['total']\n        _title =\\\n        'dataset: {0} \\n method: {1} \\n mrate: {2:.2f}% \\n interpolation:{3} \\n epochs={4} \\n number of filters={5} \\n lags = {6} \\n mse(train) = {7:.2f}, mse(test) = {8:.2f}, mse(test_base) = {9:.2f} \\n'.\\\n        format(self.learner.dataset_name,self.learner.method,self.learner.mrate_total*100,self.learner.interpolation_method,self.learner.epochs,self.learner.nof_filters,self.learner.lags,_mse1,_mse2,_mse3)\n        fig.suptitle(_title)\n        fig.tight_layout()\n        return fig\nfrom torch_geometric_temporal.dataset import ChickenpoxDatasetLoader\nloader1 = ChickenpoxDatasetLoader()\na = loader2.get_dataset(lags=1)\ntrain_dataset, test_dataset = torch_geometric_temporal.signal.temporal_signal_split(a, train_ratio=0.8)\nmindex = itstgcnsnd.rand_mindex(train_dataset,mrate=0.9)\ndataset_miss = itstgcnsnd.miss(train_dataset,mindex,mtype='rand')\ndataset_padded = itstgcnsnd.padding(dataset_miss,imputation_method='linear') # padding(train_dataset_miss,method='linear'와 같음)\nlrnr = ITStgcnLearner(dataset_padded)\nlrnr.learn(epoch=5)\n\n5/5\nev = Evaluator(lrnr,train_dataset,test_dataset)\nimport matplotlib.pyplot as plt\nfig = ev.plot('--.',h=5,max_node=5,label='complete data',alpha=0.5) \nfig.set_figwidth(20)\nfig.set_figheight(20)\nfig.tight_layout()\nfig"
  },
  {
    "objectID": "posts/2_research/2023-04-29-pedalme_GSO_st.html#random",
    "href": "posts/2_research/2023-04-29-pedalme_GSO_st.html#random",
    "title": "Padalme GSO_st",
    "section": "random",
    "text": "random\n\nplans_stgcn_rand = {\n    'max_iteration': 30, \n    'method': ['STGCN', 'IT-STGCN'], \n    'mrate': [0.3,0.6],\n    'lags': [4], \n    'nof_filters': [12], \n    'inter_method': ['linear','nearest'],\n    'epoch': [50]\n}\n\n\nplnr = itstgcnsnd.planner.PLNR_STGCN_RAND(plans_stgcn_rand,loader2,dataset_name='pedalme')\n\n\nplnr.simulate()\n\n1/30 is done\n2/30 is done\n3/30 is done\n4/30 is done\n5/30 is done\n6/30 is done\n7/30 is done\n8/30 is done\n9/30 is done\n10/30 is done\n11/30 is done\n12/30 is done\n13/30 is done\n14/30 is done\n15/30 is done\n16/30 is done\n17/30 is done\n18/30 is done\n19/30 is done\n20/30 is done\n21/30 is done\n22/30 is done\n23/30 is done\n24/30 is done\n25/30 is done\n26/30 is done\n27/30 is done\n28/30 is done\n29/30 is done\n30/30 is done\nAll results are stored in ./simulation_results/2023-07-02_07-01-12.csv"
  },
  {
    "objectID": "posts/2_research/2023-04-29-pedalme_GSO_st.html#block",
    "href": "posts/2_research/2023-04-29-pedalme_GSO_st.html#block",
    "title": "Padalme GSO_st",
    "section": "block",
    "text": "block\n\nmy_list = [[] for _ in range(15)] #pedalme\nanother_list = list(range(10,25))\nmy_list[1] = another_list\nmy_list[3] = another_list\nmy_list[4] = another_list\nmy_list[5] = another_list\nanother_list = list(range(5,20))\nmy_list[7] = another_list\nmy_list[9] = another_list\nmy_list[10] = another_list\nmy_list[11] = another_list\nmindex = my_list\n\n\n# mindex= [[],[],[],list(range(50,150)),[]]  # node 1\n# mindex= [list(range(10,100)),[],list(range(50,80)),[],[]] # node 2\n# mindex= [list(range(10,100)),[],list(range(50,80)),list(range(50,150)),[]] # node3\nplans_stgcn_block = {\n    'max_iteration': 30, \n    'method': ['STGCN', 'IT-STGCN'], \n    'mindex': [mindex],\n    'lags': [4], \n    'nof_filters': [12], \n    'inter_method': ['linear','nearest'],\n    'epoch': [50]\n}\n\n\nplnr = itstgcnsnd.planner.PLNR_STGCN_MANUAL(plans_stgcn_block,loader2,dataset_name='pedalme')\nplnr.simulate(mindex=mindex,mtype='block')\n\n1/30 is done\n2/30 is done\n3/30 is done\n4/30 is done\n5/30 is done\n6/30 is done\n7/30 is done\n8/30 is done\n9/30 is done\n10/30 is done\n11/30 is done\n12/30 is done\n13/30 is done\n14/30 is done\n15/30 is done\n16/30 is done\n17/30 is done\n18/30 is done\n19/30 is done\n20/30 is done\n21/30 is done\n22/30 is done\n23/30 is done\n24/30 is done\n25/30 is done\n26/30 is done\n27/30 is done\n28/30 is done\n29/30 is done\n30/30 is done\nAll results are stored in ./simulation_results/2023-07-02_07-19-21.csv\n\n\n\n# df1 = pd.read_csv('./simulation_results/2023-04-13_20-37-59.csv')\n\n\n# data = pd.concat([df1],axis=0);data"
  },
  {
    "objectID": "posts/2_research/2024-01-14-toy_example_figure.html",
    "href": "posts/2_research/2024-01-14-toy_example_figure.html",
    "title": "Toy Example Figure(Intro)",
    "section": "",
    "text": "import itstgcn_intro\n\nimport torch\nimport numpy as np\nimport pandas as pd\n\nimport random\n\nfrom plotnine import *\nimport pickle\n\nimport plotly.express as px\nimport plotly.graph_objects as go\n\nimport matplotlib.pyplot as plt\n\n\ndef load_data(fname):\n    with open(fname, 'rb') as outfile:\n        data_dict = pickle.load(outfile)\n    return data_dict\n\ndef save_data(data_dict,fname):\n    with open(fname,'wb') as outfile:\n        pickle.dump(data_dict,outfile)"
  },
  {
    "objectID": "posts/2_research/2024-01-14-toy_example_figure.html#data-load",
    "href": "posts/2_research/2024-01-14-toy_example_figure.html#data-load",
    "title": "Toy Example Figure(Intro)",
    "section": "Data load",
    "text": "Data load\n\n# data_dict1 = itstgcn_intro.load_data('./data/new_toy_dict.pkl')\n# data1 = itstgcn_intro.load_data('./data/new_toy_data.csv')\nT = len(data_dict1['FX'])\n# mindex = [random.sample(range(0, T), int(T*0.8)),[np.array(list(range(750,1250)))]]\nmindex = []"
  },
  {
    "objectID": "posts/2_research/2024-01-14-toy_example_figure.html#learner",
    "href": "posts/2_research/2024-01-14-toy_example_figure.html#learner",
    "title": "Toy Example Figure(Intro)",
    "section": "Learner",
    "text": "Learner\n\ntest_Classic = Intro(data=data1, data_dict=data_dict1, lags=4, mindex=mindex, mtype='block',name = 'Classic')\n\n/home/csy/Dropbox/ITTGNN_blog/posts/2_research/itstgcn_intro/utils.py:71: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at /opt/conda/conda-bld/pytorch_1682343998658/work/torch/csrc/utils/tensor_new.cpp:245.)\n\n\n\ntest_Classic.lrnr(filters=4,epoch=5)\n\n5/5\n\n\n\ntest_Proposed = Intro(data=data1, data_dict=data_dict1, lags=4, mindex=mindex, mtype='block',name = 'Proposed')\n\n\ntest_Proposed.lrnr(filters=4,epoch=5)"
  },
  {
    "objectID": "posts/2_research/2024-01-14-toy_example_figure.html#yhat",
    "href": "posts/2_research/2024-01-14-toy_example_figure.html#yhat",
    "title": "Toy Example Figure(Intro)",
    "section": "yhat",
    "text": "yhat\n\ntest_Classic.record_yhat()\n\n\ntest_Proposed.tr\n\n\ntest_Proposed.record_yhat()\n\n\ntest_Classic.custom_melt_function()\n\n\ntest_Proposed.custom_melt_function()"
  },
  {
    "objectID": "posts/2_research/2024-01-14-toy_example_figure.html#animation",
    "href": "posts/2_research/2024-01-14-toy_example_figure.html#animation",
    "title": "Toy Example Figure(Intro)",
    "section": "animation",
    "text": "animation\n\ntest_Classic.create_time_series_plot()\n\n\ntest_Proposed.create_time_series_plot()\n\n\ntest_Classic.fig[0]\n\n\ntest_Proposed.fig[3]"
  },
  {
    "objectID": "posts/2_research/2023-07-18-EbayesThresh toy ex.html",
    "href": "posts/2_research/2023-07-18-EbayesThresh toy ex.html",
    "title": "EbayesThresh Toy ex",
    "section": "",
    "text": "Import\n\nfrom itstgcn.learners import * \n\n\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport random\nimport pandas as pd\n\n\nfrom rpy2.robjects.vectors import FloatVector\nimport rpy2.robjects as robjects\nfrom rpy2.robjects.packages import importr\nimport rpy2.robjects.numpy2ri as rpyn\nebayesthresh = importr('EbayesThresh').ebayesthresh\n\n\nWhile \\({\\bf p}_y\\) serves as a consistent estimator for \\(\\mathbb{E}[|{\\bf V}^H{\\bf y}|^2]\\), it is not an efficient estimator, and therefore, improvement is needed [@djuric2018cooperative]. The traditional approach for improvement is to use the windowed periodogram.\nThe windowed periodogram is efficient in detecting specific frequencies or periods, but it may not be as efficient in estimating the underlying function. One notable paper that utilized the windowed periodogram is the one that detected the El Niño phenomenon.\nAs this structure exhibits a “sparse signal + heavy-tailed” characteristics, by applying Bayesian modeling and thresholding \\({\\bf p}_y\\), we can estimate an appropriate \\({\\bf p}_{pp}\\) as discussed in [@johnstone2004needles].\n\n\n\nBayesian Model\n\\(x_i \\sim N(\\mu_i,1)\\)\n확률변수가 잘 정의되어 있을때, 여기서 \\(\\mu_i\\)를 정하는 Baysian.\n\n\\(\\mu_i \\sim\\) 사전분포(\\(\\mu_i\\)를 뽑을 수 있는)\n\\((\\mu_i | X_i = x_i)^n_{i=1} \\sim\\) 사후분포\n\nex) \\(N(10,1) \\sim\\) 사전분포\n관측치\n\n_obs = [7.1,6.9,8.5]\n\n\nnp.mean(_obs)\n\n7.5\n\n\n관측치를 보니 평균이 10이 아닌 것 같다.\n\\(N(10-3,1) \\sim\\) 사후분포\n\n여기서, \\(10-3\\)이 posterior meman\n사후 분포를 정의할때, 이벤트의 mean이냐, median이냐로 잡는 방법은 정해진 것이 아니다.(이베이즈에서는 median으로 잡음)\n\nEbayes는 사전분포를 Heavy-tail으로 정의했다.\nheavy tail?\n\n\n\nimage.png\n\n\n\\(\\mu_x \\sim\\) Double Exponential \\(= p_{pp} + p_{ac}\\) -&gt; 혼합형(misture) = pure point + absolutely continuous\n\\(E(\\mu_i | X_i = x_i) = \\hat{\\mu}_i\\) -&gt; thresholding의 결과\n\\(f_{prior}(\\mu) = (1-w)\\delta_0(\\mu) + w \\gamma (\\mu)\\)\n\n\\(\\delta_0\\) = 디렉함수(특정값이 아니면 다 0으로 봄)\n\\(\\gamma = \\frac{a}{2} e^{-a|\\mu|}\\)\n\nEbayes의 역할 = 자동으로 \\(w\\)를 계산 혹은 추정\n\n\\(1-w\\) 확률로 \\(\\delta_0\\)를 정의, \\(w\\)의 확룔로 \\(\\gamma\\)를 정의.\n\n\\(X_i = \\mu_i + \\epsilon_i, \\epsilon_i \\sim N(0,1)\\)에서 \\(\\mu_i\\)를 찾는게 목적이다. 이게 바로 \\(\\eta\\)값\n\nEbayes로 sparse signal만 골러낼 것이다.\n평균 이상의 값에서 자를 것이다.\n\nthresh(임계치)를 잡는 게 어려울 텐데, 위에서 이베이즈가 \\(w\\)를 자동으로 잡아 확률 계산되는 방법론을 제안한 것,\n\nbaysian modeling 사용하여 heavy tail + impulse(sparse)에서 posterior median 추정하여 임계값thresh으로 \\(p\\)에서 \\(p_{pp}\\)를 추출하는 것이 GODE 목적\n\n\n\nEbayesThresh\n\nT = 100\n\n\nt = np.arange(T)/T * 10\n\n\ny_true = 3*np.sin(0.5*t) + 1.2*np.sin(1.0*t) + 0.5*np.sin(1.2*t) \n\n\ny = y_true + np.random.normal(size=T)\n\n\nplt.figure(figsize=(10,6))\nplt.plot(t,y_true)\n\n\n\n\n\n\n\n\n- 관찰한 신호\n\nplt.plot(t,y,'o')\nplt.plot(t,y_true,'--')\n\n\n\n\n\n\n\n\n- 퓨리에 변환\n\nf = np.array(y)\nif len(f.shape)==1: f = f.reshape(-1,1)\nT,N = f.shape\nPsi = make_Psi(T)\nfbar = Psi.T @ f # apply dft \n\n\nplt.plot(t,fbar**2) # periodogram \n\n\n\n\n\n\n\n\n- threshed\n\nfbar_threshed = np.stack([ebayesthresh(FloatVector(fbar[:,i])) for i in range(N)],axis=1)\nplt.plot((fbar**2)) # periodogram \nplt.plot((fbar_threshed**2)) \n\n\n\n\n\n\n\n\n\nplt.plot((fbar**2)[20:80]) # periodogram \nplt.plot((fbar_threshed**2)[20:80]) \n\n\n\n\n\n\n\n\n- 역퓨리에변환\n\nyhat = Psi @ fbar_threshed # inverse dft\n\n\nplt.figure(figsize=(10,6))\nplt.plot(t,y,'.')\nplt.plot(t,y_true,'--')\nplt.plot(t,yhat)\n\n\n\n\n\n\n\n\n\nplt.figure(figsize=(10,6))\nplt.plot(y,'.')\nplt.plot(y_true)\n\n\n\n\n\n\n\n\n\n\nResult\n\nwith plt.style.context('seaborn-white'):\n    fig, ((ax1,ax2),(ax3,ax4)) = plt.subplots(2,2,figsize=(40,15))\n    fig.suptitle('Figure 1',fontsize=40)\n    \n    ax1.plot(y, 'b.',alpha=0.5)\n    ax1.plot(y_true,'p--',label='True')\n    ax1.legend(fontsize=20,loc='upper left',facecolor='white', frameon=True)\n    \n    ax1.tick_params(axis='y', labelsize=20)\n    ax1.tick_params(axis='x', labelsize=20)\n    \n    ax2.plot(y, 'b.',alpha=0.5)\n    ax2.plot(y_true,'p--',label='True')\n    ax2.plot(yhat,label='y hat')\n    ax2.legend(fontsize=20,loc='upper left',facecolor='white', frameon=True)\n    ax2.tick_params(axis='y', labelsize=20)\n    ax2.tick_params(axis='x', labelsize=20)\n    \n    ax3.plot((fbar**2)) # periodogram \n    ax3.plot((fbar_threshed**2)) \n    ax3.tick_params(axis='y', labelsize=20)\n    ax3.tick_params(axis='x', labelsize=20)\n    ax3.axvspan(20, 80, facecolor='gray', alpha=0.2)\n\n    \n    ax4.plot(range(20, 80),(fbar**2)[20:80]) # periodogram \n    ax4.plot(range(20, 80),(fbar_threshed**2)[20:80]) \n    ax4.set_xticks(range(20, 81, 10))\n    ax4.set_xticklabels(range(20, 81, 10))\n    # ax4.set_xticklabels(['20','40','60'])\n    ax4.tick_params(axis='y', labelsize=20)\n    ax4.tick_params(axis='x', labelsize=20)\n\n\n\n\n\n\n\n\n\nfrom mpl_toolkits.axes_grid1.inset_locator import mark_inset, inset_axes\nplt.figure(figsize = (20,10))\nplt.suptitle('Figure',fontsize=40)\nax = plt.subplot(1, 1, 1)\nax.plot(range(0,100),(fbar**2))\nax.plot((fbar_threshed**2)) \naxins = inset_axes(ax, 8, 3, loc = 1, bbox_to_anchor=(0.8, 0.8),\n                   bbox_transform = ax.figure.transFigure)\naxins.plot(range(20, 80),(fbar**2)[20:80])\naxins.plot(range(20, 80),(fbar_threshed**2)[20:80]) \naxins.set_xlim(20, 80)\naxins.set_ylim(-0.1, 7)\nmark_inset(ax, axins, loc1=4, loc2=3, fc=\"none\", ec = \"0.01\")\nax.tick_params(axis='y', labelsize=20)\nax.tick_params(axis='x', labelsize=20)\naxins.tick_params(axis='y', labelsize=15)\naxins.tick_params(axis='x', labelsize=15)\n# plt.savefig('Ebayes_Toy.png')\n\n\n\n\n\n\n\n\n\nfrom matplotlib.patches import ConnectionPatch\nfig = plt.figure(figsize=(20,10))\nplt.suptitle('Figure 1',fontsize=40)\nplot1 = fig.add_subplot(2,2,(1,2))\n\nplot1.plot(range(20, 80),(fbar**2)[20:80]) # periodogram \nplot1.plot(range(20, 80),(fbar_threshed**2)[20:80]) \nplot1.set_xticks(range(20, 81, 10))\nplot1.set_xticklabels(range(20, 81, 10))\nplot1.tick_params(axis='y', labelsize=20)\nplot1.tick_params(axis='x', labelsize=20)\n\nplot3 = fig.add_subplot(2,2,(3,4)) \n\nplot3.plot((fbar**2)) # periodogram \nplot3.plot((fbar_threshed**2)) \nplot3.tick_params(axis='y', labelsize=20)\nplot3.tick_params(axis='x', labelsize=20)\nplot3.axvspan(20, 80, facecolor='gray', alpha=0.2)\n\n# plot3.fill_between((20, 80), 10, 60, facecolor= \"red\", alpha = 0.2)\nconn1 = ConnectionPatch(xyA = (20, -0.1), coordsA=plot1.transData,\n                       xyB=(20, 0), coordsB=plot3.transData, color = 'black')\nfig.add_artist(conn1)\nconn2 = ConnectionPatch(xyA = (79, -0.1), coordsA=plot1.transData,\n                       xyB=(80, 0), coordsB=plot3.transData, color = 'black')\nfig.add_artist(conn2)\nplt.show()\n\n\n\n\n\n\n\n\n\n\nIn article\n\nimport rpy2\nimport rpy2.robjects as ro \nfrom rpy2.robjects.vectors import FloatVector \nfrom rpy2.robjects.packages import importr\n\n\n%load_ext rpy2.ipython\n\n\n%%R\nlibrary(GNAR)\nlibrary(igraph)\n\nR[write to console]: Loading required package: igraph\n\nR[write to console]: \nAttaching package: ‘igraph’\n\n\nR[write to console]: The following objects are masked from ‘package:stats’:\n\n    decompose, spectrum\n\n\nR[write to console]: The following object is masked from ‘package:base’:\n\n    union\n\n\nR[write to console]: Loading required package: wordcloud\n\nR[write to console]: Loading required package: RColorBrewer\n\n\n\n\nimport rpy2\n\n\nfrom rpy2.robjects.packages import importr\n\n\nebayesthresh = importr('EbayesThresh').ebayesthresh\n\n\n#import rpy2\n#import rpy2.robjects as ro \nfrom rpy2.robjects.vectors import FloatVector\nimport rpy2.robjects as robjects\nfrom rpy2.robjects.packages import importr\nimport rpy2.robjects.numpy2ri as rpyn\nGNAR = importr('GNAR') # import GNAR \n#igraph = importr('igraph') # import igraph \nebayesthresh = importr('EbayesThresh').ebayesthresh\n\n\n%%R\nset.seed(1)\nx &lt;- rnorm(1000) + sample(c( runif(25,-7,7), rep(0,975)))\n\n\\(X_i\\)에서 \\(\\mu_i\\) 추출 가능하는 것을 증명할 예제\n\n%%R\n# png(\"Ebayes_plot1.png\", width=1600, height=800)\npar(mfrow=c(1,2))\npar(cex.axis=2) \npar(cex.lab=2)\nplot(x, type='l', xlab=\"Observed data\", ylab=\"\")\nplot(ebayesthresh(x, sdev=1),type='l', xlab=\"Estimate\", ylab=\"\")\n# dev.off()\n\n\n\n\n\n\n\n\n\nimport itstgcn\n\n\nitstgcn.make_Psi(T)\n\narray([[ 0.07106691, -0.10050378,  0.10050378, ..., -0.10050378,\n        -0.10050378,  0.07106691],\n       [ 0.10050378, -0.14206225,  0.14184765, ...,  0.14184765,\n         0.14206225, -0.10050378],\n       [ 0.10050378, -0.14184765,  0.14099032, ..., -0.14099032,\n        -0.14184765,  0.10050378],\n       ...,\n       [ 0.10050378,  0.14184765,  0.14099032, ...,  0.14099032,\n        -0.14184765, -0.10050378],\n       [ 0.10050378,  0.14206225,  0.14184765, ..., -0.14184765,\n         0.14206225,  0.10050378],\n       [ 0.07106691,  0.10050378,  0.10050378, ...,  0.10050378,\n        -0.10050378, -0.07106691]])\n\n\ndef trim(f):\n    f = np.array(f)\n    if len(f.shape)==1: f = f.reshape(-1,1)\n    T,N = f.shape\n    Psi = make_Psi(T)\n    fbar = Psi.T @ f # apply dft \n    fbar_threshed = np.stack([ebayesthresh(FloatVector(fbar[:,i])) for i in range(N)],axis=1)\n    fhat = Psi @ fbar_threshed # inverse dft \n    return fhat\n\nplt.plot(y)\n\n\n\n\n\n\n\n\n\nplt.plot(itstgcn.make_Psi(T).T@y)\n\n\n\n\n\n\n\n\n\nplt.plot(ebayesthresh(FloatVector(itstgcn.make_Psi(T).T@y)))\n\n\n\n\n\n\n\n\n\nplt.plot(itstgcn.make_Psi(T)@ebayesthresh(FloatVector(itstgcn.make_Psi(T).T@y)))\n\n\n\n\n\n\n\n\n\n_T = 1000\n\n\n_t = np.arange(_T)/_T * 10\n\n\n_x = 1.5*np.sin(2*_t)+2*np.random.rand(_T)+1.5*np.sin(4*_t)+1.5*np.sin(8*_t)\nplt.plot(_x)\n\n\n\n\n\n\n\n\n\nimport itstgcn\n\n\nclass Eval_csy:\n    def __init__(self,learner,train_dataset):\n        self.learner = learner\n        # self.learner.model.eval()\n        try:self.learner.model.eval()\n        except:pass\n        self.train_dataset = train_dataset\n        self.lags = self.learner.lags\n        rslt_tr = self.learner(self.train_dataset) \n        self.X_tr = rslt_tr['X']\n        self.y_tr = rslt_tr['y']\n        self.f_tr = torch.concat([self.train_dataset[0].x.T,self.y_tr],axis=0).float()\n        self.yhat_tr = rslt_tr['yhat']\n        self.fhat_tr = torch.concat([self.train_dataset[0].x.T,self.yhat_tr],axis=0).float()\n\n\n_node_ids = {'node1':0,'node2':1}\n\n_FX1 = np.stack([_x,_x],axis=1).tolist()\n\n_edges1 = torch.tensor([[0,1]]).tolist()\n\ndata_dict1 = {'edges':_edges1, 'node_ids':_node_ids, 'FX':_FX1}\n\ndata1 = pd.DataFrame({'x':_x,'x1':_x,'xer':_x,'xer1':_x})\n\n\nloader1 = itstgcn.DatasetLoader(data_dict1)\n\n\ndataset = loader1.get_dataset(lags=2)\n\n\nmindex = itstgcn.rand_mindex(dataset,mrate=0.7)\ndataset_miss = itstgcn.miss(dataset,mindex,mtype='rand')\n\n\ndataset_padded = itstgcn.padding(dataset_miss,interpolation_method='linear')\n\n\nlrnr = itstgcn.StgcnLearner(dataset_padded)\n\n\nlrnr.learn(filters=16,epoch=10)\n\n10/10\n\n\n\nevtor = Eval_csy(lrnr,dataset_padded)\n\n\nlrnr_2 = itstgcn.ITStgcnLearner(dataset_padded)\n\n\nlrnr_2.learn(filters=16,epoch=10)\n\n10/10\n\n\n\nevtor_2 = Eval_csy(lrnr_2,dataset_padded)\n\nPsi\n\nwith plt.style.context('seaborn-white'):\n    fig, ax1 = plt.subplots(figsize=(40,15))\n    ax1.plot(_x,'k--',label='Observed Data',lw=3)\n    ax1.legend(fontsize=40,loc='lower left',facecolor='white', frameon=True)\n    ax1.tick_params(axis='y', labelsize=40)\n    ax1.tick_params(axis='x', labelsize=40)\n    ax1.set_ylim(-6,6)\nplt.savefig('Ebayes_fst.pdf', format='pdf')\n\n\n\n\n\n\n\n\nfourier transform\n\nwith plt.style.context('seaborn-white'):\n    fig, ax1 = plt.subplots(figsize=(40,15))\n    # ax1.plot(itstgcn.make_Psi(_T).T@np.array(_x),'-',color='C1',label='Fourier Transform',lw=3)\n    ax1.stem(itstgcn.make_Psi(_T).T@np.array(_x),linefmt='C1-',basefmt='k-',label='Fourier Transform')\n    ax1.legend(fontsize=40,loc='upper right',facecolor='white', frameon=True)\n    ax1.tick_params(axis='y', labelsize=40)\n    ax1.tick_params(axis='x', labelsize=40)\nplt.savefig('Ebayes_snd.pdf', format='pdf')\n\n\n\n\n\n\n\n\n\nwith plt.style.context('seaborn-white'):\n    fig, ax1 = plt.subplots(figsize=(40,15))\n    # ax1.plot(itstgcn.make_Psi(_T).T@np.array(_x),'-',color='C1',label='Fourier Transform',lw=3)\n    ax1.stem((itstgcn.make_Psi(_T).T@np.array(_x))[:100],linefmt='C1-',basefmt='k-',label='Fourier Transform')\n    ax1.legend(fontsize=40,loc='upper right',facecolor='white', frameon=True)\n    ax1.tick_params(axis='y', labelsize=40)\n    ax1.tick_params(axis='x', labelsize=40)\nplt.savefig('Ebayes_snd_zin.pdf', format='pdf')\n\n\n\n\n\n\n\n\nEbayesthresh/trim\n\nwith plt.style.context('seaborn-white'):\n    fig, ax1 = plt.subplots(figsize=(40,15))\n    # ax1.plot(ebayesthresh(FloatVector(itstgcn.make_Psi(_T).T@np.array(_x))),'-',color='C1',label='EbayesThresh',lw=3)\n    ax1.stem(ebayesthresh(FloatVector(itstgcn.make_Psi(_T).T@np.array(_x))),linefmt='C1-',basefmt='k-',label='EbayesThresh')\n    ax1.legend(fontsize=40,loc='upper right',facecolor='white', frameon=True)\n    ax1.tick_params(axis='y', labelsize=40)\n    ax1.tick_params(axis='x', labelsize=40)\nplt.savefig('Ebayes_trd.pdf', format='pdf')\n\n\n\n\n\n\n\n\n\nwith plt.style.context('seaborn-white'):\n    fig, ax1 = plt.subplots(figsize=(40,15))\n    # ax1.plot(ebayesthresh(FloatVector(itstgcn.make_Psi(_T).T@np.array(_x))),'-',color='C1',label='EbayesThresh',lw=3)\n    ax1.stem((ebayesthresh(FloatVector(itstgcn.make_Psi(_T).T@np.array(_x))))[:100],linefmt='C1-',basefmt='k-',label='EbayesThresh')\n    ax1.legend(fontsize=40,loc='upper right',facecolor='white', frameon=True)\n    ax1.tick_params(axis='y', labelsize=40)\n    ax1.tick_params(axis='x', labelsize=40)\nplt.savefig('Ebayes_trd_zout.pdf', format='pdf')\n\n\n\n\n\n\n\n\nfhat\n\nwith plt.style.context('seaborn-white'):\n    fig, ax1 = plt.subplots(figsize=(40,15))\n    ax1.plot(_x,'k--',label='Observed Data',lw=3,alpha=0.3)\n    ax1.plot(itstgcn.make_Psi(_T)@ebayesthresh(FloatVector(itstgcn.make_Psi(_T).T@np.array(_x))),'-',color='C1',label='Inverse Fourier Transform',lw=5)\n    ax1.legend(fontsize=40,loc='lower left',facecolor='white', frameon=True)\n    ax1.tick_params(axis='y', labelsize=40)\n    ax1.tick_params(axis='x', labelsize=40)\n    ax1.set_ylim(-6,6)\nplt.savefig('Ebayes_fth.pdf', format='pdf')\n\n\n\n\n\n\n\n\n\nwith plt.style.context('seaborn-white'):\n    fig, ax1 = plt.subplots(figsize=(40,15))\n    # fig.suptitle('Figure 1(node 1)',fontsize=40)\n    ax1.plot(data1['x'][:],'-',color='C3',label='Complete Data')\n    ax1.legend(fontsize=40,loc='lower left',facecolor='white', frameon=True)\n    ax1.tick_params(axis='y', labelsize=40)\n    ax1.tick_params(axis='x', labelsize=40)\n# plt.savefig('node1_fst.png')\n\n\n\n\n\n\n\n\n\nwith plt.style.context('seaborn-white'):\n    fig, ax2 = plt.subplots(figsize=(40,15))\n    # fig.suptitle('Figure 1(node 1)',fontsize=40)\n    ax2.plot(data1['x'][:],'--',color='C5',alpha=0.5,label='Complete Data')\n    ax2.plot(torch.cat([torch.tensor(data1['x'][:4]),torch.tensor(dataset_miss.targets).reshape(-1,2)[:,0]],dim=0),'--o',color='C3',label='Observed Data',markersize=15)\n    ax2.legend(fontsize=40,loc='lower left',facecolor='white', frameon=True)\n    ax2.tick_params(axis='y', labelsize=40)\n    ax2.tick_params(axis='x', labelsize=40)\n# plt.savefig('node1_snd.png')\n\n\n\n\n\n\n\n\n\nwith plt.style.context('seaborn-white'):\n    fig, ax3 = plt.subplots(figsize=(40,15))\n    # fig.suptitle('Figure 1(node 1)',fontsize=40)    \n    ax3.plot(data1['x'][:],'--',color='C5',alpha=0.5,label='Complete Data')\n    ax3.plot(evtor_2.f_tr[:,0],'--o',color='C3',alpha=0.8,label='Interpolarion')\n    ax3.legend(fontsize=40,loc='lower left',facecolor='white', frameon=True)\n    ax3.tick_params(axis='y', labelsize=40)\n    ax3.tick_params(axis='x', labelsize=40)\n# plt.savefig('node1_3rd.png')\n\n\n\n\n\n\n\n\n\nwith plt.style.context('seaborn-white'):\n    fig, ax4 = plt.subplots(figsize=(40,15))\n    # fig.suptitle('Figure 1(node 1)',fontsize=40)\n    ax4.plot(data1['x'][:],'--',color='C5',alpha=0.5,label='Complete Data')\n    ax4.plot(evtor.fhat_tr[:,0],color='brown',lw=3,label='STGCN')\n    ax4.plot(evtor_2.fhat_tr[:,0],color='blue',lw=3,label='ITSTGCN')\n    # ax4.plot(138, -1.2, 'o', markersize=230, markerfacecolor='none', markeredgecolor='red',markeredgewidth=3)\n    # ax4.plot(220, -1.5, 'o', markersize=200, markerfacecolor='none', markeredgecolor='red',markeredgewidth=3)\n    # ax4.plot(290, -1.2, 'o', markersize=310, markerfacecolor='none', markeredgecolor='red',markeredgewidth=3)\n    # ax4.plot(455, -0.9, 'o', markersize=280, markerfacecolor='none', markeredgecolor='red',markeredgewidth=3)\n    ax4.legend(fontsize=40,loc='lower left',facecolor='white', frameon=True)\n    ax4.tick_params(axis='y', labelsize=40)\n    ax4.tick_params(axis='x', labelsize=40)\n# plt.savefig('node1_4th_1.png')\n\n\n\n\n\n\n\n\n\n\nFor Paper\n\nT = 500\n\n\nt = np.arange(T)/T * 10\n\n\ny_true = 3*np.sin(0.5*t) + 1.2*np.sin(1.0*t) + 0.5*np.sin(1.2*t) \n\n\ny = y_true + np.random.normal(size=T)\n\n\nplt.figure(figsize=(20,10))\nplt.plot(t,y_true,color='red',label = 'true')\nplt.plot(t,y,'.',color='black',label = 'f')\nplt.legend(fontsize=15,loc='lower left',facecolor='white', frameon=True)\n# plt.savefig('1.png')\n\n\n\n\n\n\n\n\n\nf = np.array(y)\nif len(f.shape)==1: f = f.reshape(-1,1)\nT,N = f.shape\nPsi = make_Psi(T)\nfbar = Psi.T @ f # apply dft \n\n\nfbar_threshed = np.stack([ebayesthresh(FloatVector(fbar[:,i])) for i in range(N)],axis=1)\n\n\nplt.figure(figsize=(20,10))\nplt.plot(fbar**2,color='black')\n# plt.savefig('1.png')\n\n\n\n\n\n\n\n\n\nplt.figure(figsize=(20,10))\nplt.plot(fbar_threshed**2,color='blue')\n# plt.savefig('2.png')\n\n\n\n\n\n\n\n\n\nplt.figure(figsize = (20,10))\nax = plt.subplot(1, 1, 1)\nax.plot(range(0,500),(fbar**2),color='black')\nax.plot((fbar_threshed**2),color='blue')\naxins = inset_axes(ax, 8, 3, loc = 1, bbox_to_anchor=(0.8, 0.8),\n                   bbox_transform = ax.figure.transFigure)\naxins.plot(range(100, 200),(fbar**2)[100:200],color='black')\naxins.plot(range(100, 200),(fbar_threshed**2)[100:200],color='blue') \naxins.set_xlim(100, 200)\naxins.set_ylim(-0.1, 7)\nmark_inset(ax, axins, loc1=4, loc2=3, fc=\"none\", ec = \"0.01\")\nax.tick_params(axis='y', labelsize=20)\nax.tick_params(axis='x', labelsize=20)\naxins.tick_params(axis='y', labelsize=15)\naxins.tick_params(axis='x', labelsize=15)\n# plt.savefig('3.png')\n\n\n\n\n\n\n\n\n\nplt.figure(figsize=(20,10))\nplt.plot((fbar_threshed**2),color='blue')\n# plt.savefig('4.png')\n\n\n\n\n\n\n\n\n\nfbar_hat = Psi @ fbar_threshed # apply dft \n\n\nplt.figure(figsize=(20,10))\nplt.plot(y_true,'r',label = 'True')\nplt.plot(y,'k.',label='f')\nplt.plot(fbar_hat,color='blue',label = 'fhat')\nplt.legend(fontsize=15,loc='lower left',facecolor='white', frameon=True)\n# plt.savefig('5.png')"
  },
  {
    "objectID": "posts/2_research/2023-01-20-Algorithm_traintest.html",
    "href": "posts/2_research/2023-01-20-Algorithm_traintest.html",
    "title": "1st ST-GCN Example dividing train and test",
    "section": "",
    "text": "Try to divide train and test(GNAR fivenet)\ndef vis(STdata):\n    N = STdata.shape[1]\n    fig, ax = plt.subplots(5,1,(15,5))\n    for n in range(N):\n        ax[n].plot(fiveVTS[:,0])\n    #fig.set_width()\n    fig.set_height(N*5) \n    return fig"
  },
  {
    "objectID": "posts/2_research/2023-01-20-Algorithm_traintest.html#st-gcn",
    "href": "posts/2_research/2023-01-20-Algorithm_traintest.html#st-gcn",
    "title": "1st ST-GCN Example dividing train and test",
    "section": "1) ST-GCN",
    "text": "1) ST-GCN\n\nmean_f_fiveVTS_train = torch.tensor(fiveVTS_train_mean).reshape(160,5,1).float()\n\n\nmean_X_fiveVTS = mean_f_fiveVTS_train[:159,:,:]\nmean_y_fiveVTS = mean_f_fiveVTS_train[1:,:,:]\n\n\nmodel = RecurrentGCN(node_features=1, filters=4)\n\noptimizer = torch.optim.Adam(model.parameters(), lr=0.01)\n\nmodel.train()\n\nfor epoch in tqdm(range(50)):\n    for time, (xt,yt) in enumerate(zip(mean_X_fiveVTS,mean_y_fiveVTS)):\n        y_hat = model(xt, edge_index, edge_attr)\n        cost = torch.mean((y_hat-yt)**2)\n        cost.backward()\n        optimizer.step()\n        optimizer.zero_grad()\n\n100%|██████████| 50/50 [00:27&lt;00:00,  1.84it/s]\n\n\n\nmean_fhat_fiveVTS = torch.stack([model(xt, edge_index, edge_attr) for xt in mean_X_fiveVTS]).detach().numpy()\n\n\nxt_test = torch.tensor(fiveVTS_test.reshape(40,5,1)[:-1,:,:]).float()\n\n\nmean_fhat_fiveVTS_forecast = torch.stack([model(xt, edge_index, edge_attr) for xt in xt_test]).detach().numpy()\n\n\nvis2(fiveVTS_test[1:],mean_fhat_fiveVTS_forecast);\n\n\n\n\n\n\n\n\n\nvis2(fiveVTS_train_mean,mean_fhat_fiveVTS);"
  },
  {
    "objectID": "posts/2_research/2023-01-20-Algorithm_traintest.html#fourier-transform",
    "href": "posts/2_research/2023-01-20-Algorithm_traintest.html#fourier-transform",
    "title": "1st ST-GCN Example dividing train and test",
    "section": "2) Fourier transform",
    "text": "2) Fourier transform\n\nw=np.zeros((159*N,159*N))\n\n\nfor i in range(159*N):\n    for j in range(159*N):\n        if i==j :\n            w[i,j] = 0\n        elif np.abs(i-j) &lt;= 1 : \n            w[i,j] = 1\n\n\n# np.fft(mean_fhat_fiveVTS[:,0,0])\n\n\n# mean_fhat_fiveVTS.shape\n\n\n# fft_result =np.stack([np.fft.fft(mean_fhat_fiveVTS[:,n,0]) for n in range(N)]).T\n\n\n# plt.plot(abs(fft_result[:,0])**2)\n\n\nd = np.array(w.sum(axis=1))\nD = np.diag(d)\nL = np.array(np.diag(1/np.sqrt(d)) @ (D-w) @ np.diag(1/np.sqrt(d)))\nlamb, Psi = np.linalg.eigh(L)\nLamb = np.diag(lamb)\n\n\nfhatbar = Psi.T @ mean_fhat_fiveVTS.reshape(159*N,1)"
  },
  {
    "objectID": "posts/2_research/2023-01-20-Algorithm_traintest.html#ebayes",
    "href": "posts/2_research/2023-01-20-Algorithm_traintest.html#ebayes",
    "title": "1st ST-GCN Example dividing train and test",
    "section": "3) Ebayes",
    "text": "3) Ebayes\n\nebayesthresh = importr('EbayesThresh').ebayesthresh\nfhatbar_threshed = ebayesthresh(FloatVector(fhatbar))\n\n\nplt.plot(fhatbar)\nplt.plot(fhatbar_threshed)"
  },
  {
    "objectID": "posts/2_research/2023-01-20-Algorithm_traintest.html#inverse-fourier-transform",
    "href": "posts/2_research/2023-01-20-Algorithm_traintest.html#inverse-fourier-transform",
    "title": "1st ST-GCN Example dividing train and test",
    "section": "4) Inverse Fourier transform",
    "text": "4) Inverse Fourier transform\n\nfhatbarhat = Psi @ fhatbar_threshed\nfhatbarhat_mean_spatio_temporal = fhatbarhat.reshape(159,N,1)\n\n\nvis2(mean_fhat_fiveVTS,fhatbarhat_mean_spatio_temporal.reshape(159,5));"
  },
  {
    "objectID": "posts/2_research/2023-01-20-Algorithm_traintest.html#st-gcn-1",
    "href": "posts/2_research/2023-01-20-Algorithm_traintest.html#st-gcn-1",
    "title": "1st ST-GCN Example dividing train and test",
    "section": "5) ST-GCN",
    "text": "5) ST-GCN\n\nfiveVTS_train_mean[seed_number1,0] = fhatbarhat_mean_spatio_temporal[seed_number1,0,0]\nfiveVTS_train_mean[seed_number2,1] = fhatbarhat_mean_spatio_temporal[seed_number2,1,0]\nfiveVTS_train_mean[seed_number3,2] = fhatbarhat_mean_spatio_temporal[seed_number3,2,0]\nfiveVTS_train_mean[seed_number4,3] = fhatbarhat_mean_spatio_temporal[seed_number4,3,0]\nfiveVTS_train_mean[seed_number5,4] = fhatbarhat_mean_spatio_temporal[seed_number5,4,0]\nvis(fiveVTS_train_mean);\n\n\n\n\n\n\n\n\n\nmodel = RecurrentGCN(node_features=1, filters=4)\n\noptimizer = torch.optim.Adam(model.parameters(), lr=0.01)\n\nmodel.train()\nfor epoch in tqdm(range(50)):\n    for time, (xt,yt) in enumerate(zip(mean_X_fiveVTS,mean_y_fiveVTS)):\n        y_hat = model(xt, edge_index, edge_attr)\n        cost = torch.mean((y_hat-yt)**2)\n        cost.backward()\n        optimizer.step()\n        optimizer.zero_grad()\n\n100%|██████████| 50/50 [00:26&lt;00:00,  1.88it/s]\n\n\n\nmean_fhat_spatio_temporal = torch.stack([model(xt, edge_index, edge_attr) for xt in mean_X_fiveVTS]).detach().numpy()\n\n\nmean_fhat_spatio_temporal_test = torch.stack([model(xt, edge_index, edge_attr) for xt in xt_test]).detach().numpy()\n\n\nvis2(fiveVTS_test[1:],mean_fhat_spatio_temporal_test);\n\n\n\n\n\n\n\n\n\nvis2(fhatbarhat_mean_spatio_temporal,mean_fhat_spatio_temporal);\n\n\n\n\n\n\n\n\n\n\nfor i in tqdm(range(50)):\n    ## GFT \n    fhatbar = Psi.T @ mean_fhat_fiveVTS.reshape(159*N,1)\n\n    ## Ebayes\n    ebayesthresh = importr('EbayesThresh').ebayesthresh\n    fhatbar_threshed = ebayesthresh(FloatVector(fhatbar))\n    #plt.plot(fhatbar)\n    #plt.plot(fhatbar_threshed)\n\n    ## inverse GFT \n    fhatbarhat = Psi @ fhatbar_threshed\n    fhatbarhat_mean_spatio_temporal = fhatbarhat.reshape(159,N,1)\n    #vis2(mean_fhat_fiveVTS,fhatbarhat_mean_spatio_temporal.reshape(159,5));\n\n    ## STGCN \n    fiveVTS_train_mean[seed_number1,0] = fhatbarhat_mean_spatio_temporal[seed_number1,0,0]\n    fiveVTS_train_mean[seed_number2,1] = fhatbarhat_mean_spatio_temporal[seed_number1,1,0]\n    fiveVTS_train_mean[seed_number3,2] = fhatbarhat_mean_spatio_temporal[seed_number1,2,0]\n    fiveVTS_train_mean[seed_number4,3] = fhatbarhat_mean_spatio_temporal[seed_number1,3,0]\n    fiveVTS_train_mean[seed_number5,4] = fhatbarhat_mean_spatio_temporal[seed_number1,4,0]\n    #vis(fiveVTS_train_mean);\n\n    #model = RecurrentGCN(node_features=1, filters=4)\n\n    #optimizer = torch.optim.Adam(model.parameters(), lr=0.01)\n\n    #model.train()\n    for epoch in range(1):\n        for time, (xt,yt) in enumerate(zip(mean_X_fiveVTS,mean_y_fiveVTS)):\n            y_hat = model(xt, edge_index, edge_attr)\n            cost = torch.mean((y_hat-yt)**2)\n            cost.backward()\n            optimizer.step()\n            optimizer.zero_grad()\n\n    mean_fhat_spatio_temporal = torch.stack([model(xt, edge_index, edge_attr) for xt in mean_X_fiveVTS]).detach().numpy()\n    mean_fhat_spatio_temporal_test = torch.stack([model(xt, edge_index, edge_attr) for xt in xt_test]).detach().numpy()\n    #vis2(fiveVTS_test[1:],mean_fhat_spatio_temporal_test);\n    #vis2(fiveVTS_train_backup,mean_fhat_spatio_temporal);\n\n100%|██████████| 50/50 [00:55&lt;00:00,  1.10s/it]\n\n\n\nvis2(fiveVTS_train_backup,mean_fhat_spatio_temporal);\n\n\n\n\n\n\n\n\n\nvis2(fiveVTS_train_backup,mean_fhat_spatio_temporal);"
  },
  {
    "objectID": "posts/2_research/2023-01-20-Algorithm_traintest.html#fourier-transform-1",
    "href": "posts/2_research/2023-01-20-Algorithm_traintest.html#fourier-transform-1",
    "title": "1st ST-GCN Example dividing train and test",
    "section": "6) Fourier transform",
    "text": "6) Fourier transform\n\nw=np.zeros((159*N,159*N))\n\n\nfor i in range(159*N):\n    for j in range(159*N):\n        if i==j :\n            w[i,j] = 0\n        elif np.abs(i-j) &lt;= 1 : \n            w[i,j] = 1\n\n\nd = np.array(w.sum(axis=1))\nD = np.diag(d)\nL = np.array(np.diag(1/np.sqrt(d)) @ (D-w) @ np.diag(1/np.sqrt(d)))\nlamb, Psi = np.linalg.eigh(L)\nLamb = np.diag(lamb)\n\n\nfhatbar = Psi.T @ mean_fhat_spatio_temporal.reshape(159*N,1)\npower = fhatbar**2"
  },
  {
    "objectID": "posts/2_research/2023-01-20-Algorithm_traintest.html#ebayes-1",
    "href": "posts/2_research/2023-01-20-Algorithm_traintest.html#ebayes-1",
    "title": "1st ST-GCN Example dividing train and test",
    "section": "7) Ebayes",
    "text": "7) Ebayes\n\nebayesthresh = importr('EbayesThresh').ebayesthresh\nfhatbar_threshed = ebayesthresh(FloatVector(fhatbar))\n\n\nplt.plot(fhatbar)\n\n\n\n\n\n\n\n\n\nplt.plot(fhatbar)\nplt.plot(fhatbar_threshed)"
  },
  {
    "objectID": "posts/2_research/2023-01-20-Algorithm_traintest.html#inverse-fourier-transform-1",
    "href": "posts/2_research/2023-01-20-Algorithm_traintest.html#inverse-fourier-transform-1",
    "title": "1st ST-GCN Example dividing train and test",
    "section": "8) Inverse Fourier transform",
    "text": "8) Inverse Fourier transform\n\nfhatbarhat = Psi @ fhatbar_threshed\nfhatbarhat_mean_spatio_temporal2 = fhatbarhat.reshape(159,N,1)\n\n\nvis2(mean_fhat_spatio_temporal,fhatbarhat_mean_spatio_temporal2.reshape(159,5));"
  },
  {
    "objectID": "posts/2_research/2023-01-20-Algorithm_traintest.html#st-gcn-2",
    "href": "posts/2_research/2023-01-20-Algorithm_traintest.html#st-gcn-2",
    "title": "1st ST-GCN Example dividing train and test",
    "section": "9) ST-GCN",
    "text": "9) ST-GCN\n\nfiveVTS_train_mean[seed_number1,0] = fhatbarhat_mean_spatio_temporal[seed_number1,0,0]\nfiveVTS_train_mean[seed_number2,1] = fhatbarhat_mean_spatio_temporal[seed_number2,1,0]\nfiveVTS_train_mean[seed_number3,2] = fhatbarhat_mean_spatio_temporal[seed_number3,2,0]\nfiveVTS_train_mean[seed_number4,3] = fhatbarhat_mean_spatio_temporal[seed_number4,3,0]\nfiveVTS_train_mean[seed_number5,4] = fhatbarhat_mean_spatio_temporal[seed_number5,4,0]\nvis(fiveVTS_train_mean);\n\n\n\n\n\n\n\n\n\nmodel = RecurrentGCN(node_features=1, filters=4)\n\noptimizer = torch.optim.Adam(model.parameters(), lr=0.01)\n\nmodel.train()\nfor epoch in tqdm(range(50)):\n    for time, (xt,yt) in enumerate(zip(mean_X_fiveVTS,mean_y_fiveVTS)):\n        y_hat = model(xt, edge_index, edge_attr)\n        cost = torch.mean((y_hat-yt)**2)\n        cost.backward()\n        optimizer.step()\n        optimizer.zero_grad()\n\n100%|██████████| 50/50 [00:27&lt;00:00,  1.84it/s]\n\n\n\nmean_fhat_spatio_temporal2 = torch.stack([model(xt, edge_index, edge_attr) for xt in mean_X_fiveVTS]).detach().numpy()\n\n\nmean_fhat_spatio_temporal_test2 = torch.stack([model(xt, edge_index, edge_attr) for xt in xt_test]).detach().numpy()\n\n\nvis2(fiveVTS_test[1:],mean_fhat_spatio_temporal_test2);\n\n\n\n\n\n\n\n\n\nvis2(fhatbarhat_mean_spatio_temporal2,mean_fhat_spatio_temporal2);"
  },
  {
    "objectID": "posts/2_research/2023-01-20-Algorithm_traintest.html#fourier-transform-2",
    "href": "posts/2_research/2023-01-20-Algorithm_traintest.html#fourier-transform-2",
    "title": "1st ST-GCN Example dividing train and test",
    "section": "10) Fourier transform",
    "text": "10) Fourier transform\n\nw=np.zeros((159*N,159*N))\n\n\nfor i in range(159*N):\n    for j in range(159*N):\n        if i==j :\n            w[i,j] = 0\n        elif np.abs(i-j) &lt;= 1 : \n            w[i,j] = 1\n\n\nd = np.array(w.sum(axis=1))\nD = np.diag(d)\nL = np.array(np.diag(1/np.sqrt(d)) @ (D-w) @ np.diag(1/np.sqrt(d)))\nlamb, Psi = np.linalg.eigh(L)\nLamb = np.diag(lamb)\n\n\nfhatbar = Psi.T @ mean_fhat_spatio_temporal.reshape(159*N,1)\npower = fhatbar**2"
  },
  {
    "objectID": "posts/2_research/2023-01-20-Algorithm_traintest.html#ebayes-2",
    "href": "posts/2_research/2023-01-20-Algorithm_traintest.html#ebayes-2",
    "title": "1st ST-GCN Example dividing train and test",
    "section": "11) Ebayes",
    "text": "11) Ebayes\n\nebayesthresh = importr('EbayesThresh').ebayesthresh\nfhatbar_threshed = ebayesthresh(FloatVector(fhatbar))"
  },
  {
    "objectID": "posts/2_research/2023-01-20-Algorithm_traintest.html#inverse-fourier-transform-2",
    "href": "posts/2_research/2023-01-20-Algorithm_traintest.html#inverse-fourier-transform-2",
    "title": "1st ST-GCN Example dividing train and test",
    "section": "12) Inverse Fourier transform",
    "text": "12) Inverse Fourier transform\n\nfhatbarhat = Psi @ fhatbar_threshed\nfhatbarhat_mean_spatio_temporal3 = fhatbarhat.reshape(159,N,1)"
  },
  {
    "objectID": "posts/2_research/2023-01-20-Algorithm_traintest.html#st-gcn-3",
    "href": "posts/2_research/2023-01-20-Algorithm_traintest.html#st-gcn-3",
    "title": "1st ST-GCN Example dividing train and test",
    "section": "13) ST-GCN",
    "text": "13) ST-GCN\n\nfiveVTS_train_mean[seed_number1,0] = fhatbarhat_mean_spatio_temporal[seed_number1,0,0]\nfiveVTS_train_mean[seed_number2,1] = fhatbarhat_mean_spatio_temporal[seed_number2,1,0]\nfiveVTS_train_mean[seed_number3,2] = fhatbarhat_mean_spatio_temporal[seed_number3,2,0]\nfiveVTS_train_mean[seed_number4,3] = fhatbarhat_mean_spatio_temporal[seed_number4,3,0]\nfiveVTS_train_mean[seed_number5,4] = fhatbarhat_mean_spatio_temporal[seed_number5,4,0]\nvis(fiveVTS_train_mean);\n\n\n\n\n\n\n\n\n\nmodel = RecurrentGCN(node_features=1, filters=4)\n\noptimizer = torch.optim.Adam(model.parameters(), lr=0.01)\n\nmodel.train()\n\nfor epoch in tqdm(range(50)):\n    for time, (xt,yt) in enumerate(zip(mean_X_fiveVTS,mean_y_fiveVTS)):\n        y_hat = model(xt, edge_index, edge_attr)\n        cost = torch.mean((y_hat-yt)**2)\n        cost.backward()\n        optimizer.step()\n        optimizer.zero_grad()\n\n100%|██████████| 50/50 [00:26&lt;00:00,  1.86it/s]\n\n\n\nmean_fhat_spatio_temporal3 = torch.stack([model(xt, edge_index, edge_attr) for xt in mean_X_fiveVTS]).detach().numpy()\n\n\nmean_fhat_spatio_temporal_test3 = torch.stack([model(xt, edge_index, edge_attr) for xt in xt_test]).detach().numpy()\n\n\nvis2(fiveVTS_test[1:],mean_fhat_spatio_temporal_test3);\n\n\n\n\n\n\n\n\n\nvis2(fhatbarhat_mean_spatio_temporal3,mean_fhat_spatio_temporal3);\n\n\n\n\n\n\n\n\n\none = []\nfor i in range(N):\n    one.append(np.mean((fiveVTS_test[1:,i] - mean_fhat_fiveVTS_forecast.reshape(39,5)[:,i])))\n\n\ntwo = []\nfor i in range(N):\n    two.append(np.mean((fiveVTS_test[1:,i] - mean_fhat_spatio_temporal_test.reshape(39,5)[:,i])))\n\n\nthree = []\nfor i in range(N):\n    three.append(np.mean((fiveVTS_test[1:,i] - mean_fhat_spatio_temporal_test2.reshape(39,5)[:,i])))\n\n\nfour = []\nfor i in range(N):\n    four.append(np.mean((fiveVTS_test[1:,i] - mean_fhat_spatio_temporal_test3.reshape(39,5)[:,i])))\n\n\npd.DataFrame({'one':one,'two':two,'three':three,'four':four})\n\n\n\n\n\n\n\n\none\ntwo\nthree\nfour\n\n\n\n\n0\n-0.196310\n-0.189000\n-0.173563\n-0.200559\n\n\n1\n-0.161632\n-0.135003\n-0.142250\n-0.159892\n\n\n2\n0.079347\n0.106893\n0.108179\n0.079011\n\n\n3\n-0.267653\n-0.244438\n-0.248220\n-0.269292\n\n\n4\n-0.162464\n-0.135709\n-0.130221\n-0.167336"
  },
  {
    "objectID": "posts/2_research/2023-01-20-Algorithm_traintest.html#st-gcn-4",
    "href": "posts/2_research/2023-01-20-Algorithm_traintest.html#st-gcn-4",
    "title": "1st ST-GCN Example dividing train and test",
    "section": "1) ST-GCN",
    "text": "1) ST-GCN\n\nlinear_f_fiveVTS_train = torch.tensor(linear_fiveVTS_train).reshape(160,5,1).float()\n\n\nlinear_X_fiveVTS = linear_f_fiveVTS_train[:159,:,:]\nlinear_y_fiveVTS = linear_f_fiveVTS_train[1:,:,:]\n\n\nmodel = RecurrentGCN(node_features=1, filters=4)\n\noptimizer = torch.optim.Adam(model.parameters(), lr=0.01)\n\nmodel.train()\n\nfor epoch in tqdm(range(50)):\n    for time, (xt,yt) in enumerate(zip(linear_X_fiveVTS,linear_y_fiveVTS)):\n        y_hat = model(xt, edge_index, edge_attr)\n        cost = torch.mean((y_hat-yt)**2)\n        cost.backward()\n        optimizer.step()\n        optimizer.zero_grad()\n\n\nlinear_fhat_fiveVTS = torch.stack([model(xt, edge_index, edge_attr) for xt in linear_X_fiveVTS]).detach().numpy()\n\n\nxt_test = torch.tensor(fiveVTS_test.reshape(40,5,1)[:-1,:,:]).float()\n\n\nlinear_fhat_fiveVTS_forecast = torch.stack([model(xt, edge_index, edge_attr) for xt in xt_test]).detach().numpy()\n\n\nvis2(fiveVTS_test[1:],linear_fhat_fiveVTS_forecast);\n\n\nvis2(linear_fiveVTS_train,linear_f_fiveVTS_train);"
  },
  {
    "objectID": "posts/2_research/2023-01-20-Algorithm_traintest.html#fourier-transform-3",
    "href": "posts/2_research/2023-01-20-Algorithm_traintest.html#fourier-transform-3",
    "title": "1st ST-GCN Example dividing train and test",
    "section": "2) Fourier transform",
    "text": "2) Fourier transform\n\nw=np.zeros((159*N,159*N))\n\n\nfor i in range(159*N):\n    for j in range(159*N):\n        if i==j :\n            w[i,j] = 0\n        elif np.abs(i-j) &lt;= 1 : \n            w[i,j] = 1\n\n\nd = np.array(w.sum(axis=1))\nD = np.diag(d)\nL = np.array(np.diag(1/np.sqrt(d)) @ (D-w) @ np.diag(1/np.sqrt(d)))\nlamb, Psi = np.linalg.eigh(L)\nLamb = np.diag(lamb)\n\n\nfhatbar = Psi.T @ linear_fhat_fiveVTS.reshape(159*N,1)\npower = fhatbar**2"
  },
  {
    "objectID": "posts/2_research/2023-01-20-Algorithm_traintest.html#ebayes-3",
    "href": "posts/2_research/2023-01-20-Algorithm_traintest.html#ebayes-3",
    "title": "1st ST-GCN Example dividing train and test",
    "section": "3) Ebayes",
    "text": "3) Ebayes\n\nplt.plot(fhatbar.reshape(159,5)[:,0]**2)\n\n\nebayesthresh = importr('EbayesThresh').ebayesthresh\nfhatbar_threshed = ebayesthresh(FloatVector(fhatbar))\n\n\nplt.plot(fhatbar)\nplt.plot(fhatbar_threshed)"
  },
  {
    "objectID": "posts/2_research/2023-01-20-Algorithm_traintest.html#inverse-fourier-transform-3",
    "href": "posts/2_research/2023-01-20-Algorithm_traintest.html#inverse-fourier-transform-3",
    "title": "1st ST-GCN Example dividing train and test",
    "section": "4) Inverse Fourier transform",
    "text": "4) Inverse Fourier transform\n\nfhatbarhat = Psi @ fhatbar_threshed\nfhatbarhat_linear_spatio_temporal = fhatbarhat.reshape(159,N,1)\n\n\nvis2(linear_fhat_fiveVTS,fhatbarhat_linear_spatio_temporal.reshape(159,5));"
  },
  {
    "objectID": "posts/2_research/2023-01-20-Algorithm_traintest.html#st-gcn-5",
    "href": "posts/2_research/2023-01-20-Algorithm_traintest.html#st-gcn-5",
    "title": "1st ST-GCN Example dividing train and test",
    "section": "5) ST-GCN",
    "text": "5) ST-GCN\n\nlinear_spatio_temporal = torch.tensor(fhatbarhat_linear_spatio_temporal).reshape(159,5,1).float()\n\n\nlinear_X_spatio_temporal = linear_spatio_temporal[:158,:,:]\nlinear_y_spatio_temporal = linear_spatio_temporal[1:,:,:]\n\n\nmodel = RecurrentGCN(node_features=1, filters=4)\n\noptimizer = torch.optim.Adam(model.parameters(), lr=0.01)\n\nmodel.train()\n\nfor epoch in tqdm(range(50)):\n    for time, (xt,yt) in enumerate(zip(linear_X_spatio_temporal,linear_y_spatio_temporal)):\n        y_hat = model(xt, edge_index, edge_attr)\n        cost = torch.mean((y_hat-yt)**2)\n        cost.backward()\n        optimizer.step()\n        optimizer.zero_grad()\n\n\nlinear_fhat_spatio_temporal = torch.stack([model(xt, edge_index, edge_attr) for xt in linear_X_spatio_temporal]).detach().numpy()\n\n\nlinear_fhat_spatio_temporal_test = torch.stack([model(xt, edge_index, edge_attr) for xt in xt_test]).detach().numpy()\n\n\nvis2(fiveVTS_test[1:],linear_fhat_spatio_temporal_test);\n\n\nvis2(fhatbarhat_linear_spatio_temporal,linear_fhat_spatio_temporal);"
  },
  {
    "objectID": "posts/2_research/2023-01-20-Algorithm_traintest.html#fourier-transform-4",
    "href": "posts/2_research/2023-01-20-Algorithm_traintest.html#fourier-transform-4",
    "title": "1st ST-GCN Example dividing train and test",
    "section": "6) Fourier transform",
    "text": "6) Fourier transform\n\nw=np.zeros((158*N,158*N))\n\n\nfor i in range(158*N):\n    for j in range(158*N):\n        if i==j :\n            w[i,j] = 0\n        elif np.abs(i-j) &lt;= 1 : \n            w[i,j] = 1\n\n\nd = np.array(w.sum(axis=1))\nD = np.diag(d)\nL = np.array(np.diag(1/np.sqrt(d)) @ (D-w) @ np.diag(1/np.sqrt(d)))\nlamb, Psi = np.linalg.eigh(L)\nLamb = np.diag(lamb)\n\n\nfhatbar = Psi.T @ linear_fhat_spatio_temporal.reshape(158*N,1)\npower = fhatbar**2"
  },
  {
    "objectID": "posts/2_research/2023-01-20-Algorithm_traintest.html#ebayes-4",
    "href": "posts/2_research/2023-01-20-Algorithm_traintest.html#ebayes-4",
    "title": "1st ST-GCN Example dividing train and test",
    "section": "7) Ebayes",
    "text": "7) Ebayes\n\nebayesthresh = importr('EbayesThresh').ebayesthresh\nfhatbar_threshed = ebayesthresh(FloatVector(fhatbar))\n\n\nplt.plot(fhatbar)\nplt.plot(fhatbar_threshed)"
  },
  {
    "objectID": "posts/2_research/2023-01-20-Algorithm_traintest.html#inverse-fourier-transform-4",
    "href": "posts/2_research/2023-01-20-Algorithm_traintest.html#inverse-fourier-transform-4",
    "title": "1st ST-GCN Example dividing train and test",
    "section": "8) Inverse Fourier transform",
    "text": "8) Inverse Fourier transform\n\nfhatbarhat = Psi @ fhatbar_threshed\nfhatbarhat_linear_spatio_temporal2 = fhatbarhat.reshape(158,N,1)\n\n\nvis2(linear_fhat_spatio_temporal,fhatbarhat_linear_spatio_temporal2.reshape(158,5));"
  },
  {
    "objectID": "posts/2_research/2023-01-20-Algorithm_traintest.html#st-gcn-6",
    "href": "posts/2_research/2023-01-20-Algorithm_traintest.html#st-gcn-6",
    "title": "1st ST-GCN Example dividing train and test",
    "section": "9) ST-GCN",
    "text": "9) ST-GCN\n\nlinear_spatio_temporal2 = torch.tensor(fhatbarhat_linear_spatio_temporal2).reshape(158,5,1).float()\n\n\nlinear_X_spatio_temporal2 = linear_spatio_temporal2[:157,:,:]\nlinear_y_spatio_temporal2 = linear_spatio_temporal2[1:,:,:]\n\n\nmodel = RecurrentGCN(node_features=1, filters=4)\n\noptimizer = torch.optim.Adam(model.parameters(), lr=0.01)\n\nmodel.train()\n\nfor epoch in tqdm(range(50)):\n    for time, (xt,yt) in enumerate(zip(linear_X_spatio_temporal2,linear_y_spatio_temporal2)):\n        y_hat = model(xt, edge_index, edge_attr)\n        cost = torch.mean((y_hat-yt)**2)\n        cost.backward()\n        optimizer.step()\n        optimizer.zero_grad()\n\n\nlinear_fhat_spatio_temporal2 = torch.stack([model(xt, edge_index, edge_attr) for xt in linear_X_spatio_temporal2]).detach().numpy()\n\n\nlinear_fhat_spatio_temporal_test2 = torch.stack([model(xt, edge_index, edge_attr) for xt in xt_test]).detach().numpy()\n\n\nvis2(fiveVTS_test[1:],linear_fhat_spatio_temporal_test2);\n\n\nvis2(fhatbarhat_linear_spatio_temporal2,linear_fhat_spatio_temporal2);\n\n\none = []\nfor i in range(N):\n    one.append(np.mean((fiveVTS_test[1:,i] - linear_fhat_fiveVTS_forecast.reshape(39,5)[:,i])))\n\n\ntwo = []\nfor i in range(N):\n    two.append(np.mean((fiveVTS_test[1:,i] - linear_fhat_spatio_temporal_test.reshape(39,5)[:,i])))\n\n\nthree = []\nfor i in range(N):\n    three.append(np.mean((fiveVTS_test[1:,i] - linear_fhat_spatio_temporal_test2.reshape(39,5)[:,i])))\n\n\npd.DataFrame({'one':one,'two':two,'three':three})"
  },
  {
    "objectID": "posts/2_research/2023-01-20-Algorithm_traintest.html#fourier-transform-5",
    "href": "posts/2_research/2023-01-20-Algorithm_traintest.html#fourier-transform-5",
    "title": "1st ST-GCN Example dividing train and test",
    "section": "10) Fourier transform",
    "text": "10) Fourier transform\n\nw=np.zeros((157*N,157*N))\n\n\nfor i in range(157*N):\n    for j in range(157*N):\n        if i==j :\n            w[i,j] = 0\n        elif np.abs(i-j) &lt;= 1 : \n            w[i,j] = 1\n\n\nd = np.array(w.sum(axis=1))\nD = np.diag(d)\nL = np.array(np.diag(1/np.sqrt(d)) @ (D-w) @ np.diag(1/np.sqrt(d)))\nlamb, Psi = np.linalg.eigh(L)\nLamb = np.diag(lamb)\n\n\nfhatbar = Psi.T @ linear_fhat_spatio_temporal2.reshape(157*N,1)"
  },
  {
    "objectID": "posts/2_research/2023-01-20-Algorithm_traintest.html#ebayes-5",
    "href": "posts/2_research/2023-01-20-Algorithm_traintest.html#ebayes-5",
    "title": "1st ST-GCN Example dividing train and test",
    "section": "11) Ebayes",
    "text": "11) Ebayes\n\nebayesthresh = importr('EbayesThresh').ebayesthresh\nfhatbar_threshed = ebayesthresh(FloatVector(fhatbar))\n\n\nplt.plot(fhatbar)\nplt.plot(fhatbar_threshed)"
  },
  {
    "objectID": "posts/2_research/2023-01-20-Algorithm_traintest.html#inverse-fourier-transform-5",
    "href": "posts/2_research/2023-01-20-Algorithm_traintest.html#inverse-fourier-transform-5",
    "title": "1st ST-GCN Example dividing train and test",
    "section": "12) Inverse Fourier transform",
    "text": "12) Inverse Fourier transform\n\nfhatbarhat = Psi @ fhatbar_threshed\nfhatbarhat_linear_spatio_temporal3 = fhatbarhat.reshape(157,N,1)\n\n\nvis2(linear_fhat_spatio_temporal2,fhatbarhat_linear_spatio_temporal3.reshape(157,5));"
  },
  {
    "objectID": "posts/2_research/2023-01-20-Algorithm_traintest.html#st-gcn-7",
    "href": "posts/2_research/2023-01-20-Algorithm_traintest.html#st-gcn-7",
    "title": "1st ST-GCN Example dividing train and test",
    "section": "13) ST-GCN",
    "text": "13) ST-GCN\n\nlinear_spatio_temporal3 = torch.tensor(fhatbarhat_linear_spatio_temporal3).reshape(157,5,1).float()\n\n\nlinear_X_spatio_temporal3 = linear_spatio_temporal3[:156,:,:]\nlinear_y_spatio_temporal3 = linear_spatio_temporal3[1:,:,:]\n\n\nmodel = RecurrentGCN(node_features=1, filters=4)\n\noptimizer = torch.optim.Adam(model.parameters(), lr=0.01)\n\nmodel.train()\n\nfor epoch in tqdm(range(50)):\n    for time, (xt,yt) in enumerate(zip(linear_X_spatio_temporal3,linear_y_spatio_temporal3)):\n        y_hat = model(xt, edge_index, edge_attr)\n        cost = torch.mean((y_hat-yt)**2)\n        cost.backward()\n        optimizer.step()\n        optimizer.zero_grad()\n\n\nlinear_fhat_spatio_temporal3 = torch.stack([model(xt, edge_index, edge_attr) for xt in linear_X_spatio_temporal3]).detach().numpy()\n\n\nlinear_fhat_spatio_temporal_test3 = torch.stack([model(xt, edge_index, edge_attr) for xt in xt_test]).detach().numpy()\n\n\nvis2(fiveVTS_test[1:],linear_fhat_spatio_temporal_test3);\n\n\nvis2(fhatbarhat_linear_spatio_temporal3,linear_fhat_spatio_temporal3);\n\n\none = []\nfor i in range(N):\n    one.append(np.mean((fiveVTS_test[1:,i] - linear_fhat_fiveVTS_forecast.reshape(39,5)[:,i])))\n\n\ntwo = []\nfor i in range(N):\n    two.append(np.mean((fiveVTS_test[1:,i] - linear_fhat_spatio_temporal_test.reshape(39,5)[:,i])))\n\n\nthree = []\nfor i in range(N):\n    three.append(np.mean((fiveVTS_test[1:,i] - linear_fhat_spatio_temporal_test2.reshape(39,5)[:,i])))\n\n\nfour = []\nfor i in range(N):\n    four.append(np.mean((fiveVTS_test[1:,i] - linear_fhat_spatio_temporal_test3.reshape(39,5)[:,i])))\n\n\npd.DataFrame({'one':one,'two':two,'three':three,'four':four})"
  },
  {
    "objectID": "posts/2_research/2023-01-26-ESTGCN_GNAR_DATA.html",
    "href": "posts/2_research/2023-01-26-ESTGCN_GNAR_DATA.html",
    "title": "Class of Method(GNAR) lag 1",
    "section": "",
    "text": "GNAR fiveNet,fivenodes"
  },
  {
    "objectID": "posts/2_research/2023-01-26-ESTGCN_GNAR_DATA.html#시나리오1-baseline",
    "href": "posts/2_research/2023-01-26-ESTGCN_GNAR_DATA.html#시나리오1-baseline",
    "title": "Class of Method(GNAR) lag 1",
    "section": "시나리오1 (Baseline)",
    "text": "시나리오1 (Baseline)\n시나리오1\n\nmissing rate: 0%\n보간방법: None\n\n\nSTGCN 으로 적합 + 예측\n\nX = torch.tensor(fiveVTS_train).reshape(int(T*0.8),N,1).float()[:int(T*0.8-1),:,:]\ny = torch.tensor(fiveVTS_train).reshape(int(T*0.8),N,1).float()[1:,:,:]\n\n\nXX = torch.tensor(fiveVTS_test.reshape(int(T*0.2),N,1)[:-1,:,:]).float()\nyy = torch.tensor(fiveVTS_test.reshape(int(T*0.2),N,1)[1:,:,:]).float()\n\n\nnet = RecurrentGCN(node_features=1, filters=4)\noptimizer = torch.optim.Adam(net.parameters(), lr=0.01)\nnet.train()\nfor epoch in tqdm(range(50)):\n    for time, (xt,yt) in enumerate(zip(X,y)):\n        yt_hat = net(xt, edge_index, edge_attr)\n        cost = torch.mean((yt_hat-yt)**2)\n        cost.backward()\n        optimizer.step()\n        optimizer.zero_grad()\n\n100%|██████████| 50/50 [00:34&lt;00:00,  1.43it/s]\n\n\n\nyhat = torch.stack([net(xt, edge_index, edge_attr) for xt in X]).detach().numpy()\nyyhat = torch.stack([net(xt, edge_index, edge_attr) for xt in XX]).detach().numpy()\n\n\nstgcn_train = yhat.squeeze() # stgcn은 stgcn에 의한 적합결과를 의미함\nstgcn_test = yyhat.squeeze() \n\n\ntrain_mse_eachnode_stgcn = (((y-yhat).squeeze())**2).mean(axis=0)\ntrain_mse_total_stgcn = (((y-yhat).squeeze())**2).mean()\ntest_mse_eachnode_stgcn = (((yy-yyhat).squeeze())**2).mean(axis=0)\ntest_mse_total_stgcn = (((yy-yyhat).squeeze())**2).mean()\n\n\n\nGNAR 으로 적합 + 예측\n-\n\n%load_ext rpy2.ipython\n\nThe rpy2.ipython extension is already loaded. To reload it, use:\n  %reload_ext rpy2.ipython\n\n\n\n%%R\nlibrary(GNAR)\nlibrary(igraph)\nlibrary(tidyverse)\n\n\n%R -i fiveVTS_train\n\n\n%%R\nanswer &lt;- GNARfit(vts = fiveVTS_train, net = fiveNet, alphaOrder = 1, betaOrder = c(1))\nprediction &lt;- predict(answer,n.ahead=40)\n\n\n%%R\ngnar_train &lt;- residuals(answer)\ngnar_test &lt;- prediction\n\n\n%R -o gnar_train\n%R -o gnar_test\n\n\ntrain_mse_eachnode_gnar = (gnar_train**2).mean(axis=0)\ntrain_mse_total_gnar = (gnar_train**2).mean()\ntest_mse_eachnode_gnar = ((fiveVTS_test - gnar_test.reshape(-1,5))**2).mean(axis=0)\ntest_mse_total_gnar = ((fiveVTS_test - gnar_test.reshape(-1,5))**2).mean()\n\n\n\n결과시각화\n\ntrain_mse_total_gnar,test_mse_total_gnar\n\n(0.9994323113693153, 1.2692101967317866)\n\n\n\nfig = plot(fiveVTS,'--.',h=4,color='gray',label='complete data',alpha=0.5)\nax = fig.get_axes()\nfor i,a in enumerate(ax):     \n    a.set_title('node{0} \\n mse(train) = {1:.2f}, mse(test) = {2:.2f} \\n mse(train) = {3:.2f}, mse(test) = {4:.2f}'.format(i,train_mse_eachnode_stgcn[i],test_mse_eachnode_stgcn[i],train_mse_eachnode_gnar[i],test_mse_eachnode_gnar[i]))\n    a.plot(range(1,160),stgcn_train[:,i],label='STCGCN (train)',color='C0')\n    a.plot(range(160,199),stgcn_test[:,i],label='STCGCN (test)',color='C0')\n    a.plot(range(1,160),gnar_train.reshape(-1,5)[:,i],label='GNAR (train)',color='C1')\n    a.plot(range(161,201),gnar_test.reshape(-1,5)[:,i],label='GNAR (test)',color='C1')\n    a.legend()\nfig.set_figwidth(14)\nfig.suptitle(\"Scenario1: STGCN \\n missing=0% \\n interpolation=None \\n\\n STGCN: mse(train) = {0:.2f}, mse(test) = {1:.2f} \\n GNAR: mse(train) = {2:.2f}, mse(test) = {3:.2f} \\n\".format(train_mse_total_stgcn,test_mse_total_stgcn,train_mse_total_gnar,test_mse_total_gnar),size=15)\nfig.tight_layout()\nfig"
  },
  {
    "objectID": "posts/2_research/2023-01-26-ESTGCN_GNAR_DATA.html#시나리오2",
    "href": "posts/2_research/2023-01-26-ESTGCN_GNAR_DATA.html#시나리오2",
    "title": "Class of Method(GNAR) lag 1",
    "section": "시나리오2",
    "text": "시나리오2\n시나리오2\n\nmissing rate: 50%\n보간방법: linear\n\n- 결측치생성 + 보간\n\n_zero = Missing(fiveVTS_train)\n_zero.miss(percent = 0.5)\n_zero.second_linear()\n\n\nmissing_index = _zero.number\ninterpolated_signal = _zero.train_linear\n\n\nfig = plot(fiveVTS,'--o',h=4,color='gray',label='complete data',alpha=0.2)\nax = fig.get_axes()\nfor i,a in enumerate(ax):     \n    a.plot(missing_index[i],fiveVTS_train[:,i][missing_index[i]],'xk',label='missing')\n    a.plot(interpolated_signal[:,i],'-',color='gray',label='linear interpolation')\n    a.legend()\nfig.set_figwidth(15)\nfig\n\n\n\n\n\n\n\n\n\nSTGCN 으로 적합 + 예측\n\nX = torch.tensor(interpolated_signal).reshape(int(T*0.8),N,1).float()[:int(T*0.8-1),:,:]\ny = torch.tensor(interpolated_signal).reshape(int(T*0.8),N,1).float()[1:,:,:]\n\n\nXX = torch.tensor(fiveVTS_test.reshape(int(T*0.2),N,1)[:-1,:,:]).float()\nyy = torch.tensor(fiveVTS_test.reshape(int(T*0.2),N,1)[1:,:,:]).float()\n\n\nnet = RecurrentGCN(node_features=1, filters=4)\noptimizer = torch.optim.Adam(net.parameters(), lr=0.01)\nnet.train()\nfor epoch in tqdm(range(50)):\n    for time, (xt,yt) in enumerate(zip(X,y)):\n        yt_hat = net(xt, edge_index, edge_attr)\n        cost = torch.mean((yt_hat-yt)**2)\n        cost.backward()\n        optimizer.step()\n        optimizer.zero_grad()\n\n100%|██████████| 50/50 [00:35&lt;00:00,  1.41it/s]\n\n\n\nyhat = torch.stack([net(xt, edge_index, edge_attr) for xt in X]).detach().numpy()\nyyhat = torch.stack([net(xt, edge_index, edge_attr) for xt in XX]).detach().numpy()\n\n\nreal_y = torch.tensor(fiveVTS_train).reshape(int(T*0.8),N,1).float()[1:,:,:]\n\ntrain_mse_eachnode_stgcn = (((real_y-yhat).squeeze())**2).mean(axis=0)\ntrain_mse_total_stgcn = (((real_y-yhat).squeeze())**2).mean()\ntest_mse_eachnode_stgcn = (((yy-yyhat).squeeze())**2).mean(axis=0)\ntest_mse_total_stgcn = (((yy-yyhat).squeeze())**2).mean()\n\n\nstgcn_train = yhat.squeeze() # stgcn은 stgcn에 의한 적합결과를 의미함\nstgcn_test = yyhat.squeeze() \n\n\n\nESTGCN 으로 적합 + 예측\n\nX = torch.tensor(interpolated_signal).reshape(int(T*0.8),N,1).float()[:int(T*0.8-1),:,:]\ny = torch.tensor(interpolated_signal).reshape(int(T*0.8),N,1).float()[1:,:,:]\n\n\nXX = torch.tensor(fiveVTS_test.reshape(int(T*0.2),N,1)[:-1,:,:]).float()\nyy = torch.tensor(fiveVTS_test.reshape(int(T*0.2),N,1)[1:,:,:]).float()\n\n- ESTGCN\n\nnet = RecurrentGCN(node_features=1, filters=4)\noptimizer = torch.optim.Adam(net.parameters(), lr=0.01)\nnet.train()\nsignal = interpolated_signal.copy()\nfor epoch in tqdm(range(50)):\n    signal = update_from_freq_domain(signal,missing_index)\n    X = torch.tensor(signal).reshape(int(T*0.8),N,1).float()[:int(T*0.8-1),:,:]\n    y = torch.tensor(signal).reshape(int(T*0.8),N,1).float()[1:,:,:]\n    for time, (xt,yt) in enumerate(zip(X,y)):        \n        yt_hat = net(xt, edge_index, edge_attr)\n        cost = torch.mean((yt_hat-yt)**2)\n        cost.backward()\n        optimizer.step()\n        optimizer.zero_grad()\n    signal = torch.concat([X.squeeze(),yt_hat.detach().squeeze().reshape(1,-1)])        \n\n100%|██████████| 50/50 [00:37&lt;00:00,  1.33it/s]\n\n\n\nyhat = torch.stack([net(xt, edge_index, edge_attr) for xt in X]).detach().numpy()\nyyhat = torch.stack([net(xt, edge_index, edge_attr) for xt in XX]).detach().numpy()\n\n\nreal_y = torch.tensor(fiveVTS_train).reshape(int(T*0.8),N,1).float()[1:,:,:]\n\ntrain_mse_eachnode_estgcn = (((real_y-yhat).squeeze())**2).mean(axis=0)\ntrain_mse_total_estgcn = (((real_y-yhat).squeeze())**2).mean()\ntest_mse_eachnode_estgcn = (((yy-yyhat).squeeze())**2).mean(axis=0)\ntest_mse_total_estgcn = (((yy-yyhat).squeeze())**2).mean()\n\n\nestgcn_train = yhat.squeeze() # stgcn은 stgcn에 의한 적합결과를 의미함\nestgcn_test = yyhat.squeeze() \n\n\n\nGNAR 으로 적합 + 예측\n-\n\nX_train1 = np.array(X).squeeze()\nX_test1 =  np.array(XX).squeeze()\n\n\n%R -i X_train1\n\n\n%%R\nanswer &lt;- GNARfit(vts = X_train1, net = fiveNet, alphaOrder = 1, betaOrder = c(1))\nprediction &lt;- predict(answer,n.ahead=40)\n\n\n%%R\ngnar_train &lt;- residuals(answer)\ngnar_test &lt;- prediction\n\n\n%R -o gnar_train\n%R -o gnar_test\n\n\ntrain_mse_eachnode_gnar = (gnar_train**2).mean(axis=0)\ntrain_mse_total_gnar = (gnar_train**2).mean()\ntest_mse_eachnode_gnar = ((X_test1 - gnar_test[1:,:])**2).mean(axis=0)\ntest_mse_total_gnar = ((X_test1 - gnar_test[1:,:])**2).mean()\n\n\n\n결과시각화\n\ntrain_mse_total_gnar,test_mse_total_gnar\n\n(0.7473098322871093, 1.3231643342748722)\n\n\n\nfig = plot(fiveVTS,'--.',h=4,color='gray',label='complete data',alpha=0.5)\nax = fig.get_axes()\nfor i,a in enumerate(ax):     \n    a.set_title('node{0} \\n STGCN: mse(train) = {1:.2f}, mse(test) = {2:.2f} \\n ESTGCN: mse(train) = {3:.2f}, mse(test) = {4:.2f}\\n GNAR: mse(train) = {5:.2f}, mse(test) = {6:.2f}'.format(i,train_mse_eachnode_stgcn[i],test_mse_eachnode_stgcn[i],train_mse_eachnode_estgcn[i],test_mse_eachnode_estgcn[i],train_mse_eachnode_gnar[i],test_mse_eachnode_gnar[i]))\n    a.plot(missing_index[i],fiveVTS_train[:,i][missing_index[i]],'xk',label='missing')\n    a.plot(interpolated_signal[:,i],'-',color='gray',label='linear interpolation')\n    a.plot(range(1,160),stgcn_train.squeeze()[:,i],'--.',label='STCGCN (train)',color='C0')\n    a.plot(range(160,199),stgcn_test.squeeze()[:,i],'--.',label='STCGCN (test)',color='C0')\n    a.plot(range(1,160),estgcn_train.squeeze()[:,i],label='ESTCGCN (train)',color='C1')\n    a.plot(range(161,200),estgcn_test.squeeze()[:,i],label='ESTCGCN (test)',color='C1')\n    a.plot(range(1,159),gnar_train[:,i],label='GNAR (train)',color='C2')\n    a.plot(range(161,201),gnar_test[:,i],label='GNAR (test)',color='C2')\n    \n    a.legend()\nfig.set_figwidth(14)\nfig.suptitle(\"Scenario2: \\n missing=50% \\n interpolation=linear \\n\\n STGCN: mse(train) = {0:.2f}, mse(test) = {1:.2f} \\n ESTGCN: mse(train) = {2:.2f}, mse(test) = {3:.2f} \\n GNAR: mse(train) = {4:.2f}, mse(test) = {5:.2f} \\n\".format(train_mse_total_stgcn,test_mse_total_stgcn,train_mse_total_estgcn,test_mse_total_estgcn,train_mse_total_gnar,test_mse_total_gnar),size=15)\nfig.tight_layout()\nfig"
  },
  {
    "objectID": "posts/2_research/2023-01-26-ESTGCN_GNAR_DATA.html#시나리오3",
    "href": "posts/2_research/2023-01-26-ESTGCN_GNAR_DATA.html#시나리오3",
    "title": "Class of Method(GNAR) lag 1",
    "section": "시나리오3",
    "text": "시나리오3\n시나리오3\n\nmissing rate: 80%\n보간방법: linear\n\n- 결측치생성 + 보간\n\n_zero = Missing(fiveVTS_train)\n_zero.miss(percent = 0.8)\n_zero.second_linear()\n\n\nmissing_index = _zero.number\ninterpolated_signal = _zero.train_linear\n\n\nfig = plot(fiveVTS,'--o',h=4,color='gray',label='complete data',alpha=0.2)\nax = fig.get_axes()\nfor i,a in enumerate(ax):     \n    a.plot(missing_index[i],fiveVTS_train[:,i][missing_index[i]],'xk',label='missing')\n    a.plot(interpolated_signal[:,i],'-',color='gray',label='linear interpolation')\n    a.legend()\nfig.set_figwidth(15)\nfig\n\n\n\n\n\n\n\n\n\nSTGCN 으로 적합 + 예측\n\nX = torch.tensor(interpolated_signal).reshape(int(T*0.8),N,1).float()[:int(T*0.8-1),:,:]\ny = torch.tensor(interpolated_signal).reshape(int(T*0.8),N,1).float()[1:,:,:]\n\n\nXX = torch.tensor(fiveVTS_test.reshape(int(T*0.2),N,1)[:-1,:,:]).float()\nyy = torch.tensor(fiveVTS_test.reshape(int(T*0.2),N,1)[1:,:,:]).float()\n\n\nnet = RecurrentGCN(node_features=1, filters=4)\noptimizer = torch.optim.Adam(net.parameters(), lr=0.01)\nnet.train()\nfor epoch in tqdm(range(50)):\n    for time, (xt,yt) in enumerate(zip(X,y)):\n        yt_hat = net(xt, edge_index, edge_attr)\n        cost = torch.mean((yt_hat-yt)**2)\n        cost.backward()\n        optimizer.step()\n        optimizer.zero_grad()\n\n100%|██████████| 50/50 [00:35&lt;00:00,  1.39it/s]\n\n\n\nyhat = torch.stack([net(xt, edge_index, edge_attr) for xt in X]).detach().numpy()\nyyhat = torch.stack([net(xt, edge_index, edge_attr) for xt in XX]).detach().numpy()\n\n\nreal_y = torch.tensor(fiveVTS_train).reshape(int(T*0.8),N,1).float()[1:,:,:]\n\ntrain_mse_eachnode_stgcn = (((real_y-yhat).squeeze())**2).mean(axis=0)\ntrain_mse_total_stgcn = (((real_y-yhat).squeeze())**2).mean()\ntest_mse_eachnode_stgcn = (((yy-yyhat).squeeze())**2).mean(axis=0)\ntest_mse_total_stgcn = (((yy-yyhat).squeeze())**2).mean()\n\n\nstgcn_train = yhat.squeeze() # stgcn은 stgcn에 의한 적합결과를 의미함\nstgcn_test = yyhat.squeeze() \n\n\n\nESTGCN 으로 적합 + 예측\n\nX = torch.tensor(interpolated_signal).reshape(int(T*0.8),N,1).float()[:int(T*0.8-1),:,:]\ny = torch.tensor(interpolated_signal).reshape(int(T*0.8),N,1).float()[1:,:,:]\n\n\nXX = torch.tensor(fiveVTS_test.reshape(int(T*0.2),N,1)[:-1,:,:]).float()\nyy = torch.tensor(fiveVTS_test.reshape(int(T*0.2),N,1)[1:,:,:]).float()\n\n- ESTGCN\n\nnet = RecurrentGCN(node_features=1, filters=4)\noptimizer = torch.optim.Adam(net.parameters(), lr=0.01)\nnet.train()\nsignal = interpolated_signal.copy()\nfor epoch in tqdm(range(50)):\n    signal = update_from_freq_domain(signal,missing_index)\n    X = torch.tensor(signal).reshape(int(T*0.8),N,1).float()[:int(T*0.8-1),:,:]\n    y = torch.tensor(signal).reshape(int(T*0.8),N,1).float()[1:,:,:]\n    for time, (xt,yt) in enumerate(zip(X,y)):        \n        yt_hat = net(xt, edge_index, edge_attr)\n        cost = torch.mean((yt_hat-yt)**2)\n        cost.backward()\n        optimizer.step()\n        optimizer.zero_grad()\n    signal = torch.concat([X.squeeze(),yt_hat.detach().squeeze().reshape(1,-1)])        \n\n100%|██████████| 50/50 [00:37&lt;00:00,  1.33it/s]\n\n\n\nyhat = torch.stack([net(xt, edge_index, edge_attr) for xt in X]).detach().numpy()\nyyhat = torch.stack([net(xt, edge_index, edge_attr) for xt in XX]).detach().numpy()\n\n\nreal_y = torch.tensor(fiveVTS_train).reshape(int(T*0.8),N,1).float()[1:,:,:]\n\ntrain_mse_eachnode_estgcn = (((real_y-yhat).squeeze())**2).mean(axis=0)\ntrain_mse_total_estgcn = (((real_y-yhat).squeeze())**2).mean()\ntest_mse_eachnode_estgcn = (((yy-yyhat).squeeze())**2).mean(axis=0)\ntest_mse_total_estgcn = (((yy-yyhat).squeeze())**2).mean()\n\n\nestgcn_train = yhat.squeeze() # stgcn은 stgcn에 의한 적합결과를 의미함\nestgcn_test = yyhat.squeeze() \n\n\n\nGNAR 으로 적합 + 예측\n-\n\nX_train1 = np.array(X).squeeze()\nX_test1 =  np.array(XX).squeeze()\n\n\n%R -i X_train1\n\n\n%%R\nanswer &lt;- GNARfit(vts = X_train1, net = fiveNet, alphaOrder = 1, betaOrder = c(1))\nprediction &lt;- predict(answer,n.ahead=40)\n\n\n%%R\ngnar_train &lt;- residuals(answer)\ngnar_test &lt;- prediction\n\n\n%R -o gnar_train\n%R -o gnar_test\n\n\ntrain_mse_eachnode_gnar = (gnar_train**2).mean(axis=0)\ntrain_mse_total_gnar = (gnar_train**2).mean()\ntest_mse_eachnode_gnar = ((X_test1 - gnar_test[1:,:])**2).mean(axis=0)\ntest_mse_total_gnar = ((X_test1 - gnar_test[1:,:])**2).mean()\n\n\n\n결과시각화\n\ntrain_mse_total_gnar,test_mse_total_gnar\n\n(0.38358787816283946, 1.3239931193379793)\n\n\n\nfig = plot(fiveVTS,'--.',h=4,color='gray',label='complete data',alpha=0.5)\nax = fig.get_axes()\nfor i,a in enumerate(ax):     \n    a.set_title('node{0} \\n STGCN: mse(train) = {1:.2f}, mse(test) = {2:.2f} \\n ESTGCN: mse(train) = {3:.2f}, mse(test) = {4:.2f}\\n GNAR: mse(train) = {5:.2f}, mse(test) = {6:.2f}'.format(i,train_mse_eachnode_stgcn[i],test_mse_eachnode_stgcn[i],train_mse_eachnode_estgcn[i],test_mse_eachnode_estgcn[i],train_mse_eachnode_gnar[i],test_mse_eachnode_gnar[i]))\n    a.plot(missing_index[i],fiveVTS_train[:,i][missing_index[i]],'xk',label='missing')\n    a.plot(interpolated_signal[:,i],'-',color='gray',label='linear interpolation')\n    a.plot(range(1,160),stgcn_train.squeeze()[:,i],'--.',label='STCGCN (train)',color='C0')\n    a.plot(range(160,199),stgcn_test.squeeze()[:,i],'--.',label='STCGCN (test)',color='C0')\n    a.plot(range(1,160),estgcn_train.squeeze()[:,i],label='ESTCGCN (train)',color='C1')\n    a.plot(range(161,200),estgcn_test.squeeze()[:,i],label='ESTCGCN (test)',color='C1')\n    a.plot(range(1,159),gnar_train[:,i],label='GNAR (train)',color='C2')\n    a.plot(range(161,201),gnar_test[:,i],label='GNAR (test)',color='C2')\n    \n    a.legend()\nfig.set_figwidth(14)\nfig.suptitle(\"Scenario3: \\n missing=80% \\n interpolation=linear \\n\\n STGCN: mse(train) = {0:.2f}, mse(test) = {1:.2f} \\n ESTGCN: mse(train) = {2:.2f}, mse(test) = {3:.2f} \\n GNAR: mse(train) = {4:.2f}, mse(test) = {5:.2f} \\n\".format(train_mse_total_stgcn,test_mse_total_stgcn,train_mse_total_estgcn,test_mse_total_estgcn,train_mse_total_gnar,test_mse_total_gnar),size=15)\nfig.tight_layout()\nfig"
  },
  {
    "objectID": "posts/2_research/2023-01-26-ESTGCN_GNAR_DATA.html#시나리오4",
    "href": "posts/2_research/2023-01-26-ESTGCN_GNAR_DATA.html#시나리오4",
    "title": "Class of Method(GNAR) lag 1",
    "section": "시나리오4",
    "text": "시나리오4\n시나리오4\n\nmissing rate: 30%\n보간방법: linear\n\n- 결측치생성 + 보간\n\n_zero = Missing(fiveVTS_train)\n_zero.miss(percent = 0.3)\n_zero.second_linear()\n\n\nmissing_index = _zero.number\ninterpolated_signal = _zero.train_linear\n\n\nfig = plot(fiveVTS,'--o',h=4,color='gray',label='complete data',alpha=0.2)\nax = fig.get_axes()\nfor i,a in enumerate(ax):     \n    a.plot(missing_index[i],fiveVTS_train[:,i][missing_index[i]],'xk',label='missing')\n    a.plot(interpolated_signal[:,i],'-',color='gray',label='linear interpolation')\n    a.legend()\nfig.set_figwidth(15)\nfig\n\n\n\n\n\n\n\n\n\nSTGCN 으로 적합 + 예측\n\nX = torch.tensor(interpolated_signal).reshape(int(T*0.8),N,1).float()[:int(T*0.8-1),:,:]\ny = torch.tensor(interpolated_signal).reshape(int(T*0.8),N,1).float()[1:,:,:]\n\n\nXX = torch.tensor(fiveVTS_test.reshape(int(T*0.2),N,1)[:-1,:,:]).float()\nyy = torch.tensor(fiveVTS_test.reshape(int(T*0.2),N,1)[1:,:,:]).float()\n\n\nnet = RecurrentGCN(node_features=1, filters=4)\noptimizer = torch.optim.Adam(net.parameters(), lr=0.01)\nnet.train()\nfor epoch in tqdm(range(50)):\n    for time, (xt,yt) in enumerate(zip(X,y)):\n        yt_hat = net(xt, edge_index, edge_attr)\n        cost = torch.mean((yt_hat-yt)**2)\n        cost.backward()\n        optimizer.step()\n        optimizer.zero_grad()\n\n100%|██████████| 50/50 [00:27&lt;00:00,  1.85it/s]\n\n\n\nyhat = torch.stack([net(xt, edge_index, edge_attr) for xt in X]).detach().numpy()\nyyhat = torch.stack([net(xt, edge_index, edge_attr) for xt in XX]).detach().numpy()\n\n\nreal_y = torch.tensor(fiveVTS_train).reshape(int(T*0.8),N,1).float()[1:,:,:]\n\ntrain_mse_eachnode_stgcn = (((real_y-yhat).squeeze())**2).mean(axis=0)\ntrain_mse_total_stgcn = (((real_y-yhat).squeeze())**2).mean()\ntest_mse_eachnode_stgcn = (((yy-yyhat).squeeze())**2).mean(axis=0)\ntest_mse_total_stgcn = (((yy-yyhat).squeeze())**2).mean()\n\n\nstgcn_train = yhat.squeeze() # stgcn은 stgcn에 의한 적합결과를 의미함\nstgcn_test = yyhat.squeeze() \n\n\n\nESTGCN 으로 적합 + 예측\n\nX = torch.tensor(interpolated_signal).reshape(int(T*0.8),N,1).float()[:int(T*0.8-1),:,:]\ny = torch.tensor(interpolated_signal).reshape(int(T*0.8),N,1).float()[1:,:,:]\n\n\nXX = torch.tensor(fiveVTS_test.reshape(int(T*0.2),N,1)[:-1,:,:]).float()\nyy = torch.tensor(fiveVTS_test.reshape(int(T*0.2),N,1)[1:,:,:]).float()\n\n- ESTGCN\n\nnet = RecurrentGCN(node_features=1, filters=4)\noptimizer = torch.optim.Adam(net.parameters(), lr=0.01)\nnet.train()\nsignal = interpolated_signal.copy()\nfor epoch in tqdm(range(50)):\n    signal = update_from_freq_domain(signal,missing_index)\n    X = torch.tensor(signal).reshape(int(T*0.8),N,1).float()[:int(T*0.8-1),:,:]\n    y = torch.tensor(signal).reshape(int(T*0.8),N,1).float()[1:,:,:]\n    for time, (xt,yt) in enumerate(zip(X,y)):        \n        yt_hat = net(xt, edge_index, edge_attr)\n        cost = torch.mean((yt_hat-yt)**2)\n        cost.backward()\n        optimizer.step()\n        optimizer.zero_grad()\n    signal = torch.concat([X.squeeze(),yt_hat.detach().squeeze().reshape(1,-1)])        \n\n100%|██████████| 50/50 [00:27&lt;00:00,  1.79it/s]\n\n\n\nyhat = torch.stack([net(xt, edge_index, edge_attr) for xt in X]).detach().numpy()\nyyhat = torch.stack([net(xt, edge_index, edge_attr) for xt in XX]).detach().numpy()\n\n\nreal_y = torch.tensor(fiveVTS_train).reshape(int(T*0.8),N,1).float()[1:,:,:]\n\ntrain_mse_eachnode_estgcn = (((real_y-yhat).squeeze())**2).mean(axis=0)\ntrain_mse_total_estgcn = (((real_y-yhat).squeeze())**2).mean()\ntest_mse_eachnode_estgcn = (((yy-yyhat).squeeze())**2).mean(axis=0)\ntest_mse_total_estgcn = (((yy-yyhat).squeeze())**2).mean()\n\n\nestgcn_train = yhat.squeeze() # stgcn은 stgcn에 의한 적합결과를 의미함\nestgcn_test = yyhat.squeeze() \n\n\n\nGNAR 으로 적합 + 예측\n-\n\nX_train1 = np.array(X).squeeze()\nX_test1 =  np.array(XX).squeeze()\n\n\n%R -i X_train1\n\n\n%%R\nanswer &lt;- GNARfit(vts = X_train1, net = fiveNet, alphaOrder = 1, betaOrder = c(1))\nprediction &lt;- predict(answer,n.ahead=40)\n\n\n%%R\ngnar_train &lt;- residuals(answer)\ngnar_test &lt;- prediction\n\n\n%R -o gnar_train\n%R -o gnar_test\n\n\ntrain_mse_eachnode_gnar = (gnar_train**2).mean(axis=0)\ntrain_mse_total_gnar = (gnar_train**2).mean()\ntest_mse_eachnode_gnar = ((X_test1 - gnar_test[1:,:])**2).mean(axis=0)\ntest_mse_total_gnar = ((X_test1 - gnar_test[1:,:])**2).mean()\n\n\n\n결과시각화\n\ntrain_mse_total_gnar,test_mse_total_gnar\n\n(0.7978462123549198, 1.3146463350699074)\n\n\n\nfig = plot(fiveVTS,'--.',h=4,color='gray',label='complete data',alpha=0.5)\nax = fig.get_axes()\nfor i,a in enumerate(ax):     \n    a.set_title('node{0} \\n STGCN: mse(train) = {1:.2f}, mse(test) = {2:.2f} \\n ESTGCN: mse(train) = {3:.2f}, mse(test) = {4:.2f}\\n GNAR: mse(train) = {5:.2f}, mse(test) = {6:.2f}'.format(i,train_mse_eachnode_stgcn[i],test_mse_eachnode_stgcn[i],train_mse_eachnode_estgcn[i],test_mse_eachnode_estgcn[i],train_mse_eachnode_gnar[i],test_mse_eachnode_gnar[i]))\n    a.plot(missing_index[i],fiveVTS_train[:,i][missing_index[i]],'xk',label='missing')\n    a.plot(interpolated_signal[:,i],'-',color='gray',label='linear interpolation')\n    a.plot(range(1,160),stgcn_train.squeeze()[:,i],'--.',label='STCGCN (train)',color='C0')\n    a.plot(range(160,199),stgcn_test.squeeze()[:,i],'--.',label='STCGCN (test)',color='C0')\n    a.plot(range(1,160),estgcn_train.squeeze()[:,i],label='ESTCGCN (train)',color='C1')\n    a.plot(range(161,200),estgcn_test.squeeze()[:,i],label='ESTCGCN (test)',color='C1')\n    a.plot(range(1,159),gnar_train[:,i],label='GNAR (train)',color='C2')\n    a.plot(range(160,200),gnar_test[:,i],label='GNAR (test)',color='C2')\n    \n    a.legend()\nfig.set_figwidth(14)\nfig.suptitle(\"Scenario3: \\n missing=80% \\n interpolation=linear \\n\\n STGCN: mse(train) = {0:.2f}, mse(test) = {1:.2f} \\n ESTGCN: mse(train) = {2:.2f}, mse(test) = {3:.2f} \\n GNAR: mse(train) = {4:.2f}, mse(test) = {5:.2f} \\n\".format(train_mse_total_stgcn,test_mse_total_stgcn,train_mse_total_estgcn,test_mse_total_estgcn,train_mse_total_gnar,test_mse_total_gnar),size=15)\nfig.tight_layout()\nfig"
  },
  {
    "objectID": "posts/2_research/2023-01-05-GNAR.html",
    "href": "posts/2_research/2023-01-05-GNAR.html",
    "title": "GNAR data",
    "section": "",
    "text": "GNAR\nGNAR\nimport rpy2\nimport rpy2.robjects as ro \nfrom rpy2.robjects.vectors import FloatVector \nfrom rpy2.robjects.packages import importr\n%load_ext rpy2.ipython\n%%R\nlibrary(GNAR)\nlibrary(igraph)\n\nR[write to console]: Loading required package: igraph\n\nR[write to console]: \nAttaching package: ‘igraph’\n\n\nR[write to console]: The following objects are masked from ‘package:stats’:\n\n    decompose, spectrum\n\n\nR[write to console]: The following object is masked from ‘package:base’:\n\n    union\n\n\nR[write to console]: Loading required package: wordcloud\n\nR[write to console]: Loading required package: RColorBrewer"
  },
  {
    "objectID": "posts/2_research/2023-01-05-GNAR.html#gnar-network-example",
    "href": "posts/2_research/2023-01-05-GNAR.html#gnar-network-example",
    "title": "GNAR data",
    "section": "2.3 GNAR network example",
    "text": "2.3 GNAR network example\n\nedge(list)\ndist(list)\n\n\n%%R\nplot(fiveNet, vertex.label = c(\"A\", \"B\", \"C\", \"D\", \"E\"))\n\n\n\n\n\n\n\n\n\n%%R\nsummary(\"fiveNet\")\n\n   Length     Class      Mode \n        1 character character \n\n\nother examples\n\nigraphtoGNAR or GNARtoigraph쓰는 예제\n\n\n%%R\nfiveNet2 &lt;- GNARtoigraph(net = fiveNet)\nsummary(fiveNet2)\n\nIGRAPH 2b4460d U-W- 5 5 -- \n+ attr: weight (e/n)\n\n\n\n%%R\nfiveNet3 &lt;- igraphtoGNAR(fiveNet2)\nall.equal(fiveNet, fiveNet3)\n\n[1] TRUE\n\n\n\n%%R\nprint(igraphtoGNAR(fiveNet2))\n\nGNARnet with 5 nodes \nedges:1--4 1--5 2--3 2--4 3--2 3--4 4--1 4--2 4--3 5--1 \n     \n edges of each of length  1 \n\n\nedge들 보고 싶을 때\nwhereas the reverse conversion would be performed as\n\n%%R\ng &lt;- make_ring(10)\nprint(igraphtoGNAR(g))\n\nGNARnet with 10 nodes \nedges:1--2 1--10 2--1 2--3 3--2 3--4 4--3 4--5 5--4 5--6 \n     6--5 6--7 7--6 7--8 8--7 8--9 9--8 9--10 10--1 10--9 \n     \n edges of each of length  1 \n\n\n\n%%R\nmake_ring(10)\n\nIGRAPH 22f6be5 U--- 10 10 -- Ring graph\n+ attr: name (g/c), mutual (g/l), circular (g/l)\n+ edges from 22f6be5:\n [1] 1-- 2 2-- 3 3-- 4 4-- 5 5-- 6 6-- 7 7-- 8 8-- 9 9--10 1--10\n\n\n이어진 방향으로 각각의 edge를 만들어주는 게 igrapphtoGNAR이다\nGNARtoigraph function으로 높은 수준의 이웃 구조를 포함한 그래프를 추출할 수 있다.\n\nas.matrix or matrixtoGNAR로 인접 행렬 구할 수 있음\n\nwe can prosucean adjacency matrix for the fiveNet obeject with\n\n%%R\nas.matrix(fiveNet)\n\n     [,1] [,2] [,3] [,4] [,5]\n[1,]    0    0    0    1    1\n[2,]    0    0    1    1    0\n[3,]    0    1    0    1    0\n[4,]    1    1    1    0    0\n[5,]    1    0    0    0    0\n\n\nand an example converting a weighted adjacency matrix to a GNARnet object is\n\n%%R\nadj &lt;- matrix(runif(9), ncol = 3, nrow = 3)\nadj[adj &lt; 0.3] &lt;- 0\nprint(matrixtoGNAR(adj))\n\nWARNING: diagonal entries present in original matrix, these will be removed\nGNARnet with 3 nodes \nedges:1--2 1--3 2--1 2--3 3--1 3--2 \n edges of unequal lengths"
  },
  {
    "objectID": "posts/2_research/2023-01-05-GNAR.html#example-gnar-model-fitting",
    "href": "posts/2_research/2023-01-05-GNAR.html#example-gnar-model-fitting",
    "title": "GNAR data",
    "section": "2.4. Example: GNAR model fitting",
    "text": "2.4. Example: GNAR model fitting\n\nGNAR로 fit과 predict 가능\n\n\n%%R\ndata(\"fiveNode\")\nanswer &lt;- GNARfit(vts = fiveVTS, net = fiveNet, alphaOrder = 2, betaOrder = c(1, 1))\nanswer\n\nModel: \nGNAR(2,[1,1]) \n\nCall:\nlm(formula = yvec ~ dmat + 0)\n\nCoefficients:\n dmatalpha1  dmatbeta1.1   dmatalpha2  dmatbeta2.1  \n    0.20624      0.50277      0.02124     -0.09523  \n\n\n\n\n파라메터 4개 가지고 있음\n\n\n%%R\nlayout(matrix(c(1, 2), 2, 1))\nplot(fiveVTS[, 1], ylab = \"Node A Time Series\")\nlines(fitted(answer)[, 1], col = 2)\nplot(fiveVTS[, 2], ylab = \"Node B Time Series\")\nlines(fitted(answer)[, 2], col = 2)\n\n\n\n\n\n\n\n\n\n%%R\nlayout(matrix(c(1, 2), 2, 1))\nplot(fiveVTS[, 3], ylab = \"Node C Time Series\")\nlines(fitted(answer)[, 3], col = 2)\nplot(fiveVTS[, 4], ylab = \"Node D Time Series\")\nlines(fitted(answer)[, 4], col = 2)\n\n\n\n\n\n\n\n\n\n각 노드의 time series(검정), fitted values from ‘answer’ model overlaid in red\n\n\n%%R\nmyresiduals &lt;- residuals(answer)[, 1]\nlayout(matrix(c(1, 2), 2, 1))\nplot(ts(residuals(answer)[, 1]), ylab = \"`answer' model residuals\")\nhist(residuals(answer)[, 1], main = \"\", xlab = \"`answer' model residuals\")\n\n\n\n\n\n\n\n\n\n%%R\nmyresiduals &lt;- residuals(answer)[, 2]\nlayout(matrix(c(1, 2), 2, 1))\nplot(ts(residuals(answer)[, 1]), ylab = \"`answer' model residuals\")\nhist(residuals(answer)[, 2], main = \"\", xlab = \"`answer' model residuals\")\n\n\n\n\n\n\n\n\n\n%%R\nmyresiduals &lt;- residuals(answer)[, 3]\nlayout(matrix(c(1, 2), 2, 1))\nplot(ts(residuals(answer)[, 1]), ylab = \"`answer' model residuals\")\nhist(residuals(answer)[, 3], main = \"\", xlab = \"`answer' model residuals\")\n\n\n\n\n\n\n\n\n\n%%R\nmyresiduals &lt;- residuals(answer)[, 4]\nlayout(matrix(c(1, 2), 2, 1))\nplot(ts(residuals(answer)[, 1]), ylab = \"`answer' model residuals\")\nhist(residuals(answer)[, 4], main = \"\", xlab = \"`answer' model residuals\")\n\n\n\n\n\n\n\n\n\nresidual plots from ‘answer’ model fit. Top: time sereies, Bottom: Histogram"
  },
  {
    "objectID": "posts/2_research/2023-01-05-GNAR.html#example-gnar-data-simulation-on-a-given-network",
    "href": "posts/2_research/2023-01-05-GNAR.html#example-gnar-data-simulation-on-a-given-network",
    "title": "GNAR data",
    "section": "2.5. Example: GNAR data simulation on a given network",
    "text": "2.5. Example: GNAR data simulation on a given network\n\nfiveNet 네트워크를 사용하여 네트워크 시계열 시뮬레이션 진행\n두 시뮬레이션 모두 sigma argument를 사용하여 표준 편차가 제어되는 표준 정규 노이즈를 사용하여 생성된다.\n\n\n%%R\nset.seed(10)\nfiveVTS2 &lt;- GNARsim(n = 200, net = fiveNet, alphaParams = list(c(0.4, 0, -0.6, 0, 0)), betaParams = list(c(0.3)))\n\n\nfiveVTS2 네트워크를 사용하여 시뮬레이션 된 것이다보니 파라메터 계수 비슷\n\n\n%%R\nprint(GNARfit(vts = fiveVTS2, net = fiveNet, alphaOrder = 1, betaOrder = 1, globalalpha = FALSE))\n\nModel: \nGNAR(1,[1]) \n\nCall:\nlm(formula = yvec ~ dmat + 0)\n\nCoefficients:\ndmatalpha1node1  dmatalpha1node2  dmatalpha1node3  dmatalpha1node4  \n        0.45902          0.13133         -0.49166          0.03828  \ndmatalpha1node5      dmatbeta1.1  \n        0.02249          0.24848  \n\n\n\n\n%%R\nset.seed(10)\nfiveVTS3 &lt;- GNARsim(n = 200, net = fiveNet, alphaParams = list(rep(0.2, 5), rep(0.3, 5)), betaParams = list(c(0.2, 0.3), c(0)))\nprint(GNARfit(vts = fiveVTS3, net = fiveNet, alphaOrder = 2, betaOrder = c(2,0)))\n\nModel: \nGNAR(2,[2,0]) \n\nCall:\nlm(formula = yvec ~ dmat + 0)\n\nCoefficients:\n dmatalpha1  dmatbeta1.1  dmatbeta1.2   dmatalpha2  \n     0.2537       0.1049       0.3146       0.2907  \n\n\n\n\n%%R\nfiveVTS4 &lt;- simulate(GNARfit(vts = fiveVTS2, net = fiveNet, alphaOrder = 1, betaOrder = 1, globalalpha = FALSE), n = 200)\nprint(GNARfit(vts = fiveVTS4, net = fiveNet, alphaOrder = 1, betaOrder = 1, globalalpha = FALSE))\n\nModel: \nGNAR(1,[1]) \n\nCall:\nlm(formula = yvec ~ dmat + 0)\n\nCoefficients:\ndmatalpha1node1  dmatalpha1node2  dmatalpha1node3  dmatalpha1node4  \n      0.4478300       -0.0008695       -0.4822675        0.0523652  \ndmatalpha1node5      dmatbeta1.1  \n     -0.0063702        0.2249530  \n\n\n\n\n위와 같이 GNAR 모델에 있는 시계열을 simulate하기 위해 GNARfit object에 대해 simulate S3 method 사용할 수 있다"
  },
  {
    "objectID": "posts/2_research/2023-01-05-GNAR.html#missing-data-and-changing-connection-weights-with-gnar-models",
    "href": "posts/2_research/2023-01-05-GNAR.html#missing-data-and-changing-connection-weights-with-gnar-models",
    "title": "GNAR data",
    "section": "2.6 Missing data and changing connection weights with GNAR models",
    "text": "2.6 Missing data and changing connection weights with GNAR models\n\nThe flexibility of GNAR modelling이 의미하는 것은 연결 가중치를 바꾸지 않고 변하는 네트워크로 missing data 를 모델링 할 수 있다.\n한 노드가 missing data 구간이 생기면 그 구간에서만 네트워크를 변화하여 weight가 변경된다.\n\n\n%%R\nfiveVTS0 &lt;- fiveVTS\nfiveVTS0[50:150, 3] &lt;- NA\nnafit &lt;- GNARfit(vts = fiveVTS0, net = fiveNet, alphaOrder = 2, betaOrder = c(1, 1))\nlayout(matrix(c(1, 2), 2, 1))\nplot(ts(fitted(nafit)[, 3]), ylab = \"Node C fitted values\")\nplot(ts(fitted(nafit)[, 4]), ylab = \"Node D fitted values\")\n\n\n\n\n\n\n\n\nA key advantage of our parsimonious GNAR model is that it models via neighborhoods across the entire data set. If a node is missing for a given time, then it does not contribute to the estimation of neighborhood parameters that the network structure suggests it should, and there are plenty of other nodes that do contribute, generally resulting in a high number of observations to estimate each coefficient. In GNAR models, missing data of this kind is not a problem.\n\n우리의 간결한 GNAR 모델의 주요 장점은 전체 데이터 세트에 걸쳐 이웃을 통해 모델링한다는 것입니다. 노드가 특정 시간 동안 누락되면 네트워크 구조가 제안하는 인접 매개 변수의 추정에 기여하지 않으며, 기여하는 다른 노드도 많아 일반적으로 각 계수를 추정하기 위한 관측치 수가 많습니다. GNAR 모델에서는 이런 종류의 데이터가 누락되는 것은 문제가 되지 않습니다."
  },
  {
    "objectID": "posts/2_research/2023-01-05-GNAR.html#stationary-conditions-for-a-gnar-process-with-fixed-network",
    "href": "posts/2_research/2023-01-05-GNAR.html#stationary-conditions-for-a-gnar-process-with-fixed-network",
    "title": "GNAR data",
    "section": "2.7. Stationary conditions for a GNAR process with fixed network",
    "text": "2.7. Stationary conditions for a GNAR process with fixed network\nTheorem 1\n\nGiven an unchanging network, \\(\\mathcal{G}\\) a sufficient condition for the GNAT model (1) to be stationary is\n\n\\[\\sum^p_{j=1}(|\\alpha_{i,j}| + \\sum^{C}_{c=1} \\sum^{s_j}_{r=1} |\\beta_{j,t,c}|)&lt;1 , \\forall_i \\in 1,\\dots, N\\]\n위 조건을 GNARsim을 이용하여 확인 할 수 있다.\n\n%%R\nset.seed(10)\nfiveVTS4 &lt;- GNARsim(n = 200, net = fiveNet, alphaParams = list(rep(0.2, 5)), betaParams = list(c(0.85)))\nc(mean(fiveVTS4[1:50, ]), mean(fiveVTS4[51:100, ]), mean(fiveVTS4[101:150, ]), mean(fiveVTS4[151:200, ]))\n\n[1]    -120.511   -1370.216  -15725.884 -180319.140\n\n\n\nThe mean increases rapidly indicating nonstationarity.\n평균이 빠르게 증가하는 것으로 보아 정상성을 띄고 있지 않다."
  },
  {
    "objectID": "posts/2_research/2023-01-05-GNAR.html#benefits-of-our-model-and-comparisons-to-others",
    "href": "posts/2_research/2023-01-05-GNAR.html#benefits-of-our-model-and-comparisons-to-others",
    "title": "GNAR data",
    "section": "2.8. Benefits of our model and comparisons to others",
    "text": "2.8. Benefits of our model and comparisons to others"
  },
  {
    "objectID": "posts/2_research/2023-01-05-GNAR.html#order-selection",
    "href": "posts/2_research/2023-01-05-GNAR.html#order-selection",
    "title": "GNAR data",
    "section": "3.1. Order selection",
    "text": "3.1. Order selection\nBayesian information criterion\n\\[BIC(p,s) = ln|\\sum^{\\hat{}}_{p,s}| + T^{-1} M ln(T)\\]\n\n%%R\nBIC(GNARfit())\n\n[1] -0.003953124\n\n\n\n%%R\nBIC(GNARfit(betaOrder = c(2, 1)))\n\n[1] 0.02251406\n\n\nAkaike information criterion\n\\[AIC(p,s) = ln|\\sum^{\\hat{}}_{p,s}| + 2T^{-1} M\\]\n\n%%R\nAIC(GNARfit())\n\n[1] -0.06991947\n\n\n\n%%R\nAIC(GNARfit(betaOrder = c(2, 1)))\n\n[1] -0.05994387"
  },
  {
    "objectID": "posts/2_research/2023-01-05-GNAR.html#model-selection-on-a-wind-network-time-series",
    "href": "posts/2_research/2023-01-05-GNAR.html#model-selection-on-a-wind-network-time-series",
    "title": "GNAR data",
    "section": "3.2. Model selection on a wind network time series",
    "text": "3.2. Model selection on a wind network time series\nthe data suite vswind that contains a number of R objects pertaining to 721 wind speeds taken at each of 102 weather stations in England and Wales. The suite contains the vector time series vswindts, the associated network vswindnet, a character vector of the weather station location names in vswindnames and coordinates of the stations in the two column matrix vswindcoords. The data originate from the UK Met Office site http://wow.metoffice.gov.uk and full details can be found in the vswind help file in the GNAR package.\n\nnodes : 102\ntime step : 721\n\n\n%%R\noldpar &lt;- par(cex = 0.75)\nwindnetplot()\npar(oldpar)\n\n\n\n\n\n\n\n\nPlot of the wind speed network\n\nblue numbers are relative distance between sites\nlabels are the site name\n\n\n%%R\nBIC(GNARfit(vts = vswindts, net = vswindnet, alphaOrder = 1, betaOrder = 0))\n\n[1] -233.3848\n\n\n\n%%R \nfiveFit &lt;- GNARfit(fiveVTS[1:160,],net=fiveNet, alphaOrder=2, betaOrder=c(2,0)) #learn \ndim(fitted(fiveFit))\n\n[1] 158   5\n\n\n\n%%R\ndummyFit &lt;- GNARfit(fiveVTS,net=fiveNet, alphaOrder=2, betaOrder=c(2,0)) #learn \ndummyFit$mod$coefficients &lt;- fiveFit$mod$coefficients\n\n\n%%R\nfitted(dummyFit)[161:200]\n\n [1]  0.01093152  0.07611113  0.50989356  0.84380803  0.90488185 -0.12703505\n [7] -0.57721780 -0.36681689 -0.26281975 -0.47712098 -0.62293008 -0.58121816\n[13] -0.81149078 -0.45403821 -0.60487041  0.28617606  0.20580455  0.19341988\n[19]  0.35296420  0.15628117  0.68350847  0.49043974  0.29627168 -0.35666858\n[25] -0.47565960  0.06692171 -0.14924170 -0.36616239 -0.49994894  0.22625500\n[31] -0.08023045  0.25371268 -0.47415540 -0.99390660 -1.16821429 -0.18438203\n[37] -1.10766872 -0.76969390  0.71828989  0.69737474\n\n\n\n%%R\nfitted(fiveFit)\n\n               [,1]         [,2]         [,3]          [,4]         [,5]\n  [1,]  0.300764573  0.731759572  0.645747192  0.6590441235  0.329408573\n  [2,]  0.584155581  0.709939766  0.504838042  0.4006542005  0.002404727\n  [3,]  0.248206643 -0.053672929  0.089912292  0.3569760840  0.886027241\n  [4,]  0.431422029 -0.596332784 -0.445500333 -0.1950437084  0.998677531\n  [5,]  0.369251545 -0.131646096 -0.151852774 -0.0307057532  0.602574799\n  [6,] -0.281163998  0.372761568  0.470612441  0.0640593261 -0.793452225\n  [7,] -0.629184394  0.122210560 -0.064188223  0.2614228537  0.027403171\n  [8,]  0.176102146  0.054605602  0.001654112 -0.0679337406 -0.041235943\n  [9,] -0.168610998 -0.212095723 -0.204125726 -0.2125261302 -0.010579734\n [10,] -0.161421278 -0.007465513  0.149274060 -0.0249484567 -0.327041759\n [11,]  0.359655447 -0.077039236 -0.039580520 -0.0457447866  0.112500734\n [12,] -0.015390804 -0.224700395 -0.236137628 -0.1566997305 -0.032236173\n [13,] -0.254595294 -0.052425427 -0.222029463 -0.1955623614 -0.090852345\n [14,] -0.133378083 -0.231175869 -0.262111004 -0.4557269171 -0.613390251\n [15,] -0.299492590 -1.329777415 -1.157380728 -0.7395871328  0.367735930\n [16,] -0.092939379 -0.649485715 -0.649077093 -0.5677985865  0.018220159\n [17,] -0.197734475  0.006055154  0.003767933  0.0142878952  0.017169293\n [18,] -0.593128663 -0.029057099  0.019930836 -0.1995170085 -0.699678586\n [19,] -0.406803183 -0.080878255 -0.076916012 -0.0142207300  0.084031465\n [20,]  0.195661742  0.636973567  0.514808183  0.1521298877 -0.565262518\n [21,] -0.646798007  0.034595763 -0.038468526 -0.2844049647 -1.200687294\n [22,] -0.858660467 -0.279197317 -0.100300563 -0.1774862659 -0.600631944\n [23,]  0.072500668  0.171516441  0.137895886  0.1253750409  0.109017776\n [24,]  0.361169443 -0.134591857 -0.172907391 -0.0276254736  0.454886946\n [25,] -0.555837928  0.021772877  0.086135588 -0.5401601843 -1.758757275\n [26,] -0.907025497 -0.516889787 -0.601369437 -0.4907248867 -0.518976355\n [27,] -0.577788165 -0.883706611 -0.990274375 -0.5655498386  0.219126828\n [28,] -0.381253476 -1.020667212 -0.974967606 -0.8163078225 -0.224880458\n [29,] -0.418889129 -1.136651964 -1.192973176 -0.8278078873  0.075705061\n [30,] -0.430848760 -0.711242749 -0.645456670 -0.7835485785 -0.902099485\n [31,] -0.408762558 -0.330133901 -0.313267801 -0.2950574688 -0.516672981\n [32,] -0.397456978 -0.177503203 -0.123190622  0.0851676997  0.175790185\n [33,] -0.321504797  0.137826136  0.185516044 -0.0469442492 -0.541534195\n [34,] -0.446890348  0.425984103  0.440639199  0.0859296090 -0.911239457\n [35,] -0.409664931  0.661582101  0.615592428  0.4648149587 -0.620671093\n [36,] -0.178853576  1.134703079  1.170584884  0.7788603566 -0.614547790\n [37,] -0.187107405  0.975055141  0.894986103  0.7556786981  0.299963760\n [38,] -0.421512891  0.230443777  0.311288224  0.3783662241  0.481463020\n [39,]  0.551866636  0.010017627 -0.031712684 -0.0291730137  0.299117468\n [40,] -0.415705371 -0.084974037 -0.102059950  0.0729357811 -0.004680301\n [41,] -0.596582277 -0.181138675 -0.037759798 -0.0710576429 -0.300907209\n [42,] -0.264388277  0.136784757  0.077185718  0.0270845136 -0.311590329\n [43,]  0.413516614  0.932841430  0.935079115  0.7161523403 -0.158898407\n [44,]  0.095801809  0.920995408  0.955252555  0.8459697803 -0.127453246\n [45,]  0.975373534  1.209886345  1.185240223  1.0015804496  0.169827608\n [46,]  0.618868253 -0.058442807 -0.058935046  0.2370795943  0.914954875\n [47,]  0.366854292  0.267881271  0.247344186  0.2400219950  0.479987689\n [48,] -0.150157867 -0.777932308 -0.717286974 -0.5794468369  0.195377648\n [49,]  0.075029321 -0.299959011 -0.355342761 -0.1920419709  0.334423437\n [50,]  0.386593451 -0.135877992 -0.072542184  0.2518678243  0.984484742\n [51,]  0.697822864 -0.311896474 -0.295669625 -0.2502382955  0.585030936\n [52,]  0.062654477 -0.191584061 -0.107314176  0.2208996954  1.056837103\n [53,]  0.728773427 -0.477836693 -0.494169917 -0.4843342717  0.451358938\n [54,]  0.585069845  0.665543312  0.719452401  0.5820396956  0.545931719\n [55,]  1.079661317  0.127297862  0.038038488  0.0677187351  0.907180658\n [56,]  0.651000271  0.443573833  0.461237609  0.2318123507  0.355931847\n [57,]  0.047340441  0.061483244 -0.142562719 -0.0074741198  0.203598849\n [58,]  0.020722674 -0.855956709 -0.772986200 -0.7280158105 -0.072139463\n [59,] -0.197216261 -0.894976325 -0.740954981 -0.7972459012 -0.237856418\n [60,] -0.335837112 -0.488349114 -0.651565734 -0.7163113371 -0.703717928\n [61,] -0.573434288 -1.566963776 -1.470953366 -0.9358838106  0.340002593\n [62,] -0.019967493 -1.006103326 -1.203028246 -0.7206978782  0.426166645\n [63,]  0.289165755 -0.676589963 -0.529091393 -0.3863962807  0.580747013\n [64,] -0.063683833 -0.320450285 -0.325613255 -0.2974046265 -0.105675029\n [65,] -0.112032969 -0.527479112 -0.523398055 -0.1590403400  0.496544412\n [66,] -0.286173235 -0.053400092  0.018863005 -0.0124208740 -0.118724115\n [67,] -0.499744957  0.041067235  0.076504951 -0.2048682261 -0.787362589\n [68,] -0.547870145 -0.732546310 -0.770409578 -0.7369449312 -0.331821251\n [69,] -0.905056316 -0.829491825 -0.850502405 -0.7607730212 -0.656264847\n [70,] -0.246440697 -0.279850678 -0.269853546 -0.5660758221 -1.111827018\n [71,] -0.186335645  0.010799425 -0.077495418  0.0601490253  0.102437820\n [72,]  0.010493539 -0.479453043 -0.453954591 -0.3426073144 -0.040090049\n [73,] -0.673580794 -0.734494634 -0.564315445 -0.8190023610 -1.019947630\n [74,] -0.640032299 -0.558498277 -0.596977420 -0.5456733025 -0.090276605\n [75,] -0.464960459 -0.529199551 -0.552331950 -0.8884973812 -1.066744453\n [76,] -0.887182659 -0.141258133 -0.110786546  0.0237603066  0.003998690\n [77,] -0.073105627 -0.207336194 -0.187193599 -0.6930066146 -1.105251019\n [78,] -0.689981040  0.027486354  0.151691133  0.2650861359 -0.203709649\n [79,]  0.258382277  0.652801148  0.545006830  0.3925781271 -0.249181067\n [80,]  0.751377138  0.239900459  0.276168396  0.6057405726  1.262601102\n [81,]  0.631390471  0.411822991  0.543334476  0.2438621480  0.112347717\n [82,] -0.034969641 -0.104184784 -0.097705561  0.1498464790  0.895833940\n [83,]  0.120113065  0.049356967  0.046034277 -0.0721309377  0.217972070\n [84,] -0.200386785 -0.203495710 -0.239586254 -0.1172887981  0.323191787\n [85,]  0.185644181 -0.395187380 -0.370733604 -0.5009300085 -0.211102667\n [86,] -0.237732516  0.276454796  0.287754635  0.2080093644 -0.223951494\n [87,]  0.216163467  0.192130141  0.177949800 -0.0753244143 -0.351364565\n [88,] -0.142649364 -0.186692214 -0.168987298 -0.1166303744  0.011404103\n [89,]  0.548618117  0.747707315  0.770581155  0.5221810333  0.172075628\n [90,]  0.028104354  0.688465275  0.636215214  0.5500772784  0.059464762\n [91,] -0.495363724 -0.071919605 -0.008154852 -0.4353087653 -1.271143508\n [92,] -1.107134875 -0.172815824 -0.303687490 -0.0844525666 -0.236711607\n [93,] -0.149354271  0.009431534  0.092873697 -0.4137230613 -1.079758945\n [94,] -0.480127162 -0.759207617 -0.733556252 -0.1155717520  0.856016011\n [95,] -0.068768871 -0.262250231 -0.276510238 -0.4068034977 -0.281317568\n [96,] -0.204766674 -0.466093264 -0.516820813 -0.4012743931 -0.036227691\n [97,]  0.450365055 -0.345449271 -0.277951368 -0.0735386028  0.626942400\n [98,]  0.825252322 -0.102070954  0.029346323  0.2224587296  0.891243876\n [99,]  0.918255191  0.974393754  0.785132509  0.9588950601  0.747082329\n[100,]  1.161471681  0.803193156  1.046464965  1.2474096963  1.536524225\n[101,]  0.796513581  0.943461644  0.698756745  0.5968311778  0.275247231\n[102,]  0.189772234  0.088167353  0.193522913 -0.1055246595 -0.205361362\n[103,] -0.486312684  0.528260131  0.609425778  0.4533837262 -0.448633125\n[104,] -0.780106474  0.619638040  0.487082196  0.2699333424 -1.074329863\n[105,]  0.001495171  0.043686101  0.071429149 -0.0068869393  0.109059794\n[106,]  0.412811020  0.589258087  0.534530373  0.1827023688 -0.169191222\n[107,]  0.230033356  0.369815780  0.441499631  0.4808802972  0.619074058\n[108,]  0.887957665  0.299276196  0.242694214  0.2120811348  0.437908215\n[109,] -0.003445154 -0.934051132 -0.730038077 -0.3220444395  0.707556500\n[110,] -0.138607950 -0.723059594 -0.801323554 -0.7383525593 -0.366013353\n[111,] -0.761794858 -0.776583246 -0.767390838 -0.6281202398 -0.286591883\n[112,] -0.143995206 -0.994654745 -0.891931956 -0.8145750541  0.194529857\n[113,] -0.158840376 -0.789802525 -0.720611341 -0.3434589195  0.705106074\n[114,]  0.213701938  0.094954893 -0.066337346 -0.1715778242 -0.301015012\n[115,]  0.353147878 -0.162438159 -0.035453134  0.2417453098  0.692188468\n[116,] -0.259026322 -0.062564122 -0.069539905  0.0494333669  0.016725122\n[117,]  0.439567933  0.334238798  0.123452083  0.2899531832  0.536404389\n[118,]  0.243694767 -0.136731679 -0.062368473 -0.0505163356  0.329751821\n[119,] -0.221760052 -0.034088224 -0.073079140 -0.0719693132  0.080492952\n[120,] -0.639870092 -0.346958117 -0.350162454 -0.6158933852 -1.052705642\n[121,]  0.191514800  0.198936857  0.188570749  0.0576140540 -0.098783032\n[122,]  0.497040376  0.104911046  0.183534206  0.3233013200  0.725368718\n[123,]  0.912741276  0.906110114  0.847102372  0.9050828507  0.840894148\n[124,]  0.761509505  0.486180954  0.657445208  0.6485641139  0.813518998\n[125,]  0.316821416 -0.083720193 -0.173493262 -0.1007134067  0.356082268\n[126,] -0.067185056 -0.753094489 -0.702750670 -0.2621385119  1.046954137\n[127,] -0.091228436 -0.675109353 -0.703565297 -0.5564405787  0.258988050\n[128,] -0.109253436 -0.444143304 -0.399640932 -0.3580561559 -0.035762392\n[129,] -0.232521560 -0.039990247 -0.095241200 -0.0443450553 -0.430664437\n[130,]  0.123446724  0.200420996  0.173518282  0.5184071044  0.313636742\n[131,]  0.285159008 -0.032242166  0.027550780  0.1048459050 -0.132334038\n[132,] -0.029094326  0.291860928  0.394652562  0.4297945912  0.077465921\n[133,]  0.607314455  0.854271821  0.946947611  0.7446008823  0.229008010\n[134,]  0.114274696  0.395094364  0.372741933  0.2933528915  0.073135295\n[135,]  0.058334948  0.020883512 -0.064495939 -0.0002935146  0.194116690\n[136,] -0.485203794  0.162128088  0.243616443  0.0173331790 -0.588154182\n[137,]  0.361414229  0.497203208  0.361158226  0.5018799742  0.573470360\n[138,]  0.819903948  0.366313425  0.363516294  0.3796662479  0.699684917\n[139,] -0.216165855 -0.335708047 -0.251520773 -0.2682301442  0.062768414\n[140,] -0.454294227 -0.359066363 -0.294969357 -0.4097376437 -0.479590775\n[141,] -0.062418172  0.009466708 -0.051169781  0.1398314442  0.275918751\n[142,]  0.546927988  0.424770686  0.388169659  0.1288546957 -0.347653081\n[143,] -0.311508529  0.345041027  0.500511866  0.3303953879 -0.189245985\n[144,]  0.156321321  0.623518356  0.518171727  0.4513620372  0.284295098\n[145,]  0.039622964  0.299954776  0.356381605  0.3469508290  0.326305082\n[146,]  0.579791654  0.695810253  0.652099702  0.5011115910  0.023065334\n[147,]  0.554655907  0.093490122  0.153079897  0.4182759318  0.767121199\n[148,]  0.384057236 -0.065678272 -0.023948715  0.0451867298  0.454508452\n[149,]  0.447874514  0.392017250  0.266888939  0.3039632644  0.540801889\n[150,]  0.497810670  0.980726335  1.014061377  0.9632510637  0.560362395\n[151,]  0.044270389  0.550595322  0.526083974  0.5090146051 -0.039702900\n[152,]  0.128989148  0.501717816  0.494582107  0.2658346703 -0.283886186\n[153,]  0.003640933  0.471547028  0.593579063  0.5105159483  0.179982706\n[154,]  0.207081614  0.420074580  0.265684904  0.2159979026 -0.161101019\n[155,] -0.478137997  0.240399554  0.349195339  0.3926704926 -0.290283502\n[156,] -0.368038032  0.588505317  0.539132547  0.2403574312 -0.831187194\n[157,] -0.452959327  0.212753187  0.402307299  0.1713284561 -0.415735323\n[158,]  0.013726373  0.132970935  0.077022243  0.0243587773 -0.211000744\n\n\n\n%%R \nfiveVTS\n#gdpVTSn2[52,]\n\n               [,1]         [,2]         [,3]         [,4]        [,5]\n  [1,] -0.106526553  1.077613724 -0.244694569  0.933710066  0.44443593\n  [2,]  0.664495737  0.935476457  1.402823610  0.826656526  0.02097885\n  [3,] -0.255977521 -1.478171940  2.160938890  1.746276180  0.80188586\n  [4,]  1.684436464  0.180662387  0.398879113 -0.418094544  0.28037722\n  [5,]  1.451205104  0.584157057 -1.700218367 -1.219724913  1.63810802\n  [6,]  0.728846398  0.536253148 -0.730310794 -0.297010221  1.04864255\n  [7,] -1.526780869  1.957817106 -0.506088728  0.470747730 -0.63167929\n  [8,]  1.269035583 -0.519870107  0.773038576 -0.176065931 -3.13354575\n  [9,] -0.403340381 -0.398760476  1.358212255 -0.755544421  1.58592625\n [10,] -0.057805476 -0.932876792  0.163580296 -0.299486667 -0.31232748\n [11,] -0.498505318  1.005323196 -0.762045202  0.136725053 -0.43412619\n [12,]  0.057551667  0.252414834 -1.585146661  0.892773199  0.64534882\n [13,] -0.235969810  0.168738145 -0.927906887 -0.201182245  0.28394360\n [14,] -0.574265201 -0.451671164  1.523378623 -1.466113999  0.88143283\n [15,] -1.120369546 -2.167955049 -0.290951250  0.951438750 -0.63430591\n [16,]  0.734315543 -1.074175821 -2.550164018 -1.843026440  0.16372069\n [17,] -0.300708705 -0.215965234 -1.251829204 -1.147084336  0.81200151\n [18,]  0.168111491  0.617846030 -0.067553835 -0.316813889 -0.57552628\n [19,] -1.130660662  1.049382642 -0.219923688 -0.838450813 -0.84289855\n [20,]  0.491603006  0.911202204 -0.037939755 -1.146905077 -0.70878526\n [21,] -1.282680651 -0.037695723  1.273889958  1.222120228  0.48792144\n [22,] -1.977860407 -1.476586870  0.618312041  0.541447907 -1.59494685\n [23,] -0.566157633  1.007129193 -0.421914435 -1.255494830 -1.57780757\n [24,]  0.427751755  0.446864227 -0.006556415  0.235046101 -0.13251371\n [25,]  0.588963964 -0.510977075 -0.208215798 -0.038859202  1.03505405\n [26,] -3.577108873  0.063814248 -0.764773488  0.750897963 -0.57677829\n [27,] -0.762998219 -0.715903539  0.070974165 -1.767866460 -0.85886671\n [28,]  1.154873570 -2.505711766 -0.235496918 -1.464120269 -1.74689908\n [29,] -0.272693870 -1.857460255 -1.012583601 -1.478438760 -0.20065611\n [30,]  0.395708110 -2.601309968 -1.102332663 -1.418525692 -0.67074519\n [31,] -1.632021790 -1.389226367 -1.406639927 -0.164463613 -0.57992151\n [32,] -0.593868925 -0.604746854 -0.889344852  0.170550009 -1.19152282\n [33,]  0.902385866  0.859337939 -0.216488189 -1.019066503 -1.20331830\n [34,] -0.755599082  1.205454399 -0.315142793 -0.182216870 -0.76219579\n [35,] -1.402617202  1.327218071  0.030109089  0.508358610 -1.19204930\n [36,] -0.248901260  0.835443000  0.673836075  1.249100802 -2.50681088\n [37,] -0.821889741  2.683690391  1.991361276  0.628557722 -0.63183483\n [38,]  0.509052231  2.387571425  3.198218488 -1.313720751  0.41614740\n [39,]  1.760264680  1.329747672  0.589082315 -0.959062662 -2.17248032\n [40,]  0.407692882 -0.494160904 -0.336544078  0.518871030  1.17098422\n [41,]  0.529542953 -0.109371157  0.276546799 -0.441917032 -1.77465267\n [42,]  0.071422121  1.390859571 -0.656166400 -1.112883997 -1.53889722\n [43,] -0.038685141  0.343631869 -0.121075284  0.189519172 -1.28519302\n [44,]  0.032507365  1.105775541  0.745569895  2.313785056 -0.54606867\n [45,]  0.326298291  1.672376452  0.867801558  1.596593083 -1.38530798\n [46,]  0.493735214  0.828445070  0.665517459  3.545540325  0.17892418\n [47,]  1.046193130  0.182022439  0.076358398 -0.564071723  2.31894117\n [48,]  0.487801423  0.402192928  0.660265277  0.160859127  0.85916399\n [49,]  0.012036883 -0.575802156 -1.375502706 -1.555497631  0.78253503\n [50,]  0.566249612 -0.679123506 -0.342116629 -0.293370938  0.16791547\n [51,]  1.983899460  0.325870466 -0.462247924 -0.144103964  0.20428669\n [52,]  0.082477869 -0.015707410 -0.827420937 -0.533338717  3.03848854\n [53,]  2.025321199  1.260059506 -0.667274520 -0.983423623 -0.25069462\n [54,] -0.182207953 -0.492329932 -1.572486971 -0.318050791  3.11590615\n [55,]  0.310016519  2.163157480  0.539075569  0.775034293  1.55599197\n [56,]  0.344219807 -0.289984010  0.019376746  0.352604618  3.88803701\n [57,] -0.628460347  0.908826573  0.839215153  0.387888048  2.79852674\n [58,]  0.219514752 -1.949662023  1.297952270  0.245509848 -0.10715958\n [59,] -0.703231049 -1.872499373 -0.948118550 -0.956147553  1.46327573\n [60,] -1.037484518 -0.353586885 -2.197040404 -1.022195776  1.03019923\n [61,] -1.671104120 -1.807816077 -0.456726722 -0.275561042  0.23349098\n [62,]  1.082454650 -2.163914674 -2.772699788 -1.867484810 -1.15276058\n [63,]  0.733526181 -2.901286567 -0.144692022 -1.533368920  0.64538823\n [64,]  0.607622782 -0.564034684 -1.024446572 -0.860302664  1.46464103\n [65,] -0.249447697 -0.622497600 -0.860157861  0.059602510 -0.28629810\n [66,]  1.242991882 -0.553213122 -0.784868076 -0.866755329 -0.50726271\n [67,]  0.025808026  1.134045937 -0.194833475 -0.766397319 -0.66714031\n [68,] -1.280038469  0.969797971 -0.529341149 -0.274648131 -0.88451275\n [69,] -0.899437152 -0.455454165 -0.889218336 -2.081201461  0.63782940\n [70,] -0.485225451 -1.579881065 -1.542956678 -0.728773133 -2.63529295\n [71,] -2.232840088 -0.666470900 -0.802215707  0.306471316  0.36000506\n [72,]  0.263283350 -0.473210196  0.858443996 -0.330691066 -0.23528971\n [73,]  0.194868023 -1.673792947 -1.157081284  0.464290012 -0.67196384\n [74,] -2.374209088  0.746939110 -1.641811447 -1.805924541  0.66216561\n [75,] -0.319164807  0.368646446 -0.675529010 -2.103798682  0.06414596\n [76,] -2.347006749 -0.926133386 -1.280596322 -0.435692565  0.21824222\n [77,]  0.446994651  1.473521493  0.734103122 -2.231131628 -1.36931119\n [78,] -2.402766578 -0.673636271 -1.514644362  0.766704957  0.57180703\n [79,]  0.591165442  1.638210347 -0.954659887  0.008337226 -2.90507678\n [80,] -0.284550885  0.812008860  0.745209777  1.099796776  0.12632330\n [81,]  2.404372924  0.494780967 -0.137672593  0.738809147  0.71597343\n [82,] -0.732204526  2.210038243 -0.355431422  0.339909653  2.46725315\n [83,]  1.437145004  1.547713162 -0.345952409 -1.578834604  0.52850356\n [84,] -0.084957358  1.234464257 -0.032386550 -0.956802127  1.29018233\n [85,]  0.630013261  0.172600953 -0.127816246 -1.056826347 -0.23795324\n [86,] -0.893457396 -0.660542921 -1.265061454  0.015612038  1.27234265\n [87,] -0.287301062  0.963686285  0.359594848  0.203580641 -0.80393259\n [88,] -1.133229025  0.151632482 -0.040554491  0.515841576  1.28839412\n [89,] -0.023953670 -0.117144878 -0.537169309 -0.192949617 -0.19885872\n [90,] -0.238994400  1.725878556  1.063475592  0.828811176  1.72692730\n [91,]  0.299232378  0.799624873  1.181753039  0.831726730 -0.84857446\n [92,] -2.481179710  0.113529319 -0.650053446  0.011672273 -0.33558946\n [93,]  0.312713660 -0.079515578  1.502108851 -2.180229770 -2.24161884\n [94,] -1.996496328 -0.407588562 -0.652585366  0.923280995 -0.15832811\n [95,]  2.250167112 -0.220512659 -0.805730284 -2.066199413 -1.28057608\n [96,] -0.763689773 -0.031020794 -0.208759418 -0.867474784  0.72626616\n [97,]  0.014431435 -1.336163148 -0.640654650 -0.333381100 -0.44435745\n [98,]  0.957691513 -0.361694475 -0.967050395  0.060214711  1.10246612\n [99,]  1.469745730  0.554733531 -1.990318074  1.277950650  0.91469843\n[100,]  1.356192755  0.363980993  1.646543813  2.153855538  0.49502358\n[101,]  3.001712443  2.666746291 -0.368684582  1.849950555  0.63675201\n[102,] -0.281234965  0.456235005  2.292473984  0.854928226  2.22619190\n[103,] -1.140727890  0.202030354 -0.219402824  0.310787559  1.38376301\n[104,] -0.101523413  1.693526806  0.088496042  0.849098718 -2.58113447\n[105,] -1.122014513  0.674017745  1.696074464  0.065284585 -2.46042966\n[106,] -0.050503692  0.476461163  0.741019471 -1.033544711  1.30742678\n[107,] -0.977589842  0.119899249  1.188456997  1.081889577  1.41965239\n[108,]  0.909422801  1.255343375  0.842509208 -0.192153747  0.71392520\n[109,]  0.441028047 -0.896440332 -0.270300473  1.985698469  1.24604331\n[110,]  1.237351903  0.285784091 -2.573210003 -1.364613642  0.30715435\n[111,] -0.812621272 -1.216797535 -1.946208168 -0.293442424  0.01613793\n[112,] -0.569590223 -0.000629766 -0.657769831 -2.422099355 -0.23850071\n[113,]  0.013135413 -0.269047393 -2.386679520 -1.612243344  1.04220642\n[114,]  1.497050345  0.473005141 -2.121414725 -1.449151157 -0.36453856\n[115,] -0.826489548 -0.635578799  0.160746530  0.588310936  0.64139235\n[116,]  1.714414681 -0.069917852 -1.565924860  1.120471391 -0.78042307\n[117,]  0.090005879  1.200734542  0.270198304 -1.404710351 -0.03628579\n[118,]  1.236752409 -1.518803758  1.237344754  1.129076932 -0.20967169\n[119,] -0.087766705  0.109758177  0.830463404 -1.215657793  2.10369911\n[120,]  0.171155851 -0.458798255  0.673636072 -0.491339577 -0.55057403\n[121,] -2.015292503 -0.870762059 -0.029309395 -0.793820162 -0.46857349\n[122,] -0.362231614 -0.364385616  0.389845581  0.862317056  0.58137591\n[123,]  1.134908449  0.528983153 -0.215140490  0.381732812  0.95051905\n[124,]  1.457741194  0.847372382  1.281677103  1.882417946  0.73169697\n[125,]  1.108355891  2.153722703 -0.310021511  0.663789759  1.50520899\n[126,]  0.071438586 -0.058006918 -0.325876606 -0.374044712  1.44845100\n[127,]  1.793527240  0.106114670 -0.893934806 -2.276541783  0.63834022\n[128,]  0.315117203 -0.717323756 -0.955019446 -1.370627155  0.39051426\n[129,] -0.043340209 -0.312945747 -1.197591049 -0.317055511 -0.20153262\n[130,] -0.102618844 -0.880170227 -0.604543790  1.138606188 -2.06784587\n[131,]  1.740909147 -0.637353850 -0.010562729  1.561563247 -2.32157478\n[132,]  0.188345255 -0.469433386 -0.998242748  1.333047029 -0.61688863\n[133,]  0.478267920  1.814870921 -0.214383410  0.211463195 -0.68793981\n[134,]  0.520210151  2.448452650 -0.463708150  1.952197392  0.18188706\n[135,] -0.269362229  1.911952464  0.242272223 -0.444547682  1.07677467\n[136,]  0.423190923 -0.410880998 -0.195993929  0.257239829 -0.28741994\n[137,] -1.026903990  1.526871656  0.360358335 -0.772639199 -0.55385790\n[138,]  1.450078506 -0.349330702  1.034884543  1.105119122 -0.44915787\n[139,]  0.550436921  0.247527774  1.257978907  0.192344787  2.62949378\n[140,] -0.191655801  0.134136826 -0.521197521 -1.048113725  0.16877213\n[141,] -0.791900380  0.321889000 -1.170523314 -0.640876817 -0.67475039\n[142,]  0.971146450  0.021298110 -0.039896748  0.046666821 -0.88362522\n[143,] -0.982608481 -0.377262589  0.171316614  1.854723760  0.99834845\n[144,] -0.495627812  2.483176264  0.354246741 -0.684488580 -0.02501307\n[145,]  0.561938527  0.993967044  1.211151634  0.257789007  0.12399797\n[146,]  0.709311790  1.260796758  0.500729309 -0.317030392 -0.10472747\n[147,]  0.242781056  0.024085820  0.200161589  2.499565048 -0.35615091\n[148,]  1.391448147  0.433674956 -0.406153463  0.535043661  0.73457838\n[149,]  0.412636662  0.657392857 -0.602350630 -0.222007914  1.28589205\n[150,]  0.464724442  0.241309629  1.384164088 -0.007823340  1.51425328\n[151,]  1.387500492  1.166650199  1.425112356  1.867514447 -0.82096695\n[152,]  0.324362289  0.546923900  1.123115609  0.598062895 -0.86820404\n[153,] -0.642847662  0.488927474  1.005998211  0.623605903  0.36832761\n[154,]  0.580613501  1.826643913  0.215912542  0.360628058 -0.61370894\n[155,] -0.144307876 -0.646151661  0.724633376  1.186826797 -0.31028196\n[156,]  0.546715317  0.754951418 -0.046634000  0.644574743 -3.02529497\n[157,] -1.292512590  1.254904828  1.493495999 -0.128867226 -0.51111891\n[158,] -0.178360098  1.914596966 -0.981080025  0.228541535 -1.75337640\n[159,] -0.292207540  0.598820101 -0.516651587  0.274304089  0.03373131\n[160,] -2.308524149  0.208852278  1.461230272 -0.500911249 -2.27810147\n[161,] -1.379791032  0.145882158  0.915799429  1.484776097 -3.50041126\n[162,] -0.687780533 -0.916656103  1.270414228  1.016692379 -0.33067553\n[163,]  0.719998169  2.802816864  1.671148575 -0.623046401  0.58186844\n[164,]  0.720895216 -0.441022235  1.355521153  0.169312739  1.37392204\n[165,]  1.255917246  1.237433250 -1.261445845  1.156903886  1.34803048\n[166,]  1.705742677 -2.082080117  1.178644006  0.035092003  2.31810280\n[167,] -1.241297515 -1.810899663  0.429577096 -1.605573403  1.82238571\n[168,] -1.733594362 -1.512615033 -0.660630798 -0.541034771 -0.51548359\n[169,]  0.792480615  1.111823458 -0.087410649 -0.471304848 -1.46894677\n[170,] -1.308490289 -0.820921717 -1.060340478 -0.848828375  0.61581232\n[171,] -0.151371781 -1.755296255 -2.178401224 -1.133293893 -0.73148664\n[172,] -1.440949271 -1.701461336  0.086620787 -0.574783622 -0.96759066\n[173,] -0.602659336  0.088764382 -1.505800731 -1.165425847 -0.71480221\n[174,]  1.275259704 -0.410086120  0.494580410 -2.991666233 -1.27793672\n[175,] -1.404478046 -0.271241519  0.442854733 -0.222198583 -0.71844124\n[176,] -0.656202533  1.203821570  0.978017687  0.126331498 -1.96978752\n[177,] -0.394150009 -0.161910841  2.176312978  0.695232617  0.89852498\n[178,]  0.942635640  0.627373149  1.433906899 -0.365531957  0.60603188\n[179,] -0.023229999  1.377273430 -0.409007667  1.061813530 -0.29478037\n[180,]  0.365704922  1.211613739 -1.923486162  0.212759987  0.96275639\n[181,] -0.005420862 -0.765759383 -1.517079711  0.902219156 -0.34235656\n[182,]  0.024961781  0.098786009 -0.242548348  0.230247435  2.58670863\n[183,]  0.827409379 -0.019279260  1.645100501 -0.517327835  1.97867114\n[184,]  0.446528306 -0.641671765  0.183926490  0.194955794  0.61934460\n[185,] -0.852794309 -0.150650614 -1.175556623 -0.749290811 -0.16935739\n[186,]  0.545053341 -0.164000579 -0.705192833 -2.243859141 -0.09017822\n[187,] -1.237547900  0.639970678 -0.071693146  0.802596589  0.36885061\n[188,] -0.400597906 -0.490175656  1.285472941  0.566619402 -0.76045074\n[189,] -0.346030030 -0.031140831  0.468958960 -0.626935003 -0.58954715\n[190,] -0.242598074 -0.643482354  0.637618122  0.729482578 -2.59565491\n[191,]  1.906927330  1.295421628 -0.473359922  0.009433931 -0.45087521\n[192,]  0.247556013  0.380624945  0.511373051 -0.303734068 -0.35216521\n[193,]  0.109182478 -1.095266046 -1.117244679  0.260695357  0.63022764\n[194,] -0.428068355  0.957493744  1.147709215 -1.806721502  0.20508136\n[195,] -2.391696093  1.006982896  0.469035946 -1.029263987 -1.23612823\n[196,] -2.210774865 -1.403493412  0.556772208 -0.600485513 -2.42048578\n[197,] -0.722601932 -0.414053540  0.988636770 -0.610321642  0.58772892\n[198,] -2.938590715 -0.559749691  0.078615069  0.010633188 -2.36841490\n[199,] -0.985951916  0.215394351 -0.119048423  0.779733910 -2.98799845\n[200,]  0.447242609  0.404155027  0.719959458 -0.442525233 -1.78159592\n\n\n\n%%R\nBIC(GNARfit(vts = vswindts, net = vswindnet, alphaOrder = 1, betaOrder = 0, globalalpha = FALSE))\n\n[1] -233.1697\n\n\n\n%%R\nBIC.Alpha2.Beta &lt;- matrix(0, ncol = 15, nrow = 15)\nfor(b1 in 0:14)\n    for(b2 in 0:14)\n        BIC.Alpha2.Beta[b1 + 1, b2 + 1] &lt;- BIC(GNARfit(vts = vswindts,\n                    net = vswindnet, alphaOrder = 2, betaOrder = c(b1, b2)))\ncontour(0:14, 0:14, log(251 + BIC.Alpha2.Beta), xlab = \"Lag 1 Neighbour Order\", ylab = \"Lag 2 Neighbour Order\")\n\nException ignored from cffi callback &lt;function _processevents at 0x7f1829767f70&gt;:\nTraceback (most recent call last):\n  File \"/home/csy/anaconda3/envs/csy/lib/python3.8/site-packages/rpy2/rinterface_lib/callbacks.py\", line 277, in _processevents\n    try:\nKeyboardInterrupt: \nException ignored from cffi callback &lt;function _processevents at 0x7f1829767f70&gt;:\nTraceback (most recent call last):\n  File \"/home/csy/anaconda3/envs/csy/lib/python3.8/site-packages/rpy2/rinterface_lib/callbacks.py\", line 277, in _processevents\n    try:\nKeyboardInterrupt: \nException ignored from cffi callback &lt;function _processevents at 0x7f1829767f70&gt;:\nTraceback (most recent call last):\n  File \"/home/csy/anaconda3/envs/csy/lib/python3.8/site-packages/rpy2/rinterface_lib/callbacks.py\", line 277, in _processevents\n    try:\nKeyboardInterrupt: \n\n\n\na set of GNAR(2,[b1,b2]) models with b1, b2 ranging from zero to 14\nContour plot of BIC values for the two-lag autoregressive model incorporating b1-stage and b2-stage neighbours at time lags one and two. Values shown are log(251 + BIC) to display clearer contours.\n\n이해 덜 됨..\n\nincreasing the lag two neighbour sets beyond first stage neighbours would appear to increase the BIC for those lag one neighbour stages greater than five\n\nchatGPT\n이 문장을 조금 더 자세히 설명하면, BIC(Bayesian Information Criterion)는 모델을 선택할 때 사용하는 지표로서, 우리가 선택한 모델이 얼마나 적합한지를 측정합니다. 이 문장에서는, 이웃 집합의 대기 시간이 증가할수록 BIC 값이 증가할 것이라고 언급하고 있습니다. 이는 우리가 선택한 모델이 적합하지 않을 가능성이 있다는 의미입니다. 그래프를 보고 있을 때, 수평 윤곽선은 BIC 값이 0인 스테이지를 의미합니다. 이는 우리가 선택한 모델이 완벽하게 적합한다는 의미입니다.\n\n%%R\ngoodmod &lt;- GNARfit(vts = vswindts, net = vswindnet, alphaOrder = 2, betaOrder = c(5, 1))\ngoodmod\n\nModel: \nGNAR(2,[5,1]) \n\nCall:\nlm(formula = yvec ~ dmat + 0)\n\nCoefficients:\n dmatalpha1  dmatbeta1.1  dmatbeta1.2  dmatbeta1.3  dmatbeta1.4  dmatbeta1.5  \n    0.56911      0.10932      0.03680      0.02332      0.02937      0.04709  \n dmatalpha2  dmatbeta2.1  \n    0.23424     -0.04872"
  },
  {
    "objectID": "posts/2_research/2023-01-05-GNAR.html#constructing-a-network-to-aid-prediction",
    "href": "posts/2_research/2023-01-05-GNAR.html#constructing-a-network-to-aid-prediction",
    "title": "GNAR data",
    "section": "3.3. Constructing a network to aid prediction",
    "text": "3.3. Constructing a network to aid prediction\nWe propose a network construction method that uses prediction error, but note here that our scope is not to estimate an underlying network, but merely to find a structure that is useful in the task of prediction.\nwe use a prediction error measure, understood as the sum of squared differences between the observations and the estimates:\n\\[\\sum^N_{i=1} (X_{i,t} - \\hat{X}_{i,t})^2\\]\n\n%%R\nprediction &lt;- predict(GNARfit(vts = fiveVTS[1:199,], net = fiveNet, alphaOrder = 2, betaOrder = c(1, 1)))\nprediction\n\nTime Series:\nStart = 1 \nEnd = 1 \nFrequency = 1 \n    Series 1  Series 2  Series 3  Series 4   Series 5\n1 -0.6427718 0.2060671 0.2525534 0.1228404 -0.8231921"
  },
  {
    "objectID": "posts/2_research/2023-01-05-GNAR.html#oecd-gdp-network-structure-aids-prediction",
    "href": "posts/2_research/2023-01-05-GNAR.html#oecd-gdp-network-structure-aids-prediction",
    "title": "GNAR data",
    "section": "4. OECD GDP: Network structure aids prediction",
    "text": "4. OECD GDP: Network structure aids prediction\nGOP growth rate time series\n\n35 countries from the OECD website\ntime series : 1961 - 2013\nT = 52\nNodes = 35\nIn this data set 20.8% (379 out of 1820) of the observations were missing due to some nodes not being included from the start.\nwe do not uese covariate information, so C=1\n\n\n%%R\nlibrary(\"fields\")\nlayout(matrix(c(1, 2), nrow = 1, ncol = 2), widths = c(4.5, 1))\nimage(t(apply(gdpVTS, 1, rev)), xaxt = \"n\", yaxt = \"n\", col = gray.colors(14), xlab = \"Year\", ylab = \"Country\")\naxis(side = 1, at = seq(from = 0, to = 1, length = 52), labels = FALSE, col.ticks = \"grey\")\naxis(side = 1, at = seq(from = 0, to = 1, length = 52)[5*(1:11)], labels = (1:52)[5*(1:11)])\naxis(side = 2, at = seq(from = 1, to = 0, length = 35), labels = colnames(gdpVTS), las = 1, cex = 0.8)\nlayout(matrix(1))\nimage.plot(zlim = range(gdpVTS, na.rm = TRUE), legend.only = TRUE, col = gray.colors(14))\n\nR[write to console]: Loading required package: spam\n\nR[write to console]: Spam version 2.8-0 (2022-01-05) is loaded.\nType 'help( Spam)' or 'demo( spam)' for a short introduction \nand overview of this package.\nHelp for individual functions is also obtained by adding the\nsuffix '.spam' to the function name, e.g. 'help( chol.spam)'.\n\nR[write to console]: \nAttaching package: ‘spam’\n\n\nR[write to console]: The following objects are masked from ‘package:base’:\n\n    backsolve, forwardsolve\n\n\nR[write to console]: Loading required package: viridis\n\nR[write to console]: Loading required package: viridisLite\n\nR[write to console]: \nTry help(fields) to get started.\n\n\n\n\n\n\n\n\n\n\nHeat plot(grey scale) of the differenced time series,\n\nwhite space indicates missing time series observations"
  },
  {
    "objectID": "posts/2_research/2023-01-05-GNAR.html#finding-a-network-to-aid-prediction",
    "href": "posts/2_research/2023-01-05-GNAR.html#finding-a-network-to-aid-prediction",
    "title": "GNAR data",
    "section": "4.1. Finding a network to aid prediction",
    "text": "4.1. Finding a network to aid prediction\n\n%%R\nnet1 &lt;- seedToNet(seed.no = seed.nos[1], nnodes = 35, graph.prob = 0.15)\nnet2 &lt;- seedToNet(seed.no = seed.nos[2], nnodes = 35, graph.prob = 0.15)\n\n\n%%R\nlayout(matrix(c(2, 1), 1, 2))\npar(mar=c(0,1,0,1))\nplot(net1, vertex.label = colnames(gdpVTS), vertex.size = 0)\nplot(net2, vertex.label = colnames(gdpVTS), vertex.size = 0)\n\n\n\n\n\n\n\n\n\nErdos-Renyi random graphs xonstructed from the first two elements of the seed.nos variable with 35 nodes and connection probability 0.15.\n자기회귀 모델인 GNAR 모델을 예측에 사용할 때, 어떤 네트워크가 가장 적합한지 조사해야 함.\n이때 각 노드의 자기 상관 함수를 이용한 초기 분석 결과, 2차 자기회귀 구성 요소가 충분할 것으로 예상되어 p = 2까지의 GNAR 모델을 시험함.\n각 시간 지연에서 최대 2개의 이웃 집합을 포함함.\n이에 따라 아래와 같은 GNAR 모델이 시험됨.\n\nGNAR(1, [0]), GNAR(1, [1]), GNAR(2, [0, 0]), GNAR(2, [1, 0]), GNAR(2, [1, 1]), GNAR(2, [2, 0]), GNAR(2, [2, 1]), 그리고 GNAR(2, [2, 2])가 시험되며, 각각 individual-\\(\\alpha\\)와 global-\\(\\alpha\\) GNAR 모델로 적합함.\n총 16개의 모델이 생성됨.\n이 중에서 전체 GDP 예측에 사용할 GNAR 모델을 선택할 것.\n연결 확률이 0.15인 10,000개의 임의의 양방향 네트워크를 생성하고, 위에서 언급한 GNAR 모델을 이용해 예측할 것.\n그래서 이 예제는 상당한 계산 시간이 필요(데스크탑 PC에서 약 90분).\n이를 위해 아래 코드에는 일부 분석만 포함.\n계산 상의 이유로, 우선 각 노드에서 표준 편차로 나눠서 잔차가 각 노드에서 동일한 분산을 가지게 함.\nseedSim 함수는 예측값과 원래 값의 제곱 차이의 합을 출력하고, 이를 예측 정확도의 측정 기준으로 사용\n\n\n\n%%R\ngdpVTSn &lt;- apply(gdpVTS, 2, function(x){x / sd(x[1:50], na.rm = TRUE)})\nalphas &lt;- c(rep(1, 2), rep(2, 6))\nbetas &lt;- list(c(0), c(1), c(0, 0), c(1, 0), c(1, 1), c(2, 0), c(2, 1), c(2, 2))\nseedSim &lt;- function(seedNo, modelNo, globalalpha){\n    net1 &lt;- seedToNet(seed.no = seedNo, nnodes = 35, graph.prob = 0.15)\n    gdpPred &lt;- predict(GNARfit(vts = gdpVTSn[1:50, ], net = net1,\n                               alphaOrder = alphas[modelNo], betaOrder = betas[[modelNo]],\n                               globalalpha = globalalpha))\n    return(sum((gdpPred - gdpVTSn[51, ])^2))\n    }\n\n\n%%R\nseedSim(seedNo = seed.nos[1], modelNo = 1, globalalpha = TRUE)\n\n[1] 23.36913\n\n\n\n%%R\nseedSim(seed.nos[1], modelNo = 3, globalalpha = TRUE)\n\n[1] 11.50739\n\n\n\n%%R\nseedSim(seed.nos[1], modelNo = 3, globalalpha = FALSE)\n\n[1] 18.96766\n\n\n\n\n\nimage.png\n\n\n\n10,000개의 임의의 네트워크와 16개의 모델로부터 시뮬레이션한 예측 오류의 박스 그래프\n(계산 시간이 길어(90분) 코드는 생략).\n일반적으로 global-α 모델은 더 낮은 예측 오류를 일으킴.\n그래서 이 버전의 GNAR 모델을 사용할 것.\n그림 9에서 첫 번째 모델인 GNAR(1, [0])과 세 번째 모델인 GNAR(2, [0, 0])의 경우, “박스 그래프”는 인접한 매개변수가 적합되지 않아 결과가 전부 동일해 짧은 수평선으로 표시됨.\n다른 global-α 모델들은 이 안에 포함되어 있기 때문에, global-α GNAR(2, [2, 2])의 예측 오류가 최소가 되는 임의의 그래프를 선택할 것.\n이는 seed.nos[921]에서 생성된 네트워크가 선택되게 됩니다.\n\n\n%%R\nnet921 &lt;- seedToNet(seed.no = seed.nos[921], nnodes = 35, graph.prob = 0.15)\nlayout(matrix(c(1), 1, 1))\nplot(net921, vertex.label = colnames(gdpVTS), vertex.size = 0)\n\n\n\n\n\n\n\n\nRandomly generated un-weighted and un-directed graph over the OECD ountries that minimises the prediction error at t = 51 using GNAR(2, [2, 2]).\n\nseed.nos[921]에서 생성된 네트워크\n네트워크에는 전부 2개 이상의 이웃을 가지고 있는 countries들이 있고, 총 97개의 edges이 있음.\n이 “921” 네트워크는 GDP 예측을 위해 생성되었기 때문에, 찾은 네트워크에 인식 가능한 구조가 있지 않을 것이라고 예상할 수 있음\n그러나 미국, 멕시코, 캐나다는 각각 8개, 8개, 6개의 edge을 가지고 있어 매우 잘 연결되어 있음.\n스웨덴과 칠레도 잘 연결되어 있으며, 각각 8개와 7개의 edge을 가지고 있습니다.\n예측 성능이 유사한 적은 개수의 edge를 가진 네트워크를 찾기 위해 테스트 될 수 있지만, 여기서는 전체 선택된 네트워크를 그대로 사용.\n이 네트워크를 사용하면 BIC를 이용해 최적의 GNAR 순서를 선택할 수 있음.\n\n\n%%R\nres &lt;- rep(NA, 8)\nfor(i in 1:8){\n    res[i] &lt;- BIC(GNARfit(gdpVTSn[1:50, ],\n                          net = seedToNet(seed.nos[921], nnodes = 35, graph.prob = 0.15),\n                          alphaOrder = alphas[i], betaOrder = betas[[i]]))}\norder(res)\n\n[1] 6 3 4 7 8 5 1 2\n\n\n\n%%R\nsort(res)\n\n[1] -64.44811 -64.32155 -64.18751 -64.12683 -64.09656 -63.86919 -60.67858\n[8] -60.54207"
  },
  {
    "objectID": "posts/2_research/2023-01-05-GNAR.html#results-and-comparisons",
    "href": "posts/2_research/2023-01-05-GNAR.html#results-and-comparisons",
    "title": "GNAR data",
    "section": "4.2. Results and comparisons",
    "text": "4.2. Results and comparisons\n\n이전 섹션의 모델을 사용해 t=52일 때의 값을 예측\n이 예측 오류를 표준 AR과 VAR 모델을 사용해 찾은 예측 오류와 비교\nGNAR 예측은 선택된 네트워크(seed.nos[921]에 해당)를 가진 GNAR(2, [2, 0]) 모델을 t=51까지의 데이터에 적합시키고, t=52일 때의 값을 예측\n우선 series를 정규화한 다음, 모델 적합으로부터 SSE를 계산합니다.\n\n\n%%R\ngdpVTSn2 &lt;- apply(gdpVTS, 2, function(x){x / sd(x[1:51], na.rm = TRUE)})\ngdpFit &lt;- GNARfit(gdpVTSn2[1:51,], net = net921, alphaOrder = 2, betaOrder = c(2, 0))\nsummary(gdpFit)\n\n\nCall:\nlm(formula = yvec2 ~ dmat2 + 0)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-3.4806 -0.5491 -0.0121  0.5013  3.1208 \n\nCoefficients:\n             Estimate Std. Error t value Pr(&gt;|t|)    \ndmat2alpha1  -0.41693    0.03154 -13.221  &lt; 2e-16 ***\ndmat2beta1.1 -0.12662    0.05464  -2.317   0.0206 *  \ndmat2beta1.2  0.28044    0.06233   4.500  7.4e-06 ***\ndmat2alpha2  -0.33282    0.02548 -13.064  &lt; 2e-16 ***\n---\nSignif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1\n\nResidual standard error: 0.8926 on 1332 degrees of freedom\n  (23 observations deleted due to missingness)\nMultiple R-squared:  0.1859,    Adjusted R-squared:  0.1834 \nF-statistic: 76.02 on 4 and 1332 DF,  p-value: &lt; 2.2e-16\n\nGNAR BIC: -62.86003\n\n\n\n%%R\nsum((predict(gdpFit) - gdpVTSn2[52, ])^2)\n\n[1] 5.737203\n\n\n이 GNAR 모델의 적합된 매개변수는 \\(\\alpha^1 = - 0.42, \\beta^1,1 = - 0.13, \\beta^1,2 = 0.28\\), 그리고 \\(\\alpha^2 = - 0.33\\)입니다.\n\n\n\nModel\nparameters\nprediction error\n\n\n\n\nGNAR(2,[2,0])\n4\n5.7\n\n\nIndividual AR(2)\n38\n8.1\n\n\nVAR(1)\n199\n26.2\n\n\n\nEstimated prediction error of differenced real GDP change at t = 52 for all 35 countries.\n우리의 방법과 CRAN forecast 패키지의 버전 8.0에서의 forecast.ar()과 auto.arima() 함수를 사용해 각 노드별로 AR 모델을 적합한 결과를 비교\n\n섹션 4.1의 자기상관 분석을 고려해 각각 35개의 개별 모델의 최대 AR 순서를 p=2로 설정\n\n\n%%R\nlibrary(\"forecast\")\narforecast &lt;- apply(gdpVTSn2[1:51, ], 2, function(x){\n            forecast(auto.arima(x[!is.na(x)], d= ,D=0,max.p = 2,max.q=0,\n                                max.P=0,max.Q = 0,stationary = TRUE, seasonal = FALSE), ic = \"bic\",\n                     allowmean = FALSE, allowdraft = FALSE, trace = FALSE, h=1)$mean\n})\nsum((arforecast - gdpVTSn2[52, ])^2)\n\nR[write to console]: Registered S3 method overwritten by 'quantmod':\n  method            from\n  as.zoo.data.frame zoo \n\n\n\n[1] 7.8974\n\n\nWe fit the model using the VAR function and then use the restrict function to reduce dimensionality further, by setting to zero any coefficient whose associated absolute t-statistic value is less than two.\n\n%%R\nlibrary(\"vars\")\ngdpVTSn2.0 &lt;- gdpVTSn2\ngdpVTSn2.0[is.na(gdpVTSn2.0)] &lt;- 0\nvarforecast &lt;- predict(restrict(VAR(gdpVTSn2.0[1:51, ], p = 1, type = \"none\")), n.ahead = 1)\n\ncompute the prediction error\n\n%%R\ngetfcst &lt;- function(x){return(x[1])}\nvarforecastpt &lt;- unlist(lapply(varforecast$fcst, getfcst))\nsum((varforecastpt - gdpVTSn2.0[52, ])^2)\n\n[1] 26.19805\n\n\nGNAR 모델은 AR과 VAR 결과보다 적은 예측 오류를 제공합니다. 이는 AR과 비교했을 때 29%가 줄어들고, VAR과 비교했을 때 78%가 줄어듭니다.\n위 절차를 반복해 2단계 앞으로의 예측을 기반으로 분석을 수행합니다.\n이 경우 다른 네트워크가 GNAR(2,[2,2]) 모델의 예측 오류를 최소화합니다.\n그러나 BIC 단계에서 GNAR(2,[0,0]) 모델이 최적으로 적합된 것을 식별하였고, 이는 네트워크 회귀 매개변수를 포함하지 않는 모델입니다.\n\n%%R\nlibrary(\"vars\")\ngdpVTSn2.0 &lt;- gdpVTSn2\ngdpVTSn2.0[is.na(gdpVTSn2.0)] &lt;- 0\nvarforecast &lt;- predict(restrict(VAR(gdpVTSn2.0[1:51, ], p = 1, type = \"none\")), n.ahead = 40)"
  },
  {
    "objectID": "posts/2_research/2024-01-28시뮬 데이터 정리.html",
    "href": "posts/2_research/2024-01-28시뮬 데이터 정리.html",
    "title": "시뮬 데이터 정리중",
    "section": "",
    "text": "Model\nFiveVTS\nChickenpox\nPedalme\nWikimath\nWindmillsmall\nMontevideoBus\n\n\n\n\nMax iter.\n30\n30\n30\n30\n30\n30\n\n\nEpochs\n50\n50\n50\n50\n50\n50\n\n\nLags\n2\n4\n4\n8\n8\n4\n\n\nInterpolation\nlinear\nlinear\nnearest\nlinear\nlinear\nnearest\n\n\nFilters\n\n\n\n\n\n\n\n\nGConvGRU\n12\n16\n12\n12\n12\n12\n\n\nGConvLSTM\n12\n32\n2\n64\n16\n12\n\n\nGCLSTM\n4\n16\n4\n64\n16\n12\n\n\nLRGCN\n4\n8\n8\n32\n12\n2\n\n\nDyGrEncoder\n12\n12\n12\n12\n12\n12\n\n\nEvolveGCNH\nNo need\nNo need\nNo need\nNo need\nNo need\nNo need\n\n\nEvolveGCNO\nNo need\nNo need\nNo need\nNo need\nNo need\nNo need\n\n\nTGCN\n12\n12\n12\n12\n12\n8\n\n\nDCRNN\n2\n16\n8\n12\n4\n12\n\n\n\nwikimath\n\n# pd.concat([pd.read_csv('./simulation_results/2024-02-25_12-06-35.csv').assign(model='GConvGRU'),\n#           pd.read_csv('./simulation_results/2024-03-04_04-25-27.csv').assign(model='GConvLSTM'),\n#           pd.read_csv('./simulation_results/2024-03-04_21-32-09.csv').assign(model='GCLSTM'),\n#           pd.read_csv('./simulation_results/2024-03-05_06-22-00.csv').assign(model='LRGCN'),\n#           pd.read_csv('./simulation_results/2024-03-04_03-27-33.csv').assign(model='DyGrEncoder'),\n#           pd.read_csv('./simulation_results/2024-03-04_15-24-07.csv').assign(model='EvolveGCNH'),\n#           pd.read_csv('./simulation_results/2024-03-06_11-25-11.csv').assign(model='EvolveGCNO'),\n#           pd.read_csv('./simulation_results/2024-03-07_09-44-51.csv').assign(model='TGCN'),\n#           pd.read_csv('./simulation_results/2024-03-05_08-53-09.csv').assign(model='DCRNN'),\n#           ]).iloc[:,1:].assign(dataset='wikimath').to_csv('wikimath_block_2024.csv')\n\nmonte_GCLSTM(0.3,0.5,0.7,0.8)\n\n# pd.concat([pd.read_csv('./simulation_results/2024-02-25_08-09-31.csv'), # 0.3\n#            pd.read_csv('./simulation_results/2024-02-25_20-39-38.csv'), # 0.5\n#            pd.read_csv('./simulation_results/2024-02-26_09-39-26.csv'), # 0.7\n#            pd.read_csv('./simulation_results/2024-02-26_19-27-03.csv'), # 0.8\n#           ]).assign(model='GCLSTM').to_csv('./GCLSTM_monte_linear.csv')\n\n\nblock\nLRGCN\n\n# pd.concat([pd.read_csv('./simulation_results/2024-02-24_17-12-48.csv'), # 1\n#            pd.read_csv('./simulation_results/2024-02-25_13-17-08.csv'), # 2\n#            pd.read_csv('./simulation_results/2024-02-26_09-24-46.csv'), # 3\n#            pd.read_csv('./simulation_results/2024-02-27_01-40-11.csv'), # 4\n#            pd.read_csv('./simulation_results/2024-02-29_08-11-48.csv'), # 5\n#            pd.read_csv('./simulation_results/2024-02-29_23-24-00.csv'), # 6\n#            pd.read_csv('./simulation_results/2024-03-01_14-20-41.csv'), # 7\n#            pd.read_csv('./simulation_results/2024-03-02_05-31-34.csv'), # 8\n#            pd.read_csv('./simulation_results/2024-03-02_20-20-07.csv'), # 9\n#            pd.read_csv('./simulation_results/2024-03-03_11-18-32.csv'), # 10\n#            pd.read_csv('./simulation_results/2024-03-04_01-27-12.csv'), # 11\n#            pd.read_csv('./simulation_results/2024-03-04_13-12-25.csv'), # 12\n#            pd.read_csv('./simulation_results/2024-03-05_00-18-50.csv'), # 13\n#            pd.read_csv('./simulation_results/2024-03-05_10-30-53.csv'), # 14\n#            pd.read_csv('./simulation_results/2024-03-05_11-20-25.csv'), # 15\n#            pd.read_csv('./simulation_results/2024-03-06_07-09-45.csv'), # 16\n#            pd.read_csv('./simulation_results/2024-03-06_19-37-46.csv'), # 17\n#            pd.read_csv('./simulation_results/2024-03-07_08-43-01.csv'), # 18\n#            pd.read_csv('./simulation_results/2024-03-07_21-55-25.csv'), # 19\n#            pd.read_csv('./simulation_results/2024-03-08_16-32-06.csv'), # 20\n#            pd.read_csv('./simulation_results/2024-03-09_05-37-54.csv'), # 21\n#            pd.read_csv('./simulation_results/2024-03-09_18-51-44.csv'), # 22\n#            pd.read_csv('./simulation_results/2024-03-10_08-07-06.csv'), # 23\n#            pd.read_csv('./simulation_results/2024-03-10_21-36-56.csv'), # 24\n#            pd.read_csv('./simulation_results/2024-03-11_11-06-11.csv'), # 25\n#            pd.read_csv('./simulation_results/2024-03-12_00-29-16.csv'), # 26\n#            pd.read_csv('./simulation_results/2024-03-12_14-03-31.csv'), # 27\n#            pd.read_csv('./simulation_results/2024-03-13_03-29-05.csv'), # 28\n#            pd.read_csv('./simulation_results/2024-03-13_15-00-58.csv'), # 29\n#            pd.read_csv('./simulation_results/2024-03-14_02-08-48.csv'), # 30\n#           ]).assign(model='LRGCN').iloc[:,1:].assign(dataset='windmillsmall').to_csv('LRGCN_block_windmillsmall.csv')\n\nGCLSTM\n\n# pd.concat([pd.read_csv('./simulation_results/2024-02-25_01-56-18.csv'), # 1\n#            pd.read_csv('./simulation_results/2024-02-25_21-45-51.csv'), # 2\n#            pd.read_csv('./simulation_results/2024-02-26_16-26-55.csv'), # 3\n#            pd.read_csv('./simulation_results/2024-02-27_07-25-35.csv'), # 4\n#            pd.read_csv('./simulation_results/2024-02-29_07-57-46.csv'), # 5\n#            pd.read_csv('./simulation_results/2024-02-29_22-33-16.csv'), # 6\n#            pd.read_csv('./simulation_results/2024-03-01_13-11-27.csv'), # 7\n#            pd.read_csv('./simulation_results/2024-03-02_03-46-31.csv'), # 8\n#            pd.read_csv('./simulation_results/2024-03-02_18-06-47.csv'), # 9\n#            pd.read_csv('./simulation_results/2024-03-03_08-23-50.csv'), # 10\n#            pd.read_csv('./simulation_results/2024-03-03_22-37-18.csv'), # 11\n#            pd.read_csv('./simulation_results/2024-03-04_10-31-53.csv'), # 12\n#            pd.read_csv('./simulation_results/2024-03-04_21-17-46.csv'), # 13\n#            pd.read_csv('./simulation_results/2024-03-05_09-39-49.csv'), # 14\n#            pd.read_csv('./simulation_results/2024-03-06_14-07-10.csv'), # 15\n#            pd.read_csv('./simulation_results/2024-03-07_01-57-28.csv'), # 16\n#            pd.read_csv('./simulation_results/2024-03-08_03-34-59.csv'), # 17\n#            pd.read_csv('./simulation_results/2024-03-08_20-00-11.csv'), # 18\n#            pd.read_csv('./simulation_results/2024-03-09_08-17-39.csv'), # 19\n#            pd.read_csv('./simulation_results/2024-03-09_20-19-39.csv'), # 20\n#            pd.read_csv('./simulation_results/2024-03-10_08-25-07.csv'), # 21\n#            pd.read_csv('./simulation_results/2024-03-10_20-34-35.csv'), # 22\n#            pd.read_csv('./simulation_results/2024-03-11_08-47-02.csv'), # 23\n#            pd.read_csv('./simulation_results/2024-03-11_21-08-05.csv'), # 24\n#            pd.read_csv('./simulation_results/2024-03-12_09-08-18.csv'), # 25\n#            pd.read_csv('./simulation_results/2024-03-12_21-26-10.csv'), # 26\n#            pd.read_csv('./simulation_results/2024-03-13_09-03-20.csv'), # 27\n#            pd.read_csv('./simulation_results/2024-03-13_19-13-30.csv'), # 28\n#            pd.read_csv('./simulation_results/2024-03-14_05-40-23.csv'), # 29\n#            pd.read_csv('./simulation_results/2024-03-16_11-17-42.csv'), # 30\n#           ]).assign(model='GCLSTM').iloc[:,1:].assign(dataset='windmillsmall').to_csv('GCLSTM_block_windmillsmall.csv')\n\nGConvGRU\n\n# pd.concat([pd.read_csv('./simulation_results/windmillsmall_block_186_GConcGRU.csv').iloc[:,1:-1], # 1 ~ 14\n#            pd.read_csv('./simulation_results/2024-03-06_09-57-41.csv'), # 15\n#            pd.read_csv('./simulation_results/2024-03-06_23-23-04.csv'), # 16\n#            pd.read_csv('./simulation_results/2024-03-07_13-14-28.csv'), # 17\n#            pd.read_csv('./simulation_results/2024-03-08_05-30-38.csv'), # 18\n#            pd.read_csv('./simulation_results/2024-03-08_22-39-57.csv'), # 19\n#            pd.read_csv('./simulation_results/2024-03-09_13-00-53.csv'), # 20\n#            pd.read_csv('./simulation_results/2024-03-10_03-39-13.csv'), # 21\n#            pd.read_csv('./simulation_results/2024-03-10_18-13-23.csv'), # 22\n#            pd.read_csv('./simulation_results/2024-03-11_08-35-25.csv'), # 23\n#            pd.read_csv('./simulation_results/2024-03-11_22-52-22.csv'), # 24\n#            pd.read_csv('./simulation_results/2024-03-12_13-24-38.csv'), # 25\n#            pd.read_csv('./simulation_results/2024-03-13_03-57-13.csv'), # 26\n#            pd.read_csv('./simulation_results/2024-03-13_16-30-36.csv'), # 27\n#            pd.read_csv('./simulation_results/2024-03-14_04-59-42.csv'), # 28\n#            pd.read_csv('./simulation_results/2024-03-14_13-42-47.csv'), # 29\n#            pd.read_csv('./simulation_results/2024-03-14_21-51-17.csv'), # 30\n#           ]).assign(model = 'GConvGRU').iloc[:,1:].assign(dataset='windmillsmall').to_csv('GConvGRU_block_windmillsmall.csv')\n\nDCRNN\n\n# pd.concat([pd.read_csv('./simulation_results/windmillsmall_block_186_DCRNN.csv').iloc[:,1:-1], # 1 ~ 17\n#            pd.read_csv('./simulation_results/2024-03-07_01-11-41.csv'), # 18\n#            pd.read_csv('./simulation_results/2024-03-07_12-57-06.csv'), # 19\n#            pd.read_csv('./simulation_results/2024-03-08_01-14-30.csv'), # 20\n#            pd.read_csv('./simulation_results/2024-03-08_18-22-08.csv'), # 21\n#            pd.read_csv('./simulation_results/2024-03-09_06-22-33.csv'), # 22\n#            pd.read_csv('./simulation_results/2024-03-09_18-25-50.csv'), # 23\n#            pd.read_csv('./simulation_results/2024-03-10_06-19-54.csv'), # 24\n#            pd.read_csv('./simulation_results/2024-03-10_18-11-23.csv'), # 25\n#            pd.read_csv('./simulation_results/2024-03-11_06-07-29.csv'), # 26\n#            pd.read_csv('./simulation_results/2024-03-11_18-13-03.csv'), # 27\n#            pd.read_csv('./simulation_results/2024-03-12_06-07-16.csv'), # 28\n#            pd.read_csv('./simulation_results/2024-03-12_18-23-58.csv'), # 29\n#            pd.read_csv('./simulation_results/2024-03-13_06-16-02.csv'), # 30\n#            ]).assign(model='DCRNN').iloc[:,1:].assign(dataset='windmillsmall').to_csv('DCRNN_block_windmillsmall.csv')\n\nDyGrEncoder\n\n# pd.concat([pd.read_csv('./simulation_results/windmillsmall_block_186_DyGrEncoder.csv').iloc[:,1:-1], # 1 ~ 13\n#            pd.read_csv('./simulation_results/2024-03-07_23-58-29.csv'), # 14\n#            pd.read_csv('./simulation_results/2024-03-08_19-46-08.csv'), # 15\n#            pd.read_csv('./simulation_results/2024-03-09_09-47-20.csv'), # 16\n#            pd.read_csv('./simulation_results/2024-03-10_00-06-03.csv'), # 17\n#            pd.read_csv('./simulation_results/2024-03-10_13-54-25.csv'), # 18\n#            pd.read_csv('./simulation_results/2024-03-11_03-56-49.csv'), # 19\n#            pd.read_csv('./simulation_results/2024-03-11_17-20-01.csv'), # 20\n#            pd.read_csv('./simulation_results/2024-03-12_07-21-14.csv'), # 21\n#            pd.read_csv('./simulation_results/2024-03-12_21-02-09.csv'), # 22\n#            pd.read_csv('./simulation_results/2024-03-13_09-54-25.csv'), # 23\n#            pd.read_csv('./simulation_results/2024-03-13_20-33-27.csv'), # 24\n#            pd.read_csv('./simulation_results/2024-03-14_06-54-42.csv'), # 25\n#            pd.read_csv('./simulation_results/2024-03-14_13-27-47.csv'), # 26\n#            pd.read_csv('./simulation_results/2024-03-14_19-20-53.csv'), # 27\n#            pd.read_csv('./simulation_results/2024-03-15_00-13-07.csv'), # 28\n#            pd.read_csv('./simulation_results/2024-03-15_04-12-46.csv'), # 29\n#            pd.read_csv('./simulation_results/2024-03-15_08-13-11.csv'), # 30\n#           ]).assign(model='DyGrEncoder').iloc[:,1:].assign(dataset='windmillsmall').to_csv('DyGrEncoder_block_windmillsmall.csv')\n\nEvolveGCNO\n\n# pd.concat([pd.read_csv('./simulation_results/windmillsmall_block_186_EvolveGCNO.csv').iloc[:,1:-1], # 1 ~ 10\n#            pd.read_csv('./simulation_results/2024-03-09_16-20-03.csv'), # 11\n#            pd.read_csv('./simulation_results/2024-03-09_22-18-27.csv'), # 12\n#            pd.read_csv('./simulation_results/2024-03-10_04-05-47.csv'), # 13\n#            pd.read_csv('./simulation_results/2024-03-10_09-52-52.csv'), # 14\n#            pd.read_csv('./simulation_results/2024-03-10_15-48-25.csv'), # 15\n#            pd.read_csv('./simulation_results/2024-03-10_21-31-36.csv'), # 16\n#            pd.read_csv('./simulation_results/2024-03-11_03-39-58.csv'), # 17\n#            pd.read_csv('./simulation_results/2024-03-11_09-44-28.csv'), # 18\n#            pd.read_csv('./simulation_results/2024-03-11_15-35-47.csv'), # 19\n#            pd.read_csv('./simulation_results/2024-03-11_21-41-00.csv'), # 20\n#            pd.read_csv('./simulation_results/2024-03-12_03-33-46.csv'), # 21\n#            pd.read_csv('./simulation_results/2024-03-12_09-38-00.csv'), # 22\n#            pd.read_csv('./simulation_results/2024-03-12_15-44-12.csv'), # 23\n#            pd.read_csv('./simulation_results/2024-03-12_21-56-48.csv'), # 24\n#            pd.read_csv('./simulation_results/2024-03-13_03-53-51.csv'), # 25\n#            pd.read_csv('./simulation_results/2024-03-13_09-56-41.csv'), # 26\n#            pd.read_csv('./simulation_results/2024-03-13_17-29-51.csv'), # 27\n#            pd.read_csv('./simulation_results/2024-03-13_23-49-15.csv'), # 28\n#            pd.read_csv('./simulation_results/2024-03-14_05-47-19.csv'), # 29\n#            pd.read_csv('./simulation_results/2024-03-14_12-53-09.csv'), # 30\n#           ]).assign(model='EvolveGCNO').iloc[:,1:].assign(dataset='windmillsmall').to_csv('EvolveGCNO_block_windmillsmall.csv')\n\nEvolveGCNH\n\n# pd.concat([pd.read_csv('./simulation_results/2024-03-17_14-13-44.csv'), # 1\n#            pd.read_csv('./simulation_results/2024-03-17_14-31-15.csv'), # 2\n#            pd.read_csv('./simulation_results/2024-03-17_16-06-39.csv'), # 3\n#            pd.read_csv('./simulation_results/2024-03-17_16-06-51.csv'), # 4\n#            pd.read_csv('./simulation_results/2024-03-17_16-06-58.csv'), # 5\n#            pd.read_csv('./simulation_results/2024-03-17_12-54-45.csv'), # 6\n#            pd.read_csv('./simulation_results/2024-03-17_13-05-25.csv'), # 7\n#            pd.read_csv('./simulation_results/2024-03-17_13-06-51.csv'), # 8\n#            pd.read_csv('./simulation_results/2024-03-17_13-52-59.csv'), # 9\n#            pd.read_csv('./simulation_results/2024-03-17_13-57-46.csv'), # 10\n#            pd.read_csv('./simulation_results/2024-03-18_01-58-50.csv'), # 11\n#            pd.read_csv('./simulation_results/2024-03-18_02-12-30.csv'), # 12\n#            pd.read_csv('./simulation_results/2024-03-18_05-06-04.csv'), # 13\n#            pd.read_csv('./simulation_results/2024-03-18_05-06-08.csv'), # 14\n#            pd.read_csv('./simulation_results/2024-03-18_05-48-26.csv'), # 15\n#            pd.read_csv('./simulation_results/2024-03-18_05-48-58.csv'), # 16\n#            pd.read_csv('./simulation_results/2024-03-18_05-50-22.csv'), # 17\n#            pd.read_csv('./simulation_results/2024-03-18_05-51-37.csv'), # 18\n#            pd.read_csv('./simulation_results/2024-03-18_06-42-35.csv'), # 19\n#            pd.read_csv('./simulation_results/2024-03-18_13-32-59.csv'), # 20\n#            pd.read_csv('./simulation_results/2024-03-18_14-12-41.csv'), # 21\n#            pd.read_csv('./simulation_results/2024-03-18_17-55-26.csv'), # 22\n#            pd.read_csv('./simulation_results/2024-03-18_18-08-51.csv'), # 23\n#            pd.read_csv('./simulation_results/2024-03-19_01-19-07.csv'), # 24\n#            pd.read_csv('./simulation_results/2024-03-19_02-02-35.csv'), # 25\n#            pd.read_csv('./simulation_results/2024-03-19_06-51-50.csv'), # 26\n#            pd.read_csv('./simulation_results/2024-03-19_07-03-53.csv'), # 27\n#            pd.read_csv('./simulation_results/2024-03-19_07-15-32.csv'), # 28\n#            pd.read_csv('./simulation_results/2024-03-18_23-01-44.csv'), # 29\n#            pd.read_csv('./simulation_results/2024-03-18_23-12-21.csv'), # 30\n#           ]).assign(model='EvolveGCNH').iloc[:,1:].assign(dataset='windmillsmall').to_csv('EvolveGCNH_block_windmillsmall.csv')\n\nGConvLSTM\n\n# pd.concat([pd.read_csv('./simulation_results/windmillsmall_block_186_GConvLSTM.csv').iloc[:,1:-1], # 1 ~ 12\n#            pd.read_csv('./simulation_results/2024-03-09_19-05-13.csv'), # 13\n#            pd.read_csv('./simulation_results/2024-03-10_03-19-57.csv'), # 14\n#            pd.read_csv('./simulation_results/2024-03-10_11-23-58.csv'), # 15\n#            pd.read_csv('./simulation_results/2024-03-10_19-48-00.csv'), # 16\n#            pd.read_csv('./simulation_results/2024-03-11_04-21-33.csv'), # 17\n#            pd.read_csv('./simulation_results/2024-03-11_12-50-30.csv'), # 18\n#            pd.read_csv('./simulation_results/2024-03-11_21-16-02.csv'), # 19\n#            pd.read_csv('./simulation_results/2024-03-12_05-51-52.csv'), # 20\n#            pd.read_csv('./simulation_results/2024-03-12_14-24-24.csv'), # 21\n#            pd.read_csv('./simulation_results/2024-03-12_22-46-04.csv'), # 22\n#            pd.read_csv('./simulation_results/2024-03-13_07-06-17.csv'), # 23\n#            pd.read_csv('./simulation_results/2024-03-13_16-58-36.csv'), # 24\n#            pd.read_csv('./simulation_results/2024-03-14_01-56-44.csv'), # 25\n#            pd.read_csv('./simulation_results/2024-03-14_11-26-31.csv'), # 26\n#            pd.read_csv('./simulation_results/2024-03-14_18-01-05.csv'), # 27\n#            pd.read_csv('./simulation_results/2024-03-14_23-43-02.csv'), # 28\n#            pd.read_csv('./simulation_results/2024-03-15_05-14-29.csv'), # 29\n#            pd.read_csv('./simulation_results/2024-03-15_10-39-31.csv'), # 30\n#           ]).assign(model='GConvLSTM').iloc[:,1:].assign(dataset='windmillsmall').to_csv('GConvLSTM_block_windmillsmall.csv')\n\nTGCN\n\n# pd.concat([pd.read_csv('./simulation_results/windmillsmall_block_186_TGCN.csv').iloc[:,1:-1], # 1 ~ 11\n#            pd.read_csv('./simulation_results/2024-03-09_17-07-28.csv'), # 12\n#            pd.read_csv('./simulation_results/2024-03-09_23-39-03.csv'), # 13\n#            pd.read_csv('./simulation_results/2024-03-10_06-04-49.csv'), # 14\n#            pd.read_csv('./simulation_results/2024-03-10_12-31-46.csv'), # 15\n#            pd.read_csv('./simulation_results/2024-03-10_19-17-35.csv'), # 16\n#            pd.read_csv('./simulation_results/2024-03-11_01-54-27.csv'), # 17\n#            pd.read_csv('./simulation_results/2024-03-11_08-36-34.csv'), # 18\n#            pd.read_csv('./simulation_results/2024-03-11_15-14-56.csv'), # 19\n#            pd.read_csv('./simulation_results/2024-03-11_22-06-57.csv'), # 20\n#            pd.read_csv('./simulation_results/2024-03-12_04-58-46.csv'), # 21\n#            pd.read_csv('./simulation_results/2024-03-12_11-39-48.csv'), # 22\n#            pd.read_csv('./simulation_results/2024-03-12_18-16-49.csv'), # 23\n#            pd.read_csv('./simulation_results/2024-03-13_07-47-23.csv'), # 24\n#            pd.read_csv('./simulation_results/2024-03-13_15-47-57.csv'), # 25\n#            pd.read_csv('./simulation_results/2024-03-13_23-12-57.csv'), # 26\n#            pd.read_csv('./simulation_results/2024-03-14_06-31-10.csv'), # 27\n#            pd.read_csv('./simulation_results/2024-03-14_13-36-34.csv'), # 28\n#            pd.read_csv('./simulation_results/2024-03-14_18-40-20.csv'), # 29\n#            pd.read_csv('./simulation_results/2024-03-16_02-27-03.csv'), # 30\n#           ]).assign(model='TGCN').iloc[:,1:].assign(dataset='windmillsmall').to_csv('TGCN_block_windmillsmall.csv')\n\n\nrandom\nGConvLSTM\n\n# pd.concat([pd.read_csv('./simulation_results/2024-01-24_01-17-39.csv'), # 1\n#            pd.read_csv('./simulation_results/2024-01-24_08-15-26.csv'), # 2\n#            pd.read_csv('./simulation_results/2024-01-24_14-17-11.csv'), # 3\n#            pd.read_csv('./simulation_results/2024-01-25_01-05-51.csv'), # 4\n#            pd.read_csv('./simulation_results/2024-01-25_12-18-29.csv'), # 5\n#            pd.read_csv('./simulation_results/2024-01-25_23-41-45.csv'), # 6\n#            pd.read_csv('./simulation_results/2024-01-26_12-37-25.csv'), # 7\n#            pd.read_csv('./simulation_results/2024-01-26_21-14-21.csv'), # 8\n#            pd.read_csv('./simulation_results/2024-01-27_08-50-46.csv'), # 9\n#            pd.read_csv('./simulation_results/2024-01-27_19-02-29.csv'), # 10\n#            pd.read_csv('./simulation_results/2024-01-28_08-29-10.csv'), # 11\n#            pd.read_csv('./simulation_results/2024-01-28_17-22-21.csv'), # 12\n#            pd.read_csv('./simulation_results/2024-01-29_05-59-51.csv'), # 13\n#            pd.read_csv('./simulation_results/2024-01-29_15-49-49.csv'), # 14\n#            pd.read_csv('./simulation_results/2024-01-30_04-23-51.csv'), # 15\n#            pd.read_csv('./simulation_results/2024-01-30_23-11-56.csv'), # 16\n#            pd.read_csv('./simulation_results/2024-01-31_14-01-41.csv'), # 17\n#            pd.read_csv('./simulation_results/2024-02-01_04-15-58.csv'), # 18\n#            pd.read_csv('./simulation_results/2024-02-01_19-29-23.csv'), # 19\n#            pd.read_csv('./simulation_results/2024-02-02_09-52-22.csv'), # 20\n#            pd.read_csv('./simulation_results/2024-02-03_00-11-56.csv'), # 21\n#            pd.read_csv('./simulation_results/2024-02-03_14-38-41.csv'), # 22\n#            pd.read_csv('./simulation_results/2024-02-04_05-42-18.csv'), # 23\n#            pd.read_csv('./simulation_results/2024-02-04_20-15-56.csv'), # 24\n#            pd.read_csv('./simulation_results/2024-02-05_10-59-26.csv'), # 25\n#            pd.read_csv('./simulation_results/2024-02-06_02-03-46.csv'), # 26\n#            pd.read_csv('./simulation_results/2024-02-06_16-24-31.csv'), # 27\n#            pd.read_csv('./simulation_results/2024-02-07_04-29-11.csv'), # 28\n#            pd.read_csv('./simulation_results/2024-02-07_15-48-58.csv'), # 29\n#            pd.read_csv('./simulation_results/2024-02-08_02-55-19.csv'), # 30\n#           ]).assign(model='GConvLSTM').iloc[:,1:].assign(dataset='windmillsmall').to_csv('./GConvLSTM_50_windmillsmall.csv')\n\n\n# pd.concat([pd.read_csv('./simulation_results/2024-02-08_14-55-12.csv'), # 1\n#            pd.read_csv('./simulation_results/2024-02-09_03-05-24.csv'), # 2\n#            pd.read_csv('./simulation_results/2024-02-09_14-21-53.csv'), # 3\n#            pd.read_csv('./simulation_results/2024-02-10_01-25-30.csv'), # 4\n#            pd.read_csv('./simulation_results/2024-02-10_12-24-53.csv'), # 5\n#            pd.read_csv('./simulation_results/2024-02-10_23-38-23.csv'), # 6\n#            pd.read_csv('./simulation_results/2024-02-11_10-34-58.csv'), # 7\n#            pd.read_csv('./simulation_results/2024-02-11_21-30-49.csv'), # 8\n#            pd.read_csv('./simulation_results/2024-02-12_08-15-15.csv'), # 9\n#            pd.read_csv('./simulation_results/2024-02-12_19-05-28.csv'), # 10\n#            pd.read_csv('./simulation_results/2024-02-13_06-14-23.csv'), # 11\n#            pd.read_csv('./simulation_results/2024-02-13_17-17-50.csv'), # 12\n#            pd.read_csv('./simulation_results/2024-02-14_04-08-14.csv'), # 13\n#            pd.read_csv('./simulation_results/2024-02-14_15-21-16.csv'), # 14\n#            pd.read_csv('./simulation_results/2024-02-15_02-13-19.csv'), # 15\n#            pd.read_csv('./simulation_results/2024-02-15_13-23-31.csv'), # 16\n#            pd.read_csv('./simulation_results/2024-02-16_00-12-37.csv'), # 17\n#            pd.read_csv('./simulation_results/2024-02-16_11-11-35.csv'), # 18\n#            pd.read_csv('./simulation_results/2024-02-18_06-06-11.csv'), # 19\n#            pd.read_csv('./simulation_results/2024-02-18_17-02-58.csv'), # 20\n#            pd.read_csv('./simulation_results/2024-02-19_04-03-13.csv'), # 21\n#            pd.read_csv('./simulation_results/2024-02-16_22-05-00.csv'), # 22\n#            pd.read_csv('./simulation_results/2024-02-19_14-47-18.csv'), # 23\n#            pd.read_csv('./simulation_results/2024-02-20_01-34-46.csv'), # 24\n#            pd.read_csv('./simulation_results/2024-02-20_12-36-59.csv'), # 25\n#            pd.read_csv('./simulation_results/2024-02-20_23-53-12.csv'), # 26\n#            pd.read_csv('./simulation_results/2024-02-21_12-26-29.csv'), # 27\n#            pd.read_csv('./simulation_results/2024-02-22_05-21-26.csv'), # 28\n#            pd.read_csv('./simulation_results/2024-02-22_23-13-28.csv'), # 29\n#            pd.read_csv('./simulation_results/2024-02-23_20-56-31.csv'), # 30\n#            ]).assign(model='GConvLSTM').iloc[:,1:].assign(dataset='windmillsmall').to_csv('./GConvLSTM_80_windmillsmall.csv')\n\nGCLSTM\n\n# pd.concat([pd.read_csv('./simulation_results/2024-01-30_19-56-08.csv'), # 1\n#            pd.read_csv('./simulation_results/2024-01-31_06-41-15.csv'), # 2\n#            pd.read_csv('./simulation_results/2024-01-31_17-33-23.csv'), # 3\n#            pd.read_csv('./simulation_results/2024-02-01_15-53-59.csv'), # 4\n#            pd.read_csv('./simulation_results/2024-02-02_02-39-21.csv'), # 5\n#            pd.read_csv('./simulation_results/2024-02-02_13-50-54.csv'), # 6\n#            pd.read_csv('./simulation_results/2024-02-03_01-08-09.csv'), # 7\n#            pd.read_csv('./simulation_results/2024-02-03_11-55-58.csv'), # 8\n#            pd.read_csv('./simulation_results/2024-02-03_22-53-58.csv'), # 9\n#            pd.read_csv('./simulation_results/2024-02-04_09-45-33.csv'), # 10\n#            pd.read_csv('./simulation_results/2024-02-05_07-29-54.csv'), # 11\n#            pd.read_csv('./simulation_results/2024-02-05_18-27-43.csv'), # 12\n#            pd.read_csv('./simulation_results/2024-02-06_05-10-19.csv'), # 13\n#            pd.read_csv('./simulation_results/2024-02-06_16-09-48.csv'), # 14\n#            pd.read_csv('./simulation_results/2024-02-07_01-44-15.csv'), # 15\n#            pd.read_csv('./simulation_results/2024-02-07_10-31-09.csv'), # 16\n#            pd.read_csv('./simulation_results/2024-02-07_19-20-00.csv'), # 17\n#            pd.read_csv('./simulation_results/2024-02-08_04-03-03.csv'), # 18\n#            pd.read_csv('./simulation_results/2024-02-08_13-44-17.csv'), # 19\n#            pd.read_csv('./simulation_results/2024-02-08_23-34-57.csv'), # 20\n#            pd.read_csv('./simulation_results/2024-02-09_08-13-53.csv'), # 21\n#            pd.read_csv('./simulation_results/2024-02-09_16-50-46.csv'), # 22\n#            pd.read_csv('./simulation_results/2024-02-10_01-33-38.csv'), # 23\n#            pd.read_csv('./simulation_results/2024-02-10_10-15-07.csv'), # 24\n#            pd.read_csv('./simulation_results/2024-02-10_18-56-22.csv'), # 25\n#            pd.read_csv('./simulation_results/2024-02-11_03-47-47.csv'), # 26\n#            pd.read_csv('./simulation_results/2024-02-11_12-34-32.csv'), # 27\n#            pd.read_csv('./simulation_results/2024-02-11_21-29-30.csv'), # 28\n#            pd.read_csv('./simulation_results/2024-02-13_01-34-32.csv'), # 29\n#            pd.read_csv('./simulation_results/2024-02-13_10-20-47.csv'), # 30\n#           ]).assign(model='GCLSTM').iloc[:,1:].assign(dataset='windmillsmall').to_csv('./GCLSTM_50_windmillsmall.csv')\n\n\n# pd.concat([pd.read_csv('./simulation_results/2024-02-12_06-02-29.csv'), # 1\n#            pd.read_csv('./simulation_results/2024-02-12_14-49-15.csv'), # 2\n#            pd.read_csv('./simulation_results/2024-02-13_19-01-09.csv'), # 3\n#            pd.read_csv('./simulation_results/2024-02-14_03-53-08.csv'), # 4\n#            pd.read_csv('./simulation_results/2024-02-14_12-33-03.csv'), # 5\n#            pd.read_csv('./simulation_results/2024-02-14_21-12-00.csv'), # 6\n#            pd.read_csv('./simulation_results/2024-02-15_05-57-01.csv'), # 7\n#            pd.read_csv('./simulation_results/2024-02-15_14-47-07.csv'), # 8\n#            pd.read_csv('./simulation_results/2024-02-15_23-34-42.csv'), # 9\n#            pd.read_csv('./simulation_results/2024-02-16_08-17-16.csv'), # 10\n#            pd.read_csv('./simulation_results/2024-02-16_17-04-04.csv'), # 11\n#            pd.read_csv('./simulation_results/2024-02-17_02-00-38.csv'), # 12\n#            pd.read_csv('./simulation_results/2024-02-18_04-02-09.csv'), # 13\n#            pd.read_csv('./simulation_results/2024-02-18_12-26-08.csv'), # 14\n#            pd.read_csv('./simulation_results/2024-02-18_20-43-29.csv'), # 15\n#            pd.read_csv('./simulation_results/2024-02-19_05-12-45.csv'), # 16\n#            pd.read_csv('./simulation_results/2024-02-19_13-37-38.csv'), # 17\n#            pd.read_csv('./simulation_results/2024-02-19_21-54-58.csv'), # 18\n#            pd.read_csv('./simulation_results/2024-02-20_06-41-33.csv'), # 19\n#            pd.read_csv('./simulation_results/2024-02-20_15-33-13.csv'), # 20\n#            pd.read_csv('./simulation_results/2024-02-21_00-19-54.csv'), # 21\n#            pd.read_csv('./simulation_results/2024-02-21_09-27-21.csv'), # 22\n#            pd.read_csv('./simulation_results/2024-02-21_21-58-24.csv'), # 23\n#            pd.read_csv('./simulation_results/2024-02-22_08-49-41.csv'), # 24\n#            pd.read_csv('./simulation_results/2024-02-22_22-10-54.csv'), # 25\n#            pd.read_csv('./simulation_results/2024-02-23_12-32-05.csv'), # 26\n#            pd.read_csv('./simulation_results/2024-02-24_03-59-48.csv'), # 27\n#            pd.read_csv('./simulation_results/2024-02-24_22-40-38.csv'), # 28\n#            pd.read_csv('./simulation_results/2024-02-25_17-56-58.csv'), # 29\n#            pd.read_csv('./simulation_results/2024-02-26_12-54-23.csv'), # 30\n#           ]).assign(model='GCLSTM').iloc[:,1:].assign(dataset='windmillsmall').to_csv('./GCLSTM_80_windmillsmall.csv')\n\nDyGrEncoder\n\n# pd.concat([pd.read_csv('./simulation_results/2024-01-24_23-09-51.csv'), # 1\n#            pd.read_csv('./simulation_results/2024-01-25_07-49-42.csv'), # 2\n#            pd.read_csv('./simulation_results/2024-01-26_01-37-53.csv'), # 3\n#            pd.read_csv('./simulation_results/2024-01-26_12-20-37.csv'), # 4\n#            pd.read_csv('./simulation_results/2024-01-26_20-56-38.csv'), # 5\n#            pd.read_csv('./simulation_results/2024-01-27_05-43-47.csv'), # 6\n#            pd.read_csv('./simulation_results/2024-01-27_16-32-59.csv'), # 7\n#            pd.read_csv('./simulation_results/2024-01-28_03-29-56.csv'), # 8\n#            pd.read_csv('./simulation_results/2024-01-28_13-38-03.csv'), # 9\n#            pd.read_csv('./simulation_results/2024-01-28_22-37-03.csv'), # 10\n#            pd.read_csv('./simulation_results/2024-01-29_08-53-56.csv'), # 11\n#            pd.read_csv('./simulation_results/2024-01-29_19-20-44.csv'), # 12\n#            pd.read_csv('./simulation_results/2024-01-30_05-35-24.csv'), # 13\n#            pd.read_csv('./simulation_results/2024-01-30_16-11-37.csv'), # 14\n#            pd.read_csv('./simulation_results/2024-01-31_03-32-21.csv'), # 15\n#            pd.read_csv('./simulation_results/2024-01-31_15-01-32.csv'), # 16\n#            pd.read_csv('./simulation_results/2024-02-01_02-38-14.csv'), # 17\n#            pd.read_csv('./simulation_results/2024-02-01_14-16-41.csv'), # 18\n#            pd.read_csv('./simulation_results/2024-02-02_01-53-34.csv'), # 19\n#            pd.read_csv('./simulation_results/2024-02-02_13-11-36.csv'), # 20\n#            pd.read_csv('./simulation_results/2024-02-03_00-45-57.csv'), # 21\n#            pd.read_csv('./simulation_results/2024-02-03_12-03-51.csv'), # 22\n#            pd.read_csv('./simulation_results/2024-02-03_23-21-15.csv'), # 23\n#            pd.read_csv('./simulation_results/2024-02-04_10-38-15.csv'), # 24\n#            pd.read_csv('./simulation_results/2024-02-04_22-27-16.csv'), # 25\n#            pd.read_csv('./simulation_results/2024-02-05_09-51-32.csv'), # 26\n#            pd.read_csv('./simulation_results/2024-02-05_21-09-34.csv'), # 27\n#            pd.read_csv('./simulation_results/2024-02-06_08-30-16.csv'), # 28\n#            pd.read_csv('./simulation_results/2024-02-06_19-58-25.csv'), # 29\n#            pd.read_csv('./simulation_results/2024-02-08_19-29-39.csv'), # 30\n#           ]).assign(model='DyGrEncoder').iloc[:,1:].assign(dataset='windmillsmall').to_csv('./DyGrEncoder_50_windmillsmall.csv')\n\nLRGCN\n\n# pd.concat([pd.read_csv('./simulation_results/2024-01-25_00-26-15.csv'), # 1\n#            pd.read_csv('./simulation_results/2024-01-25_10-04-35.csv'), # 2\n#            pd.read_csv('./simulation_results/2024-01-25_19-42-50.csv'), # 3\n#            pd.read_csv('./simulation_results/2024-01-26_06-17-49.csv'), # 4\n#            pd.read_csv('./simulation_results/2024-01-26_16-19-15.csv'), # 5\n#            pd.read_csv('./simulation_results/2024-01-27_01-56-42.csv'), # 6\n#            pd.read_csv('./simulation_results/2024-01-27_12-31-42.csv'), # 7\n#            pd.read_csv('./simulation_results/2024-01-27_23-41-30.csv'), # 8\n#            pd.read_csv('./simulation_results/2024-01-28_10-50-18.csv'), # 9\n#            pd.read_csv('./simulation_results/2024-01-28_20-31-17.csv'), # 10\n#            pd.read_csv('./simulation_results/2024-01-29_07-11-35.csv'), # 11\n#            pd.read_csv('./simulation_results/2024-01-29_18-05-25.csv'), # 12\n#            pd.read_csv('./simulation_results/2024-01-30_04-49-08.csv'), # 13\n#            pd.read_csv('./simulation_results/2024-01-30_20-31-22.csv'), # 14\n#            pd.read_csv('./simulation_results/2024-01-31_08-00-27.csv'), # 15\n#            pd.read_csv('./simulation_results/2024-01-31_19-44-35.csv'), # 16\n#            pd.read_csv('./simulation_results/2024-02-01_07-39-17.csv'), # 17\n#            pd.read_csv('./simulation_results/2024-02-01_19-37-34.csv'), # 18\n#            pd.read_csv('./simulation_results/2024-02-02_06-52-35.csv'), # 19\n#            pd.read_csv('./simulation_results/2024-02-02_18-45-51.csv'), # 20\n#            pd.read_csv('./simulation_results/2024-02-03_06-20-39.csv'), # 21\n#            pd.read_csv('./simulation_results/2024-02-03_17-55-52.csv'), # 22\n#            pd.read_csv('./simulation_results/2024-02-04_05-24-54.csv'), # 23\n#            pd.read_csv('./simulation_results/2024-02-04_16-55-08.csv'), # 24\n#            pd.read_csv('./simulation_results/2024-02-05_04-24-45.csv'), # 25\n#            pd.read_csv('./simulation_results/2024-02-05_16-07-40.csv'), # 26\n#            pd.read_csv('./simulation_results/2024-02-06_03-30-29.csv'), # 27\n#            pd.read_csv('./simulation_results/2024-02-06_15-15-44.csv'), # 28\n#            pd.read_csv('./simulation_results/2024-02-07_01-39-04.csv'), # 29\n#            pd.read_csv('./simulation_results/2024-02-08_19-19-05.csv'), # 30\n#           ]).assign(model='LRGCN').iloc[:,1:].assign(dataset='windmillsmall').to_csv('./LRGCN_50_windmillsmall.csv')\n\n\n# pd.concat([pd.read_csv('./simulation_results/2024-02-07_11-03-03.csv'), # 1\n#            pd.read_csv('./simulation_results/2024-02-07_20-27-31.csv'), # 2\n#            pd.read_csv('./simulation_results/2024-02-08_05-56-58.csv'), # 3\n#            pd.read_csv('./simulation_results/2024-02-09_04-47-47.csv'), # 4\n#            pd.read_csv('./simulation_results/2024-02-09_14-20-24.csv'), # 5\n#            pd.read_csv('./simulation_results/2024-02-09_23-33-55.csv'), # 6\n#            pd.read_csv('./simulation_results/2024-02-10_08-58-42.csv'), # 7\n#            pd.read_csv('./simulation_results/2024-02-10_18-19-14.csv'), # 8\n#            pd.read_csv('./simulation_results/2024-02-11_03-42-11.csv'), # 9\n#            pd.read_csv('./simulation_results/2024-02-11_13-04-46.csv'), # 10\n#            pd.read_csv('./simulation_results/2024-02-11_22-29-17.csv'), # 11\n#            pd.read_csv('./simulation_results/2024-02-12_07-55-55.csv'), # 12\n#            pd.read_csv('./simulation_results/2024-02-12_17-16-34.csv'), # 13\n#            pd.read_csv('./simulation_results/2024-02-13_02-34-46.csv'), # 14\n#            pd.read_csv('./simulation_results/2024-02-13_11-56-39.csv'), # 15\n#            pd.read_csv('./simulation_results/2024-02-13_21-22-01.csv'), # 16\n#            pd.read_csv('./simulation_results/2024-02-14_06-39-55.csv'), # 17\n#            pd.read_csv('./simulation_results/2024-02-14_16-06-08.csv'), # 18\n#            pd.read_csv('./simulation_results/2024-02-15_01-27-59.csv'), # 19\n#            pd.read_csv('./simulation_results/2024-02-15_10-50-32.csv'), # 20\n#            pd.read_csv('./simulation_results/2024-02-15_20-12-40.csv'), # 21\n#            pd.read_csv('./simulation_results/2024-02-16_05-28-41.csv'), # 22\n#            pd.read_csv('./simulation_results/2024-02-16_14-51-25.csv'), # 23\n#            pd.read_csv('./simulation_results/2024-02-17_00-18-20.csv'), # 24\n#            pd.read_csv('./simulation_results/2024-02-18_05-02-37.csv'), # 25\n#            pd.read_csv('./simulation_results/2024-02-18_14-41-32.csv'), # 26\n#            pd.read_csv('./simulation_results/2024-02-19_00-17-37.csv'), # 27\n#            pd.read_csv('./simulation_results/2024-02-19_09-58-13.csv'), # 28\n#            pd.read_csv('./simulation_results/2024-02-19_19-41-43.csv'), # 29\n#            pd.read_csv('./simulation_results/2024-02-21_21-50-08.csv'), # 30\n#           ]).assign(model='LRGCN').iloc[:,1:].assign(dataset='windmillsmall').to_csv('./LRGCN_80_windmillsmall.csv')\n\nEvolveGCNH 50%\n\n# pd.concat([pd.read_csv('./simulation_results/EvolveGCNH50_186.csv').iloc[:,1:], # 186 7번\n#            pd.read_csv('./simulation_results/2024-02-20_04-53-21.csv'), # 8\n#            pd.read_csv('./simulation_results/2024-02-20_13-32-30.csv'), # 9\n#            pd.read_csv('./simulation_results/2024-02-20_21-59-56.csv'), # 10\n#            pd.read_csv('./simulation_results/2024-02-21_06-26-32.csv'), # 11\n#            pd.read_csv('./simulation_results/2024-02-21_18-00-55.csv'), # 12\n#            pd.read_csv('./simulation_results/2024-02-22_05-09-44.csv'), # 13\n#            pd.read_csv('./simulation_results/2024-02-22_16-52-55.csv'), # 14\n#            pd.read_csv('./simulation_results/2024-02-23_00-02-38.csv'), # 15\n#            pd.read_csv('./simulation_results/2024-02-23_07-04-20.csv'), # 16\n#            pd.read_csv('./simulation_results/2024-02-23_14-51-19.csv'), # 17\n#            pd.read_csv('./simulation_results/2024-02-23_21-29-45.csv'), # 18\n#            pd.read_csv('./simulation_results/2024-02-24_06-36-54.csv'), # 19\n#            pd.read_csv('./simulation_results/2024-02-24_14-11-22.csv'), # 20\n#            pd.read_csv('./simulation_results/2024-02-24_15-29-41.csv'), # 21\n#            pd.read_csv('./simulation_results/2024-02-25_01-43-07.csv'), # 22\n#            pd.read_csv('./simulation_results/2024-02-25_09-19-21.csv'), # 23\n#            pd.read_csv('./simulation_results/2024-02-25_10-46-27.csv'), # 24\n#            pd.read_csv('./simulation_results/2024-02-25_21-24-31.csv'), # 25\n#            pd.read_csv('./simulation_results/2024-02-26_04-22-25.csv'), # 26\n#            pd.read_csv('./simulation_results/2024-02-26_05-55-13.csv'), # 27\n#            pd.read_csv('./simulation_results/2024-02-26_16-10-19.csv'), # 28\n#            pd.read_csv('./simulation_results/2024-02-26_20-59-29.csv'), # 29\n#            pd.read_csv('./simulation_results/2024-02-27_07-09-24.csv'), # 30\n#           ]).assign(model='EvolveGCNH').iloc[:,1:].assign(dataset='windmillsmall').to_csv('./EvolveGCNH_50_windmillsmall.csv')\n\n0.8\n\n# pd.concat([pd.read_csv('./simulation_results/2024-02-21_20-00-40.csv'), # 1\n#            pd.read_csv('./simulation_results/2024-02-22_06-49-22.csv'), # 2\n#            pd.read_csv('./simulation_results/2024-02-22_19-10-32.csv'), # 3\n#            pd.read_csv('./simulation_results/2024-02-23_09-22-34.csv'), # 4\n#            pd.read_csv('./simulation_results/2024-02-23_23-22-41.csv'), # 5\n#            pd.read_csv('./simulation_results/2024-02-24_16-25-19.csv'), # 6\n#            pd.read_csv('./simulation_results/2024-02-25_12-06-11.csv'), # 7\n#            pd.read_csv('./simulation_results/2024-02-26_07-30-00.csv'), # 8\n#            pd.read_csv('./simulation_results/2024-02-26_23-08-42.csv'), # 9\n#            pd.read_csv('./simulation_results/2024-02-29_07-31-17.csv'), # 10\n#            pd.read_csv('./simulation_results/2024-02-29_07-32-46.csv'), # 11\n#            pd.read_csv('./simulation_results/2024-02-29_07-45-00.csv'), # 12\n#            pd.read_csv('./simulation_results/2024-02-29_21-34-56.csv'), # 13\n#            pd.read_csv('./simulation_results/2024-02-29_21-38-54.csv'), # 14\n#            pd.read_csv('./simulation_results/2024-02-29_21-50-11.csv'), # 15\n#            pd.read_csv('./simulation_results/2024-03-01_11-36-34.csv'), # 16\n#            pd.read_csv('./simulation_results/2024-03-01_11-52-14.csv'), # 17\n#            pd.read_csv('./simulation_results/2024-03-01_12-21-29.csv'), # 18\n#            pd.read_csv('./simulation_results/2024-03-02_01-21-50.csv'), # 19\n#            pd.read_csv('./simulation_results/2024-03-02_01-52-37.csv'), # 20\n#            pd.read_csv('./simulation_results/2024-03-02_02-21-24.csv'), # 21\n#            pd.read_csv('./simulation_results/2024-03-02_15-11-17.csv'), # 22\n#            pd.read_csv('./simulation_results/2024-03-02_16-14-14.csv'), # 23\n#            pd.read_csv('./simulation_results/2024-03-02_16-30-17.csv'), # 24\n#            pd.read_csv('./simulation_results/2024-03-03_05-12-00.csv'), # 25\n#            pd.read_csv('./simulation_results/2024-03-03_06-11-43.csv'), # 26\n#            pd.read_csv('./simulation_results/2024-03-03_07-01-03.csv'), # 27\n#            pd.read_csv('./simulation_results/2024-03-03_19-12-43.csv'), # 28\n#            pd.read_csv('./simulation_results/2024-03-03_21-08-43.csv'), # 29\n#            pd.read_csv('./simulation_results/2024-03-04_09-44-53.csv'), # 30\n#           ]).assign(model='EvolveGCNH').iloc[:,1:].assign(dataset='windmillsmall').to_csv('./EvolveGCNH_8_windmillsmall.csv')\n\n\nmonte(linear)\nblock\n\n# pd.concat([pd.read_csv('./simulation_results/2024-01-30_06-39-52.csv').assign(model='LRGCN'),\n#            pd.read_csv('./simulation_results/monte_random.csv'),\n#            pd.read_csv('./simulation_results/2024-01-26_05-21-17.csv').assign(model='GComvGRU'), # epoch 15(1)\n#            pd.read_csv('./simulation_results/2024-01-26_09-48-48.csv').assign(model='GComvGRU'), # epoch 15(2)\n#            pd.read_csv('./simulation_results/2024-01-27_17-54-13.csv').assign(model='GConvLSTM'), # epoch 30\n#            pd.read_csv('./simulation_results/2024-01-27_23-03-18.csv').assign(model='GCLSTM'), # epoch 30\n#            pd.read_csv('./simulation_results/2024-01-28_03-57-09.csv').assign(model='DyGrEncoder'), # epoch 30\n#            pd.read_csv('./simulation_results/2024-01-28_10-28-48.csv').assign(model='DCRNN'), # epoch 30\n#            pd.read_csv('./simulation_results/2024-01-29_00-25-33.csv').assign(model='LRGCN'), # epoch 30\n#            pd.read_csv('./simulation_results/2024-01-29_03-54-40.csv').assign(model='EvolveGCNH'), # epoch 30\n#            pd.read_csv('./simulation_results/2024-01-29_06-34-15.csv').assign(model='EvolveGCNO'), # epoch 30\n#            pd.read_csv('./simulation_results/2024-01-29_12-04-23.csv').assign(model='TGCN'), # epoch 30\n#           ]).iloc[:,1:].to_csv('./monte_linear.csv',index=False)\n\n\npedalme\n\n# pd.concat([pd.read_csv('./simulation_results/2024-02-01_12-21-45.csv').assign(model='GComvGRU'),\n#            pd.read_csv('./simulation_results/2024-02-01_13-06-50.csv').assign(model='DyGrEncoder'),\n#            pd.read_csv('./simulation_results/2024-02-01_13-25-34.csv').assign(model='EvolveGCNH'),\n#            pd.read_csv('./simulation_results/2024-02-01_13-38-43.csv').assign(model='EvolveGCNO'),\n#            pd.read_csv('./simulation_results/2024-02-01_14-02-20.csv').assign(model='TGCN'),\n#            pd.read_csv('./simulation_results/2024-02-01_14-50-24.csv').assign(model='GConvLSTM'),\n#            pd.read_csv('./simulation_results/2024-02-01_15-12-44.csv').assign(model='GCLSTM'),\n#            pd.read_csv('./simulation_results/2024-02-01_16-07-18.csv').assign(model='LRGCN'),\n#            pd.read_csv('./simulation_results/2024-02-01_16-27-28.csv').assign(model='DCRNN'),\n#            pd.read_csv('./simulation_results/2024-02-01_16-37-48.csv').assign(model='GComvGRU'),\n#            pd.read_csv('./simulation_results/2024-02-01_16-48-51.csv').assign(model='DyGrEncoder'),\n#            pd.read_csv('./simulation_results/2024-02-01_16-53-04.csv').assign(model='EvolveGCNH'),\n#            pd.read_csv('./simulation_results/2024-02-01_16-56-04.csv').assign(model='EvolveGCNO'),\n#            pd.read_csv('./simulation_results/2024-02-01_17-01-04.csv').assign(model='TGCN'),\n#            pd.read_csv('./simulation_results/2024-02-01_17-11-54.csv').assign(model='GConvLSTM'),\n#            pd.read_csv('./simulation_results/2024-02-01_17-17-03.csv').assign(model='GCLSTM'),\n#            pd.read_csv('./simulation_results/2024-02-01_17-23-48.csv').assign(model='LRGCN'),\n#            pd.read_csv('./simulation_results/2024-02-01_17-27-22.csv').assign(model='DCRNN'),\n#           ]).iloc[:,1:].to_csv('./pedalme_linear.csv',index=False)"
  },
  {
    "objectID": "posts/2_research/2023-06-13-DCRNN_Simulation_boxplot_reshape.html",
    "href": "posts/2_research/2023-06-13-DCRNN_Simulation_boxplot_reshape.html",
    "title": "DCRNN_Simulation_reshape",
    "section": "",
    "text": "Simulation Study"
  },
  {
    "objectID": "posts/2_research/2023-06-13-DCRNN_Simulation_boxplot_reshape.html#baseline",
    "href": "posts/2_research/2023-06-13-DCRNN_Simulation_boxplot_reshape.html#baseline",
    "title": "DCRNN_Simulation_reshape",
    "section": "Baseline",
    "text": "Baseline\n\ndata.query(\"method!='GNAR' and mrate==0\").plot.box(backend='plotly',x='mrate',color='method',y='mse',facet_col='lags',facet_row='nof_filters',height=600)"
  },
  {
    "objectID": "posts/2_research/2023-06-13-DCRNN_Simulation_boxplot_reshape.html#random",
    "href": "posts/2_research/2023-06-13-DCRNN_Simulation_boxplot_reshape.html#random",
    "title": "DCRNN_Simulation_reshape",
    "section": "Random",
    "text": "Random\n\ndata.query(\"method!='GNAR' and mtype =='rand'\").plot.box(backend='plotly',x='mrate',color='method',y='mse',facet_col='inter_method',facet_row='nof_filters',height=600)"
  },
  {
    "objectID": "posts/2_research/2023-06-13-DCRNN_Simulation_boxplot_reshape.html#block",
    "href": "posts/2_research/2023-06-13-DCRNN_Simulation_boxplot_reshape.html#block",
    "title": "DCRNN_Simulation_reshape",
    "section": "Block",
    "text": "Block\n\ndata.query(\"method!='GNAR' and mtype =='block' \").plot.box(backend='plotly',x='mrate',color='method',y='mse',facet_col='inter_method',facet_row='nof_filters',height=600)"
  },
  {
    "objectID": "posts/2_research/2023-06-13-DCRNN_Simulation_boxplot_reshape.html#baseline-1",
    "href": "posts/2_research/2023-06-13-DCRNN_Simulation_boxplot_reshape.html#baseline-1",
    "title": "DCRNN_Simulation_reshape",
    "section": "Baseline",
    "text": "Baseline\n\ndata.query(\"method!='GNAR' and mrate ==0 \").plot.box(backend='plotly',x='mrate',color='method',y='mse',facet_col='nof_filters',height=600)"
  },
  {
    "objectID": "posts/2_research/2023-06-13-DCRNN_Simulation_boxplot_reshape.html#random-1",
    "href": "posts/2_research/2023-06-13-DCRNN_Simulation_boxplot_reshape.html#random-1",
    "title": "DCRNN_Simulation_reshape",
    "section": "Random",
    "text": "Random\n\ndata.query(\"method!='GNAR' and mtype =='rand'\").plot.box(backend='plotly',x='mrate',color='method',y='mse',facet_col='nof_filters',facet_row='inter_method',height=800)"
  },
  {
    "objectID": "posts/2_research/2023-06-13-DCRNN_Simulation_boxplot_reshape.html#block-1",
    "href": "posts/2_research/2023-06-13-DCRNN_Simulation_boxplot_reshape.html#block-1",
    "title": "DCRNN_Simulation_reshape",
    "section": "Block",
    "text": "Block\n\ndata.query(\"method!='GNAR' and mtype =='block'\").plot.box(backend='plotly',x='mrate',color='method',y='mse',facet_col='inter_method',facet_row='lags',height=800)"
  },
  {
    "objectID": "posts/2_research/2023-06-13-DCRNN_Simulation_boxplot_reshape.html#baseline-2",
    "href": "posts/2_research/2023-06-13-DCRNN_Simulation_boxplot_reshape.html#baseline-2",
    "title": "DCRNN_Simulation_reshape",
    "section": "Baseline",
    "text": "Baseline\n\ndata.query(\"mrate ==0 and lags!=2\").plot.box(backend='plotly',x='epoch',color='method',y='mse',facet_col='nof_filters',facet_row='lags',height=400)"
  },
  {
    "objectID": "posts/2_research/2023-06-13-DCRNN_Simulation_boxplot_reshape.html#random-2",
    "href": "posts/2_research/2023-06-13-DCRNN_Simulation_boxplot_reshape.html#random-2",
    "title": "DCRNN_Simulation_reshape",
    "section": "random",
    "text": "random\n\ndata.query(\"method!='GNAR' and mtype =='rand' and lags!=2\").plot.box(backend='plotly',x='mrate',color='method',y='mse',facet_col='nof_filters',facet_row='inter_method',height=800)"
  },
  {
    "objectID": "posts/2_research/2023-06-13-DCRNN_Simulation_boxplot_reshape.html#block-2",
    "href": "posts/2_research/2023-06-13-DCRNN_Simulation_boxplot_reshape.html#block-2",
    "title": "DCRNN_Simulation_reshape",
    "section": "block",
    "text": "block\n\ndata.query(\"method!='GNAR' and mtype =='block' and lags!=2 \").plot.box(backend='plotly',x='mrate',color='method',y='mse',facet_col='nof_filters',facet_row='inter_method',height=800)"
  },
  {
    "objectID": "posts/2_research/2023-06-13-DCRNN_Simulation_boxplot_reshape.html#weight-matrix-time-node-고려한-결과",
    "href": "posts/2_research/2023-06-13-DCRNN_Simulation_boxplot_reshape.html#weight-matrix-time-node-고려한-결과",
    "title": "DCRNN_Simulation_reshape",
    "section": "weight matrix time, node 고려한 결과",
    "text": "weight matrix time, node 고려한 결과\n\ndf1 = pd.read_csv('./simulation_results/2023-06-16_21-09-11.csv')\ndf2 = pd.read_csv('./simulation_results/2023-06-16_21-30-17.csv')\n\n\ndata2 = pd.concat([df1,df2],axis=0)\n\n\ndata2.to_csv('./simulation_results/Real_simulation_reshape/DCRNN_pedalme_Simulation_itstgcnsnd.csv',index=False)\n\n\ndata2 = pd.read_csv('./simulation_results/Real_simulation_reshape/DCRNN_pedalme_Simulation_itstgcnsnd.csv')\n\n\ndata2.query(\"mtype=='rand'\").plot.box(backend='plotly',x='mrate',color='method',y='mse',facet_col='inter_method',facet_row='nof_filters',height=1000)\n\n                                                \n\n\n\ndata2.query(\"mtype=='block'\").plot.box(backend='plotly',x='mrate',color='method',y='mse',facet_col='inter_method',facet_row='nof_filters',height=1000)"
  },
  {
    "objectID": "posts/2_research/2023-06-13-DCRNN_Simulation_boxplot_reshape.html#baseline-3",
    "href": "posts/2_research/2023-06-13-DCRNN_Simulation_boxplot_reshape.html#baseline-3",
    "title": "DCRNN_Simulation_reshape",
    "section": "Baseline",
    "text": "Baseline\n\ndata.query(\"mrate==0 and method!='GNAR'\").plot.box(backend='plotly',x='mrate',color='method',y='mse',height=600)"
  },
  {
    "objectID": "posts/2_research/2023-06-13-DCRNN_Simulation_boxplot_reshape.html#random-3",
    "href": "posts/2_research/2023-06-13-DCRNN_Simulation_boxplot_reshape.html#random-3",
    "title": "DCRNN_Simulation_reshape",
    "section": "random",
    "text": "random\n\ndata.query(\"mtype=='rand' and mrate !=0 and method!='GNAR'\").plot.box(backend='plotly',x='mrate',color='method',y='mse',facet_col='lags',facet_row='nof_filters',height=800)"
  },
  {
    "objectID": "posts/2_research/2023-06-13-DCRNN_Simulation_boxplot_reshape.html#block-3",
    "href": "posts/2_research/2023-06-13-DCRNN_Simulation_boxplot_reshape.html#block-3",
    "title": "DCRNN_Simulation_reshape",
    "section": "block",
    "text": "block\n\ndata.query(\"mtype=='block' and method!='GNAR'\").plot.box(backend='plotly',x='mrate',color='method',y='mse',facet_col='lags',facet_row='nof_filters',height=1200)"
  },
  {
    "objectID": "posts/2_research/2023-06-13-DCRNN_Simulation_boxplot_reshape.html#missing-values-on-the-same-nodes",
    "href": "posts/2_research/2023-06-13-DCRNN_Simulation_boxplot_reshape.html#missing-values-on-the-same-nodes",
    "title": "DCRNN_Simulation_reshape",
    "section": "missing values on the same nodes",
    "text": "missing values on the same nodes\n\ndf1 = pd.read_csv('./simulation_results/2023-06-19_15-43-50.csv') # STGCN IT-STGCN block\ndf2 = pd.read_csv('./simulation_results/2023-06-19_18-35-04.csv') # STGCN IT-STGCN\ndf3 = pd.read_csv('./simulation_results/2023-06-19_22-00-09.csv') \n\n\ndata = pd.concat([df1,df2,df3],axis=0)\n\n\ndata.to_csv('./simulation_results/Real_simulation_reshape/DCRNN_wikimath_GSO_st.csv',index=False)\n\n\ndata = pd.read_csv('./simulation_results/Real_simulation_reshape/DCRNN_wikimath_GSO_st.csv')\n\n\ndata.query(\"method!='GNAR' and mtype =='block' \").plot.box(backend='plotly',x='mrate',color='method',y='mse',facet_col='inter_method',facet_row='lags',height=800)"
  },
  {
    "objectID": "posts/2_research/2023-06-13-DCRNN_Simulation_boxplot_reshape.html#baseline-4",
    "href": "posts/2_research/2023-06-13-DCRNN_Simulation_boxplot_reshape.html#baseline-4",
    "title": "DCRNN_Simulation_reshape",
    "section": "Baseline",
    "text": "Baseline\n\ndata.query(\"method!='GNAR' and mrate ==0 \").plot.box(backend='plotly',x='mrate',color='method',y='mse',facet_col='lags',height=800)"
  },
  {
    "objectID": "posts/2_research/2023-06-13-DCRNN_Simulation_boxplot_reshape.html#random-4",
    "href": "posts/2_research/2023-06-13-DCRNN_Simulation_boxplot_reshape.html#random-4",
    "title": "DCRNN_Simulation_reshape",
    "section": "random",
    "text": "random\n\ndata.query(\"method!='GNAR' and mtype =='rand' \").plot.box(backend='plotly',x='mrate',color='method',y='mse',facet_col='lags',facet_row='inter_method',height=800)"
  },
  {
    "objectID": "posts/2_research/2023-06-13-DCRNN_Simulation_boxplot_reshape.html#block-4",
    "href": "posts/2_research/2023-06-13-DCRNN_Simulation_boxplot_reshape.html#block-4",
    "title": "DCRNN_Simulation_reshape",
    "section": "block",
    "text": "block\n\ndata.query(\"method!='GNAR' and mtype =='block' \").plot.box(backend='plotly',x='mrate',color='method',y='mse',facet_col='lags',facet_row='inter_method',height=800)"
  },
  {
    "objectID": "posts/2_research/2023-06-13-DCRNN_Simulation_boxplot_reshape.html#baseline-5",
    "href": "posts/2_research/2023-06-13-DCRNN_Simulation_boxplot_reshape.html#baseline-5",
    "title": "DCRNN_Simulation_reshape",
    "section": "Baseline",
    "text": "Baseline\n\ndata.query(\"mrate==0 and method!='GNAR'\").plot.box(backend='plotly',x='mrate',color='method',y='mse',facet_col='lags',facet_row='nof_filters',height=600)"
  },
  {
    "objectID": "posts/2_research/2023-06-13-DCRNN_Simulation_boxplot_reshape.html#random-5",
    "href": "posts/2_research/2023-06-13-DCRNN_Simulation_boxplot_reshape.html#random-5",
    "title": "DCRNN_Simulation_reshape",
    "section": "random",
    "text": "random\n\ndata.query(\"mtype=='rand' and mrate !=0 and method!='GNAR'\").plot.box(backend='plotly',x='mrate',color='method',y='mse',facet_col='lags',facet_row='inter_method',height=800)"
  },
  {
    "objectID": "posts/2_research/2023-06-13-DCRNN_Simulation_boxplot_reshape.html#block-5",
    "href": "posts/2_research/2023-06-13-DCRNN_Simulation_boxplot_reshape.html#block-5",
    "title": "DCRNN_Simulation_reshape",
    "section": "block",
    "text": "block\n\ndata.query(\"mtype=='block' and method!='GNAR'\").plot.box(backend='plotly',x='mrate',color='method',y='mse',facet_col='lags',facet_row='nof_filters',height=1200)"
  },
  {
    "objectID": "posts/2_research/2023-04-27-simulation_table.html",
    "href": "posts/2_research/2023-04-27-simulation_table.html",
    "title": "Simulation Tables",
    "section": "",
    "text": "Simulation Tables"
  },
  {
    "objectID": "posts/2_research/2023-04-27-simulation_table.html#baseline",
    "href": "posts/2_research/2023-04-27-simulation_table.html#baseline",
    "title": "Simulation Tables",
    "section": "Baseline",
    "text": "Baseline\n\ndata.query(\"dataset=='fivenodes' and mtype!='block'\")['mrate'].unique()\n\narray([0.7 , 0.75, 0.8 , 0.85, 0.  ])\n\n\n\npd.merge(data.query(\"dataset=='fivenodes' and mtype!='rand' and mtype!='block'\").groupby(['nof_filters','method','lags'])['mse'].mean().reset_index(),\n         data.query(\"dataset=='fivenodes' and mtype!='rand' and mtype!='block'\").groupby(['nof_filters','method','lags'])['mse'].std().reset_index(),\n         on=['method','nof_filters','lags']).rename(columns={'mse_x':'mean','mse_y':'std'}).round(3)\n\n\n\n\n\n\n\n\nnof_filters\nmethod\nlags\nmean\nstd\n\n\n\n\n0\n12.0\nIT-STGCN\n2\n1.168\n0.030\n\n\n1\n12.0\nSTGCN\n2\n1.173\n0.036\n\n\n2\n16.0\nIT-STGCN\n2\n1.166\n0.039\n\n\n3\n16.0\nSTGCN\n2\n1.165\n0.040\n\n\n\n\n\n\n\n\npd.merge(data.query(\"dataset=='fivenodes' and mtype!='rand' and mtype!='block'\").groupby(['nof_filters','lags'])['mse'].mean().reset_index(),\n         data.query(\"dataset=='fivenodes' and mtype!='rand' and mtype!='block'\").groupby(['nof_filters','lags'])['mse'].std().reset_index(),\n         on=['nof_filters','lags']).rename(columns={'mse_x':'mean','mse_y':'std'}).round(3)\n\n\n\n\n\n\n\n\nnof_filters\nlags\nmean\nstd\n\n\n\n\n0\n12.0\n2\n1.170\n0.033\n\n\n1\n16.0\n2\n1.165\n0.039\n\n\n\n\n\n\n\n\npd.merge(data_DCRNN_fivenodes.query(\"dataset=='fivenodes' and mtype!='rand' and mtype!='block'\").groupby(['nof_filters','lags'])['mse'].mean().reset_index(),\n         data_DCRNN_fivenodes.query(\"dataset=='fivenodes' and mtype!='rand' and mtype!='block'\").groupby(['nof_filters','lags'])['mse'].std().reset_index(),\n         on=['nof_filters','lags']).rename(columns={'mse_x':'mean','mse_y':'std'}).round(3)\n\n\n\n\n\n\n\n\nnof_filters\nlags\nmean\nstd\n\n\n\n\n0\n16\n2\n1.247\n0.005"
  },
  {
    "objectID": "posts/2_research/2023-04-27-simulation_table.html#random",
    "href": "posts/2_research/2023-04-27-simulation_table.html#random",
    "title": "Simulation Tables",
    "section": "Random",
    "text": "Random\n\ndata.query(\"dataset=='fivenodes' and mtype=='rand'and method=='GNAR'\")['mse'].unique().round(3)\n\narray([1.407])\n\n\n\npd.merge(data.query(\"dataset=='fivenodes' and mtype=='rand'\").groupby(['mrate','nof_filters','method','lags'])['mse'].mean().reset_index(),\n         data.query(\"dataset=='fivenodes' and mtype=='rand'\").groupby(['mrate','nof_filters','method','lags'])['mse'].std().reset_index(),\n         on=['method','nof_filters','mrate','lags']).rename(columns={'mse_x':'mean','mse_y':'std'}).round(3).query(\"nof_filters==12\")\n\n\n\n\n\n\n\n\nmrate\nnof_filters\nmethod\nlags\nmean\nstd\n\n\n\n\n0\n0.70\n12.0\nIT-STGCN\n2\n1.200\n0.070\n\n\n1\n0.70\n12.0\nSTGCN\n2\n1.213\n0.083\n\n\n4\n0.75\n12.0\nIT-STGCN\n2\n1.188\n0.060\n\n\n5\n0.75\n12.0\nSTGCN\n2\n1.239\n0.102\n\n\n8\n0.80\n12.0\nIT-STGCN\n2\n1.221\n0.083\n\n\n9\n0.80\n12.0\nSTGCN\n2\n1.226\n0.105\n\n\n12\n0.85\n12.0\nIT-STGCN\n2\n1.227\n0.085\n\n\n13\n0.85\n12.0\nSTGCN\n2\n1.291\n0.252\n\n\n\n\n\n\n\n\npd.merge(data.query(\"dataset=='fivenodes' and mtype=='rand'\").groupby(['mrate','nof_filters','method','lags'])['mse'].mean().reset_index(),\n         data.query(\"dataset=='fivenodes' and mtype=='rand'\").groupby(['mrate','nof_filters','method','lags'])['mse'].std().reset_index(),\n         on=['method','nof_filters','mrate','lags']).rename(columns={'mse_x':'mean','mse_y':'std'}).round(3).query(\"nof_filters!=12\")\n\n\n\n\n\n\n\n\nmrate\nnof_filters\nmethod\nlags\nmean\nstd\n\n\n\n\n2\n0.70\n16.0\nIT-STGCN\n2\n1.201\n0.068\n\n\n3\n0.70\n16.0\nSTGCN\n2\n1.227\n0.094\n\n\n6\n0.75\n16.0\nIT-STGCN\n2\n1.231\n0.110\n\n\n7\n0.75\n16.0\nSTGCN\n2\n1.201\n0.072\n\n\n10\n0.80\n16.0\nIT-STGCN\n2\n1.232\n0.092\n\n\n11\n0.80\n16.0\nSTGCN\n2\n1.292\n0.148\n\n\n14\n0.85\n16.0\nIT-STGCN\n2\n1.286\n0.297\n\n\n15\n0.85\n16.0\nSTGCN\n2\n1.362\n0.239\n\n\n\n\n\n\n\n\npd.merge(data_DCRNN_fivenodes.query(\"dataset=='fivenodes' and mtype=='rand'\").groupby(['mrate','nof_filters','method','lags'])['mse'].mean().reset_index(),\n         data_DCRNN_fivenodes.query(\"dataset=='fivenodes' and mtype=='rand'\").groupby(['mrate','nof_filters','method','lags'])['mse'].std().reset_index(),\n         on=['method','nof_filters','mrate','lags']).rename(columns={'mse_x':'mean','mse_y':'std'}).round(3).query(\"nof_filters!=12 and mrate!=0.3\")\n\n\n\n\n\n\n\n\nmrate\nnof_filters\nmethod\nlags\nmean\nstd\n\n\n\n\n2\n0.8\n16\nIT-STGCN\n2\n1.478\n1.245\n\n\n3\n0.8\n16\nSTGCN\n2\n1.491\n0.302"
  },
  {
    "objectID": "posts/2_research/2023-04-27-simulation_table.html#block",
    "href": "posts/2_research/2023-04-27-simulation_table.html#block",
    "title": "Simulation Tables",
    "section": "Block",
    "text": "Block\n\ndata.query(\"dataset=='fivenodes' and mtype=='block'and method=='GNAR'\")['mse'].unique().round(3)\n\narray([1.407])\n\n\n\ndata.query(\"dataset=='fivenodes' and mtype=='block'\")\n\n\n\n\n\n\n\n\ndataset\nmethod\nRecurrentGCN\nmrate\nmtype\nlags\nnof_filters\ninter_method\nepoch\nmse\n\n\n\n\n600\nfivenodes\nGNAR\nGConvGRU\n0.125\nblock\n2\nNaN\ncubic\nNaN\n1.406830\n\n\n601\nfivenodes\nGNAR\nGConvGRU\n0.125\nblock\n2\nNaN\nlinear\nNaN\n1.406830\n\n\n602\nfivenodes\nGNAR\nGConvGRU\n0.125\nblock\n2\nNaN\ncubic\nNaN\n1.406830\n\n\n603\nfivenodes\nGNAR\nGConvGRU\n0.125\nblock\n2\nNaN\nlinear\nNaN\n1.406830\n\n\n604\nfivenodes\nGNAR\nGConvGRU\n0.125\nblock\n2\nNaN\ncubic\nNaN\n1.406830\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n967\nfivenodes\nIT-STGCN\nGConvGRU\n0.300\nblock\n2\n16.0\nlinear\n150.0\n1.135442\n\n\n968\nfivenodes\nSTGCN\nGConvGRU\n0.300\nblock\n2\n12.0\nlinear\n150.0\n1.203593\n\n\n969\nfivenodes\nSTGCN\nGConvGRU\n0.300\nblock\n2\n16.0\nlinear\n150.0\n1.220799\n\n\n970\nfivenodes\nIT-STGCN\nGConvGRU\n0.300\nblock\n2\n12.0\nlinear\n150.0\n1.111655\n\n\n971\nfivenodes\nIT-STGCN\nGConvGRU\n0.300\nblock\n2\n16.0\nlinear\n150.0\n1.197438\n\n\n\n\n372 rows × 10 columns\n\n\n\n\npd.merge(data.query(\"dataset=='fivenodes' and mtype=='block'\").groupby(['mrate','nof_filters','method'])['mse'].mean().reset_index(),\n         data.query(\"dataset=='fivenodes' and mtype=='block'\").groupby(['mrate','nof_filters','method'])['mse'].std().reset_index(),\n         on=['method','nof_filters','mrate']).rename(columns={'mse_x':'mean','mse_y':'std'}).round(3)\n\n\n\n\n\n\n\n\nmrate\nnof_filters\nmethod\nmean\nstd\n\n\n\n\n0\n0.125\n12.0\nIT-STGCN\n4.308\n3.333\n\n\n1\n0.125\n12.0\nSTGCN\n6.722\n5.755\n\n\n2\n0.125\n16.0\nIT-STGCN\n4.633\n3.737\n\n\n3\n0.125\n16.0\nSTGCN\n6.858\n5.814\n\n\n4\n0.300\n12.0\nIT-STGCN\n1.178\n0.032\n\n\n5\n0.300\n12.0\nSTGCN\n1.232\n0.040\n\n\n6\n0.300\n16.0\nIT-STGCN\n1.163\n0.050\n\n\n7\n0.300\n16.0\nSTGCN\n1.232\n0.053\n\n\n\n\n\n\n\n\nepoch 별 보기\n\ndf1 = pd.read_csv('./simulation_results/fivenodes/fivenodes_STGCN_ITSTGCN_random_epoch50.csv')\ndf2 = pd.read_csv('./simulation_results/fivenodes/fivenodes_STGCN_ITSTGCN_random_epoch100.csv')\ndf3 = pd.read_csv('./simulation_results/fivenodes/fivenodes_STGCN_ITSTGCN_random_epoch150.csv')\ndf4 = pd.read_csv('./simulation_results/fivenodes/fivenodes_STGCN_ITSTGCN_random_epoch200.csv')\n\n\ndf_gnar = pd.read_csv('./simulation_results/fivenodes/fivenodes_GNAR_random.csv')\n\n\ndata_temp = pd.concat([df1,df2,df3,df4,df_gnar],axis=0)\n\nSTGCN은 nearest에서 mse가 낮았다.\n\ndata_temp.query(\"method=='STGCN' and mtype=='rand' and mrate==0.8 and lags==2 and inter_method=='linear' and nof_filters==4\").\\\ngroupby(['method','epoch','mrate','lags','nof_filters','inter_method'])['mse'].mean().reset_index()['mse'].mean()\n\n1.182556539773941\n\n\n\ndata_temp.query(\"method=='STGCN' and mtype=='rand' and mrate==0.8 and lags==2 and inter_method=='linear' and nof_filters==4\").\\\ngroupby(['method','epoch','mrate','lags','nof_filters','inter_method'])['mse'].mean().reset_index()['mse'].std()\n\n0.012169932740213692\n\n\n\ndata_temp.query(\"method=='IT-STGCN' and mtype=='rand' and mrate==0.8 and lags==2 and inter_method=='linear' and nof_filters==4\").\\\ngroupby(['method','epoch','mrate','lags','nof_filters','inter_method'])['mse'].mean().reset_index()['mse'].mean()\n\n1.1747438261906304\n\n\n\ndata_temp.query(\"method=='IT-STGCN' and mtype=='rand' and mrate==0.8 and lags==2 and inter_method=='linear' and nof_filters==4\").\\\ngroupby(['method','epoch','mrate','lags','nof_filters','inter_method'])['mse'].mean().reset_index()['mse'].std()\n\n0.007602895892378366"
  },
  {
    "objectID": "posts/2_research/2023-04-27-simulation_table.html#baseline-1",
    "href": "posts/2_research/2023-04-27-simulation_table.html#baseline-1",
    "title": "Simulation Tables",
    "section": "Baseline",
    "text": "Baseline\n\npd.merge(data.query(\"dataset=='chickenpox' and mtype!='rand' and mtype!='block'\").groupby(['nof_filters','method'])['mse'].mean().reset_index(),\n         data.query(\"dataset=='chickenpox' and mtype!='rand' and mtype!='block'\").groupby(['nof_filters','method'])['mse'].std().reset_index(),\n         on=['method','nof_filters']).rename(columns={'mse_x':'mean','mse_y':'std'}).round(3).query(\"nof_filters==16\")\n\n\n\n\n\n\n\n\nnof_filters\nmethod\nmean\nstd\n\n\n\n\n0\n16.0\nIT-STGCN\n1.008\n0.010\n\n\n1\n16.0\nSTGCN\n1.009\n0.008\n\n\n\n\n\n\n\n\npd.merge(data_DCRNN_chickenpox.query(\"dataset=='chickenpox' and mtype!='rand' and mtype!='block'\").groupby(['nof_filters','method'])['mse'].mean().reset_index(),\n         data_DCRNN_chickenpox.query(\"dataset=='chickenpox' and mtype!='rand' and mtype!='block'\").groupby(['nof_filters','method'])['mse'].std().reset_index(),\n         on=['method','nof_filters']).rename(columns={'mse_x':'mean','mse_y':'std'}).round(3).query(\"nof_filters==16\")\n\n\n\n\n\n\n\n\nnof_filters\nmethod\nmean\nstd\n\n\n\n\n0\n16\nIT-STGCN\n0.953\n0.005\n\n\n1\n16\nSTGCN\n0.953\n0.006"
  },
  {
    "objectID": "posts/2_research/2023-04-27-simulation_table.html#random-1",
    "href": "posts/2_research/2023-04-27-simulation_table.html#random-1",
    "title": "Simulation Tables",
    "section": "Random",
    "text": "Random\n\ndata.query(\"dataset=='chickenpox' and mtype=='rand'and method=='GNAR'\")['mse'].unique().round(3)\n\narray([1.427])\n\n\n\npd.merge(data.query(\"dataset=='chickenpox' and mtype=='rand'\").groupby(['mrate','inter_method','nof_filters','method'])['mse'].mean().reset_index(),\n         data.query(\"dataset=='chickenpox' and mtype=='rand'\").groupby(['mrate','inter_method','nof_filters','method'])['mse'].std().reset_index(),\n         on=['method','inter_method','mrate','nof_filters']).rename(columns={'mse_x':'mean','mse_y':'std'}).round(3).query(\"mrate==0.3 and nof_filters==16\")\n\n\n\n\n\n\n\n\nmrate\ninter_method\nnof_filters\nmethod\nmean\nstd\n\n\n\n\n0\n0.3\ncubic\n16.0\nIT-STGCN\n1.019\n0.011\n\n\n1\n0.3\ncubic\n16.0\nSTGCN\n1.059\n0.013\n\n\n6\n0.3\nlinear\n16.0\nIT-STGCN\n1.015\n0.009\n\n\n7\n0.3\nlinear\n16.0\nSTGCN\n1.040\n0.014\n\n\n\n\n\n\n\n\npd.merge(data_DCRNN_chickenpox.query(\"dataset=='chickenpox' and mtype=='rand'\").groupby(['mrate','inter_method','nof_filters','method'])['mse'].mean().reset_index(),\n         data_DCRNN_chickenpox.query(\"dataset=='chickenpox' and mtype=='rand'\").groupby(['mrate','inter_method','nof_filters','method'])['mse'].std().reset_index(),\n         on=['method','inter_method','mrate','nof_filters']).rename(columns={'mse_x':'mean','mse_y':'std'}).round(3).query(\"mrate==0.3 and nof_filters==16\")\n\n\n\n\n\n\n\n\nmrate\ninter_method\nnof_filters\nmethod\nmean\nstd\n\n\n\n\n0\n0.3\ncubic\n16\nIT-STGCN\n0.985\n0.008\n\n\n1\n0.3\ncubic\n16\nSTGCN\n1.053\n0.008\n\n\n2\n0.3\nlinear\n16\nIT-STGCN\n0.983\n0.007\n\n\n3\n0.3\nlinear\n16\nSTGCN\n1.028\n0.012\n\n\n\n\n\n\n\n\npd.merge(data_DCRNN_chickenpox.query(\"dataset=='chickenpox' and mtype=='rand'\").groupby(['mrate','inter_method','nof_filters','method'])['mse'].mean().reset_index(),\n         data_DCRNN_chickenpox.query(\"dataset=='chickenpox' and mtype=='rand'\").groupby(['mrate','inter_method','nof_filters','method'])['mse'].std().reset_index(),\n         on=['method','inter_method','mrate','nof_filters']).rename(columns={'mse_x':'mean','mse_y':'std'}).round(3).query(\"mrate==0.4 and nof_filters==16\")\n\n\n\n\n\n\n\n\nmrate\ninter_method\nnof_filters\nmethod\nmean\nstd\n\n\n\n\n4\n0.4\ncubic\n16\nIT-STGCN\n0.995\n0.009\n\n\n5\n0.4\ncubic\n16\nSTGCN\n1.069\n0.011\n\n\n6\n0.4\nlinear\n16\nIT-STGCN\n0.994\n0.008\n\n\n7\n0.4\nlinear\n16\nSTGCN\n1.038\n0.011\n\n\n\n\n\n\n\n\npd.merge(data.query(\"dataset=='chickenpox' and mtype=='rand'\").groupby(['mrate','inter_method','nof_filters','method'])['mse'].mean().reset_index(),\n         data.query(\"dataset=='chickenpox' and mtype=='rand'\").groupby(['mrate','inter_method','nof_filters','method'])['mse'].std().reset_index(),\n         on=['method','inter_method','mrate','nof_filters']).rename(columns={'mse_x':'mean','mse_y':'std'}).round(3).query(\"mrate==0.4 and nof_filters==16\")\n\n\n\n\n\n\n\n\nmrate\ninter_method\nnof_filters\nmethod\nmean\nstd\n\n\n\n\n12\n0.4\ncubic\n16.0\nIT-STGCN\n1.021\n0.009\n\n\n13\n0.4\ncubic\n16.0\nSTGCN\n1.084\n0.025\n\n\n18\n0.4\nlinear\n16.0\nIT-STGCN\n1.020\n0.009\n\n\n19\n0.4\nlinear\n16.0\nSTGCN\n1.051\n0.014\n\n\n\n\n\n\n\n\npd.merge(data.query(\"dataset=='chickenpox' and mtype=='rand'\").groupby(['mrate','inter_method','nof_filters','method'])['mse'].mean().reset_index(),\n         data.query(\"dataset=='chickenpox' and mtype=='rand'\").groupby(['mrate','inter_method','nof_filters','method'])['mse'].std().reset_index(),\n         on=['method','inter_method','mrate','nof_filters']).rename(columns={'mse_x':'mean','mse_y':'std'}).round(3).query(\"mrate==0.5 and nof_filters==16\")\n\n\n\n\n\n\n\n\nmrate\ninter_method\nnof_filters\nmethod\nmean\nstd\n\n\n\n\n24\n0.5\ncubic\n16.0\nIT-STGCN\n1.027\n0.012\n\n\n25\n0.5\ncubic\n16.0\nSTGCN\n1.128\n0.042\n\n\n30\n0.5\nlinear\n16.0\nIT-STGCN\n1.026\n0.014\n\n\n31\n0.5\nlinear\n16.0\nSTGCN\n1.071\n0.016\n\n\n\n\n\n\n\n\npd.merge(data_DCRNN_chickenpox.query(\"dataset=='chickenpox' and mtype=='rand'\").groupby(['mrate','inter_method','nof_filters','method'])['mse'].mean().reset_index(),\n         data_DCRNN_chickenpox.query(\"dataset=='chickenpox' and mtype=='rand'\").groupby(['mrate','inter_method','nof_filters','method'])['mse'].std().reset_index(),\n         on=['method','inter_method','mrate','nof_filters']).rename(columns={'mse_x':'mean','mse_y':'std'}).round(3).query(\"mrate==0.5 and nof_filters==16\")\n\n\n\n\n\n\n\n\nmrate\ninter_method\nnof_filters\nmethod\nmean\nstd\n\n\n\n\n8\n0.5\ncubic\n16\nIT-STGCN\n1.011\n0.007\n\n\n9\n0.5\ncubic\n16\nSTGCN\n1.080\n0.019\n\n\n10\n0.5\nlinear\n16\nIT-STGCN\n1.008\n0.007\n\n\n11\n0.5\nlinear\n16\nSTGCN\n1.055\n0.010\n\n\n\n\n\n\n\n\npd.merge(data.query(\"dataset=='chickenpox' and mtype=='rand'\").groupby(['mrate','inter_method','nof_filters','method'])['mse'].mean().reset_index(),\n         data.query(\"dataset=='chickenpox' and mtype=='rand'\").groupby(['mrate','inter_method','nof_filters','method'])['mse'].std().reset_index(),\n         on=['method','inter_method','mrate','nof_filters']).rename(columns={'mse_x':'mean','mse_y':'std'}).round(3).query(\"mrate==0.8 and nof_filters==16\")\n\n\n\n\n\n\n\n\nmrate\ninter_method\nnof_filters\nmethod\nmean\nstd\n\n\n\n\n36\n0.8\ncubic\n16.0\nIT-STGCN\n1.206\n0.117\n\n\n37\n0.8\ncubic\n16.0\nSTGCN\n1.266\n0.152\n\n\n42\n0.8\nlinear\n16.0\nIT-STGCN\n1.101\n0.034\n\n\n43\n0.8\nlinear\n16.0\nSTGCN\n1.166\n0.059\n\n\n\n\n\n\n\n\npd.merge(data_DCRNN_chickenpox.query(\"dataset=='chickenpox' and mtype=='rand'\").groupby(['mrate','inter_method','nof_filters','method'])['mse'].mean().reset_index(),\n         data_DCRNN_chickenpox.query(\"dataset=='chickenpox' and mtype=='rand'\").groupby(['mrate','inter_method','nof_filters','method'])['mse'].std().reset_index(),\n         on=['method','inter_method','mrate','nof_filters']).rename(columns={'mse_x':'mean','mse_y':'std'}).round(3).query(\"mrate==0.8 and nof_filters==16\")\n\n\n\n\n\n\n\n\nmrate\ninter_method\nnof_filters\nmethod\nmean\nstd\n\n\n\n\n12\n0.8\ncubic\n16\nIT-STGCN\n1.181\n0.142\n\n\n13\n0.8\ncubic\n16\nSTGCN\n1.417\n0.663\n\n\n14\n0.8\nlinear\n16\nIT-STGCN\n1.058\n0.015\n\n\n15\n0.8\nlinear\n16\nSTGCN\n1.102\n0.027\n\n\n\n\n\n\n\n\npd.merge(data.query(\"dataset=='chickenpox' and mtype=='rand'\").groupby(['mrate','inter_method','nof_filters','method'])['mse'].mean().reset_index(),\n         data.query(\"dataset=='chickenpox' and mtype=='rand'\").groupby(['mrate','inter_method','nof_filters','method'])['mse'].std().reset_index(),\n         on=['method','inter_method','mrate','nof_filters']).rename(columns={'mse_x':'mean','mse_y':'std'}).round(3).query(\"mrate==0.9 and nof_filters==16\")\n\n\n\n\n\n\n\n\nmrate\ninter_method\nnof_filters\nmethod\nmean\nstd\n\n\n\n\n48\n0.9\ncubic\n16.0\nIT-STGCN\n1.228\n0.199\n\n\n49\n0.9\ncubic\n16.0\nSTGCN\n1.283\n0.222\n\n\n54\n0.9\nlinear\n16.0\nIT-STGCN\n1.251\n0.106\n\n\n55\n0.9\nlinear\n16.0\nSTGCN\n1.265\n0.148\n\n\n\n\n\n\n\n\npd.merge(data_DCRNN_chickenpox.query(\"dataset=='chickenpox' and mtype=='rand'\").groupby(['mrate','inter_method','nof_filters','method'])['mse'].mean().reset_index(),\n         data_DCRNN_chickenpox.query(\"dataset=='chickenpox' and mtype=='rand'\").groupby(['mrate','inter_method','nof_filters','method'])['mse'].std().reset_index(),\n         on=['method','inter_method','mrate','nof_filters']).rename(columns={'mse_x':'mean','mse_y':'std'}).round(3).query(\"mrate==0.9 and nof_filters==16\")\n\n\n\n\n\n\n\n\nmrate\ninter_method\nnof_filters\nmethod\nmean\nstd\n\n\n\n\n16\n0.9\ncubic\n16\nIT-STGCN\n2.372\n1.841\n\n\n17\n0.9\ncubic\n16\nSTGCN\n1.596\n0.648\n\n\n18\n0.9\nlinear\n16\nIT-STGCN\n1.090\n0.045\n\n\n19\n0.9\nlinear\n16\nSTGCN\n1.179\n0.127"
  },
  {
    "objectID": "posts/2_research/2023-04-27-simulation_table.html#block-1",
    "href": "posts/2_research/2023-04-27-simulation_table.html#block-1",
    "title": "Simulation Tables",
    "section": "Block",
    "text": "Block\n\ndata.query(\"dataset=='chickenpox' and mtype=='block'and method=='GNAR'\")['mse'].unique()\n\narray([1.42749429])\n\n\n\npd.merge(data.query(\"dataset=='chickenpox' and mtype=='block'\").groupby(['inter_method','mrate','nof_filters','method'])['mse'].mean().reset_index(),\n         data.query(\"dataset=='chickenpox' and mtype=='block'\").groupby(['inter_method','mrate','nof_filters','method'])['mse'].std().reset_index(),\n         on=['method','inter_method','mrate','nof_filters']).rename(columns={'mse_x':'mean','mse_y':'std'}).round(3).query(\"nof_filters==16\")\n\n\n\n\n\n\n\n\ninter_method\nmrate\nnof_filters\nmethod\nmean\nstd\n\n\n\n\n0\ncubic\n0.288\n16.0\nIT-STGCN\n1.052\n0.028\n\n\n1\ncubic\n0.288\n16.0\nSTGCN\n1.052\n0.023\n\n\n6\nlinear\n0.288\n16.0\nIT-STGCN\n1.008\n0.005\n\n\n7\nlinear\n0.288\n16.0\nSTGCN\n1.011\n0.008"
  },
  {
    "objectID": "posts/2_research/2023-04-27-simulation_table.html#baseline-2",
    "href": "posts/2_research/2023-04-27-simulation_table.html#baseline-2",
    "title": "Simulation Tables",
    "section": "Baseline",
    "text": "Baseline\n\npd.merge(data.query(\"dataset=='pedalme' and mtype!='rand' and mtype!='block'\").groupby(['lags','nof_filters','method'])['mse'].mean().reset_index(),\n         data.query(\"dataset=='pedalme' and mtype!='rand' and mtype!='block'\").groupby(['lags','nof_filters','method'])['mse'].std().reset_index(),\n         on=['method','lags','nof_filters']).rename(columns={'mse_x':'mean','mse_y':'std'}).round(3).query(\"lags==4\")\n\n\n\n\n\n\n\n\nlags\nnof_filters\nmethod\nmean\nstd\n\n\n\n\n0\n4\n12.0\nIT-STGCN\n1.241\n0.04\n\n\n1\n4\n12.0\nSTGCN\n1.271\n0.04\n\n\n\n\n\n\n\n\n(1.241+1.271)/2\n\n1.256\n\n\n\npd.merge(data_DCRNN_pedalme.query(\"dataset=='pedalme' and mtype!='rand' and mtype!='block'\").groupby(['lags','nof_filters','method'])['mse'].mean().reset_index(),\n         data_DCRNN_pedalme.query(\"dataset=='pedalme' and mtype!='rand' and mtype!='block'\").groupby(['lags','nof_filters','method'])['mse'].std().reset_index(),\n         on=['method','lags','nof_filters']).rename(columns={'mse_x':'mean','mse_y':'std'}).round(3)\n\n\n\n\n\n\n\n\nlags\nnof_filters\nmethod\nmean\nstd\n\n\n\n\n0\n4\n12\nIT-STGCN\n1.204\n0.020\n\n\n1\n4\n12\nSTGCN\n1.203\n0.022\n\n\n\n\n\n\n\n\n(1.204+1.203)/2\n\n1.2035"
  },
  {
    "objectID": "posts/2_research/2023-04-27-simulation_table.html#random-2",
    "href": "posts/2_research/2023-04-27-simulation_table.html#random-2",
    "title": "Simulation Tables",
    "section": "Random",
    "text": "Random\n\ndata.query(\"dataset=='pedalme' and method=='GNAR' and mtype=='rand'\").groupby(['mrate','lags','inter_method','method'])['mse'].mean().reset_index().query(\" lags==4\")\n\n\n\n\n\n\n\n\nmrate\nlags\ninter_method\nmethod\nmse\n\n\n\n\n0\n0.3\n4\ncubic\nGNAR\n1.302679\n\n\n1\n0.3\n4\nlinear\nGNAR\n1.302679\n\n\n2\n0.4\n4\ncubic\nGNAR\n1.302679\n\n\n3\n0.4\n4\nlinear\nGNAR\n1.302679\n\n\n4\n0.5\n4\ncubic\nGNAR\n1.302679\n\n\n5\n0.5\n4\nlinear\nGNAR\n1.302679\n\n\n6\n0.6\n4\ncubic\nGNAR\n1.302679\n\n\n7\n0.6\n4\nlinear\nGNAR\n1.302679\n\n\n\n\n\n\n\n\npd.merge(data.query(\"dataset=='pedalme' and mtype=='rand'\").groupby(['mrate','lags','inter_method','method'])['mse'].mean().reset_index(),\n         data.query(\"dataset=='pedalme' and mtype=='rand'\").groupby(['mrate','lags','inter_method','method'])['mse'].std().reset_index(),\n         on=['method','mrate','lags','inter_method']).rename(columns={'mse_x':'mean','mse_y':'std'}).round(3).query(\"mrate==0.3 and lags==4\")\n\n\n\n\n\n\n\n\nmrate\nlags\ninter_method\nmethod\nmean\nstd\n\n\n\n\n0\n0.3\n4\ncubic\nGNAR\n1.303\n0.000\n\n\n1\n0.3\n4\ncubic\nIT-STGCN\n1.314\n0.109\n\n\n2\n0.3\n4\ncubic\nSTGCN\n1.363\n0.115\n\n\n3\n0.3\n4\nlinear\nGNAR\n1.303\n0.000\n\n\n4\n0.3\n4\nlinear\nIT-STGCN\n1.323\n0.094\n\n\n5\n0.3\n4\nlinear\nSTGCN\n1.380\n0.127\n\n\n\n\n\n\n\n\npd.merge(data_DCRNN_pedalme.query(\"dataset=='pedalme' and mtype=='rand'\").groupby(['mrate','lags','inter_method','method'])['mse'].mean().reset_index(),\n         data_DCRNN_pedalme.query(\"dataset=='pedalme' and mtype=='rand'\").groupby(['mrate','lags','inter_method','method'])['mse'].std().reset_index(),\n         on=['method','mrate','lags','inter_method']).rename(columns={'mse_x':'mean','mse_y':'std'}).round(3).query(\"mrate==0.3 and  lags==4\")\n\n\n\n\n\n\n\n\nmrate\nlags\ninter_method\nmethod\nmean\nstd\n\n\n\n\n0\n0.3\n4\ncubic\nIT-STGCN\n1.223\n0.031\n\n\n1\n0.3\n4\ncubic\nSTGCN\n1.248\n0.039\n\n\n2\n0.3\n4\nlinear\nIT-STGCN\n1.227\n0.026\n\n\n3\n0.3\n4\nlinear\nSTGCN\n1.242\n0.031\n\n\n4\n0.3\n4\nnearest\nIT-STGCN\n1.231\n0.032\n\n\n5\n0.3\n4\nnearest\nSTGCN\n1.229\n0.032\n\n\n\n\n\n\n\n\npd.merge(data.query(\"dataset=='pedalme' and mtype=='rand'\").groupby(['mrate','lags','inter_method','method'])['mse'].mean().reset_index(),\n         data.query(\"dataset=='pedalme' and mtype=='rand'\").groupby(['mrate','lags','inter_method','method'])['mse'].std().reset_index(),\n         on=['method','mrate','lags','inter_method']).rename(columns={'mse_x':'mean','mse_y':'std'}).round(3).query(\"mrate==0.4 and lags==4\")\n\n\n\n\n\n\n\n\nmrate\nlags\ninter_method\nmethod\nmean\nstd\n\n\n\n\n10\n0.4\n4\ncubic\nGNAR\n1.303\n0.000\n\n\n11\n0.4\n4\ncubic\nIT-STGCN\n1.331\n0.112\n\n\n12\n0.4\n4\ncubic\nSTGCN\n1.342\n0.108\n\n\n13\n0.4\n4\nlinear\nGNAR\n1.303\n0.000\n\n\n14\n0.4\n4\nlinear\nIT-STGCN\n1.375\n0.154\n\n\n15\n0.4\n4\nlinear\nSTGCN\n1.397\n0.193\n\n\n\n\n\n\n\n\npd.merge(data_DCRNN_pedalme.query(\"dataset=='pedalme' and mtype=='rand'\").groupby(['mrate','lags','inter_method','method'])['mse'].mean().reset_index(),\n         data_DCRNN_pedalme.query(\"dataset=='pedalme' and mtype=='rand'\").groupby(['mrate','lags','inter_method','method'])['mse'].std().reset_index(),\n         on=['method','mrate','lags','inter_method']).rename(columns={'mse_x':'mean','mse_y':'std'}).round(3).query(\" mrate==0.4 and lags==4\")\n\n\n\n\n\n\n\n\nmrate\nlags\ninter_method\nmethod\nmean\nstd\n\n\n\n\n6\n0.4\n4\ncubic\nIT-STGCN\n1.230\n0.030\n\n\n7\n0.4\n4\ncubic\nSTGCN\n1.257\n0.051\n\n\n8\n0.4\n4\nlinear\nIT-STGCN\n1.231\n0.032\n\n\n9\n0.4\n4\nlinear\nSTGCN\n1.251\n0.040\n\n\n10\n0.4\n4\nnearest\nIT-STGCN\n1.235\n0.031\n\n\n11\n0.4\n4\nnearest\nSTGCN\n1.241\n0.033\n\n\n\n\n\n\n\n\npd.merge(data.query(\"dataset=='pedalme' and mtype=='rand'\").groupby(['mrate','lags','inter_method','method'])['mse'].mean().reset_index(),\n         data.query(\"dataset=='pedalme' and mtype=='rand'\").groupby(['mrate','lags','inter_method','method'])['mse'].std().reset_index(),\n         on=['method','mrate','lags','inter_method']).rename(columns={'mse_x':'mean','mse_y':'std'}).round(3).query(\"mrate==0.5 and lags==4\")\n\n\n\n\n\n\n\n\nmrate\nlags\ninter_method\nmethod\nmean\nstd\n\n\n\n\n20\n0.5\n4\ncubic\nGNAR\n1.303\n0.000\n\n\n21\n0.5\n4\ncubic\nIT-STGCN\n1.328\n0.108\n\n\n22\n0.5\n4\ncubic\nSTGCN\n1.367\n0.114\n\n\n23\n0.5\n4\nlinear\nGNAR\n1.303\n0.000\n\n\n24\n0.5\n4\nlinear\nIT-STGCN\n1.377\n0.138\n\n\n25\n0.5\n4\nlinear\nSTGCN\n1.326\n0.129\n\n\n\n\n\n\n\n\npd.merge(data_DCRNN_pedalme.query(\"dataset=='pedalme' and mtype=='rand'\").groupby(['mrate','lags','inter_method','method'])['mse'].mean().reset_index(),\n         data_DCRNN_pedalme.query(\"dataset=='pedalme' and mtype=='rand'\").groupby(['mrate','lags','inter_method','method'])['mse'].std().reset_index(),\n         on=['method','mrate','lags','inter_method']).rename(columns={'mse_x':'mean','mse_y':'std'}).round(3).query(\" mrate==0.5 and lags==4\")\n\n\n\n\n\n\n\n\nmrate\nlags\ninter_method\nmethod\nmean\nstd\n\n\n\n\n12\n0.5\n4\ncubic\nIT-STGCN\n1.251\n0.034\n\n\n13\n0.5\n4\ncubic\nSTGCN\n1.279\n0.095\n\n\n14\n0.5\n4\nlinear\nIT-STGCN\n1.241\n0.037\n\n\n15\n0.5\n4\nlinear\nSTGCN\n1.268\n0.052\n\n\n16\n0.5\n4\nnearest\nIT-STGCN\n1.245\n0.034\n\n\n17\n0.5\n4\nnearest\nSTGCN\n1.256\n0.043\n\n\n\n\n\n\n\n\npd.merge(data.query(\"dataset=='pedalme' and mtype=='rand'\").groupby(['mrate','lags','inter_method','method'])['mse'].mean().reset_index(),\n         data.query(\"dataset=='pedalme' and mtype=='rand'\").groupby(['mrate','lags','inter_method','method'])['mse'].std().reset_index(),\n         on=['method','mrate','lags','inter_method']).rename(columns={'mse_x':'mean','mse_y':'std'}).round(3).query(\"mrate==0.6 and lags==4\")\n\n\n\n\n\n\n\n\nmrate\nlags\ninter_method\nmethod\nmean\nstd\n\n\n\n\n30\n0.6\n4\ncubic\nGNAR\n1.303\n0.000\n\n\n31\n0.6\n4\ncubic\nIT-STGCN\n1.300\n0.063\n\n\n32\n0.6\n4\ncubic\nSTGCN\n1.352\n0.106\n\n\n33\n0.6\n4\nlinear\nGNAR\n1.303\n0.000\n\n\n34\n0.6\n4\nlinear\nIT-STGCN\n1.416\n0.169\n\n\n35\n0.6\n4\nlinear\nSTGCN\n1.326\n0.106\n\n\n\n\n\n\n\n\npd.merge(data_DCRNN_pedalme.query(\"dataset=='pedalme' and mtype=='rand'\").groupby(['mrate','lags','inter_method','method'])['mse'].mean().reset_index(),\n         data_DCRNN_pedalme.query(\"dataset=='pedalme' and mtype=='rand'\").groupby(['mrate','lags','inter_method','method'])['mse'].std().reset_index(),\n         on=['method','mrate','lags','inter_method']).rename(columns={'mse_x':'mean','mse_y':'std'}).round(3).query(\"mrate==0.6 and  lags==4\")\n\n\n\n\n\n\n\n\nmrate\nlags\ninter_method\nmethod\nmean\nstd\n\n\n\n\n18\n0.6\n4\ncubic\nIT-STGCN\n1.259\n0.052\n\n\n19\n0.6\n4\ncubic\nSTGCN\n1.313\n0.193\n\n\n20\n0.6\n4\nlinear\nIT-STGCN\n1.243\n0.036\n\n\n21\n0.6\n4\nlinear\nSTGCN\n1.280\n0.064\n\n\n22\n0.6\n4\nnearest\nIT-STGCN\n1.254\n0.037\n\n\n23\n0.6\n4\nnearest\nSTGCN\n1.271\n0.050"
  },
  {
    "objectID": "posts/2_research/2023-04-27-simulation_table.html#block-2",
    "href": "posts/2_research/2023-04-27-simulation_table.html#block-2",
    "title": "Simulation Tables",
    "section": "Block",
    "text": "Block\n\npd.merge(data.query(\"dataset=='pedalme' and mtype=='block'\").groupby(['mrate','lags','inter_method','method'])['mse'].mean().reset_index(),\n         data.query(\"dataset=='pedalme' and mtype=='block'\").groupby(['mrate','lags','inter_method','method'])['mse'].std().reset_index(),\n         on=['method','mrate','lags','inter_method']).rename(columns={'mse_x':'mean','mse_y':'std'}).round(3).query(\"lags==4\")\n\n\n\n\n\n\n\n\nmrate\nlags\ninter_method\nmethod\nmean\nstd\n\n\n\n\n4\n0.143\n4\ncubic\nGNAR\n1.303\n0.000\n\n\n5\n0.143\n4\ncubic\nIT-STGCN\n1.284\n0.053\n\n\n6\n0.143\n4\ncubic\nSTGCN\n1.288\n0.071\n\n\n7\n0.143\n4\nlinear\nGNAR\n1.303\n0.000\n\n\n14\n0.286\n4\ncubic\nGNAR\n1.303\n0.000\n\n\n15\n0.286\n4\ncubic\nIT-STGCN\n1.304\n0.050\n\n\n16\n0.286\n4\ncubic\nSTGCN\n1.377\n0.061\n\n\n17\n0.286\n4\nlinear\nGNAR\n1.303\n0.000\n\n\n18\n0.286\n4\nlinear\nIT-STGCN\n1.335\n0.062\n\n\n19\n0.286\n4\nlinear\nSTGCN\n1.350\n0.056"
  },
  {
    "objectID": "posts/2_research/2023-04-27-simulation_table.html#w_st",
    "href": "posts/2_research/2023-04-27-simulation_table.html#w_st",
    "title": "Simulation Tables",
    "section": "W_st",
    "text": "W_st\n\ndata_pedalme_wst = pd.read_csv('./simulation_results/Real_simulation/pedalme_Simulation_itstgcnsnd.csv')\n\n\npd.merge(data_pedalme_wst.query(\"mtype=='rand'\").groupby(['mrate','lags','inter_method','method'])['mse'].mean().reset_index(),\n         data_pedalme_wst.query(\"mtype=='rand'\").groupby(['mrate','lags','inter_method','method'])['mse'].std().reset_index(),\n         on=['method','mrate','lags','inter_method']).rename(columns={'mse_x':'mean','mse_y':'std'}).round(3).query(\"lags==4\")\n\n\n\n\n\n\n\n\nmrate\nlags\ninter_method\nmethod\nmean\nstd\n\n\n\n\n0\n0.3\n4\ncubic\nIT-STGCN\n1.353\n0.141\n\n\n1\n0.3\n4\ncubic\nSTGCN\n1.360\n0.131\n\n\n2\n0.3\n4\nlinear\nIT-STGCN\n1.337\n0.122\n\n\n3\n0.3\n4\nlinear\nSTGCN\n1.353\n0.117\n\n\n4\n0.3\n4\nnearest\nIT-STGCN\n1.316\n0.122\n\n\n5\n0.3\n4\nnearest\nSTGCN\n1.403\n0.134\n\n\n12\n0.4\n4\ncubic\nIT-STGCN\n1.332\n0.166\n\n\n13\n0.4\n4\ncubic\nSTGCN\n1.344\n0.123\n\n\n14\n0.4\n4\nlinear\nIT-STGCN\n1.355\n0.139\n\n\n15\n0.4\n4\nlinear\nSTGCN\n1.393\n0.168\n\n\n16\n0.4\n4\nnearest\nIT-STGCN\n1.386\n0.128\n\n\n17\n0.4\n4\nnearest\nSTGCN\n1.341\n0.129\n\n\n24\n0.5\n4\ncubic\nIT-STGCN\n1.312\n0.152\n\n\n25\n0.5\n4\ncubic\nSTGCN\n1.362\n0.129\n\n\n26\n0.5\n4\nlinear\nIT-STGCN\n1.344\n0.177\n\n\n27\n0.5\n4\nlinear\nSTGCN\n1.335\n0.117\n\n\n28\n0.5\n4\nnearest\nIT-STGCN\n1.335\n0.153\n\n\n29\n0.5\n4\nnearest\nSTGCN\n1.350\n0.129\n\n\n36\n0.6\n4\ncubic\nIT-STGCN\n1.346\n0.151\n\n\n37\n0.6\n4\ncubic\nSTGCN\n1.398\n0.103\n\n\n38\n0.6\n4\nlinear\nIT-STGCN\n1.365\n0.177\n\n\n39\n0.6\n4\nlinear\nSTGCN\n1.353\n0.087\n\n\n40\n0.6\n4\nnearest\nIT-STGCN\n1.402\n0.269\n\n\n41\n0.6\n4\nnearest\nSTGCN\n1.339\n0.111\n\n\n48\n0.7\n4\ncubic\nIT-STGCN\n1.377\n0.173\n\n\n49\n0.7\n4\ncubic\nSTGCN\n1.363\n0.097\n\n\n50\n0.7\n4\nlinear\nIT-STGCN\n1.355\n0.144\n\n\n51\n0.7\n4\nlinear\nSTGCN\n1.288\n0.063\n\n\n52\n0.7\n4\nnearest\nIT-STGCN\n1.383\n0.157\n\n\n53\n0.7\n4\nnearest\nSTGCN\n1.334\n0.124\n\n\n\n\n\n\n\n\npd.merge(data_pedalme_wst.query(\"mtype=='block'\").groupby(['mrate','lags','inter_method','method'])['mse'].mean().reset_index(),\n         data_pedalme_wst.query(\"mtype=='block'\").groupby(['mrate','lags','inter_method','method'])['mse'].std().reset_index(),\n         on=['method','mrate','lags','inter_method']).rename(columns={'mse_x':'mean','mse_y':'std'}).round(3).query(\"lags==4\")\n\n\n\n\n\n\n\n\nmrate\nlags\ninter_method\nmethod\nmean\nstd\n\n\n\n\n6\n0.286\n4\ncubic\nIT-STGCN\n1.260\n0.063\n\n\n7\n0.286\n4\ncubic\nSTGCN\n1.417\n0.065\n\n\n8\n0.286\n4\nlinear\nIT-STGCN\n1.276\n0.065\n\n\n9\n0.286\n4\nlinear\nSTGCN\n1.288\n0.055\n\n\n10\n0.286\n4\nnearest\nIT-STGCN\n1.275\n0.061\n\n\n11\n0.286\n4\nnearest\nSTGCN\n1.312\n0.061"
  },
  {
    "objectID": "posts/2_research/2023-04-27-simulation_table.html#baseline-3",
    "href": "posts/2_research/2023-04-27-simulation_table.html#baseline-3",
    "title": "Simulation Tables",
    "section": "Baseline",
    "text": "Baseline\n\ndata.query(\"dataset=='wikimath' and mrate==0\").groupby(['lags','nof_filters','method'])['mse'].mean().reset_index().round(3).query(\"lags==8\")\n\n\n\n\n\n\n\n\nlags\nnof_filters\nmethod\nmse\n\n\n\n\n4\n8\n12.0\nIT-STGCN\n0.771\n\n\n5\n8\n12.0\nSTGCN\n0.772\n\n\n\n\n\n\n\n\ndata_DCRNN_wikimath.query(\"dataset=='wikimath' and mrate==0\").groupby(['lags','nof_filters','method'])['mse'].mean().reset_index().round(3).query(\"lags==8\")\n\n\n\n\n\n\n\n\nlags\nnof_filters\nmethod\nmse\n\n\n\n\n0\n8\n12\nIT-STGCN\n0.778\n\n\n1\n8\n12\nSTGCN\n0.759"
  },
  {
    "objectID": "posts/2_research/2023-04-27-simulation_table.html#random-3",
    "href": "posts/2_research/2023-04-27-simulation_table.html#random-3",
    "title": "Simulation Tables",
    "section": "Random",
    "text": "Random\n\npd.merge(data.query(\"dataset=='wikimath' and mtype=='rand'\").groupby(['mrate','lags','method'])['mse'].mean().reset_index(),\n         data.query(\"dataset=='wikimath' and mtype=='rand'\").groupby(['mrate','lags','method'])['mse'].std().reset_index(),\n         on=['method','mrate','lags']).rename(columns={'mse_x':'mean','mse_y':'std'}).round(3).query(\"lags==8\")\n\n\n\n\n\n\n\n\nmrate\nlags\nmethod\nmean\nstd\n\n\n\n\n6\n0.3\n8\nIT-STGCN\n0.781\n0.012\n\n\n7\n0.3\n8\nSTGCN\n0.779\n0.013\n\n\n14\n0.5\n8\nIT-STGCN\n0.802\n0.041\n\n\n15\n0.5\n8\nSTGCN\n0.806\n0.020\n\n\n\n\n\n\n\n\npd.merge(data_DCRNN_wikimath.query(\"dataset=='wikimath' and mtype=='rand'\").groupby(['mrate','lags','method'])['mse'].mean().reset_index(),\n         data_DCRNN_wikimath.query(\"dataset=='wikimath' and mtype=='rand'\").groupby(['mrate','lags','method'])['mse'].std().reset_index(),\n         on=['method','mrate','lags']).rename(columns={'mse_x':'mean','mse_y':'std'}).round(3).query(\"lags==8\")\n\n\n\n\n\n\n\n\nmrate\nlags\nmethod\nmean\nstd\n\n\n\n\n0\n0.3\n8\nIT-STGCN\n0.759\n0.021\n\n\n1\n0.3\n8\nSTGCN\n0.774\n0.030"
  },
  {
    "objectID": "posts/2_research/2023-04-27-simulation_table.html#block-3",
    "href": "posts/2_research/2023-04-27-simulation_table.html#block-3",
    "title": "Simulation Tables",
    "section": "Block",
    "text": "Block\n\npd.merge(data.query(\"dataset=='wikimath' and mtype=='block'\").groupby(['mrate','lags','method'])['mse'].mean().reset_index(),\n         data.query(\"dataset=='wikimath' and mtype=='block'\").groupby(['mrate','lags','method'])['mse'].std().reset_index(),\n         on=['method','mrate','lags']).rename(columns={'mse_x':'mean','mse_y':'std'})\n\n\n\n\n\n\n\n\nmrate\nlags\nmethod\nmean\nstd\n\n\n\n\n0\n0.003841\n2\nIT-STGCN\n0.810475\n0.033897\n\n\n1\n0.003841\n2\nSTGCN\n0.801502\n0.015510\n\n\n2\n0.003841\n4\nIT-STGCN\n0.779852\n0.013188\n\n\n3\n0.003841\n4\nSTGCN\n0.779816\n0.019309"
  },
  {
    "objectID": "posts/2_research/2023-04-27-simulation_table.html#missing-values-on-the-same-nodes",
    "href": "posts/2_research/2023-04-27-simulation_table.html#missing-values-on-the-same-nodes",
    "title": "Simulation Tables",
    "section": "missing values on the same nodes",
    "text": "missing values on the same nodes\n\ndata_wikimath_st = pd.read_csv('./simulation_results/Real_simulation/wikimath_GSO_st.csv')\n\n\npd.merge(data_wikimath_st.groupby(['mrate','lags','method'])['mse'].mean().reset_index(),\n        data_wikimath_st.groupby(['mrate','lags','method'])['mse'].std().reset_index(),\n         on=['method','mrate','lags']).rename(columns={'mse_x':'mean','mse_y':'std'}).round(3)\n\n\n\n\n\n\n\n\nmrate\nlags\nmethod\nmean\nstd\n\n\n\n\n0\n0.123\n4\nIT-STGCN\n0.774\n0.008\n\n\n1\n0.123\n4\nSTGCN\n0.766\n0.010\n\n\n2\n0.738\n4\nIT-STGCN\n0.851\n0.029\n\n\n3\n0.738\n4\nSTGCN\n0.831\n0.031"
  },
  {
    "objectID": "posts/2_research/2023-04-27-simulation_table.html#baseline-4",
    "href": "posts/2_research/2023-04-27-simulation_table.html#baseline-4",
    "title": "Simulation Tables",
    "section": "Baseline",
    "text": "Baseline\n\npd.merge(data.query(\"dataset=='windmillsmall' and mrate==0\").groupby(['lags','method'])['mse'].mean().reset_index(),\n         data.query(\"dataset=='windmillsmall' and mrate==0\").groupby(['lags','method'])['mse'].std().reset_index(),\n         on=['method','lags']).rename(columns={'mse_x':'mean','mse_y':'std'}).round(3)\n\n\n\n\n\n\n\n\nlags\nmethod\nmean\nstd\n\n\n\n\n0\n8\nGNAR\n1.649\n0.000\n\n\n1\n8\nIT-STGCN\n1.006\n0.006\n\n\n2\n8\nSTGCN\n1.001\n0.003\n\n\n\n\n\n\n\n\npd.merge(data_DCRNN_windmillsmall.query(\"dataset=='windmillsmall' and mrate==0\").groupby(['lags','method'])['mse'].mean().reset_index(),\n         data_DCRNN_windmillsmall.query(\"dataset=='windmillsmall' and mrate==0\").groupby(['lags','method'])['mse'].std().reset_index(),\n         on=['method','lags']).rename(columns={'mse_x':'mean','mse_y':'std'}).round(3)\n\n\n\n\n\n\n\n\nlags\nmethod\nmean\nstd\n\n\n\n\n0\n8\nIT-STGCN\n1.000\nNaN\n\n\n1\n8\nSTGCN\n1.001\nNaN"
  },
  {
    "objectID": "posts/2_research/2023-04-27-simulation_table.html#random-4",
    "href": "posts/2_research/2023-04-27-simulation_table.html#random-4",
    "title": "Simulation Tables",
    "section": "Random",
    "text": "Random\n\ndata.query(\"dataset=='windmillsmall' and mtype=='rand'\")\n\n\n\n\n\n\n\n\ndataset\nmethod\nRecurrentGCN\nmrate\nmtype\nlags\nnof_filters\ninter_method\nepoch\nmse\n\n\n\n\n7097\nwindmillsmall\nSTGCN\nGConvGRU\n0.7\nrand\n8\n12.0\nlinear\n50.0\n1.436077\n\n\n7098\nwindmillsmall\nIT-STGCN\nGConvGRU\n0.7\nrand\n8\n12.0\nlinear\n50.0\n1.290896\n\n\n7099\nwindmillsmall\nSTGCN\nGConvGRU\n0.7\nrand\n8\n12.0\nlinear\n50.0\n1.410098\n\n\n7100\nwindmillsmall\nIT-STGCN\nGConvGRU\n0.7\nrand\n8\n12.0\nlinear\n50.0\n1.176981\n\n\n7101\nwindmillsmall\nSTGCN\nGConvGRU\n0.7\nrand\n8\n12.0\nlinear\n50.0\n1.447851\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n7180\nwindmillsmall\nGNAR\nGConvGRU\n0.7\nrand\n8\nNaN\nlinear\nNaN\n1.649230\n\n\n7181\nwindmillsmall\nGNAR\nGConvGRU\n0.7\nrand\n8\nNaN\nlinear\nNaN\n1.649230\n\n\n7182\nwindmillsmall\nGNAR\nGConvGRU\n0.7\nrand\n8\nNaN\nlinear\nNaN\n1.649230\n\n\n7183\nwindmillsmall\nGNAR\nGConvGRU\n0.7\nrand\n8\nNaN\nlinear\nNaN\n1.649230\n\n\n7184\nwindmillsmall\nGNAR\nGConvGRU\n0.7\nrand\n8\nNaN\nlinear\nNaN\n1.649230\n\n\n\n\n88 rows × 10 columns\n\n\n\n\npd.merge(data.query(\"dataset=='windmillsmall' and mtype=='rand'\").groupby(['mrate','lags','method'])['mse'].mean().reset_index(),\n         data.query(\"dataset=='windmillsmall' and mtype=='rand'\").groupby(['mrate','lags','method'])['mse'].std().reset_index(),\n         on=['method','mrate','lags']).rename(columns={'mse_x':'mean','mse_y':'std'}).round(3)\n\n\n\n\n\n\n\n\nmrate\nlags\nmethod\nmean\nstd\n\n\n\n\n0\n0.7\n8\nGNAR\n1.649\n0.000\n\n\n1\n0.7\n8\nIT-STGCN\n1.178\n0.054\n\n\n2\n0.7\n8\nSTGCN\n1.410\n0.075\n\n\n\n\n\n\n\n\npd.merge(data_DCRNN_windmillsmall.query(\"dataset=='windmillsmall' and mtype=='rand'\").groupby(['mrate','lags','method'])['mse'].mean().reset_index(),\n         data_DCRNN_windmillsmall.query(\"dataset=='windmillsmall' and mtype=='rand'\").groupby(['mrate','lags','method'])['mse'].std().reset_index(),\n         on=['method','mrate','lags']).rename(columns={'mse_x':'mean','mse_y':'std'}).round(3)\n\n\n\n\n\n\n\n\nmean\nmrate\nlags\nmethod\nstd"
  },
  {
    "objectID": "posts/2_research/2023-04-27-simulation_table.html#block-4",
    "href": "posts/2_research/2023-04-27-simulation_table.html#block-4",
    "title": "Simulation Tables",
    "section": "Block",
    "text": "Block\n\npd.merge(data.query(\"dataset=='windmillsmall' and mtype=='block'\").groupby(['mrate','lags','method'])['mse'].mean().reset_index(),\n         data.query(\"dataset=='windmillsmall' and mtype=='block'\").groupby(['mrate','lags','method'])['mse'].std().reset_index(),\n         on=['method','mrate','lags']).rename(columns={'mse_x':'mean','mse_y':'std'}).round(3)\n\n\n\n\n\n\n\n\nmrate\nlags\nmethod\nmean\nstd\n\n\n\n\n0\n0.325\n8\nGNAR\n1.649\n0.000\n\n\n1\n0.325\n8\nIT-STGCN\n1.015\n0.009\n\n\n2\n0.325\n8\nSTGCN\n1.017\n0.008"
  },
  {
    "objectID": "posts/2_research/2023-04-27-simulation_table.html#baseline-5",
    "href": "posts/2_research/2023-04-27-simulation_table.html#baseline-5",
    "title": "Simulation Tables",
    "section": "Baseline",
    "text": "Baseline\n\nround(data.query(\"dataset=='monte' and mrate==0\")['mse'].mean(),3),round(data_monte.query(\"mrate==0\")['mse'].std(),3)\n\n(0.97, 0.024)"
  },
  {
    "objectID": "posts/2_research/2023-04-27-simulation_table.html#random-5",
    "href": "posts/2_research/2023-04-27-simulation_table.html#random-5",
    "title": "Simulation Tables",
    "section": "Random",
    "text": "Random\n\ndata.query(\"dataset=='monte' and mtype=='rand'\")\n\n\n\n\n\n\n\n\ndataset\nmethod\nRecurrentGCN\nmrate\nmtype\nlags\nnof_filters\ninter_method\nepoch\nmse\n\n\n\n\n7307\nmonte\nSTGCN\nGConvGRU\n0.3\nrand\n8\n12.0\nlinear\n50.0\n0.972491\n\n\n7308\nmonte\nIT-STGCN\nGConvGRU\n0.3\nrand\n8\n12.0\nlinear\n50.0\n0.974410\n\n\n7309\nmonte\nSTGCN\nGConvGRU\n0.3\nrand\n8\n12.0\nlinear\n50.0\n0.968628\n\n\n7310\nmonte\nIT-STGCN\nGConvGRU\n0.3\nrand\n8\n12.0\nlinear\n50.0\n0.976796\n\n\n7311\nmonte\nSTGCN\nGConvGRU\n0.3\nrand\n8\n12.0\nlinear\n50.0\n0.973314\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n7872\nmonte\nIT-STGCN\nGConvGRU\n0.9\nrand\n8\n12.0\nlinear\n50.0\n1.030140\n\n\n7873\nmonte\nSTGCN\nGConvGRU\n0.9\nrand\n8\n12.0\nlinear\n50.0\n1.040731\n\n\n7874\nmonte\nIT-STGCN\nGConvGRU\n0.9\nrand\n8\n12.0\nlinear\n50.0\n1.041819\n\n\n7875\nmonte\nSTGCN\nGConvGRU\n0.9\nrand\n8\n12.0\nlinear\n50.0\n1.023531\n\n\n7876\nmonte\nIT-STGCN\nGConvGRU\n0.9\nrand\n8\n12.0\nlinear\n50.0\n1.032515\n\n\n\n\n504 rows × 10 columns\n\n\n\n\npd.merge(data.query(\"dataset=='monte' and mtype=='rand'\").groupby(['mrate','lags','method'])['mse'].mean().reset_index(),\n         data.query(\"dataset=='monte' and mtype=='rand'\").groupby(['mrate','lags','method'])['mse'].std().reset_index(),\n         on=['method','mrate','lags']).rename(columns={'mse_x':'mean','mse_y':'std'})\n\n\n\n\n\n\n\n\nmrate\nlags\nmethod\nmean\nstd\n\n\n\n\n0\n0.3\n4\nGNAR\n1.061937\n0.000000\n\n\n1\n0.3\n4\nIT-STGCN\n0.971925\n0.001871\n\n\n2\n0.3\n4\nSTGCN\n0.965885\n0.002552\n\n\n3\n0.3\n8\nGNAR\n1.068464\n0.000000\n\n\n4\n0.3\n8\nIT-STGCN\n0.974867\n0.003233\n\n\n5\n0.3\n8\nSTGCN\n0.972051\n0.002383\n\n\n6\n0.4\n4\nGNAR\n1.061937\n0.000000\n\n\n7\n0.4\n4\nIT-STGCN\n0.976348\n0.001374\n\n\n8\n0.4\n4\nSTGCN\n0.967480\n0.002974\n\n\n9\n0.4\n8\nGNAR\n1.068464\n0.000000\n\n\n10\n0.4\n8\nIT-STGCN\n0.978809\n0.002110\n\n\n11\n0.4\n8\nSTGCN\n0.973098\n0.002613\n\n\n12\n0.8\n4\nGNAR\n1.061937\n0.000000\n\n\n13\n0.8\n4\nIT-STGCN\n1.006583\n0.003297\n\n\n14\n0.8\n4\nSTGCN\n0.999715\n0.006909\n\n\n15\n0.8\n8\nGNAR\n1.068464\n0.000000\n\n\n16\n0.8\n8\nIT-STGCN\n1.004450\n0.002953\n\n\n17\n0.8\n8\nSTGCN\n1.005238\n0.005777\n\n\n18\n0.9\n4\nGNAR\n1.061937\n0.000000\n\n\n19\n0.9\n4\nIT-STGCN\n1.034162\n0.005611\n\n\n20\n0.9\n4\nSTGCN\n1.034995\n0.006551\n\n\n21\n0.9\n8\nGNAR\n1.068464\n0.000000\n\n\n22\n0.9\n8\nIT-STGCN\n1.030283\n0.007979\n\n\n23\n0.9\n8\nSTGCN\n1.031538\n0.009285"
  },
  {
    "objectID": "posts/2_research/2023-04-27-simulation_table.html#block-5",
    "href": "posts/2_research/2023-04-27-simulation_table.html#block-5",
    "title": "Simulation Tables",
    "section": "Block",
    "text": "Block\n\npd.merge(data.query(\"dataset=='monte' and mtype=='block'\").groupby(['mrate','lags','method'])['mse'].mean().reset_index(),\n         data.query(\"dataset=='monte' and mtype=='block'\").groupby(['mrate','lags','method'])['mse'].std().reset_index(),\n         on=['method','mrate','lags']).rename(columns={'mse_x':'mean','mse_y':'std'})\n\n\n\n\n\n\n\n\nmrate\nlags\nmethod\nmean\nstd\n\n\n\n\n0\n0.149142\n4\nGNAR\n1.061937\n0.000000\n\n\n1\n0.149142\n4\nIT-STGCN\n0.963990\n0.002194\n\n\n2\n0.149142\n4\nSTGCN\n0.965297\n0.001611\n\n\n3\n0.149142\n8\nGNAR\n1.068464\n0.000000\n\n\n4\n0.149142\n8\nIT-STGCN\n0.971647\n0.002860\n\n\n5\n0.149142\n8\nSTGCN\n0.971700\n0.001672"
  },
  {
    "objectID": "posts/2_research/2023-06-25-EvolveGCNO_Simulation_boxplot_reshape.html",
    "href": "posts/2_research/2023-06-25-EvolveGCNO_Simulation_boxplot_reshape.html",
    "title": "EvolveGCNO_Simulation_reshape",
    "section": "",
    "text": "Simulation Study"
  },
  {
    "objectID": "posts/2_research/2023-06-25-EvolveGCNO_Simulation_boxplot_reshape.html#baseline",
    "href": "posts/2_research/2023-06-25-EvolveGCNO_Simulation_boxplot_reshape.html#baseline",
    "title": "EvolveGCNO_Simulation_reshape",
    "section": "Baseline",
    "text": "Baseline\n\ndata.query(\"method!='GNAR' and mrate==0\").plot.box(backend='plotly',x='mrate',color='method',y='mse',facet_col='lags',facet_row='nof_filters',height=600)"
  },
  {
    "objectID": "posts/2_research/2023-06-25-EvolveGCNO_Simulation_boxplot_reshape.html#random",
    "href": "posts/2_research/2023-06-25-EvolveGCNO_Simulation_boxplot_reshape.html#random",
    "title": "EvolveGCNO_Simulation_reshape",
    "section": "Random",
    "text": "Random\n\ndata.query(\"method!='GNAR' and mtype =='rand'\").plot.box(backend='plotly',x='mrate',color='method',y='mse',facet_col='inter_method',facet_row='nof_filters',height=600)"
  },
  {
    "objectID": "posts/2_research/2023-06-25-EvolveGCNO_Simulation_boxplot_reshape.html#block",
    "href": "posts/2_research/2023-06-25-EvolveGCNO_Simulation_boxplot_reshape.html#block",
    "title": "EvolveGCNO_Simulation_reshape",
    "section": "Block",
    "text": "Block\n\ndata.query(\"method!='GNAR' and mtype =='block' \").plot.box(backend='plotly',x='mrate',color='method',y='mse',facet_col='inter_method',facet_row='nof_filters',height=600)"
  },
  {
    "objectID": "posts/2_research/2023-06-25-EvolveGCNO_Simulation_boxplot_reshape.html#baseline-1",
    "href": "posts/2_research/2023-06-25-EvolveGCNO_Simulation_boxplot_reshape.html#baseline-1",
    "title": "EvolveGCNO_Simulation_reshape",
    "section": "Baseline",
    "text": "Baseline\n\ndata.query(\"method!='GNAR' and mrate ==0 \").plot.box(backend='plotly',x='mrate',color='method',y='mse',facet_col='nof_filters',height=600)"
  },
  {
    "objectID": "posts/2_research/2023-06-25-EvolveGCNO_Simulation_boxplot_reshape.html#random-1",
    "href": "posts/2_research/2023-06-25-EvolveGCNO_Simulation_boxplot_reshape.html#random-1",
    "title": "EvolveGCNO_Simulation_reshape",
    "section": "Random",
    "text": "Random\n\ndata.query(\"method!='GNAR' and mtype =='rand'\").plot.box(backend='plotly',x='mrate',color='method',y='mse',facet_col='nof_filters',facet_row='inter_method',height=800)"
  },
  {
    "objectID": "posts/2_research/2023-06-25-EvolveGCNO_Simulation_boxplot_reshape.html#block-1",
    "href": "posts/2_research/2023-06-25-EvolveGCNO_Simulation_boxplot_reshape.html#block-1",
    "title": "EvolveGCNO_Simulation_reshape",
    "section": "Block",
    "text": "Block\n\ndata.query(\"method!='GNAR' and mtype =='block'\").plot.box(backend='plotly',x='mrate',color='method',y='mse',facet_col='inter_method',facet_row='lags',height=800)"
  },
  {
    "objectID": "posts/2_research/2023-06-25-EvolveGCNO_Simulation_boxplot_reshape.html#baseline-2",
    "href": "posts/2_research/2023-06-25-EvolveGCNO_Simulation_boxplot_reshape.html#baseline-2",
    "title": "EvolveGCNO_Simulation_reshape",
    "section": "Baseline",
    "text": "Baseline\n\ndata.query(\"mrate ==0 and lags!=2\").plot.box(backend='plotly',x='epoch',color='method',y='mse',facet_col='nof_filters',facet_row='lags',height=400)"
  },
  {
    "objectID": "posts/2_research/2023-06-25-EvolveGCNO_Simulation_boxplot_reshape.html#random-2",
    "href": "posts/2_research/2023-06-25-EvolveGCNO_Simulation_boxplot_reshape.html#random-2",
    "title": "EvolveGCNO_Simulation_reshape",
    "section": "random",
    "text": "random\n\ndata.query(\"method!='GNAR' and mtype =='rand' and lags!=2\").plot.box(backend='plotly',x='mrate',color='method',y='mse',facet_col='nof_filters',facet_row='inter_method',height=800)"
  },
  {
    "objectID": "posts/2_research/2023-06-25-EvolveGCNO_Simulation_boxplot_reshape.html#block-2",
    "href": "posts/2_research/2023-06-25-EvolveGCNO_Simulation_boxplot_reshape.html#block-2",
    "title": "EvolveGCNO_Simulation_reshape",
    "section": "block",
    "text": "block\n\ndata.query(\"method!='GNAR' and mtype =='block' and lags!=2 \").plot.box(backend='plotly',x='mrate',color='method',y='mse',facet_col='nof_filters',facet_row='inter_method',height=800)"
  },
  {
    "objectID": "posts/2_research/2023-06-25-EvolveGCNO_Simulation_boxplot_reshape.html#weight-matrix-time-node-고려한-결과",
    "href": "posts/2_research/2023-06-25-EvolveGCNO_Simulation_boxplot_reshape.html#weight-matrix-time-node-고려한-결과",
    "title": "EvolveGCNO_Simulation_reshape",
    "section": "weight matrix time, node 고려한 결과",
    "text": "weight matrix time, node 고려한 결과\n\ndf1 = pd.read_csv('./simulation_results/2023-06-27_00-26-15.csv')\ndf2 = pd.read_csv('./simulation_results/2023-06-27_00-42-14.csv')\n\n\ndata2 = pd.concat([df1,df2],axis=0)\n\n\ndata2.to_csv('./simulation_results/Real_simulation_reshape/EvolveGCNO_pedalme_Simulation_itstgcnsnd.csv',index=False)\n\n\ndata2 = pd.read_csv('./simulation_results/Real_simulation_reshape/EvolveGCNO_pedalme_Simulation_itstgcnsnd.csv')\n\n\ndata2.query(\"mtype=='rand'\").plot.box(backend='plotly',x='mrate',color='method',y='mse',facet_col='inter_method',facet_row='nof_filters',height=1000)\n\n                                                \n\n\n\ndata2.query(\"mtype=='block'\").plot.box(backend='plotly',x='mrate',color='method',y='mse',facet_col='inter_method',facet_row='nof_filters',height=1000)"
  },
  {
    "objectID": "posts/2_research/2023-06-25-EvolveGCNO_Simulation_boxplot_reshape.html#baseline-3",
    "href": "posts/2_research/2023-06-25-EvolveGCNO_Simulation_boxplot_reshape.html#baseline-3",
    "title": "EvolveGCNO_Simulation_reshape",
    "section": "Baseline",
    "text": "Baseline\n\ndata.query(\"mrate==0 and method!='GNAR'\").plot.box(backend='plotly',x='mrate',color='method',y='mse',height=600)"
  },
  {
    "objectID": "posts/2_research/2023-06-25-EvolveGCNO_Simulation_boxplot_reshape.html#random-3",
    "href": "posts/2_research/2023-06-25-EvolveGCNO_Simulation_boxplot_reshape.html#random-3",
    "title": "EvolveGCNO_Simulation_reshape",
    "section": "random",
    "text": "random\n\ndata.query(\"mtype=='rand' and mrate !=0 and method!='GNAR'\").plot.box(backend='plotly',x='mrate',color='method',y='mse',facet_col='lags',facet_row='nof_filters',height=800)"
  },
  {
    "objectID": "posts/2_research/2023-06-25-EvolveGCNO_Simulation_boxplot_reshape.html#block-3",
    "href": "posts/2_research/2023-06-25-EvolveGCNO_Simulation_boxplot_reshape.html#block-3",
    "title": "EvolveGCNO_Simulation_reshape",
    "section": "block",
    "text": "block\n\ndata.query(\"mtype=='block' and method!='GNAR'\").plot.box(backend='plotly',x='mrate',color='method',y='mse',facet_col='lags',facet_row='nof_filters',height=1200)"
  },
  {
    "objectID": "posts/2_research/2023-06-25-EvolveGCNO_Simulation_boxplot_reshape.html#missing-values-on-the-same-nodes",
    "href": "posts/2_research/2023-06-25-EvolveGCNO_Simulation_boxplot_reshape.html#missing-values-on-the-same-nodes",
    "title": "EvolveGCNO_Simulation_reshape",
    "section": "missing values on the same nodes",
    "text": "missing values on the same nodes\n\ndf1 = pd.read_csv('./simulation_results/2023-06-29_18-21-43.csv') # STGCN IT-STGCN block\ndf2 = pd.read_csv('./simulation_results/2023-06-29_13-25-16.csv') # STGCN IT-STGCN\ndf3 = pd.read_csv('./simulation_results/2023-06-29_15-50-57.csv') \n\n\ndata = pd.concat([df1,df2,df3],axis=0)\n\n\ndata.to_csv('./simulation_results/Real_simulation_reshape/EvolveGCNO_wikimath_GSO_st.csv',index=False)\n\n\ndata = pd.read_csv('./simulation_results/Real_simulation_reshape/EvolveGCNO_wikimath_GSO_st.csv')\n\n\ndata.query(\"method!='GNAR' and mtype =='block' \").plot.box(backend='plotly',x='mrate',color='method',y='mse',facet_col='inter_method',facet_row='lags',height=800)"
  },
  {
    "objectID": "posts/2_research/2023-06-25-EvolveGCNO_Simulation_boxplot_reshape.html#baseline-4",
    "href": "posts/2_research/2023-06-25-EvolveGCNO_Simulation_boxplot_reshape.html#baseline-4",
    "title": "EvolveGCNO_Simulation_reshape",
    "section": "Baseline",
    "text": "Baseline\n\ndata.query(\"method!='GNAR' and mrate ==0 \").plot.box(backend='plotly',x='mrate',color='method',y='mse',facet_col='lags',height=800)"
  },
  {
    "objectID": "posts/2_research/2023-06-25-EvolveGCNO_Simulation_boxplot_reshape.html#random-4",
    "href": "posts/2_research/2023-06-25-EvolveGCNO_Simulation_boxplot_reshape.html#random-4",
    "title": "EvolveGCNO_Simulation_reshape",
    "section": "random",
    "text": "random\n\ndata.query(\"method!='GNAR' and mtype =='rand' \").plot.box(backend='plotly',x='mrate',color='method',y='mse',facet_col='lags',facet_row='inter_method',height=800)"
  },
  {
    "objectID": "posts/2_research/2023-06-25-EvolveGCNO_Simulation_boxplot_reshape.html#block-4",
    "href": "posts/2_research/2023-06-25-EvolveGCNO_Simulation_boxplot_reshape.html#block-4",
    "title": "EvolveGCNO_Simulation_reshape",
    "section": "block",
    "text": "block\n\ndata.query(\"method!='GNAR' and mtype =='block' \").plot.box(backend='plotly',x='mrate',color='method',y='mse',facet_col='lags',facet_row='inter_method',height=800)"
  },
  {
    "objectID": "posts/2_research/2023-06-25-EvolveGCNO_Simulation_boxplot_reshape.html#baseline-5",
    "href": "posts/2_research/2023-06-25-EvolveGCNO_Simulation_boxplot_reshape.html#baseline-5",
    "title": "EvolveGCNO_Simulation_reshape",
    "section": "Baseline",
    "text": "Baseline\n\ndata.query(\"mrate==0 and method!='GNAR'\").plot.box(backend='plotly',x='mrate',color='method',y='mse',facet_col='lags',facet_row='nof_filters',height=600)"
  },
  {
    "objectID": "posts/2_research/2023-06-25-EvolveGCNO_Simulation_boxplot_reshape.html#random-5",
    "href": "posts/2_research/2023-06-25-EvolveGCNO_Simulation_boxplot_reshape.html#random-5",
    "title": "EvolveGCNO_Simulation_reshape",
    "section": "random",
    "text": "random\n\ndata.query(\"mtype=='rand' and mrate !=0 and method!='GNAR'\").plot.box(backend='plotly',x='mrate',color='method',y='mse',facet_col='lags',facet_row='inter_method',height=800)"
  },
  {
    "objectID": "posts/2_research/2023-06-25-EvolveGCNO_Simulation_boxplot_reshape.html#block-5",
    "href": "posts/2_research/2023-06-25-EvolveGCNO_Simulation_boxplot_reshape.html#block-5",
    "title": "EvolveGCNO_Simulation_reshape",
    "section": "block",
    "text": "block\n\ndata.query(\"mtype=='block' and method!='GNAR'\").plot.box(backend='plotly',x='mrate',color='method',y='mse',facet_col='lags',facet_row='nof_filters',height=1200)"
  },
  {
    "objectID": "posts/2_research/2023-01-26-guebin.html",
    "href": "posts/2_research/2023-01-26-guebin.html",
    "title": "Class of Method",
    "section": "",
    "text": "Class"
  },
  {
    "objectID": "posts/2_research/2023-01-26-guebin.html#시나리오1-baseline",
    "href": "posts/2_research/2023-01-26-guebin.html#시나리오1-baseline",
    "title": "Class of Method",
    "section": "시나리오1 (Baseline)",
    "text": "시나리오1 (Baseline)\n시나리오1\n\nmissing rate: 0%\n보간방법: None\n\n\nSTGCN 으로 적합 + 예측\n\nX = torch.tensor(fiveVTS_train).reshape(int(T*0.8),N,1).float()[:int(T*0.8-1),:,:]\ny = torch.tensor(fiveVTS_train).reshape(int(T*0.8),N,1).float()[1:,:,:]\n\n\nXX = torch.tensor(fiveVTS_test.reshape(int(T*0.2),N,1)[:-1,:,:]).float()\nyy = torch.tensor(fiveVTS_test.reshape(int(T*0.2),N,1)[1:,:,:]).float()\n\n\nnet = RecurrentGCN(node_features=1, filters=4)\noptimizer = torch.optim.Adam(net.parameters(), lr=0.01)\nnet.train()\nfor epoch in tqdm(range(50)):\n    for time, (xt,yt) in enumerate(zip(X,y)):\n        yt_hat = net(xt, edge_index, edge_attr)\n        cost = torch.mean((yt_hat-yt)**2)\n        cost.backward()\n        optimizer.step()\n        optimizer.zero_grad()\n\n100%|██████████| 50/50 [01:39&lt;00:00,  2.00s/it]\n\n\n\nyhat = torch.stack([net(xt, edge_index, edge_attr) for xt in X]).detach().numpy()\nyyhat = torch.stack([net(xt, edge_index, edge_attr) for xt in XX]).detach().numpy()\n\n\nstgcn_train = yhat.squeeze() # stgcn은 stgcn에 의한 적합결과를 의미함\nstgcn_test = yyhat.squeeze() \n\n\ntrain_mse_eachnode = (((y-yhat).squeeze())**2).mean(axis=0)\ntrain_mse_total = (((y-yhat).squeeze())**2).mean()\ntest_mse_eachnode = (((yy-yyhat).squeeze())**2).mean(axis=0)\ntest_mse_total = (((yy-yyhat).squeeze())**2).mean()\n\n\n\nGNAR 으로 적합 + 예측\n-\n\n\n결과시각화\n\nfig = plot(fiveVTS,'--.',h=4,color='gray',label='complete data',alpha=0.5)\nax = fig.get_axes()\nfor i,a in enumerate(ax):     \n    a.set_title('node{0} \\n mse(train) = {1:.2f}, mse(test) = {2:.2f}'.format(i,train_mse_eachnode[i],test_mse_eachnode[i]))\n    a.plot(range(1,160),stgcn_train[:,i],label='STCGCN (train)',color='C0')\n    a.plot(range(161,200),stgcn_test[:,i],label='STCGCN (test)',color='C1')\n    a.legend()\nfig.set_figwidth(14)\nfig.suptitle(\"Scenario1: STGCN \\n missing=0% \\n interpolation=None \\n mse(train) = {0:.2f}, mse(test) = {1:.2f} \\n\".format(train_mse_total,test_mse_total),size=15)\nfig.tight_layout()\nfig"
  },
  {
    "objectID": "posts/2_research/2023-01-26-guebin.html#시나리오2",
    "href": "posts/2_research/2023-01-26-guebin.html#시나리오2",
    "title": "Class of Method",
    "section": "시나리오2",
    "text": "시나리오2\n시나리오2\n\nmissing rate: 50%\n보간방법: linear\n\n- 결측치생성 + 보간\n\n_zero = Missing(fiveVTS_train)\n_zero.miss(percent = 0.5)\n_zero.second_linear()\n\n\nmissing_index = _zero.number\ninterpolated_signal = _zero.train_linear\n\n\nfig = plot(fiveVTS,'--o',h=4,color='gray',label='complete data',alpha=0.2)\nax = fig.get_axes()\nfor i,a in enumerate(ax):     \n    a.plot(missing_index[i],fiveVTS_train[:,i][missing_index[i]],'xk',label='missing')\n    a.plot(interpolated_signal[:,i],'-',color='gray',label='linear interpolation')\n    a.legend()\nfig.set_figwidth(15)\nfig\n\n\n\n\n\n\n\n\n\nSTGCN 으로 적합 + 예측\n\nX = torch.tensor(interpolated_signal).reshape(int(T*0.8),N,1).float()[:int(T*0.8-1),:,:]\ny = torch.tensor(interpolated_signal).reshape(int(T*0.8),N,1).float()[1:,:,:]\n\n\nXX = torch.tensor(fiveVTS_test.reshape(int(T*0.2),N,1)[:-1,:,:]).float()\nyy = torch.tensor(fiveVTS_test.reshape(int(T*0.2),N,1)[1:,:,:]).float()\n\n\nnet = RecurrentGCN(node_features=1, filters=4)\noptimizer = torch.optim.Adam(net.parameters(), lr=0.01)\nnet.train()\nfor epoch in tqdm(range(50)):\n    for time, (xt,yt) in enumerate(zip(X,y)):\n        yt_hat = net(xt, edge_index, edge_attr)\n        cost = torch.mean((yt_hat-yt)**2)\n        cost.backward()\n        optimizer.step()\n        optimizer.zero_grad()\n\n100%|██████████| 50/50 [01:31&lt;00:00,  1.82s/it]\n\n\n\nyhat = torch.stack([net(xt, edge_index, edge_attr) for xt in X]).detach().numpy()\nyyhat = torch.stack([net(xt, edge_index, edge_attr) for xt in XX]).detach().numpy()\n\n\nreal_y = torch.tensor(fiveVTS_train).reshape(int(T*0.8),N,1).float()[1:,:,:]\n\ntrain_mse_eachnode_stgcn = (((real_y-yhat).squeeze())**2).mean(axis=0)\ntrain_mse_total_stgcn = (((real_y-yhat).squeeze())**2).mean()\ntest_mse_eachnode_stgcn = (((yy-yyhat).squeeze())**2).mean(axis=0)\ntest_mse_total_stgcn = (((yy-yyhat).squeeze())**2).mean()\n\n\nstgcn_train = yhat.squeeze() # stgcn은 stgcn에 의한 적합결과를 의미함\nstgcn_test = yyhat.squeeze() \n\n\n\nESTGCN 으로 적합 + 예측\n\nX = torch.tensor(interpolated_signal).reshape(int(T*0.8),N,1).float()[:int(T*0.8-1),:,:]\ny = torch.tensor(interpolated_signal).reshape(int(T*0.8),N,1).float()[1:,:,:]\n\n\nXX = torch.tensor(fiveVTS_test.reshape(int(T*0.2),N,1)[:-1,:,:]).float()\nyy = torch.tensor(fiveVTS_test.reshape(int(T*0.2),N,1)[1:,:,:]).float()\n\n- ESTGCN\n\nnet = RecurrentGCN(node_features=1, filters=4)\noptimizer = torch.optim.Adam(net.parameters(), lr=0.01)\nnet.train()\nsignal = interpolated_signal.copy()\nfor epoch in tqdm(range(50)):\n    signal = update_from_freq_domain(signal,missing_index)\n    X = torch.tensor(signal).reshape(int(T*0.8),N,1).float()[:int(T*0.8-1),:,:]\n    y = torch.tensor(signal).reshape(int(T*0.8),N,1).float()[1:,:,:]\n    for time, (xt,yt) in enumerate(zip(X,y)):        \n        yt_hat = net(xt, edge_index, edge_attr)\n        cost = torch.mean((yt_hat-yt)**2)\n        cost.backward()\n        optimizer.step()\n        optimizer.zero_grad()\n    signal = torch.concat([X.squeeze(),yt_hat.detach().squeeze().reshape(1,-1)])        \n\n100%|██████████| 50/50 [01:29&lt;00:00,  1.80s/it]\n\n\n\nyhat = torch.stack([net(xt, edge_index, edge_attr) for xt in X]).detach().numpy()\nyyhat = torch.stack([net(xt, edge_index, edge_attr) for xt in XX]).detach().numpy()\n\n\nreal_y = torch.tensor(fiveVTS_train).reshape(int(T*0.8),N,1).float()[1:,:,:]\n\ntrain_mse_eachnode_estgcn = (((real_y-yhat).squeeze())**2).mean(axis=0)\ntrain_mse_total_estgcn = (((real_y-yhat).squeeze())**2).mean()\ntest_mse_eachnode_estgcn = (((yy-yyhat).squeeze())**2).mean(axis=0)\ntest_mse_total_estgcn = (((yy-yyhat).squeeze())**2).mean()\n\n\nestgcn_train = yhat.squeeze() # stgcn은 stgcn에 의한 적합결과를 의미함\nestgcn_test = yyhat.squeeze() \n\n\n\n결과시각화\n\nfig = plot(fiveVTS,'--.',h=4,color='gray',label='complete data',alpha=0.5)\nax = fig.get_axes()\nfor i,a in enumerate(ax):     \n    a.set_title('node{0} \\n STGCN: mse(train) = {1:.2f}, mse(test) = {2:.2f} \\n ESTGCN: mse(train) = {3:.2f}, mse(test) = {4:.2f}'.format(i,train_mse_eachnode_stgcn[i],test_mse_eachnode_stgcn[i],train_mse_eachnode_estgcn[i],test_mse_eachnode_estgcn[i]))\n    a.plot(missing_index[i],fiveVTS_train[:,i][missing_index[i]],'xk',label='missing')\n    a.plot(interpolated_signal[:,i],'-',color='gray',label='linear interpolation')\n    a.plot(range(1,160),stgcn_train.squeeze()[:,i],'--.',label='STCGCN (train)',color='C0')\n    a.plot(range(161,200),stgcn_test.squeeze()[:,i],'--.',label='STCGCN (test)',color='C0')\n    a.plot(range(1,160),estgcn_train.squeeze()[:,i],label='ESTCGCN (train)',color='C1')\n    a.plot(range(161,200),estgcn_test.squeeze()[:,i],label='ESTCGCN (test)',color='C1')\n    \n    a.legend()\nfig.set_figwidth(14)\nfig.suptitle(\"Scenario2: \\n missing=50% \\n interpolation=linear \\n\\n STGCN: mse(train) = {0:.2f}, mse(test) = {1:.2f} \\n ESTGCN: mse(train) = {2:.2f}, mse(test) = {3:.2f} \\n\".format(train_mse_total_stgcn,test_mse_total_stgcn,train_mse_total_estgcn,test_mse_total_estgcn),size=15)\nfig.tight_layout()\nfig"
  },
  {
    "objectID": "posts/2_research/2023-01-26-guebin.html#시나리오3",
    "href": "posts/2_research/2023-01-26-guebin.html#시나리오3",
    "title": "Class of Method",
    "section": "시나리오3",
    "text": "시나리오3\n시나리오3\n\nmissing rate: 80%\n보간방법: linear\n\n- 결측치생성 + 보간\n\n_zero = Missing(fiveVTS_train)\n_zero.miss(percent = 0.8)\n_zero.second_linear()\n\n\nmissing_index = _zero.number\ninterpolated_signal = _zero.train_linear\n\n\nfig = plot(fiveVTS,'--o',h=4,color='gray',label='complete data',alpha=0.2)\nax = fig.get_axes()\nfor i,a in enumerate(ax):     \n    a.plot(missing_index[i],fiveVTS_train[:,i][missing_index[i]],'xk',label='missing')\n    a.plot(interpolated_signal[:,i],'-',color='gray',label='linear interpolation')\n    a.legend()\nfig.set_figwidth(15)\nfig\n\n\n\n\n\n\n\n\n\nSTGCN 으로 적합 + 예측\n\nX = torch.tensor(interpolated_signal).reshape(int(T*0.8),N,1).float()[:int(T*0.8-1),:,:]\ny = torch.tensor(interpolated_signal).reshape(int(T*0.8),N,1).float()[1:,:,:]\n\n\nXX = torch.tensor(fiveVTS_test.reshape(int(T*0.2),N,1)[:-1,:,:]).float()\nyy = torch.tensor(fiveVTS_test.reshape(int(T*0.2),N,1)[1:,:,:]).float()\n\n\nnet = RecurrentGCN(node_features=1, filters=4)\noptimizer = torch.optim.Adam(net.parameters(), lr=0.01)\nnet.train()\nfor epoch in tqdm(range(50)):\n    for time, (xt,yt) in enumerate(zip(X,y)):\n        yt_hat = net(xt, edge_index, edge_attr)\n        cost = torch.mean((yt_hat-yt)**2)\n        cost.backward()\n        optimizer.step()\n        optimizer.zero_grad()\n\n100%|██████████| 50/50 [01:27&lt;00:00,  1.76s/it]\n\n\n\nyhat = torch.stack([net(xt, edge_index, edge_attr) for xt in X]).detach().numpy()\nyyhat = torch.stack([net(xt, edge_index, edge_attr) for xt in XX]).detach().numpy()\n\n\nreal_y = torch.tensor(fiveVTS_train).reshape(int(T*0.8),N,1).float()[1:,:,:]\n\ntrain_mse_eachnode_stgcn = (((real_y-yhat).squeeze())**2).mean(axis=0)\ntrain_mse_total_stgcn = (((real_y-yhat).squeeze())**2).mean()\ntest_mse_eachnode_stgcn = (((yy-yyhat).squeeze())**2).mean(axis=0)\ntest_mse_total_stgcn = (((yy-yyhat).squeeze())**2).mean()\n\n\nstgcn_train = yhat.squeeze() # stgcn은 stgcn에 의한 적합결과를 의미함\nstgcn_test = yyhat.squeeze() \n\n\n\nESTGCN 으로 적합 + 예측\n\nX = torch.tensor(interpolated_signal).reshape(int(T*0.8),N,1).float()[:int(T*0.8-1),:,:]\ny = torch.tensor(interpolated_signal).reshape(int(T*0.8),N,1).float()[1:,:,:]\n\n\nXX = torch.tensor(fiveVTS_test.reshape(int(T*0.2),N,1)[:-1,:,:]).float()\nyy = torch.tensor(fiveVTS_test.reshape(int(T*0.2),N,1)[1:,:,:]).float()\n\n- ESTGCN\n\nnet = RecurrentGCN(node_features=1, filters=4)\noptimizer = torch.optim.Adam(net.parameters(), lr=0.01)\nnet.train()\nsignal = interpolated_signal.copy()\nfor epoch in tqdm(range(50)):\n    signal = update_from_freq_domain(signal,missing_index)\n    X = torch.tensor(signal).reshape(int(T*0.8),N,1).float()[:int(T*0.8-1),:,:]\n    y = torch.tensor(signal).reshape(int(T*0.8),N,1).float()[1:,:,:]\n    for time, (xt,yt) in enumerate(zip(X,y)):        \n        yt_hat = net(xt, edge_index, edge_attr)\n        cost = torch.mean((yt_hat-yt)**2)\n        cost.backward()\n        optimizer.step()\n        optimizer.zero_grad()\n    signal = torch.concat([X.squeeze(),yt_hat.detach().squeeze().reshape(1,-1)])        \n\n100%|██████████| 50/50 [01:32&lt;00:00,  1.86s/it]\n\n\n\nyhat = torch.stack([net(xt, edge_index, edge_attr) for xt in X]).detach().numpy()\nyyhat = torch.stack([net(xt, edge_index, edge_attr) for xt in XX]).detach().numpy()\n\n\nreal_y = torch.tensor(fiveVTS_train).reshape(int(T*0.8),N,1).float()[1:,:,:]\n\ntrain_mse_eachnode_estgcn = (((real_y-yhat).squeeze())**2).mean(axis=0)\ntrain_mse_total_estgcn = (((real_y-yhat).squeeze())**2).mean()\ntest_mse_eachnode_estgcn = (((yy-yyhat).squeeze())**2).mean(axis=0)\ntest_mse_total_estgcn = (((yy-yyhat).squeeze())**2).mean()\n\n\nestgcn_train = yhat.squeeze() # stgcn은 stgcn에 의한 적합결과를 의미함\nestgcn_test = yyhat.squeeze() \n\n\n\n결과시각화\n\nfig = plot(fiveVTS,'--.',h=4,color='gray',label='complete data',alpha=0.5)\nax = fig.get_axes()\nfor i,a in enumerate(ax):     \n    a.set_title('node{0} \\n STGCN: mse(train) = {1:.2f}, mse(test) = {2:.2f} \\n ESTGCN: mse(train) = {3:.2f}, mse(test) = {4:.2f}'.format(i,train_mse_eachnode_stgcn[i],test_mse_eachnode_stgcn[i],train_mse_eachnode_estgcn[i],test_mse_eachnode_estgcn[i]))\n    a.plot(missing_index[i],fiveVTS_train[:,i][missing_index[i]],'xk',label='missing')\n    a.plot(interpolated_signal[:,i],'-',color='gray',label='linear interpolation')\n    a.plot(range(1,160),stgcn_train.squeeze()[:,i],'--.',label='STCGCN (train)',color='C0')\n    a.plot(range(161,200),stgcn_test.squeeze()[:,i],'--.',label='STCGCN (test)',color='C0')\n    a.plot(range(1,160),estgcn_train.squeeze()[:,i],label='ESTCGCN (train)',color='C1')\n    a.plot(range(161,200),estgcn_test.squeeze()[:,i],label='ESTCGCN (test)',color='C1')\n    \n    a.legend()\nfig.set_figwidth(14)\nfig.suptitle(\"Scenario3: \\n missing=80% \\n interpolation=linear \\n\\n STGCN: mse(train) = {0:.2f}, mse(test) = {1:.2f} \\n ESTGCN: mse(train) = {2:.2f}, mse(test) = {3:.2f} \\n\".format(train_mse_total_stgcn,test_mse_total_stgcn,train_mse_total_estgcn,test_mse_total_estgcn),size=15)\nfig.tight_layout()\nfig"
  },
  {
    "objectID": "posts/2_research/2023-05-27-AddingtheRecurrentGCNmodels.html",
    "href": "posts/2_research/2023-05-27-AddingtheRecurrentGCNmodels.html",
    "title": "Adding the RecurrentGCN models",
    "section": "",
    "text": "RecurrentGCN\n\n\nImport\n\nimport itstgcntry\nimport torch\nimport itstgcntry.planner \n\nTry\n\nimport pandas as pd\n\n\npd.read_csv('./simulation_results/2023-05-29_11-18-32.csv').groupby(['RecurrentGCN','method','mrate'])['mse'].mean().reset_index()\n\n\n\n\n\n\n\n\nRecurrentGCN\nmethod\nmrate\nmse\n\n\n\n\n0\nDCRNN\nIT-STGCN\n0.8\n1.517346\n\n\n1\nDCRNN\nSTGCN\n0.8\n1.521906\n\n\n2\nEvolveGCNH\nIT-STGCN\n0.8\n1.338225\n\n\n3\nEvolveGCNH\nSTGCN\n0.8\n1.337022\n\n\n4\nEvolveGCNO\nIT-STGCN\n0.8\n1.370455\n\n\n5\nEvolveGCNO\nSTGCN\n0.8\n1.326990\n\n\n6\nGCLSTM\nIT-STGCN\n0.8\n1.529536\n\n\n7\nGCLSTM\nSTGCN\n0.8\n1.600452\n\n\n8\nGConvGRU\nIT-STGCN\n0.8\n1.567661\n\n\n9\nGConvGRU\nSTGCN\n0.8\n1.654162\n\n\n10\nGConvLSTM\nIT-STGCN\n0.8\n1.494284\n\n\n11\nGConvLSTM\nSTGCN\n0.8\n1.633645\n\n\n12\nLRGCN\nIT-STGCN\n0.8\n1.512042\n\n\n13\nLRGCN\nSTGCN\n0.8\n1.604186\n\n\n14\nMPNNLSTM\nIT-STGCN\n0.8\n1.386678\n\n\n15\nMPNNLSTM\nSTGCN\n0.8\n1.351526\n\n\n16\nTGCN\nIT-STGCN\n0.8\n1.279269\n\n\n17\nTGCN\nSTGCN\n0.8\n1.278411\n\n\n\n\n\n\n\n\npd.read_csv('./simulation_results/2023-05-29_12-31-06.csv').groupby(['RecurrentGCN','method','mrate'])['mse'].mean().reset_index()\n\n\n\n\n\n\n\n\nRecurrentGCN\nmethod\nmrate\nmse\n\n\n\n\n0\nDCRNN\nIT-STGCN\n0.6\n1.457874\n\n\n1\nDCRNN\nSTGCN\n0.6\n1.523312\n\n\n2\nEvolveGCNH\nIT-STGCN\n0.6\n1.265695\n\n\n3\nEvolveGCNH\nSTGCN\n0.6\n1.290008\n\n\n4\nEvolveGCNO\nIT-STGCN\n0.6\n1.288295\n\n\n5\nEvolveGCNO\nSTGCN\n0.6\n1.308886\n\n\n6\nGCLSTM\nIT-STGCN\n0.6\n1.304539\n\n\n7\nGCLSTM\nSTGCN\n0.6\n1.537681\n\n\n8\nGConvGRU\nIT-STGCN\n0.6\n1.402607\n\n\n9\nGConvGRU\nSTGCN\n0.6\n1.582745\n\n\n10\nGConvLSTM\nIT-STGCN\n0.6\n1.299263\n\n\n11\nGConvLSTM\nSTGCN\n0.6\n1.534498\n\n\n12\nLRGCN\nIT-STGCN\n0.6\n1.295239\n\n\n13\nLRGCN\nSTGCN\n0.6\n1.527917\n\n\n14\nMPNNLSTM\nIT-STGCN\n0.6\n1.287649\n\n\n15\nMPNNLSTM\nSTGCN\n0.6\n1.307589\n\n\n16\nTGCN\nIT-STGCN\n0.6\n1.276318\n\n\n17\nTGCN\nSTGCN\n0.6\n1.262256\n\n\n\n\n\n\n\n\n\nGConvGRU\n\ndata_dict = itstgcntry.load_data('./data/fivenodes.pkl')\nloader = itstgcntry.DatasetLoader(data_dict)\n\n\ndataset = loader.get_dataset(lags=2)\ntrain_dataset, test_dataset = itstgcntry.temporal_signal_split(dataset, train_ratio=0.8)\n\n\nmindex = [list(range(10,100)),[],list(range(50,80)),[],[]]\ntrain_dataset_miss = itstgcntry.miss(train_dataset,mindex,mtype='block')\ntrain_dataset_padded = itstgcntry.padding(train_dataset_miss) # padding(train_dataset_miss,method='linear'와 같음)\n\n/home/csy/Dropbox/blog/posts/GCN/itstgcntry/utils.py:72: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at /opt/conda/conda-bld/pytorch_1682343998658/work/torch/csrc/utils/tensor_new.cpp:245.)\n  lags = torch.tensor(train_dataset.features).shape[-1]\n\n\n\nmindex = itstgcntry.rand_mindex(train_dataset,mrate=0.5)\ntrain_dataset_miss = itstgcntry.miss(train_dataset,mindex,mtype='rand')\ntrain_dataset_padded = itstgcntry.padding(train_dataset_miss) # padding(train_dataset_miss,method='linear'와 같음)\n\n- 학습\n\nlrnr = itstgcntry.StgcnLearner(train_dataset_padded)\n\n\nmodel = itstgcntry.GConvGRU_RecurrentGCN(train_dataset_padded,filters=1)\n\n\nlrnr.learn(model,epoch=5)\n\n5/5\n\n\n\nlrnr1 = itstgcntry.ITStgcnLearner(train_dataset_padded)\n\n\nmodel1 = itstgcntry.GConvGRU_RecurrentGCN(train_dataset_padded,filters=1)\n\n\nlrnr1.learn(model1,epoch=5)\n\n5/5\n\n\n- 적합값\n\n# lrnr(train_dataset_padded) \n# lrnr(test_dataset)['yhat'].shape\n\n\n실행하면 X,y,yhat 출력\n\n- 모형 평가 및 시각화\n\nevtor = itstgcntry.Evaluator(lrnr,train_dataset_padded,test_dataset)\n\n\nfig = evtor.plot('--.',h=5,max_node=3,label='complete data',alpha=0.5) # max_nodes 는 1보다 커야함\nfig.set_figwidth(12)\nfig.set_figheight(12)\nfig.tight_layout()\nfig\n\n\n\n\n\n\n\n\n\nevtor1 = itstgcntry.Evaluator(lrnr1,train_dataset_padded,test_dataset)\n\n\nfig = evtor1.plot('--.',h=5,max_node=3,label='complete data',alpha=0.5) # max_nodes 는 1보다 커야함\nfig.set_figwidth(12)\nfig.set_figheight(12)\nfig.tight_layout()\nfig\n\n\n\n\n\n\n\n\n\n\nDCRNN\n\ndata_dict = itstgcntry.load_data('./data/fivenodes.pkl')\nloader = itstgcntry.DatasetLoader(data_dict)\n\n\ndataset = loader.get_dataset(lags=2)\ntrain_dataset, test_dataset = itstgcntry.temporal_signal_split(dataset, train_ratio=0.8)\n\n\nmindex = [list(range(10,100)),[],list(range(50,80)),[],[]]\ntrain_dataset_miss = itstgcntry.miss(train_dataset,mindex,mtype='block')\ntrain_dataset_padded = itstgcntry.padding(train_dataset_miss) # padding(train_dataset_miss,method='linear'와 같음)\n\n\nmindex = itstgcntry.rand_mindex(train_dataset,mrate=0.5)\ntrain_dataset_miss = itstgcntry.miss(train_dataset,mindex,mtype='rand')\ntrain_dataset_padded = itstgcntry.padding(train_dataset_miss) # padding(train_dataset_miss,method='linear'와 같음)\n\n- 학습\n\nlrnr = itstgcntry.StgcnLearner(train_dataset_padded)\n\n\nmodel = itstgcntry.DCRNN_RecurrentGCN(train_dataset_padded,filters=1)\n\n\nlrnr.learn(model,epoch=50)\n\n50/50\n\n\n\nlrnr1 = itstgcntry.ITStgcnLearner(train_dataset_padded)\n\n\nmodel1 = itstgcntry.DCRNN_RecurrentGCN(train_dataset_padded,filters=1)\n\n\nlrnr1.learn(model1,epoch=50)\n\n50/50\n\n\n- 적합값\n\n# lrnr(train_dataset_padded) \n# lrnr(test_dataset)['yhat'].shape\n\n\n실행하면 X,y,yhat 출력\n\n- 모형 평가 및 시각화\n\nevtor = itstgcntry.Evaluator(lrnr,train_dataset_padded,test_dataset)\n\n\nfig = evtor.plot('--.',h=5,max_node=3,label='complete data',alpha=0.5) # max_nodes 는 1보다 커야함\nfig.set_figwidth(12)\nfig.set_figheight(12)\nfig.tight_layout()\nfig\n\n\n\n\n\n\n\n\n\nevtor1 = itstgcntry.Evaluator(lrnr1,train_dataset_padded,test_dataset)\n\n\nfig = evtor1.plot('--.',h=5,max_node=3,label='complete data',alpha=0.5) # max_nodes 는 1보다 커야함\nfig.set_figwidth(12)\nfig.set_figheight(12)\nfig.tight_layout()\nfig\n\n\n\n\n\n\n\n\n\n\nEvolveGCNH\n\ndata_dict = itstgcntry.load_data('./data/fivenodes.pkl')\nloader = itstgcntry.DatasetLoader(data_dict)\n\n\ndataset = loader.get_dataset(lags=2)\ntrain_dataset, test_dataset = itstgcntry.temporal_signal_split(dataset, train_ratio=0.8)\n\n\nmindex = [list(range(10,100)),[],list(range(50,80)),[],[]]\ntrain_dataset_miss = itstgcntry.miss(train_dataset,mindex,mtype='block')\ntrain_dataset_padded = itstgcntry.padding(train_dataset_miss) # padding(train_dataset_miss,method='linear'와 같음)\n\n\nmindex = itstgcntry.rand_mindex(train_dataset,mrate=0.5)\ntrain_dataset_miss = itstgcntry.miss(train_dataset,mindex,mtype='rand')\ntrain_dataset_padded = itstgcntry.padding(train_dataset_miss) # padding(train_dataset_miss,method='linear'와 같음)\n\n- 학습\n\nlrnr = itstgcntry.StgcnLearner(train_dataset_padded)\n\n\nmodel = itstgcntry.EvolveGCNH_RecurrentGCN(train_dataset_padded)\n\n\nlrnr.learn(model,epoch=50)\n\n50/50\n\n\n\nlrnr1 = itstgcntry.ITStgcnLearner(train_dataset_padded)\n\n\nmodel1 = itstgcntry.EvolveGCNH_RecurrentGCN(train_dataset_padded)\n\n\nlrnr1.learn(model1,epoch=50)\n\n50/50\n\n\n- 적합값\n\n# lrnr(train_dataset_padded) \n# lrnr(test_dataset)['yhat'].shape\n\n\n실행하면 X,y,yhat 출력\n\n- 모형 평가 및 시각화\n\nevtor = itstgcntry.Evaluator(lrnr,train_dataset_padded,test_dataset)\n\n\nfig = evtor.plot('--.',h=5,max_node=3,label='complete data',alpha=0.5) # max_nodes 는 1보다 커야함\nfig.set_figwidth(12)\nfig.set_figheight(12)\nfig.tight_layout()\nfig\n\n\n\n\n\n\n\n\n\nevtor1 = itstgcntry.Evaluator(lrnr1,train_dataset_padded,test_dataset)\n\n\nfig = evtor1.plot('--.',h=5,max_node=3,label='complete data',alpha=0.5) # max_nodes 는 1보다 커야함\nfig.set_figwidth(12)\nfig.set_figheight(12)\nfig.tight_layout()\nfig\n\n\n\n\n\n\n\n\n\n\nEvolveGCNO\n\ndata_dict = itstgcntry.load_data('./data/fivenodes.pkl')\nloader = itstgcntry.DatasetLoader(data_dict)\n\n\ndataset = loader.get_dataset(lags=2)\ntrain_dataset, test_dataset = itstgcntry.temporal_signal_split(dataset, train_ratio=0.8)\n\n\nmindex = [list(range(10,100)),[],list(range(50,80)),[],[]]\ntrain_dataset_miss = itstgcntry.miss(train_dataset,mindex,mtype='block')\ntrain_dataset_padded = itstgcntry.padding(train_dataset_miss) # padding(train_dataset_miss,method='linear'와 같음)\n\n\nmindex = itstgcntry.rand_mindex(train_dataset,mrate=0.5)\ntrain_dataset_miss = itstgcntry.miss(train_dataset,mindex,mtype='rand')\ntrain_dataset_padded = itstgcntry.padding(train_dataset_miss) # padding(train_dataset_miss,method='linear'와 같음)\n\n- 학습\n\nlrnr = itstgcntry.StgcnLearner(train_dataset_padded)\n\n\nmodel = itstgcntry.EvolveGCNO_RecurrentGCN(train_dataset_padded)\n\n\nlrnr.learn(model,epoch=50)\n\n50/50\n\n\n\nlrnr1 = itstgcntry.ITStgcnLearner(train_dataset_padded)\n\n\nmodel1 = itstgcntry.EvolveGCNO_RecurrentGCN(train_dataset_padded)\n\n\nlrnr1.learn(model1,epoch=50)\n\n50/50\n\n\n- 적합값\n\n# lrnr(train_dataset_padded) \n# lrnr(test_dataset)['yhat'].shape\n\n\n실행하면 X,y,yhat 출력\n\n- 모형 평가 및 시각화\n\nevtor = itstgcntry.Evaluator(lrnr,train_dataset_padded,test_dataset)\n\n\nfig = evtor.plot('--.',h=5,max_node=3,label='complete data',alpha=0.5) # max_nodes 는 1보다 커야함\nfig.set_figwidth(12)\nfig.set_figheight(12)\nfig.tight_layout()\nfig\n\n\n\n\n\n\n\n\n\nevtor1 = itstgcntry.Evaluator(lrnr1,train_dataset_padded,test_dataset)\n\n\nfig = evtor1.plot('--.',h=5,max_node=3,label='complete data',alpha=0.5) # max_nodes 는 1보다 커야함\nfig.set_figwidth(12)\nfig.set_figheight(12)\nfig.tight_layout()\nfig\n\n\n\n\n\n\n\n\n\n\nGCLSTM\n\ndata_dict = itstgcntry.load_data('./data/fivenodes.pkl')\nloader = itstgcntry.DatasetLoader(data_dict)\n\n\ndataset = loader.get_dataset(lags=2)\ntrain_dataset, test_dataset = itstgcntry.temporal_signal_split(dataset, train_ratio=0.8)\n\n\nmindex = [list(range(10,100)),[],list(range(50,80)),[],[]]\ntrain_dataset_miss = itstgcntry.miss(train_dataset,mindex,mtype='block')\ntrain_dataset_padded = itstgcntry.padding(train_dataset_miss) # padding(train_dataset_miss,method='linear'와 같음)\n\n\nmindex = itstgcntry.rand_mindex(train_dataset,mrate=0.5)\ntrain_dataset_miss = itstgcntry.miss(train_dataset,mindex,mtype='rand')\ntrain_dataset_padded = itstgcntry.padding(train_dataset_miss) # padding(train_dataset_miss,method='linear'와 같음)\n\n- 학습\n\nlrnr = itstgcntry.StgcnLearner(train_dataset_padded)\n\n\nmodel = itstgcntry.GCLSTM_RecurrentGCN(train_dataset_padded, filters=1)\n\n\nlrnr.learn(model,epoch=50)\n\n50/50\n\n\n\nlrnr1 = itstgcntry.ITStgcnLearner(train_dataset_padded)\n\n\nmodel1 = itstgcntry.GCLSTM_RecurrentGCN(train_dataset_padded, filters=1)\n\n\nlrnr1.learn(model1,epoch=50)\n\n50/50\n\n\n- 적합값\n\n# lrnr(train_dataset_padded) \n# lrnr(test_dataset)['yhat'].shape\n\n\n실행하면 X,y,yhat 출력\n\n- 모형 평가 및 시각화\n\nevtor = itstgcntry.Evaluator(lrnr,train_dataset_padded,test_dataset)\n\n\nfig = evtor.plot('--.',h=5,max_node=3,label='complete data',alpha=0.5) # max_nodes 는 1보다 커야함\nfig.set_figwidth(12)\nfig.set_figheight(12)\nfig.tight_layout()\nfig\n\n\n\n\n\n\n\n\n\nevtor1 = itstgcntry.Evaluator(lrnr1,train_dataset_padded,test_dataset)\n\n\nfig = evtor1.plot('--.',h=5,max_node=3,label='complete data',alpha=0.5) # max_nodes 는 1보다 커야함\nfig.set_figwidth(12)\nfig.set_figheight(12)\nfig.tight_layout()\nfig\n\n\n\n\n\n\n\n\n\n\nGConvLSTM\n\ndata_dict = itstgcntry.load_data('./data/fivenodes.pkl')\nloader = itstgcntry.DatasetLoader(data_dict)\n\n\ndataset = loader.get_dataset(lags=2)\ntrain_dataset, test_dataset = itstgcntry.temporal_signal_split(dataset, train_ratio=0.8)\n\n\nmindex = [list(range(10,100)),[],list(range(50,80)),[],[]]\ntrain_dataset_miss = itstgcntry.miss(train_dataset,mindex,mtype='block')\ntrain_dataset_padded = itstgcntry.padding(train_dataset_miss) # padding(train_dataset_miss,method='linear'와 같음)\n\n\nmindex = itstgcntry.rand_mindex(train_dataset,mrate=0.5)\ntrain_dataset_miss = itstgcntry.miss(train_dataset,mindex,mtype='rand')\ntrain_dataset_padded = itstgcntry.padding(train_dataset_miss) # padding(train_dataset_miss,method='linear'와 같음)\n\n- 학습\n\nlrnr = itstgcntry.StgcnLearner(train_dataset_padded)\n\n\nmodel = itstgcntry.GConvLSTM_RecurrentGCN(train_dataset_padded, filters=1)\n\n\nlrnr.learn(model,epoch=50)\n\n50/50\n\n\n\nlrnr1 = itstgcntry.ITStgcnLearner(train_dataset_padded)\n\n\nmodel1 = itstgcntry.GConvLSTM_RecurrentGCN(train_dataset_padded, filters=1)\n\n\nlrnr1.learn(model1,epoch=50)\n\n50/50\n\n\n- 적합값\n\n# lrnr(train_dataset_padded) \n# lrnr(test_dataset)['yhat'].shape\n\n\n실행하면 X,y,yhat 출력\n\n- 모형 평가 및 시각화\n\nevtor = itstgcntry.Evaluator(lrnr,train_dataset_padded,test_dataset)\n\n\nfig = evtor.plot('--.',h=5,max_node=3,label='complete data',alpha=0.5) # max_nodes 는 1보다 커야함\nfig.set_figwidth(12)\nfig.set_figheight(12)\nfig.tight_layout()\nfig\n\n\n\n\n\n\n\n\n\nevtor1 = itstgcntry.Evaluator(lrnr1,train_dataset_padded,test_dataset)\n\n\nfig = evtor1.plot('--.',h=5,max_node=3,label='complete data',alpha=0.5) # max_nodes 는 1보다 커야함\nfig.set_figwidth(12)\nfig.set_figheight(12)\nfig.tight_layout()\nfig\n\n\n\n\n\n\n\n\n\n\nLRGCN\n\ndata_dict = itstgcntry.load_data('./data/fivenodes.pkl')\nloader = itstgcntry.DatasetLoader(data_dict)\n\n\ndataset = loader.get_dataset(lags=2)\ntrain_dataset, test_dataset = itstgcntry.temporal_signal_split(dataset, train_ratio=0.8)\n\n\nmindex = [list(range(10,100)),[],list(range(50,80)),[],[]]\ntrain_dataset_miss = itstgcntry.miss(train_dataset,mindex,mtype='block')\ntrain_dataset_padded = itstgcntry.padding(train_dataset_miss) # padding(train_dataset_miss,method='linear'와 같음)\n\n\nmindex = itstgcntry.rand_mindex(train_dataset,mrate=0.5)\ntrain_dataset_miss = itstgcntry.miss(train_dataset,mindex,mtype='rand')\ntrain_dataset_padded = itstgcntry.padding(train_dataset_miss) # padding(train_dataset_miss,method='linear'와 같음)\n\n- 학습\n\nlrnr = itstgcntry.StgcnLearner(train_dataset_padded)\n\n\nmodel = itstgcntry.LRGCN_RecurrentGCN(train_dataset_padded, filters=1)\n\n\nlrnr.learn(model,epoch=50)\n\n50/50\n\n\n\nlrnr1 = itstgcntry.ITStgcnLearner(train_dataset_padded)\n\n\nmodel1 = itstgcntry.LRGCN_RecurrentGCN(train_dataset_padded, filters=1)\n\n\nlrnr1.learn(model1,epoch=50)\n\n50/50\n\n\n- 적합값\n\n# lrnr(train_dataset_padded) \n# lrnr(test_dataset)['yhat'].shape\n\n\n실행하면 X,y,yhat 출력\n\n- 모형 평가 및 시각화\n\nevtor = itstgcntry.Evaluator(lrnr,train_dataset_padded,test_dataset)\n\n\nfig = evtor.plot('--.',h=5,max_node=3,label='complete data',alpha=0.5) # max_nodes 는 1보다 커야함\nfig.set_figwidth(12)\nfig.set_figheight(12)\nfig.tight_layout()\nfig\n\n\n\n\n\n\n\n\n\nevtor1 = itstgcntry.Evaluator(lrnr1,train_dataset_padded,test_dataset)\n\n\nfig = evtor1.plot('--.',h=5,max_node=3,label='complete data',alpha=0.5) # max_nodes 는 1보다 커야함\nfig.set_figwidth(12)\nfig.set_figheight(12)\nfig.tight_layout()\nfig\n\n\n\n\n\n\n\n\n\n\nMPNNLSTM\n\ndata_dict = itstgcntry.load_data('./data/fivenodes.pkl')\nloader = itstgcntry.DatasetLoader(data_dict)\n\n\ndataset = loader.get_dataset(lags=2)\ntrain_dataset, test_dataset = itstgcntry.temporal_signal_split(dataset, train_ratio=0.8)\n\n\nmindex = [list(range(10,100)),[],list(range(50,80)),[],[]]\ntrain_dataset_miss = itstgcntry.miss(train_dataset,mindex,mtype='block')\ntrain_dataset_padded = itstgcntry.padding(train_dataset_miss) # padding(train_dataset_miss,method='linear'와 같음)\n\n\nmindex = itstgcntry.rand_mindex(train_dataset,mrate=0.5)\ntrain_dataset_miss = itstgcntry.miss(train_dataset,mindex,mtype='rand')\ntrain_dataset_padded = itstgcntry.padding(train_dataset_miss) # padding(train_dataset_miss,method='linear'와 같음)\n\n- 학습\n\nlrnr = itstgcntry.StgcnLearner(train_dataset_padded)\n\n\nmodel = itstgcntry.MPNNLSTM_RecurrentGCN(train_dataset_padded, filters=1)\n\n\nlrnr.learn(model,epoch=50)\n\n50/50\n\n\n\nlrnr1 = itstgcntry.ITStgcnLearner(train_dataset_padded)\n\n\nmodel1 = itstgcntry.MPNNLSTM_RecurrentGCN(train_dataset_padded, filters=1)\n\n\nlrnr1.learn(model1,epoch=50)\n\n50/50\n\n\n- 적합값\n\n# lrnr(train_dataset_padded) \n# lrnr(test_dataset)['yhat'].shape\n\n\n실행하면 X,y,yhat 출력\n\n- 모형 평가 및 시각화\n\nevtor = itstgcntry.Evaluator(lrnr,train_dataset_padded,test_dataset)\n\n\nfig = evtor.plot('--.',h=5,max_node=3,label='complete data',alpha=0.5) # max_nodes 는 1보다 커야함\nfig.set_figwidth(12)\nfig.set_figheight(12)\nfig.tight_layout()\nfig\n\n\n\n\n\n\n\n\n\nevtor1 = itstgcntry.Evaluator(lrnr1,train_dataset_padded,test_dataset)\n\n\nfig = evtor1.plot('--.',h=5,max_node=3,label='complete data',alpha=0.5) # max_nodes 는 1보다 커야함\nfig.set_figwidth(12)\nfig.set_figheight(12)\nfig.tight_layout()\nfig\n\n\n\n\n\n\n\n\n\n\nTGCN\n\ndata_dict = itstgcntry.load_data('./data/fivenodes.pkl')\nloader = itstgcntry.DatasetLoader(data_dict)\n\n\ndataset = loader.get_dataset(lags=2)\ntrain_dataset, test_dataset = itstgcntry.temporal_signal_split(dataset, train_ratio=0.8)\n\n\nmindex = [list(range(10,100)),[],list(range(50,80)),[],[]]\ntrain_dataset_miss = itstgcntry.miss(train_dataset,mindex,mtype='block')\ntrain_dataset_padded = itstgcntry.padding(train_dataset_miss) # padding(train_dataset_miss,method='linear'와 같음)\n\n\nmindex = itstgcntry.rand_mindex(train_dataset,mrate=0.5)\ntrain_dataset_miss = itstgcntry.miss(train_dataset,mindex,mtype='rand')\ntrain_dataset_padded = itstgcntry.padding(train_dataset_miss) # padding(train_dataset_miss,method='linear'와 같음)\n\n- 학습\n\nlrnr = itstgcntry.StgcnLearner(train_dataset_padded)\n\n\nmodel = itstgcntry.TGCN_RecurrentGCN(train_dataset_padded, filters=32)\n\n\nlrnr.learn(model,epoch=50)\n\n50/50\n\n\n\nlrnr1 = itstgcntry.ITStgcnLearner(train_dataset_padded)\n\n\nmodel1 = itstgcntry.TGCN_RecurrentGCN(train_dataset_padded, filters=32)\n\n\nlrnr1.learn(model1,epoch=50)\n\n50/50\n\n\n- 적합값\n\n# lrnr(train_dataset_padded) \n# lrnr(test_dataset)['yhat'].shape\n\n\n실행하면 X,y,yhat 출력\n\n- 모형 평가 및 시각화\n\nevtor = itstgcntry.Evaluator(lrnr,train_dataset_padded,test_dataset)\n\n\nfig = evtor.plot('--.',h=5,max_node=3,label='complete data',alpha=0.5) # max_nodes 는 1보다 커야함\nfig.set_figwidth(12)\nfig.set_figheight(12)\nfig.tight_layout()\nfig\n\n\n\n\n\n\n\n\n\nevtor1 = itstgcntry.Evaluator(lrnr1,train_dataset_padded,test_dataset)\n\n\nfig = evtor1.plot('--.',h=5,max_node=3,label='complete data',alpha=0.5) # max_nodes 는 1보다 커야함\nfig.set_figwidth(12)\nfig.set_figheight(12)\nfig.tight_layout()\nfig"
  },
  {
    "objectID": "posts/3_result/3_table/2024-01-08-Experiments_section.html",
    "href": "posts/3_result/3_table/2024-01-08-Experiments_section.html",
    "title": "Research Sections of Experiments",
    "section": "",
    "text": "https://arxiv.org/pdf/1609.02907.pdf%EF%BC%89\nRelated work\ngcn은 cnn이 graph로 효율적으로 확장된 분야로 증명되어 왔다.\n그래프 도메인에서 , 특히 입력 데이터가 시간 변수인 것을 처리하기 위해 동적 그래프의 개념이 사용되었으며, 이 그래프는 시간에 따라 정보가 변하기 때문에 시간과 공간의 정보를 캡쳐하여 사용한다면 효과적일 것이다.\n이에 dcrnn과 stgcn,st-gcn이 제안되었고, 이러한 방법이다.\nst data와 관련하여 method가 제안될때 다양한 방법으로 irregular에서 regular로 변환하여 사용하려는 시도가 되어져 왔고 대부분 single imputation하는 방법을 사용했다. (simple handle 용어 모호)\n하지만 이는 데이터를 예측하거나 분류할때 영향을 끼쳐 낮은 accuracy로 이끌 수 있다.\n이를 개선할 수 있는 방법이 제안된다면 기존에 제안된 st data에 사용된 방법들의 더 정확한 예측값을 얻는 등 효과적인 결과를 이끌 수 있을 것이다.\n이에 우리 연구에서는 missing 값을 처리 후 데이터의 normal trend를 활용하여 self consistecy 속성을 이용해 데이터의 예측 정확도를 키울 수 있는 접근 방식을 제안한다.\n데이터 섹션에 추가할 것 train/test 비율 - The validation set is comprised of the final 20% of the training data.\nimport plotly.io as pio\npd.options.plotting.backend = \"plotly\"\npio.templates.default = \"plotly_white\"\n\nNameError: name 'pd' is not defined"
  },
  {
    "objectID": "posts/3_result/3_table/2024-01-08-Experiments_section.html#trend",
    "href": "posts/3_result/3_table/2024-01-08-Experiments_section.html#trend",
    "title": "Research Sections of Experiments",
    "section": "1.1 Trend",
    "text": "1.1 Trend\n\n실험 결과에 대한 첫번 째 접근은 다음과 같다. Figure 1의 결과는 각 결측값 비율에 따라 정리한 데이터셋에 대한 결과이다. Classic과 Proposed는 결측값 비율이 점점 증가하면서 평균제곱오차도 함께 증가하는 경향을 보였다. 특히, Classic과 Proposed 각각에 대한 trendline을 비교해본 결과, 결측값 비율이 증가할 수록 Classic에 비해 우리가 제안한 방법((Proposed)의 평균제곱오차가 느리게 증가하는 경향을 띄었다. 반면, Classic 모델은 오히려 빠르게 증가하는 경향을 띄었다. 결측값 비율이 커질수록 우리가 제안하는 방법이 비교적 낮은 오차 내에서 값을 예측해낸다는 것을 알 수 있었다.\nThe initial approach to the experimental results is as follows. Figure \\(\\ref{}\\) illustrates the outcomes for six datasets organized based on varying levels of missing data, with nine models employed. As the missing data rates increase, both Classic and Proposed models exhibit a tendency for the mean squared error (MSE) to rise. Particularly noteworthy is the comparison between the trendlines of Classic and Proposed: as the rate of missing values increases, the MSE of the Proposed method tends to increase more gradually compared to Classic. In contrast, the Classic model displays a more rapid increase in MSE. This comparison suggests that as the proportion of missing values grows, our proposed method (Proposed) tends to predict values with a slower increase in MSE compared to the Classic model. It becomes evident that as the missing data rate becomes higher, our proposed method performs relatively well, providing predictions with lower errors compared to the Classic model.\n\n\nz# tidydata = pd.concat([df.query('dataset==\"fivenodes\" and model==\"GConvGRU\" and mtype==\"rand\"')])\n# tidydata['mrate'] = tidydata['mrate'].astype(str)\n# tidydata = tidydata.sort_values(by=['model','mrate'])\n\n# fig = px.box(tidydata,x='mrate',y='mse',color='method',width=70, log_y=True)\n\n\n# fig.layout['xaxis']['title']['text']='Missing Rate'\n# fig.layout['yaxis']['title']['text']='Mean Square Error'\n\n# fig.data[0]['marker']['color'] = 'blue'\n# fig.data[0]['name'] = 'Classic'\n\n# fig.data[1]['marker']['color'] = 'red'\n# fig.data[1]['name'] = 'Proposed'\n\n# fig.layout['legend']['title']=''\n\n# fig.update_layout(template=\"seaborn\")\n# fig.update_layout(title_text=\"Randomly Missing Values on FiveVTS\")\n# fig.update_layout(height=600, width=1800)\n\n# fig.add_trace(go.Scatter(x=tidydata.query('method==\"STGCN\"').mrate.unique(), y=(tidydata.query('method==\"STGCN\"')).groupby('mrate')['mse'].mean(), \n#                          mode=\"lines\", name=\"trendline of Classic\", line=dict(color=\"blue\",width=2)))\n# fig.add_trace(go.Scatter(x=tidydata.query('method==\"STGCN\"').mrate.unique(), y=(tidydata.query('method!=\"STGCN\"')).groupby('mrate')['mse'].mean(), \n#                          mode=\"lines\", name=\"trendline of Classic\", line=dict(color=\"red\",width=2)))\n\n# fig.update_layout(legend=dict(\n#     yanchor=\"top\",\n#     y=0.99,\n#     xanchor=\"left\",\n#     x=0.01\n# ))\n\n# fig\n\nNameError: name 'z' is not defined"
  },
  {
    "objectID": "posts/3_result/3_table/2024-01-08-Experiments_section.html#section",
    "href": "posts/3_result/3_table/2024-01-08-Experiments_section.html#section",
    "title": "Research Sections of Experiments",
    "section": "1.2",
    "text": "1.2\n1의 내용에 대한 확장으로. FiveVTS 데이터 셋에서 GConvGRU 방법 이외에 시도한 다른 Classic 모델들에 대한 결과이다. GConvGRU, GConvLSTM, GCLSTM, LRGCN, DyGrEncoder, EvolveGCNH, EvolveGCNO, TGCN, DCRNN 모델의 결과이며. 첫번째 섹션에서 제시한 결과와 비슷하게 결측값 비율이 증가할 수록 예측값에 대한 오차가 증가하였으며. Proposed가 더 낮은 값을 띄는 경향을 보인 것을 확인할 수 있었다. 다른 데이터 셋에 대한 결과도 결측값이 증가할 수록 오차가 증가하되, 우리가 제안한 방법에 대한 mse값이 낮은 비슷한 경향을 보였으며, 나머지 모델에서 나온 6개의 데이터 셋에 대한 결과는 부록에서 확인할 수 있다.\n\n# tidydata = pd.DataFrame(df.query(\"dataset=='fivenodes' and mtype=='rand'\"))\n\n# tidydata['model'] = pd.Categorical(tidydata['model'], categories=[\"GConvGRU\", \"GConvLSTM\", \"GCLSTM\", \"LRGCN\", \"DyGrEncoder\", \"EvolveGCNH\", \"EvolveGCNO\", \"TGCN\", \"DCRNN\"])\n# tidydata['method'] = pd.Categorical(tidydata['method'], categories=['STGCN', 'IT-STGCN'])\n\n# tidydata['mrate'] = tidydata['mrate'].astype(str)\n# tidydata = tidydata.sort_values(by=['model','mrate'])\n\n# fig = px.box(tidydata,x='mrate',y='mse',color='method',log_y=True,facet_col='model',facet_col_wrap=3)\n\n# fig.layout['xaxis']['title']['text']=''\n# fig.layout['xaxis2']['title']['text']='Missing Rate'\n# fig.layout['xaxis3']['title']['text']=''\n# fig.layout['yaxis']['title']['text']=''\n# fig.layout['yaxis4']['title']['text']='MSE(log scale)'\n# fig.layout['yaxis7']['title']['text']=''\n# fig.layout['legend']['title']='Method'\n\n# fig.layout.xaxis4.showticklabels=True\n# fig.layout.xaxis5.showticklabels=True\n# fig.layout.xaxis6.showticklabels=True\n# fig.layout.xaxis7.showticklabels=True\n# fig.layout.xaxis8.showticklabels=True\n# fig.layout.xaxis9.showticklabels=True\n\n# fig.layout.yaxis2.showticklabels=True\n# fig.layout.yaxis3.showticklabels=True\n# fig.layout.yaxis5.showticklabels=True\n# fig.layout.yaxis6.showticklabels=True\n# fig.layout.yaxis8.showticklabels=True\n# fig.layout.yaxis9.showticklabels=True\n\n# fig.layout['legend']['title']=''\n\n# for i in range(0, 9):\n#     fig.data[i]['marker']['color'] = 'blue'\n#     fig.data[i]['name'] = 'Classic'\n# for i in range(9, 18):    \n#     fig.data[i]['marker']['color'] = 'red'\n#     fig.data[i]['name'] = 'Proposed'\n\n# fig.update_layout(legend=dict(x=1, y=1, traceorder='normal', orientation='v'))\n# fig.update_layout(template=\"seaborn\")\n# fig.update_layout(title_text=\"Models on FiveVTS\")\n# fig.update_layout(height=1200, width=1900)\n\n# fig.for_each_annotation(lambda a: a.update(text=a.text.split(\"=\")[-1]))\n# fig.update_layout(legend=dict(\n#     yanchor=\"top\",\n#     y=0.99,\n#     xanchor=\"left\",\n#     x=0.01\n# ))\n\n# fig\n# # with open('fivenodes_fig.pkl', 'wb') as file:\n# #     pickle.dump(fig, file)\n# # with open('fivenodes_fig.pkl', 'rb') as file:\n# #     fivenodes_fig = pickle.load(file)\n\n# # fivenodes_fig"
  },
  {
    "objectID": "posts/3_result/3_table/2024-01-08-Experiments_section.html#six-datasets",
    "href": "posts/3_result/3_table/2024-01-08-Experiments_section.html#six-datasets",
    "title": "Research Sections of Experiments",
    "section": "1.3 Six Datasets",
    "text": "1.3 Six Datasets\n\n이번 섹션에서는 데이터의 다양성을 고려하기 위하여 사용된 총 6개의 데이터에 대해 결측값 비율이 높을때(70% 혹은 80%) GConvGRU 모델에서 예측과 제안된 방법의 비교 결과를 제시하였다. 대부분의 데이터셋의 결과는 Classic 모델보다 Proposed 결과의 mse 값이 낮게 분포되어 있음을 확인하였다.\n\n\n# tidydata = pd.concat([df[(df['dataset']=='fivenodes') & (df['mtype']=='rand') & (df['inter_method']=='linear') & (df['nof_filters']==12) & \n#            (df['lags']==2) & (df['epoch']==50) & (df['model']=='GConvGRU') & (df['mrate'].isin([0.8]))],\n#         df.query(\"dataset=='chickenpox' & mtype=='rand' & inter_method == 'linear' & nof_filters==16 & lags==4 & epoch==50 & model=='GConvGRU' & (mrate == 0.8)\"),\n#           df.query(\"dataset=='pedalme' & mtype=='rand' & inter_method == 'nearest' & nof_filters==12 & lags==4 & epoch==50 & model=='GConvGRU' & (mrate == 0.8)\"),\n#           df.query(\"dataset=='wikimath' & mtype=='rand' & inter_method == 'linear' & nof_filters==12 & lags==8 & epoch==50 & model=='GConvGRU' & (mrate == 0.8)\"),\n#           df.query(\"dataset=='windmillsmall' & mtype=='rand' & inter_method == 'linear' & nof_filters==12 & lags==8 & epoch==50 & model=='GConvGRU' & (mrate == 0.7)\"),\n#           df.query(\"dataset=='monte' & mtype=='rand' & inter_method == 'nearest' & nof_filters==12 & lags==4 & epoch==50 & model=='GConvGRU' & (mrate == 0.8)\")])\n\n# # tidydata = df.query(\"dataset=='fivenodes' and mtype=='rand' and mrate in [0.7,0.8]\")\n# tidydata['model'] = pd.Categorical(tidydata['model'], categories=[\"GConvGRU\", \"GConvLSTM\", \"GCLSTM\", \"LRGCN\", \"DyGrEncoder\", \"EvolveGCNH\", \"EvolveGCNO\", \"TGCN\", \"DCRNN\"])\n# categories=[\"fivenodes\", \"chickenpox\", \"pedalme\", \"wikimath\", \"windmillsmall\", \"monte\"]\n# tidydata['method'] = pd.Categorical(tidydata['method'], categories=['STGCN', 'IT-STGCN'])\n\n# tidydata = tidydata.sort_values(by='dataset', key=lambda x: x.map({dataset: i for i, dataset in enumerate(categories)}))\n\n# fig = px.strip(tidydata,x='dataset',y='mse',color='method', log_y=True,stripmode='overlay')\n\n# fig.layout['xaxis']['title']['text']='Missing Rate'\n# fig.layout['yaxis']['title']['text']='MSE(log scale)'\n\n# # fig.layout.annotations[0].text = 'FiveVTS'\n# # fig.layout.annotations[1].text = 'Chickenpox'\n# # fig.layout.annotations[2].text = 'Pedalme'\n# # fig.layout.annotations[3].text = 'Wikimath'\n# # fig.layout.annotations[4].text = 'Windmillsmall'\n# # fig.layout.annotations[5].text = 'MontevideoBus'\n\n# # fig.layout['legend']['title']=''\n\n# fig.data[0]['marker']['color'] = 'blue'\n# fig.data[0]['name'] = 'Classic'\n# fig.data[1]['marker']['color'] = 'red'\n# fig.data[1]['name'] = 'Proposed'\n\n# fig.update_layout(template=\"seaborn\")\n# fig.update_layout(title_text=\"GConvGRU on datasets\")\n# # fig.update_yaxes(matches=None)\n# fig.update_layout(height=800, width=1900)\n# fig.update_layout(legend=dict(\n#     yanchor=\"top\",\n#     y=0.99,\n#     xanchor=\"left\",\n#     x=0.01\n#  ))\n# fig\n\n\nDCRNN 모델의 wikimath 데이터에서 결측값 비율이 낮을 때 대비 높을 때 비교\n\n결측값 비율이 커지니 차이가 커진 mse 값 분포\n내용\n\nwikimath 데이터셋에서 DCRNN 결과에 대하여 결측값 비율이 작은 경우(30%) 대비 큰 경(80%)에 대하여 비교해보았다. wikimath 데이터셋에서 결측값을 주지 않았을떄, mse 값은 0.936±0.002(mean±sd)와 같았다. figure에서 보다시피, 결측값 비율이 30%으로 작을 때는 오차값이 작게 증가하였지만, 결측값의 비율이 80%로 높아졌을떄는 mse값이 확연히 높아진 것을 확인할 수 있었으며, 특히 classic 보다 proposed 의 값이 낮은 모습도 확인할 수 있었다.\n\n\n\n# # tidydata = pd.concat([df[(df['dataset']=='fivenodes') & (df['mtype']=='rand') & (df['inter_method']=='linear') & (df['nof_filters']==12) & \n# #            (df['lags']==2) & (df['epoch']==50) & (df['model']=='GConvGRU') & (df['mrate'].isin([0.3,0.8]))]])\n# tidydata = df.query(\"model=='DCRNN' and dataset=='wikimath' and mrate in [0.3,0.8]\")\n# tidydata['method'] = pd.Categorical(tidydata['method'], categories=['STGCN', 'IT-STGCN'])\n\n# tidydata['mrate'] = tidydata['mrate'].astype(str)\n# tidydata = tidydata.sort_values(by=['model','mrate'])\n\n# fig = px.box(tidydata,x='mrate',y='mse',width=10, points='all',log_y=True,facet_col='method')\n\n# fig.layout['xaxis2']['title']['text']='Missing Rate'\n# fig.layout['xaxis']['title']['text']='Missing Rate'\n# fig.layout['yaxis']['title']['text']='MSE(log scale)'\n\n# fig.data[0]['marker']['color'] = 'blue'\n# fig.data[1]['marker']['color'] = 'red'\n# fig.layout['legend']['title']=''\n\n# fig.layout.annotations[0].text = 'Classic'\n# fig.layout.annotations[1].text = 'Proposed'\n\n# fig.update_layout(template=\"seaborn\")\n# fig.update_layout(title_text=\"DCRNN on Wikimath\")\n# fig.update_layout(height=1000, width=1500)\n# fig.update_layout(legend=dict(\n#     yanchor=\"top\",\n#     y=0.99,\n#     xanchor=\"left\",\n#     x=0.01\n#  ))\n\n# fig\n\n\n\nfig = df.query(\"mtype=='rand'\").query(\"dataset != 'windmillsmall'\")\\\n.groupby([\"method\",\"dataset\",\"mrate\",\"model\"]).agg({'mse':'mean'}).reset_index()\\\n.plot.line(\n    x='mrate',\n    y='mse',\n    color='method',\n    facet_row='model',\n    facet_col='dataset',\n    width=1200,\n    height=1500,\n    \n)\n\nfor scatter in fig.data:\n    scatter['mode'] = 'lines+markers'\n    scatter['line']['dash'] = 'dashdot'\nfor annotation in fig.layout['annotations']:\n    annotation['text'] = annotation['text'].replace('dataset=','')\n    annotation['text'] = annotation['text'].replace('model=','')\nfor k in [k for k in fig.layout if 'xaxis' in k]:\n    fig.layout[k]['title']['text'] = None \nfor k in [k for k in fig.layout if 'yaxis' in k]:\n    fig.layout[k]['title']['text'] = None \nfig.update_yaxes(showticklabels=True,matches=None)\nfig.update_xaxes(showticklabels=True,matches=None)\n\nNameError: name 'df' is not defined\n\n\n\nbig = df.query(\"mtype=='rand'\").query(\"dataset == 'wikimath'\").query(\"model == 'GConvLSTM'\").sort_values(by='mrate')\\\n.assign(mrate_jittered = lambda df: np.array(df['mrate'])+np.random.randn(len(df['mrate']))*0.01)\nsmall = big.groupby([\"dataset\",\"mrate\",\"method\"]).agg({'mse':'mean'}).reset_index().rename({'mse':'mse_mean'},axis=1)\ntidydata = big.merge(small)\ntidydata['mrate'] = tidydata['mrate'].astype(str)\nfig = px.scatter(\n    tidydata,\n    y='mse',\n    x='mrate_jittered',\n    opacity=0.3,\n    color='method',\n    width=750,\n    height=500,\n)\n_fig1 = px.scatter(\n    tidydata,\n    y='mse_mean',\n    x='mrate',\n    color='method',\n)\n_fig1.data[0]['mode']='markers+lines'\n_fig1.data[0]['marker']['size'] = 10\n_fig1.data[0]['line']['width'] = 3\n_fig1.data[0]['line']['dash'] = 'dashdot'\n_fig1.data[1]['mode']='markers+lines'\n_fig1.data[1]['marker']['size'] = 10\n_fig1.data[1]['line']['width'] = 3\n_fig1.data[1]['line']['dash'] = 'dashdot'\n_fig2 = px.violin(\n    tidydata.query(\"method=='STGCN'\"),\n    y='mse',\n    x='mrate',\n)\n_fig2.data[0]['opacity']=0.7\n_fig2.data[0]['marker']['color']='#636efa'\n_fig3 = px.violin(\n    tidydata.query(\"method=='IT-STGCN'\"),\n    y='mse',\n    x='mrate',\n)\n_fig3.data[0]['opacity']=0.7\n_fig3.data[0]['marker']['color']='#EF553B'\n_fig3\nfor g in _fig1.data:\n    fig.add_trace(g)\nfor g in _fig2.data:\n    fig.add_trace(g)\nfor g in _fig3.data:\n    fig.add_trace(g)\nfig.data[0]['showlegend'] =False\nfig.data[1]['showlegend'] =False\nfig.layout['xaxis']['title']['text']='Missign Rate'\nfig.layout['yaxis']['title']['text']='MSE'\nfig.layout['legend']['title']['text']=\"\"\nfig.layout['title']['text']='Wikimath/GConvLSTM'\nfig\n\nNameError: name 'df' is not defined"
  },
  {
    "objectID": "posts/3_result/3_table/2024-01-08-Experiments_section.html#time",
    "href": "posts/3_result/3_table/2024-01-08-Experiments_section.html#time",
    "title": "Research Sections of Experiments",
    "section": "2.1 Time",
    "text": "2.1 Time\n\ndf_dataset = pd.DataFrame([[i,0,0] for i in set(df.dataset)])\ndf_dataset.columns = ['dataset','node','time']\n\ndataset_values = {\n    'fivenodes': {'time': 200, 'node': 5},\n    'chickenpox': {'time': 522, 'node': 20},\n    'pedalme': {'time': 36, 'node': 15},\n    'wikimath': {'time': 731, 'node': 1068},\n    'windmillsmall': {'time': 17472, 'node': 11},\n    'monte': {'time': 744, 'node': 675},\n}\n\nfor dataset, values in dataset_values.items():\n    df_dataset.loc[df_dataset['dataset'] == dataset, ['time', 'node']] = values.values()\n\ndf_dataset\n\ndf_dataset\n\nNameError: name 'pd' is not defined\n\n\ntime이 적은 pedalme 데이터와 time이 많은 windmillsmall 데이터의 결측값 비율이 높을 때(70% 혹은 80%) 비교\n\n내용\n\n이 섹션에서는 train 데이터 양에 따라 학습 결과를 논의해보고자 한다. 데이터가 적은 pedalme(node=15, time=36) 데이터는 proposed method 가 dramatic한 낮은 error값을 보이지 않았다. 반면에 time이 긴 windmillsmall(node=11, time=17472)은 proposed method가 결측값이 많을 때 mse 값이 낮게 나온 모습을 볼 수 있었다. underline function을 추정하여 true 겂을 따라가려는 우리 방법론이 효율적이기 위해서 데이터 양이 많을 경우가 좋다. time이 길거나(예를 들어 오랜 기간 어떤 기계의 센서에서 측정된 값) node가 많은(측정 지점이 많은) 경우가 실제 시공간 데이터의 경우 많을 것을 기대하기 떄문에, proposed의 효과가 있을 경우가 존재할 것이다.\nAccording to the table (dataset explanation), the Time for the ‘Pedalme’ dataset is 36, while the time for the ‘Windmillsmall’ dataset is 17,472. Thus, the two datasets have different amounts of data. We compared between two datasets, and the result is on Figure 555. The variations observed in Figure 555 for each dataset indicate that the disparity between methods is relatively small for the ‘Pedalme’ dataset, whereas the difference is more pronounced for the ‘Windmillsmall’ dataset.This phenomenon arises as the increasing quantity of data facilitates learning patterns and trends. Consequently, our approach signifies its effectiveness when there is a substantial amount of data in the time aspect.\n\n\n\n# pedalme node = 15 time = 36\n# windmill node = 11 time = 17,472\n\ntidydata = pd.concat([\n          df.query(\"dataset=='pedalme' & mtype=='rand'  & model=='GCLSTM' & (mrate == 0.8)\"),\n          df.query(\"dataset=='windmillsmall' & mtype=='rand' &  model=='GCLSTM' & (mrate == 0.7)\")])\n\ntidydata['method'] = pd.Categorical(tidydata['method'], categories=['STGCN', 'IT-STGCN'])\n\ntidydata = tidydata.sort_values(by=['dataset','model','mrate'])\n\nfig = px.box(tidydata,x='dataset',y='mse',color='method',width=70, log_y=True, points=\"all\")\n\nfig.layout['xaxis']['title']['text']=''\nfig.layout['yaxis']['title']['text']='MSE(log scale)'\n\nfig.data[0]['marker']['color'] = 'blue'\nfig.data[0]['name'] = 'Classic'\nfig.data[1]['marker']['color'] = 'red'\nfig.data[1]['name'] = 'Proposed'\n\n# fig.layout.annotations[0].text = 'Pedalme'\n# fig.layout.annotations[1].text = 'Windmillsmall'\n\nfig.layout['legend']['title']=''\n\nfig.update_layout(template=\"seaborn\")\nfig.update_layout(title_text=\"\")\nfig.update_layout(height=800, width=1800)\n\nfig.update_layout(legend=dict(\n    yanchor=\"top\",\n    y=0.99,\n    xanchor=\"left\",\n    x=0.01\n ))\n\nfig\n\nNameError: name 'pd' is not defined"
  },
  {
    "objectID": "posts/3_result/3_table/2024-01-08-Experiments_section.html#node",
    "href": "posts/3_result/3_table/2024-01-08-Experiments_section.html#node",
    "title": "Research Sections of Experiments",
    "section": "2.2 Node",
    "text": "2.2 Node\n\n# wikimath nodes = 1068 time = 731\n# monte nodes = 675 time = 744\n\ntidydata = pd.concat([\n          df.query(\"dataset=='wikimath' & mtype=='rand'  & model=='GCLSTM' & (mrate == 0.8)\"),\n          df.query(\"dataset=='monte' & mtype=='rand' &  model=='GCLSTM' & (mrate == 0.8)\")])\n\ntidydata['method'] = pd.Categorical(tidydata['method'], categories=['STGCN', 'IT-STGCN'])\n\ntidydata = tidydata.sort_values(by=['dataset','model','mrate'])\n\nfig = px.box(tidydata,x='dataset',y='mse',color='method',width=70, log_y=True, points=\"all\")\n\nfig.layout['xaxis']['title']['text']='Missing Rate'\nfig.layout['yaxis']['title']['text']='MSE(log scale)'\n\nfig.data[0]['marker']['color'] = 'blue'\nfig.data[0]['name'] = 'Classic'\nfig.data[1]['marker']['color'] = 'red'\nfig.data[1]['name'] = 'Proposed'\n\nfig.layout['legend']['title']=''\n\nfig.update_layout(template=\"seaborn\")\nfig.update_layout(title_text=\"\")\nfig.update_layout(height=800, width=1800)\n\nfig.update_layout(legend=dict(\n    yanchor=\"top\",\n    y=0.99,\n    xanchor=\"left\",\n    x=0.01\n ))\n\nfig\n\nNameError: name 'pd' is not defined\n\n\n\n# fivenodes = pd.concat([\n#     df[(df['dataset']=='fivenodes') & (df['mtype']=='rand') & (df['inter_method']=='linear') & (df['nof_filters']==12) & \n#        (df['lags']==2) & (df['epoch']==50) & (df['model']=='GConvGRU') & (df['mrate'].isin([0.3, 0.5, 0.6, 0.7, 0.8]))],\n#     df[(df['dataset']=='fivenodes') & (df['mtype']=='rand') & (df['inter_method']=='linear') & (df['nof_filters']==12) & \n#        (df['lags']==2) & (df['epoch']==50) & (df['model']=='GConvLSTM') & (df['mrate'].isin([0.3, 0.5, 0.6, 0.7, 0.8]))],\n#     df[(df['dataset']=='fivenodes') & (df['mtype']=='rand') & (df['inter_method']=='linear') & (df['nof_filters']==4) & \n#        (df['lags']==2) & (df['epoch']==50) & (df['model']=='GCLSTM') & (df['mrate'].isin([0.3, 0.5, 0.6, 0.7, 0.8]))],\n#     df[(df['dataset']=='fivenodes') & (df['mtype']=='rand') & (df['inter_method']=='linear') & (df['nof_filters']==4) & \n#        (df['lags']==2) & (df['epoch']==50) & (df['model']=='LRGCN') & (df['mrate'].isin([0.3, 0.5, 0.6, 0.7, 0.8]))],\n#     df[(df['dataset']=='fivenodes') & (df['mtype']=='rand') & (df['inter_method']=='linear') & (df['nof_filters']==12) & \n#        (df['lags']==2) & (df['epoch']==50) & (df['model']=='DyGrEncoder') & (df['mrate'].isin([0.3, 0.5, 0.6, 0.7, 0.8]))],\n#     df[(df['dataset']=='fivenodes') & (df['mtype']=='rand') & (df['inter_method']=='linear') & (df['lags']==2) & \n#        (df['epoch']==50) & (df['model']=='EvolveGCNH') & (df['mrate'].isin([0.3, 0.5, 0.6, 0.7, 0.8]))],\n#     df[(df['dataset']=='fivenodes') & (df['mtype']=='rand') & (df['inter_method']=='linear') & (df['lags']==2) & \n#        (df['epoch']==50) & (df['model']=='EvolveGCNO') & (df['mrate'].isin([0.3, 0.5, 0.6, 0.7, 0.8]))],\n#     df[(df['dataset']=='fivenodes') & (df['mtype']=='rand') & (df['inter_method']=='linear') & (df['nof_filters']==12) & \n#        (df['lags']==2) & (df['epoch']==50) & (df['model']=='TGCN') & (df['mrate'].isin([0.3, 0.5, 0.6, 0.7, 0.8]))],\n#     df[(df['dataset']=='fivenodes') & (df['mtype']=='rand') & (df['inter_method']=='linear') & (df['nof_filters']==2) & \n#        (df['lags']==2) & (df['epoch']==50) & (df['model']=='DCRNN') & (df['mrate'].isin([0.3, 0.5, 0.6, 0.7, 0.8]))]\n# ])\n# fivenodes['nodes'] = 5\n# fivenodes['time'] = 200\n# fivenodes = fivenodes.query(\"mrate==0.8\")\n\n# chickenpox = pd.concat([\n#     df.query(\"dataset=='chickenpox' & mtype=='rand' & inter_method == 'linear' & nof_filters==16 & lags==4 & epoch==50 & model=='GConvGRU' & (mrate == 0.3 | mrate == 0.5 | mrate == 0.6 | mrate == 0.8)\"),\n#     df.query(\"dataset=='chickenpox' & mtype=='rand' & inter_method == 'linear' & nof_filters==32 & lags==4 & epoch==50 & model=='GConvLSTM' & (mrate == 0.3 | mrate == 0.5 | mrate == 0.6 | mrate == 0.8)\"),\n#     df.query(\"dataset=='chickenpox' & mtype=='rand' & inter_method == 'linear' & nof_filters==16 & lags==4 & epoch==50 & model=='GCLSTM' & (mrate == 0.3 | mrate == 0.5 | mrate == 0.6 | mrate == 0.8)\"),\n#     df.query(\"dataset=='chickenpox' & mtype=='rand' & inter_method == 'linear' & nof_filters==8 & lags==4 & epoch==50 & model=='LRGCN' & (mrate == 0.3 | mrate == 0.5 | mrate == 0.6 | mrate == 0.8)\"),\n#     df.query(\"dataset=='chickenpox' & mtype=='rand' & inter_method == 'linear' & nof_filters==12 & lags==4 & epoch==50 & model=='DyGrEncoder' & (mrate == 0.3 | mrate == 0.5 | mrate == 0.6 | mrate == 0.8)\"),\n#     df.query(\"dataset=='chickenpox' & mtype=='rand' & inter_method == 'linear' & lags==4 & epoch==50 & model=='EvolveGCNH' & (mrate == 0.3 | mrate == 0.5 | mrate == 0.6 | mrate == 0.8)\"),\n#     df.query(\"dataset=='chickenpox' & mtype=='rand' & inter_method == 'linear' & lags==4 & epoch==50 & model=='EvolveGCNO' & (mrate == 0.3 | mrate == 0.5 | mrate == 0.6 | mrate == 0.8)\"),\n#     df.query(\"dataset=='chickenpox' & mtype=='rand' & inter_method == 'linear' & nof_filters==12 & lags==4 & epoch==50 & model=='TGCN' & (mrate == 0.3 | mrate == 0.5 | mrate == 0.6 | mrate == 0.8)\"),\n#     df.query(\"dataset=='chickenpox' & mtype=='rand' & inter_method == 'linear' & nof_filters==16 & lags==4 & epoch==50 & model=='DCRNN' & (mrate == 0.3 | mrate == 0.5 | mrate == 0.6 | mrate == 0.8)\")\n# ])\n# chickenpox['nodes'] = 20\n# chickenpox['time'] = 522\n# chickenpox = chickenpox.query(\"mrate==0.8\")\n\n# pedalme = pd.concat([\n#     df.query(\"dataset=='pedalme' & mtype=='rand' & inter_method == 'nearest' & nof_filters==12 & lags==4 & epoch==50 & model=='GConvGRU' & (mrate == 0.3 | mrate == 0.5 | mrate == 0.6 | mrate == 0.8)\"),\n#     df.query(\"dataset=='pedalme' & mtype=='rand' & inter_method == 'nearest' & nof_filters==2 & lags==4 & epoch==50 & model=='GConvLSTM' & (mrate == 0.3 | mrate == 0.5 | mrate == 0.6 | mrate == 0.8)\"),\n#     df.query(\"dataset=='pedalme' & mtype=='rand' & inter_method == 'nearest' & nof_filters==4 & lags==4 & epoch==50 & model=='GCLSTM' & (mrate == 0.3 | mrate == 0.5 | mrate == 0.6 | mrate == 0.8)\"),\n#     df.query(\"dataset=='pedalme' & mtype=='rand' & inter_method == 'nearest' & nof_filters==8 & lags==4 & epoch==50 & model=='LRGCN' & (mrate == 0.3 | mrate == 0.5 | mrate == 0.6 | mrate == 0.8)\"),\n#     df.query(\"dataset=='pedalme' & mtype=='rand' & inter_method == 'nearest' & nof_filters==12 & lags==4 & epoch==50 & model=='DyGrEncoder' & (mrate == 0.3 | mrate == 0.5 | mrate == 0.6 | mrate == 0.8)\"),\n#     df.query(\"dataset=='pedalme' & mtype=='rand' & inter_method == 'nearest' & lags==4 & epoch==50 & model=='EvolveGCNH' & (mrate == 0.3 | mrate == 0.5 | mrate == 0.6 | mrate == 0.8)\"),\n#     df.query(\"dataset=='pedalme' & mtype=='rand' & inter_method == 'nearest' & lags==4 & epoch==50 & model=='EvolveGCNO' & (mrate == 0.3 | mrate == 0.5 | mrate == 0.6 | mrate == 0.8)\"),\n#     df.query(\"dataset=='pedalme' & mtype=='rand' & inter_method == 'nearest' & nof_filters==12 & lags==4 & epoch==50 & model=='TGCN' & (mrate == 0.3 | mrate == 0.5 | mrate == 0.6 | mrate == 0.8)\"),\n#     df.query(\"dataset=='pedalme' & mtype=='rand' & inter_method == 'nearest' & nof_filters==8 & lags==4 & epoch==50 & model=='DCRNN' & (mrate == 0.3 | mrate == 0.5 | mrate == 0.6 | mrate == 0.8)\")\n# ], ignore_index=True)\n# pedalme['nodes'] = 15\n# pedalme['time'] = 36\n\n# pedalme = pedalme.query(\"mrate==0.8\")\n\n# wikimath = pd.concat([\n#     df.query(\"dataset=='wikimath' & mtype=='rand' & inter_method == 'linear' & nof_filters==12 & lags==8 & epoch==50 & model=='GConvGRU' & (mrate == 0.3 | mrate == 0.5 | mrate == 0.6 | mrate == 0.8)\"),\n#     df.query(\"dataset=='wikimath' & mtype=='rand' & inter_method == 'linear' & nof_filters==64 & lags==8 & epoch==50 & model=='GConvLSTM' & (mrate == 0.3 | mrate == 0.5 | mrate == 0.6 | mrate == 0.8)\"),\n#     df.query(\"dataset=='wikimath' & mtype=='rand' & inter_method == 'linear' & nof_filters==64 & lags==8 & epoch==50 & model=='GCLSTM' & (mrate == 0.3 | mrate == 0.5 | mrate == 0.6 | mrate == 0.8)\"),\n#     df.query(\"dataset=='wikimath' & mtype=='rand' & inter_method == 'linear' & nof_filters==32 & lags==8 & epoch==50 & model=='LRGCN' & (mrate == 0.3 | mrate == 0.5 | mrate == 0.6 | mrate == 0.8)\"),\n#     df.query(\"dataset=='wikimath' & mtype=='rand' & inter_method == 'linear' & nof_filters==12 & lags==8 & epoch==50 & model=='DyGrEncoder' & (mrate == 0.3 | mrate == 0.5 | mrate == 0.6 | mrate == 0.8)\"),\n#     df.query(\"dataset=='wikimath' & mtype=='rand' & inter_method == 'linear' & lags==8 & epoch==50 & model=='EvolveGCNH' & (mrate == 0.3 | mrate == 0.5 | mrate == 0.6 | mrate == 0.8)\"),\n#     df.query(\"dataset=='wikimath' & mtype=='rand' & inter_method == 'linear' & lags==8 & epoch==50 & model=='EvolveGCNO' & (mrate == 0.3 | mrate == 0.5 | mrate == 0.6 | mrate == 0.8)\"),\n#     df.query(\"dataset=='wikimath' & mtype=='rand' & inter_method == 'linear' & nof_filters==12 & lags==8 & epoch==50 & model=='TGCN' & (mrate == 0.3 | mrate == 0.5 | mrate == 0.6 | mrate == 0.8)\"),\n#     df.query(\"dataset=='wikimath' & mtype=='rand' & inter_method == 'linear' & nof_filters==12 & lags==8 & epoch==50 & model=='DCRNN' & (mrate == 0.3 | mrate == 0.5 | mrate == 0.6 | mrate == 0.8)\")\n# ], ignore_index=True)\n# wikimath['nodes'] = 1068\n# wikimath['time'] = 731\n# wikimath = wikimath.query(\"mrate==0.8\")\n\n# windmillsmall = pd.concat([\n#     df.query(\"dataset=='windmillsmall' & mtype=='rand' & inter_method == 'linear' & nof_filters==12 & lags==8 & epoch==50 & model=='GConvGRU' & (mrate == 0.3 | mrate == 0.5 | mrate == 0.6 | mrate == 0.7)\"),\n#     df.query(\"dataset=='windmillsmall' & mtype=='rand' & inter_method == 'linear' & nof_filters==16 & lags==8 & epoch==50 & model=='GConvLSTM' & (mrate == 0.3 | mrate == 0.5 | mrate == 0.6 | mrate == 0.7)\"),\n#     df.query(\"dataset=='windmillsmall' & mtype=='rand' & inter_method == 'linear' & nof_filters==16 & lags==8 & epoch==50 & model=='GCLSTM' & (mrate == 0.3 | mrate == 0.5 | mrate == 0.6 | mrate == 0.7)\"),\n#     df.query(\"dataset=='windmillsmall' & mtype=='rand' & inter_method == 'linear' & nof_filters==12 & lags==8 & epoch==50 & model=='LRGCN' & (mrate == 0.3 | mrate == 0.5 | mrate == 0.6 | mrate == 0.7)\"),\n#     df.query(\"dataset=='windmillsmall' & mtype=='rand' & inter_method == 'linear' & nof_filters==12 & lags==8 & epoch==50 & model=='DyGrEncoder' & (mrate == 0.3 | mrate == 0.5 | mrate == 0.6 | mrate == 0.7)\"),\n#     df.query(\"dataset=='windmillsmall' & mtype=='rand' & inter_method == 'linear' & lags==8 & epoch==50 & model=='EvolveGCNH' & (mrate == 0.3 | mrate == 0.5 | mrate == 0.6 | mrate == 0.7)\"),\n#     df.query(\"dataset=='windmillsmall' & mtype=='rand' & inter_method == 'linear' & lags==8 & epoch==50 & model=='EvolveGCNO' & (mrate == 0.3 | mrate == 0.5 | mrate == 0.6 | mrate == 0.7)\"),\n#     df.query(\"dataset=='windmillsmall' & mtype=='rand' & inter_method == 'linear' & nof_filters==12 & lags==8 & epoch==50 & model=='TGCN' & (mrate == 0.3 | mrate == 0.5 | mrate == 0.6 | mrate == 0.7)\"),\n#     df.query(\"dataset=='windmillsmall' & mtype=='rand' & inter_method == 'linear' & nof_filters==4 & lags==8 & epoch==50 & model=='DCRNN' & (mrate == 0.3 | mrate == 0.5 | mrate == 0.6 | mrate == 0.7)\")\n# ], ignore_index=True)\n# windmillsmall['nodes'] = 11\n# windmillsmall['time'] = 17472\n# windmillsmall = windmillsmall.query(\"mrate==0.7\")\n\n# monte = pd.concat([\n#     df.query(\"dataset=='monte' & mtype=='rand' & inter_method == 'nearest' & nof_filters==12 & lags==4 & epoch==50 & model=='GConvGRU' & (mrate == 0.3 | mrate == 0.5 | mrate == 0.7 | mrate == 0.8)\"),\n#     df.query(\"dataset=='monte' & mtype=='rand' & inter_method == 'nearest' & nof_filters==12 & lags==4 & epoch==50 & model=='GConvLSTM' & (mrate == 0.3 | mrate == 0.5 | mrate == 0.7 | mrate == 0.8)\"),\n#     df.query(\"dataset=='monte' & mtype=='rand' & inter_method == 'nearest' & nof_filters==12 & lags==4 & epoch==50 & model=='GCLSTM' & (mrate == 0.3 | mrate == 0.5 | mrate == 0.7 | mrate == 0.8)\"),\n#     df.query(\"dataset=='monte' & mtype=='rand' & inter_method == 'nearest' & nof_filters==2 & lags==4 & epoch==50 & model=='LRGCN' & (mrate == 0.3 | mrate == 0.5 | mrate == 0.7 | mrate == 0.8)\"),\n#     df.query(\"dataset=='monte' & mtype=='rand' & inter_method == 'nearest' & nof_filters==12 & lags==4 & epoch==50 & model=='DyGrEncoder' & (mrate == 0.3 | mrate == 0.5 | mrate == 0.7 | mrate == 0.8)\"),\n#     df.query(\"dataset=='monte' & mtype=='rand' & inter_method == 'nearest' & lags==4 & epoch==50 & model=='EvolveGCNH' & (mrate == 0.3 | mrate == 0.5 | mrate == 0.7 | mrate == 0.8)\"),\n#     df.query(\"dataset=='monte' & mtype=='rand' & inter_method == 'nearest' & lags==4 & epoch==50 & model=='EvolveGCNO' & (mrate == 0.3 | mrate == 0.5 | mrate == 0.7 | mrate == 0.8)\"),\n#     df.query(\"dataset=='monte' & mtype=='rand' & inter_method == 'nearest' & nof_filters==8 & lags==4 & epoch==50 & model=='TGCN' & (mrate == 0.3 | mrate == 0.5 | mrate == 0.7 | mrate == 0.8)\"),\n#     df.query(\"dataset=='monte' & mtype=='rand' & inter_method == 'nearest' & nof_filters==12 & lags==4 & epoch==50 & model=='DCRNN' & (mrate == 0.3 | mrate == 0.5 | mrate == 0.7 | mrate == 0.8)\")\n# ], ignore_index=True)\n# monte['nodes'] = 675\n# monte['time'] = 744\n# monte = monte.query(\"mrate==0.8\")\n\n# tidydata = pd.concat([fivenodes, chickenpox, pedalme, wikimath,windmillsmall, monte])\n\n# tidydata = tidydata.sort_values(by=['time'])\n# tidydata['model'] = pd.Categorical(tidydata['model'], categories=[\"GConvGRU\", \"GConvLSTM\", \"GCLSTM\", \"LRGCN\", \"DyGrEncoder\", \"EvolveGCNH\", \"EvolveGCNO\", \"TGCN\", \"DCRNN\"])\n# tidydata['method'] = pd.Categorical(tidydata['method'], categories=['STGCN', 'IT-STGCN'])\n# tidydata['time'] = tidydata['time'].astype(str)\n\n# fig = px.strip(tidydata,x='time',y='mse',log_y=True,color='model',facet_col='method')\n# fig.layout.annotations[0].text = 'Classic'\n# fig.layout.annotations[1].text = 'Proposed'\n\n# fig.layout.xaxis.title.text = ''\n# fig.layout.xaxis2.title.text = ''\n\n# fig.layout.yaxis2.showticklabels = True\n\n# fig.update_layout(template=\"seaborn\")\n# fig.update_layout(title_text=\"\")\n# fig.update_layout(height=1000, width=1800)\n# fig.update_layout(legend=dict(\n#     yanchor=\"top\",\n#     y=0.99,\n#     xanchor=\"left\",\n#     x=0.01\n#  ))\n# fig"
  },
  {
    "objectID": "posts/3_result/3_table/2024-01-08-Experiments_section.html#linear-vs-nearest",
    "href": "posts/3_result/3_table/2024-01-08-Experiments_section.html#linear-vs-nearest",
    "title": "Research Sections of Experiments",
    "section": "3.1 Linear vs Nearest",
    "text": "3.1 Linear vs Nearest\nInterpolation 에 따라, block은 시각적으로 보이기 좋지 않은 예시\n\ntt = nearest_fivenodes.query(\"dataset=='fivenodes'  and method!='GNAR' and mtype=='rand'  and nof_filters==12 and lags==2 and epoch==50 and model=='GConvGRU' and mrate in [0.3  , 0.5  , 0.6 ,0.7  , 0.8]\")\ntt['mrate'] = tt['mrate'].astype(str)\ntt = tt.sort_values(by=['model','mrate'])\ntt['method'] = pd.Categorical(tt['method'], categories=['STGCN', 'IT-STGCN'])\n\nfig = px.box(tt,x='mrate',y='mse',color='method',facet_col='inter_method')\n\nfig.layout['xaxis']['title']['text']='Missing Rate'\nfig.layout['yaxis']['title']['text']='MSE(log scale)'\n\nfig.data[0]['marker']['color'] = 'blue'\nfig.data[0]['name'] = 'Classic'\nfig.data[1]['marker']['color'] = 'blue'\nfig.data[1]['name'] = 'Classic'\nfig.data[2]['marker']['color'] = 'red'\nfig.data[2]['name'] = 'Proposed'\nfig.data[3]['marker']['color'] = 'red'\nfig.data[3]['name'] = 'Proposed'\n\nfig.layout['legend']['title']=''\n\nfig.update_layout(template=\"seaborn\")\nfig.update_layout(title_text=\"\")\nfig.update_layout(height=800, width=1800)\n\nfig.update_layout(legend=dict(\n    yanchor=\"top\",\n    y=0.99,\n    xanchor=\"left\",\n    x=0.01\n ))\nfig\n\nNameError: name 'nearest_fivenodes' is not defined\n\n\n\nnearest_fivenodes.model.unique()\n\nNameError: name 'nearest_fivenodes' is not defined\n\n\n\ntt = nearest_pedalme.query(\"dataset=='pedalme' and method!='GNAR' and mtype=='rand' and nof_filters==12 and lags==4 and epoch==50 and model=='GConvGRU' and mrate in [0.3,0.5,0.6,0.8]\")\ntt['mrate'] = tt['mrate'].astype(str)\ntt = tt.sort_values(by=['model','mrate'])\nfig = px.box(tt,x='mrate',y='mse',color='method',facet_col='inter_method')\nfig.layout['xaxis']['title']['text']='Missing Rate'\nfig.layout['yaxis']['title']['text']='MSE(log scale)'\n\nfig.data[0]['marker']['color'] = 'blue'\nfig.data[0]['name'] = 'Classic'\nfig.data[1]['marker']['color'] = 'blue'\nfig.data[1]['name'] = 'Classic'\nfig.data[2]['marker']['color'] = 'red'\nfig.data[2]['name'] = 'Proposed'\nfig.data[3]['marker']['color'] = 'red'\nfig.data[3]['name'] = 'Proposed'\n\nfig.layout['legend']['title']=''\n\nfig.update_layout(template=\"seaborn\")\nfig.update_layout(title_text=\"\")\nfig.update_layout(height=800, width=1800)\n\nfig.update_layout(legend=dict(\n    yanchor=\"top\",\n    y=0.99,\n    xanchor=\"left\",\n    x=0.01\n ))\nfig\n\nNameError: name 'nearest_pedalme' is not defined"
  },
  {
    "objectID": "posts/3_result/3_table/2024-01-08-Experiments_section.html#block-vs-random",
    "href": "posts/3_result/3_table/2024-01-08-Experiments_section.html#block-vs-random",
    "title": "Research Sections of Experiments",
    "section": "4.1 Block vs random",
    "text": "4.1 Block vs random\n\ntt=df.query(\"mtype in ['rand','block'] and mrate in [0.2877697841726618, 0.3] and dataset=='chickenpox'\")\ntt['mrate'] = tt['mrate'].astype(str)\ntt = tt.sort_values(by=['model','mrate'])\n\nfig = px.box(tt,x='mrate',y='mse',color='method')\nfig.layout['xaxis']['title']['text']='Missing Rate'\nfig.layout['yaxis']['title']['text']='MSE'\n\nfig.data[0]['marker']['color'] = 'blue'\nfig.data[0]['name'] = 'Classic'\nfig.data[1]['marker']['color'] = 'red'\nfig.data[1]['name'] = 'Proposed'\n\nfig.layout['legend']['title']=''\n\nfig.update_layout(template=\"seaborn\")\nfig.update_layout(title_text=\"\")\nfig.update_layout(height=800, width=1800)\n\nfig.update_layout(legend=dict(\n    yanchor=\"top\",\n    y=0.99,\n    xanchor=\"left\",\n    x=0.01\n ))\nfig\n\nNameError: name 'df' is not defined\n\n\n\ntt=df.query(\"mtype in ['rand','block'] and mrate in [0.2857142857142857, 0.3] and model=='LRGCN' and dataset=='pedalme'\")\ntt['mrate'] = tt['mrate'].astype(str)\ntt = tt.sort_values(by=['model','mrate'])\n\nfig = px.box(tt,x='mrate',y='mse',color='method')\nfig.layout['xaxis']['title']['text']='Missing Rate'\nfig.layout['yaxis']['title']['text']='MSE'\n\nfig.data[0]['marker']['color'] = 'blue'\nfig.data[0]['name'] = 'Classic'\nfig.data[1]['marker']['color'] = 'red'\nfig.data[1]['name'] = 'Proposed'\n\nfig.layout['legend']['title']=''\n\nfig.update_layout(template=\"seaborn\")\nfig.update_layout(title_text=\"\")\nfig.update_layout(height=800, width=1800)\n\nfig.update_layout(legend=dict(\n    yanchor=\"top\",\n    y=0.99,\n    xanchor=\"left\",\n    x=0.01\n ))\nfig\n\nNameError: name 'df' is not defined"
  },
  {
    "objectID": "posts/3_result/3_figure/2023-07-20-ITSTGCN_data_magement_figure.html",
    "href": "posts/3_result/3_figure/2023-07-20-ITSTGCN_data_magement_figure.html",
    "title": "Data management Figure for ITSTGCN",
    "section": "",
    "text": "library(ggplot2)\n\n\nlibrary(dplyr)\n\n\nAttaching package: ‘dplyr’\n\n\nThe following objects are masked from ‘package:stats’:\n\n    filter, lag\n\n\nThe following objects are masked from ‘package:base’:\n\n    intersect, setdiff, setequal, union\n\n\n\n\n\ndf &lt;- read.csv(\"../Data/df_fig.csv\")\n\n\nhead(df)\n\n\nA data.frame: 6 × 12\n\n\n\nX\ndataset\nmethod\nmrate\nmtype\nlags\nnof_filters\ninter_method\nepoch\nmse\ncalculation_time\nmodel\n\n\n\n&lt;int&gt;\n&lt;chr&gt;\n&lt;chr&gt;\n&lt;dbl&gt;\n&lt;chr&gt;\n&lt;int&gt;\n&lt;dbl&gt;\n&lt;chr&gt;\n&lt;dbl&gt;\n&lt;dbl&gt;\n&lt;dbl&gt;\n&lt;chr&gt;\n\n\n\n\n1\n0\nfivenodes\nSTGCN\n0.0\n\n2\n12\n\n50\n0.7293743\n80.98522\nGConvGRU\n\n\n2\n1\nfivenodes\nSTGCN\n0.0\n\n2\n12\n\n50\n0.7290817\n80.89179\nGConvGRU\n\n\n3\n2\nfivenodes\nSTGCN\n0.7\nrand\n2\n12\nlinear\n50\n1.8922616\n81.97655\nGConvGRU\n\n\n4\n3\nfivenodes\nSTGCN\n0.7\nrand\n2\n12\nnearest\n50\n2.2112885\n87.80387\nGConvGRU\n\n\n5\n4\nfivenodes\nSTGCN\n0.8\nrand\n2\n12\nlinear\n50\n2.0728178\n103.64874\nGConvGRU\n\n\n6\n5\nfivenodes\nSTGCN\n0.8\nrand\n2\n12\nnearest\n50\n2.5664744\n98.34010\nGConvGRU"
  },
  {
    "objectID": "posts/3_result/3_figure/2023-07-20-ITSTGCN_data_magement_figure.html#후보-1",
    "href": "posts/3_result/3_figure/2023-07-20-ITSTGCN_data_magement_figure.html#후보-1",
    "title": "Data management Figure for ITSTGCN",
    "section": "후보 1",
    "text": "후보 1\n\nggplot(fivenodes, aes(x=mrate,y= mse,group=mrate)) + facet_wrap(model~method) + \ngeom_boxplot(fill='grey',color='black',width=0.7,outlier.color = 'darkblue',outlier.shape = 2) + theme_classic()\n# ggsave(\"random_list_fivenodes.png\")"
  },
  {
    "objectID": "posts/3_result/3_figure/2023-07-20-ITSTGCN_data_magement_figure.html#후보-2",
    "href": "posts/3_result/3_figure/2023-07-20-ITSTGCN_data_magement_figure.html#후보-2",
    "title": "Data management Figure for ITSTGCN",
    "section": "후보 2",
    "text": "후보 2\n\nggplot(fivenodes, aes(x=mrate,y= log10(mse),group=mrate)) + facet_wrap(model~method,,ncol=4) + \ngeom_boxplot(fill='grey',color='black',width=0.7,outlier.color = 'darkblue',outlier.shape = 2) + \ntheme_classic() \n# ggsave(\"random_list_fivenodes.png\")"
  },
  {
    "objectID": "posts/3_result/3_figure/2023-07-20-ITSTGCN_data_magement_figure.html#후보-3",
    "href": "posts/3_result/3_figure/2023-07-20-ITSTGCN_data_magement_figure.html#후보-3",
    "title": "Data management Figure for ITSTGCN",
    "section": "후보 3",
    "text": "후보 3\n\nggplot(fivenodes, aes(x = mrate, y = log10(mse), group = mrate)) +\n  geom_boxplot(fill = 'grey', color = 'black', width = 0.7, outlier.color = 'darkblue', outlier.shape = 2) +\n  theme(strip.background = element_blank(), strip.placement = \"outside\", panel.grid = element_blank()) +\n  facet_wrap(model ~ method, ncol = 4, scales = \"free_y\",) +\n  xlab(\"Missing Rate\") +\n  ylab(\"MSE(log scale)\") +\n  scale_x_continuous(labels=c('','','','',''))\n# ggsave(\"random_list_fivenodes.pdf\")"
  },
  {
    "objectID": "posts/3_result/3_figure/2023-07-20-ITSTGCN_data_magement_figure.html#후보-4",
    "href": "posts/3_result/3_figure/2023-07-20-ITSTGCN_data_magement_figure.html#후보-4",
    "title": "Data management Figure for ITSTGCN",
    "section": "후보 4",
    "text": "후보 4\n\np1 = ggplot(fivenodes, aes(x = mrate, y = log10(mse), group = interaction(mrate, method))) +\n  geom_boxplot(aes(fill = method), color = 'black', width = 0.7, outlier.color = 'black', outlier.shape = 2) +\n  scale_fill_manual(values = c(\"steelblue\", \"pink\"), labels=c('Classic', 'Proposed')) +\n  theme(strip.background = element_blank(), strip.placement = \"outside\", panel.grid = element_blank(),\n       legend.position = \"\") +\n  labs(x = \" \", y = \"\", fill = \"\") +\n  scale_x_continuous(labels = c('', '', '', '', '')) +\n  facet_wrap(~model, ncol = 3, scales = \"free_y\")\n# ggsave(\"random_list_fivenodes.pdf\")\n\n\nggplot(fivenodes, aes(x = mrate, y = log10(mse), group = interaction(mrate, method))) +\n  geom_boxplot(aes(fill = method), color = 'black', width = 0.7, outlier.color = 'black', outlier.shape = 2) +\n  scale_fill_manual(values = c(\"steelblue\", \"pink\"), labels=c('Classic', 'Proposed')) +\n  theme(strip.background = element_blank(), strip.placement = \"outside\", panel.grid = element_blank(),\n       legend.position = 'bottom') +\n  labs(x = \"Missing Rate\", y = \"MSE(log scale)\", fill = \"\") +\n  scale_x_continuous(labels = c('', '', '', '', '')) +\n  facet_wrap(~model, ncol = 3, scales = \"free_y\")\nggsave(\"random_list_fivenodes.pdf\")\n\nSaving 6.67 x 6.67 in image"
  },
  {
    "objectID": "posts/3_result/3_figure/2024-02-29-실험결과정리.html",
    "href": "posts/3_result/3_figure/2024-02-29-실험결과정리.html",
    "title": "실험결과정리",
    "section": "",
    "text": "import pandas as pd\nimport numpy as np\n\nimport plotly.express as px\nimport plotly.graph_objects as go\nimport pickle\n\nimport plotly.io as pio\n\n\npd.options.plotting.backend = \"plotly\"\npio.templates.default = \"plotly_white\"\n\n\ndf = pd.concat([pd.read_csv('../Data/Body_Results.csv'),\n            pd.read_csv('../Data/chicken_inter_final.csv'), # chickenpox cubic + nearest mrate 0.8 에서\n            pd.read_csv('../Data/R_GNAR_results.csv'), # GNAR을 R에서 돌린 결과 RANDOM, BLOCK\n            pd.concat([pd.read_csv('../Data/DCRNN_50_windmillsmall.csv'), # 50% windmillsmall 결과들\n                       pd.read_csv('../Data/GConvGRU_50_windmillsmall.csv'),\n                       pd.read_csv('../Data/GConvLSTM_50_windmillsmall.csv'),\n                       pd.read_csv('../Data/LRGCN_50_windmillsmall.csv'),\n                       pd.read_csv('../Data/DyGrEncoder_50_windmillsmall.csv'),\n                       pd.read_csv('../Data/TGCN_50_windmillsmall.csv'),\n                       pd.read_csv('../Data/GCLSTM_50_windmillsmall.csv'),\n                       pd.read_csv('../Data/EvolveGCNO_50_windmillsmall.csv'),\n                       pd.read_csv('../Data/EvolveGCNH_50_windmillsmall.csv'),\n                      ]),\n            pd.concat([pd.read_csv('../Data/DCRNN_80_windmillsmall.csv'), # 80% windmillsmall 결과들\n                       pd.read_csv('../Data/LRGCN_80_windmillsmall.csv'),\n                       pd.read_csv('../Data/DyGrEncoder_80_windmillsmall.csv'),\n                       pd.read_csv('../Data/GConvLSTM_80_windmillsmall.csv'),\n                       pd.read_csv('../Data/GConvGRU_80_windmillsmall.csv'),\n                       pd.read_csv('../Data/TGCN_80_windmillsmall.csv'),\n                       pd.read_csv('../Data/GCLSTM_80_windmillsmall.csv'),\n                       pd.read_csv('../Data/EvolveGCNO_80_windmillsmall.csv'),\n                       pd.read_csv('../Data/EvolveGCNH_80_windmillsmall.csv')\n                      ]),\n            pd.concat([pd.read_csv('../Data/DCRNN_block_windmillsmall.csv'), # block windmillsmall 결과물\n                      pd.read_csv('../Data/LRGCN_block_windmillsmall.csv'),\n                      pd.read_csv('../Data/GConvGRU_block_windmillsmall.csv'),\n                      pd.read_csv('../Data/EvolveGCNO_block_windmillsmall.csv'),\n                      pd.read_csv('../Data/DyGrEncoder_block_windmillsmall.csv'),\n                      pd.read_csv('../Data/GConvLSTM_block_windmillsmall.csv'),\n                      pd.read_csv('../Data/GCLSTM_block_windmillsmall.csv'),\n                      pd.read_csv('../Data/TGCN_block_windmillsmall.csv'),\n                      pd.read_csv('../Data/EvolveGCNH_block_windmillsmall.csv')\n                      ])\n               ]).assign(model = lambda df: df.model.apply(lambda x: 'GConvGRU' if x == \"GComvGRU\" else x))\ndf = df.iloc[:,:-1]"
  },
  {
    "objectID": "posts/3_result/3_figure/2024-02-29-실험결과정리.html#시각화3-1",
    "href": "posts/3_result/3_figure/2024-02-29-실험결과정리.html#시각화3-1",
    "title": "실험결과정리",
    "section": "시각화3-1",
    "text": "시각화3-1\n\nV = 노드의 수, T = 데이터 수로 봤을때, Ratio = T/V를 계산하여 x축에 놓고, classic method와 proposed method 의 mse 차이를 계산하여 y축에 놓았다.\n\n\ntidydata = df.query('mtype != \"block\"').query('dataset != \"fivenodes\"')\\\n.groupby([\"method\",\"dataset\",\"node\",\"time\"]).agg({'mse':'mean'}).reset_index()\\\n.assign(ratio = lambda df: df['time']/df['node'])\\\n.pivot_table(index=['dataset','ratio'] ,columns=['method'],values='mse')\\\n.assign(mse_diff = lambda df: df['STGNN']- df['IT-STGNN']).loc[:,'mse_diff']\\\n.reset_index()\ntidydata\nfig = px.scatter(\n    tidydata,\n    x='ratio',\n    log_x=True,\n    y='mse_diff',\n    text='dataset',\n    width=625,\n    height=425,\n    # trendline=\"ols\",\n    # trendline_options=dict(log_x=True),\n    # trendline_color_override=\"grey\"\n)\nfig.data[0]['textposition'] = ['top right'] + ['bottom right'] + ['top right'] + ['bottom right'] + ['top left']\nfig.data[0]['marker']['size'] = 8\nfig.data[0].text = ['Chickenpox', 'MontevideoBus', 'Pedalme', 'Wikimath', 'Windmillsmall']\nfig.layout['xaxis']['title']['text']='Ratio'\nfig.layout['yaxis']['title']['text']='MSE difference'\nfig"
  },
  {
    "objectID": "posts/3_result/3_figure/2024-02-29-실험결과정리.html#시각화3-2",
    "href": "posts/3_result/3_figure/2024-02-29-실험결과정리.html#시각화3-2",
    "title": "실험결과정리",
    "section": "시각화3-2",
    "text": "시각화3-2\n\ny축은 mse값을 나타내고, missing rate에 상관없이 randomly missing rate을 다르게 한 실험 결과들의 데이터셋 별 모델 별 mse의 순위를 나타내는 figure이다.\n\n\ndef func(x):\n    if 'IT-STGCN' in x:\n        return 'IT-STGNN'\n    elif 'GNAR' in x:\n        return 'GNAR'\n    else: \n        return 'STGNN'\n\n\ntidydata = df2.query('mtype != \"block\"').query('method!=\"GNAR\"').groupby([\"method\",\"dataset\",\"mrate\",\"model\"]).agg({'mse':'mean'}).reset_index()\\\n.pivot_table(index=['model','method'],columns=['dataset'],values='mse').stack().reset_index().rename({0:'mse'},axis=1)\ntidydata = pd.concat([df.sort_values('mse').reset_index(drop=True).reset_index() for _,df in tidydata.groupby([\"dataset\"])])\n#---#\nfig = px.bar(\n    tidydata,\n    x='index',\n    y='mse',\n    color='method',\n    facet_col='dataset',\n    facet_col_wrap=2,\n    text='model',\n    height=800\n)\nfig\n\nFutureWarning:\n\nIn a future version of pandas, a length 1 tuple will be returned when iterating over a groupby with a grouper equal to a list of length 1. Don't supply a list with a single grouper to avoid this warning."
  },
  {
    "objectID": "posts/1_studies/2023-05-11-CPUvsGPU.html",
    "href": "posts/1_studies/2023-05-11-CPUvsGPU.html",
    "title": "PyG Geometric Temporal CPU vs GPU",
    "section": "",
    "text": "CPU vs GPU\n\n\n!nvidia-smi\n\nFri May 12 06:42:19 2023       \n+-----------------------------------------------------------------------------+\n| NVIDIA-SMI 495.46       Driver Version: 495.46       CUDA Version: 11.5     |\n|-------------------------------+----------------------+----------------------+\n| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n|                               |                      |               MIG M. |\n|===============================+======================+======================|\n|   0  NVIDIA GeForce ...  Off  | 00000000:65:00.0 Off |                  N/A |\n|  0%   37C    P8    35W / 420W |     19MiB / 24268MiB |      0%      Default |\n|                               |                      |                  N/A |\n+-------------------------------+----------------------+----------------------+\n                                                                               \n+-----------------------------------------------------------------------------+\n| Processes:                                                                  |\n|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n|        ID   ID                                                   Usage      |\n|=============================================================================|\n|    0   N/A  N/A      1062      G   /usr/lib/xorg/Xorg                  9MiB |\n|    0   N/A  N/A      1309      G   /usr/bin/gnome-shell                8MiB |\n+-----------------------------------------------------------------------------+\n\n\n\nimport time\n\n\nimport torch\n\n\ntorch.cuda.is_available()\n\nTrue\n\n\n\nfrom torch_geometric_temporal.dataset import ChickenpoxDatasetLoader\nfrom torch_geometric_temporal.signal import temporal_signal_split\n\nloader = ChickenpoxDatasetLoader()\n\ndataset = loader.get_dataset()\n\ntrain_dataset, test_dataset = temporal_signal_split(dataset, train_ratio=0.8)\n\n\nimport torch\nimport torch.nn.functional as F\nfrom torch_geometric_temporal.nn.recurrent import DCRNN\n\nclass RecurrentGCN(torch.nn.Module):\n    def __init__(self, node_features):\n        super(RecurrentGCN, self).__init__()\n        self.recurrent = DCRNN(node_features, 32, 1)\n        self.linear = torch.nn.Linear(32, 1)\n\n    def forward(self, x, edge_index, edge_weight):\n        h = self.recurrent(x, edge_index, edge_weight)\n        h = F.relu(h)\n        h = self.linear(h)\n        return h\n\n\nfrom tqdm import tqdm\n\n\nGPU\n\nmodel = RecurrentGCN(node_features = 4)\n\n\noptimizer = torch.optim.Adam(model.parameters(), lr=0.01)\n\n\ntime.time()\n\n1683806883.6646736\n\n\n\nt1=time.time()\nfor epoch in tqdm(range(200)):\n    cost = 0\n    for time, snapshot in enumerate(train_dataset):\n        y_hat = model(snapshot.x, snapshot.edge_index, snapshot.edge_attr).to(\"cuda:0\")\n        cost = cost + torch.mean((y_hat-snapshot.y.to(\"cuda:0\"))**2)\n    cost = cost / (time+1)\n    cost.backward()\n    optimizer.step()\n    optimizer.zero_grad()\n\n100%|██████████| 200/200 [03:05&lt;00:00,  1.08it/s]\n\n\n\nimport time\n\n\nt2=time.time()\n\n\nt2-t1\n\n185.09801506996155\n\n\n\nmodel.eval()\ncost = 0\nfor time, snapshot in enumerate(test_dataset):\n    y_hat = model(snapshot.x, snapshot.edge_index, snapshot.edge_attr)\n    cost = cost + torch.mean((y_hat-snapshot.y)**2)\ncost = cost / (time+1)\ncost = cost.item()\nprint(\"MSE: {:.4f}\".format(cost))\n\nMSE: 1.0507\n\n\n\n\nCPU\n\nmodel = RecurrentGCN(node_features = 4)\n\n\noptimizer = torch.optim.Adam(model.parameters(), lr=0.01)\n\n\nimport time\n\n\nt1=time.time()\nfor epoch in tqdm(range(200)):\n    cost = 0\n    for time, snapshot in enumerate(train_dataset):\n        y_hat = model(snapshot.x, snapshot.edge_index, snapshot.edge_attr)\n        cost = cost + torch.mean((y_hat-snapshot.y)**2)\n    cost = cost / (time+1)\n    cost.backward()\n    optimizer.step()\n    optimizer.zero_grad()\n\n100%|██████████| 200/200 [05:08&lt;00:00,  1.54s/it]\n\n\n\nimport time\n\n\nt2=time.time()\n\n\nt2-t1\n\n308.58231496810913\n\n\n\nmodel.eval()\ncost = 0\nfor time, snapshot in enumerate(test_dataset):\n    y_hat = model(snapshot.x, snapshot.edge_index, snapshot.edge_attr)\n    cost = cost + torch.mean((y_hat-snapshot.y)**2)\ncost = cost / (time+1)\ncost = cost.item()\nprint(\"MSE: {:.4f}\".format(cost))\n\nMSE: 1.0542"
  },
  {
    "objectID": "posts/1_studies/2024-03-12-toy_algorithm.html",
    "href": "posts/1_studies/2024-03-12-toy_algorithm.html",
    "title": "Toy example / Algorithm",
    "section": "",
    "text": "https://www.researchgate.net/profile/Liang-Liu-42/publication/339714816_Regression_multiple_imputation_for_missing_data_analysis/links/607f43e32fb9097c0cf91255/Regression-multiple-imputation-for-missing-data-analysis.pdf\nhttps://arxiv.org/pdf/1511.03512.pdf\nhttps://citeseerx.ist.psu.edu/document?repid=rep1&type=pdf&doi=576cd01f8e2c289651ae0a86374ac6d6abb787c1\n\n이베이즈 파이썬화?\n논문 코드 레이어화\n\n코드 이해..\n\n튜토리얼 완성 pip으로 가능하게 github 파서\n그림 고민\n\n연구/논문 조정\n\n논리 전개(논문) - 개요\ncam 논문 라이팅\n\n\n\n모두 undirected data이고, 데이터셋 중 한 개만 directed(Wiki)임.\n\n\n\n\n\n\n\n\n\n\n\nModel\nNode_feature\nEdge Index\nEdge Weight\nEdge Type\n\n\n\n\nGConvGRU\nO, Node features\nO\nO\nX\n\n\nGConvLSTM\nO, Node features\nO\nO\nX\n\n\nGCLSTM\nO, Node features\nO\nO\nX\n\n\nLRGCN\nO, Node features\nO\nO\nO\n\n\nDyGrEncoder\nO, Node features\nO\nO\nX\n\n\nEvolveGCNH\nO, Node embedding\nO\nO\nX\n\n\nEvolveGCNO\nO, Node embedding\nO\nO\nX\n\n\nTGCN\nO, Node features\nO\nO\nX\n\n\nDCRNN\nO, Node features\nO\nO\nX\n\n\n\nDefine Spatio Temporal Graph data\n\n\n\n\n\n\n\nNotations\nDefinitions or Description\n\n\n\n\n\\({\\cal G}\\)\nInput Graph\n\n\n\\(T\\)\nthe length of the time interval\n\n\n\\(\\cal{V}\\)\na set of Verteces\n\n\n\\(V\\)\na set of an index node\n\n\n\\(N\\)\nthe number of Verteces\n\n\n\\(\\cal{E}\\)\na set of undirected Edges\n\n\n\\(\\textbf{y}\\)\na graph signal, a function defined on the vertices of the graph \\(\\cal{G}\\)\n\n\n\\(\\textbf{W}\\)\na weighted adjacency matrix\n\n\n\n\nGraph \\({\\cal G} = (\\cup_{t \\in {\\cal T}} {\\cal V}_t, \\cup_{t \\in {\\cal T}} {\\cal E}_t, \\textbf{W})\\) ,\\({\\cal T}:=\\{1,\\dots,T\\}\\)\n\n\\(T\\) is the length of the time interval.\n\\(\\cal{V}\\) is a set of Verteces. \\({\\cal V} = \\{v_0,v_1,\\cdots,v_{N-1}\\}\\)\n\\(V\\) is a set of an index node of \\(V = \\{ 0, 1, \\dots, N-1 \\}\\)\n\n\\(N\\) is the number of Verteces. $|| = $\\(N\\)\n\n\\(\\cal{E}\\) is a set of undirected Edges\n\n${E} { ( x,y ) | x,y $\\(\\text{ and } x \\ne y \\}\\)\n\n\na graph signal \\(\\textbf{y}\\): \\({\\cal V}_t \\to \\mathbb{R}^N\\)\n\n\\(\\textbf{y}\\) is a function defined on the vertices of the graph \\(\\cal{G}\\)\n\n\\(\\textbf{y} = [y_{0,1},y_{0,2}, \\cdots, y_{1,1},y_{1,2}, \\cdots , y_{v,t}]^T\\), \\(v \\in V\\), \\(t \\in {\\cal T}:=\\{1,\\dots,T\\}\\)\n\n\n\\(\\textbf{W}_{T \\times T}\\) is a weighted adjacency matrix and interpreted as a graph shift operator.\n\nSuppose Graph \\(\\cal{G}\\) is a undirected cyclic graph and has time series periodic data, which allows to consider graph adjacency matrix as \\(\\begin{bmatrix} 0 & 1 & \\cdots & 0 \\\\ 1 & 0 & \\cdots & 0 \\\\ \\vdots & \\vdots & \\ddots & \\vdots \\\\ 0 & \\cdots & 1 & 0 \\end{bmatrix}\\)\n\n\n\nDefine Complete Data\n\nDefine \\({\\cal Y} = ({\\cal O} , {\\cal M})\\)\n\n\nA incomplete data \\({\\cal O}\\)\n\n\\({\\cal Y} = \\{ y_{v,t}: t \\in {\\cal T}, v \\in V\\}\\)\n\\({\\cal Y} = \\textbf{Y}_{N \\times T}\\)\n\\({\\cal Y} = \\textbf{y}\\)\n\nA missing data \\({\\cal M} = \\{ y_{v,t_v}: t_v \\in {\\cal M}_v, v \\in V \\}\\)\n\n\\({\\cal M}_{v_1}\\): \\(v_1\\) 노드에서 결측인 \\(t\\)들의 집합\n\nA observed data \\({\\cal O} = \\{ y_{v,t_v}: t_v \\in {\\cal O}_v, v \\in V \\}\\).\n\n\\({\\cal O}_{v_1}\\) : \\(v_1\\) 노드에서 관측된 \\(t\\)들의 집합\n\n\n\nConduct the linear interpolation and Update \\({\\cal M}\\) by \\(\\tilde{{\\cal Y}} = ( {\\cal O} , \\tilde{ {\\cal M} } )\\)\n\n\n\\({\\cal M}\\) is imputated by \\(\\tilde{{\\cal M}}\\).\n\n\nRepeat steps Laplacian, Eigenvalue Decomposition, Graph Fourier Transform, Ebayes Thresholding, Inverse Graph Fourier Transform, respectively\n\nDefine nomalized Laplacian matrix \\(\\textbf{L}\\) from graph shift operator \\(W\\).\n\n\n\\(\\textbf{L} = \\textbf{D}^{-1/2} (\\textbf{I} - \\textbf{W}) \\textbf{D}^{-1/2} = \\textbf{I} - \\bar{\\textbf{W}}\\)\n\n\nCalculate Eigenvalue Decomposition\n\n\n\\(\\textbf{L} = \\mathbf{V} \\Lambda \\mathbf{V}^H\\)\n\n\nApply Graph Fourier Transform\n\n\n\\({{\\mathbf V}}^H {\\bar {\\textbf Y}} = \\tilde{\\textbf{Y}}\\)\n\nTransforming a function from the time domain to the frequency domain.\n\\(p = E([{{\\mathbf V}}^H {\\bar {\\textbf Y}}]^2)\\)\n\n\n\nEstimate power of \\(\\bar{\\textbf{Y}}\\)\n\n\n\\(\\hat{p} := \\frac{1}{{\\cal R}} \\sum^{\\cal R}_{r=1} |\\bar{y}_r|^2\\)\n\nestimation of \\(p\\)\n\\(\\bar{Y}\\) is a set of vectors $_{i} : $\\(\\to \\mathbb{R}^N\\), \\(i \\in T\\)\n\\({\\cal R}\\): a finite set of realization of the process \\(\\bar{y}\\)\n\n\n\nGet \\(\\hat{\\textbf{p}}_{tr}\\) by thresholding \\(\\hat{p}\\) with ebayes thresholding\n\n\nEstimate \\({\\hat p}_{tr}\\) by getting median of this posterior distribution of \\({\\hat p}\\)\n\n\nCalculate \\(\\hat{\\textbf{Y}}_{tr}\\) from Inverse Graph Fourier Transform\n\n\nGet \\(\\hat{\\cal{M}}\\) from values of a set of Indexed missing data on \\(\\hat{\\textbf{Y}}_{tr}\\)\n\nDefine \\(\\hat{{\\cal Y}} = ({\\cal O}, \\hat{{\\cal M}})\\)\n\n\nLearn STGCN with \\(\\hat{{\\cal Y}}\\), and Get \\(\\hat{{\\cal Y}}_{stgcn}\\)\n\n\nSTGCN function \\(:= f(\\cdot)\\)\nGet \\(\\hat{{\\cal M}}^{(1)}\\) from values of a set of Indexed missing data on \\(\\hat{{\\cal Y}}_{tgnn}\\)\nImputatation from \\(\\hat{{\\cal M}}\\) to \\(\\hat{{\\cal M}}^{(1)}\\)\n\n\\(\\hat{{\\cal Y}}^{(1)} = ({\\cal O}, \\hat{{\\cal M}}^{(1)})\\)\n\n\n\nRepeat every steps\n\n\nGet \\(\\hat{{\\cal Y}}^{({\\cal L})} = ({\\cal O}, \\hat{{\\cal M}}^{({\\cal L})})\\)\n\\({\\cal L}\\) is the iteration.\n\n\\({\\cal L} :=\\{ 2,\\cdots , L\\}\\)\n\n\n\nCalculate MSE\nCaculate MSE = \\(\\frac{1}{NT} \\sum^N_{v=1} \\sum^T_{t=1} (y_{v,t} - (\\hat{y}_{v,t})^{(L)}) ^2\\)\n\n\\(\\hat{{\\cal Y}}^{(L)} = ({\\cal O}, \\hat{{\\cal M}}^{(L)})\\)\n\\({\\cal Y}_{com} = \\{ y_{v,t}: v \\in {\\cal V}, t \\in {\\cal T} \\}\\)\n\n\nAlgorithm 1: GFT, EbayesThresh and STGCN\nInput \\(\\tilde{\\cal{Y}} = ( {\\cal O} , \\tilde{ {\\cal M} } )\\)(\\(={\\tilde{\\textbf Y}}\\))\nOutput \\(\\hat{{\\cal Y}}^{(L)}\\)\nfor \\(l\\) =1, \\({\\cal L} :=\\{ 1,2,\\cdots , L\\}\\) do\nApply GFT \\({\\mathbf V}^H {\\tilde {\\textbf Y}} = \\bar{\\textbf{Y}}\\)\n\\(\\hat{p} \\leftarrow \\frac{1}{{\\cal R}} \\sum^{\\cal R}_{r=1} |\\bar{y}_r|^2\\)\nEbayesThresh \\(\\breve{\\textbf{p}} \\leftarrow \\hat{\\textbf{p}}\\)\n\\({\\breve {\\textbf Y}} = \\sqrt{\\breve{\\textbf{p}}}\\)\nApply Inverse GFT \\({\\mathbf V} {\\breve {\\textbf Y}} = \\breve{\\textbf{Y}}_{tr}\\)\n\\({\\cal{\\tilde{M}}} \\leftarrow \\breve{ {\\cal M}_{tr} }\\)\n\\(\\bar{\\cal{Y}} = ( {\\cal O} , \\breve{ {\\cal M}_{tr} } )\\)\n\\(\\text{ STGCN } (\\bar{\\cal{Y}}) \\leftarrow \\bar{{\\cal Y}}^{(l)}_{gcn}\\)\n\\({\\bar {\\cal M} }^{(l)}_{gcn} \\leftarrow \\hat{ {\\cal M} }\\)\nend for\n\nAlgorithm 2: Self Consistency\nInput \\(\\hat{g}_{obs}(\\cdot) = \\tilde{\\cal{Y}}\\)\nOutput \\(\\hat{g}_{com}(\\cdot) = {\\cal \\bar{Y}}^{(l)}\\)\n\\(E(\\bar{g}_{com} |{\\cal O}; g = \\bar{g}_{obs}) = \\bar{g}_{obs}\\)\n\\({\\cal \\bar{Y}}^{(l)} \\leftarrow \\tilde{\\cal{Y}}\\)\n\nAlgorithm 3: EbayesThresh\nInput \\(\\hat{\\textbf{p}}\\)\nOutput \\(\\breve{\\textbf{p}}\\)\n\\(\\hat{\\textbf{p}} = \\textbf{p}_{pp} + \\textbf{p}_{ac}\\)\n\\(\\breve{\\textbf{p}} \\leftarrow \\textbf{p}_{pp}\\)\n\nToy Example\nDefine data\nGraph \\({\\cal G} = (V, E,\\)\\(\\textbf{W})\\)\n\n\\(\\cal{V}\\) is a set of Verteces. \\({\\cal V} = \\{ v_0, v_1, v_2 \\}\\), \\(|{\\cal V}| = 3\\), \\(V = \\{ 0,1,2\\}\\)\n\\(\\cal{E}\\) is a set of undirected Edges. \\({\\cal E} =\\{ ( 0, 1), ( 0, 2) \\}\\)\na graph signal \\(\\textbf{y}\\): \\({\\cal V}_t \\to \\mathbb{R}^3\\)\n\\(T = 10\\)\n\\({\\cal Y} = \\{y_{0,1},y_{0,2},\\cdots,y_{2,9},y_{2,10} \\}\\)\n\n\\({\\cal Y} = \\textbf{y}\\)\n\n\\(\\textbf{W}_{10 \\times 10}\\) is a weighted adjacency matrix.\n\n\\(\\textbf{W}_{10 \\times 10} = \\begin{bmatrix} 0 & 1 & 0 &0 & 0 & 0 &0 & 0 & 0 & 0 \\\\ 1 & 0 & 1 & 0 & 0 & 0 & 0 & 0 &0&0 \\\\ 0 & 1 & 0 & 1 & 0 & 0 & 0 & 0 &0&0\\\\ 0 & 0 & 1 & 0 & 1 & 0 & 0 & 0 & 0 & 0\\\\ 0 & 0 & 0 & 1 & 0 & 1 & 0 & 0 & 0 & 0\\\\ 0 & 0 & 0 & 0 & 1 & 0 & 1 & 0 & 0 & 0\\\\ 0 & 0 & 0 & 0 & 0 & 1 & 0 & 1 & 0 & 0\\\\ 0 & 0 & 0 & 0 & 0 & 0 & 1 & 0 & 1 & 0\\\\ 0 & 0 & 0 & 0 & 0 & 0 & 0 & 1 & 0 & 1\\\\ 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 1 & 0 \\end{bmatrix}\\)\n\n\\(y_{v,t} \\to y_{v,t-1}\\)이 이어져 있다고 가정하면 위와 같이 \\(\\textbf{W}\\)가 정의 됌.\n\\(v\\) 자기 자신끼리는 time series peridic data로 이어져 있다고 가정\n\nIncomplete data\n\\({\\cal y} = ( {\\cal O}, {\\cal M} )\\), 노드 3개(0,1,2), 타임 10 \\(\\therefore 3 \\times 10= 30\\)\n\nA incomplete data \\({\\cal Y} = \\{ y_{0,0},y_{0,1},\\cdots, y_{1,0},y_{1,1},\\cdots, y_{2,0},y_{2,1},\\cdots \\}\\)\n\n\\({\\cal Y} =\\) \\(\\textbf{Y}_{3 \\times 10}\\)\n\nA missing data \\({\\cal M} = \\{ y_{0,3}, y_{0,4},\\cdots, y_{1,3},y_{1,8},\\cdots,y_{2,4},y_{2,7},\\cdots \\}\\) -&gt; 임의로 노드별로 50%의 결측값이 뿌려져 있다고 가정\nA observed data \\({\\cal O} = \\{ y_{0,0},\\cdots,y_{1,0},\\cdots,y_{2,0}\\cdots \\}\\). -&gt; 결측값이 아닌 모든 관찰된 값\n\n\nConduct the linear interpolation \\(\\bar{{\\cal Y}} = ( {\\cal O} , \\tilde{ {\\cal M} } )\\)\n\n\n\\({\\cal M}\\) is imputated by \\(\\bar{{\\cal M}}\\).\n\nRepeat\n행렬 확인차 여기 예제에서만 기호 오른쪽 아래에 행 * 열 입력\n그래프 라플라시안\n\n\\(\\textbf{L}_{10 \\times 10} = \\textbf{D}^{-1/2}_{10 \\times 10} (\\textbf{I}_{10 \\times 10} - \\textbf{W}_{10 \\times 10}) \\textbf{D}^{-1/2}_{10 \\times 10} = \\textbf{I}_{10 \\times 10} - \\tilde{\\textbf{W}}_{10 \\times 1|0}\\)\n\n고유값 분해\n\n\\(\\textbf{L}_{10 \\times 10} = \\mathbf{V}_{10 \\times 10} \\Lambda_{10 \\times 10} \\mathbf{V}^H_{10 \\times 10}\\)\n\n그래프 푸리에 변환\n\n\\({\\mathbf{V}}^H_{10 \\times 10}\\) \\(\\bar{\\textbf{Y}}_{10 \\times 3}\\) \\(= \\tilde{\\textbf{Y}}_{10 \\times 3}\\)\n\n이베이즈\n\n\\(\\tilde{y}^2\\) = \\(\\hat{p}\\) and \\(\\hat{p}_{tr}\\) = \\(10 \\times 3\\) matrix\n노드 수만큼 3번 반복\n\n역 그래프 푸리에 변환\n\n\\(\\textbf{V}_{10 \\times 10} \\tilde{\\textbf{Y}}^{'}_{10 \\times 3} = \\hat{\\textbf{Y}}_{tr,10 \\times 3}\\)\n\\(\\hat{{\\cal Y}}_{tr} = \\hat{\\textbf{Y}}_{tr,10 \\times 3}\\)\n여기서 구한 \\(\\hat{\\cal{Y}}_{tr}\\) = \\((\\hat{\\cal{O}},\\hat{\\cal{M}})\\) 기존에 구한 \\(\\bar{{\\cal Y}} = ( {\\cal O} , \\tilde{ {\\cal M} } )\\)\n결측값 부분만 imputation \\(\\hat{{\\cal Y}} = ({\\cal O}, \\hat{{\\cal M}})\\)\n\n모델로 학습\n\n\\(\\hat{{\\cal Y}}\\)로 TGNN 학습 후 \\(\\hat{{\\cal Y}}_{tgnn}\\) 얻고 거기서 \\(\\hat{{\\cal M}}^{(1)}\\) 얻기\n\\(\\hat{{\\cal M}}\\)을 \\(\\hat{{\\cal M}}^{(1)}\\)로 대체\n\n\\(\\hat{{\\cal Y}}^{(1)} = ({\\cal O}, \\hat{{\\cal M}}^{(1)})\\)\n\n\nepoch = 20 가정\n\\(\\hat{{\\cal Y}}^{(20)} = ({\\cal O}, \\hat{{\\cal M}}^{(20)})\\) 을 얻음\n\\(\\hat{{\\cal Y}}^{(20)} = ({\\cal O}, \\hat{{\\cal M}}^{(20)})\\) 를 이용해서 에러 구하기\nMSE = \\(\\frac{1}{30} \\sum^3_{v=1} \\sum^{10}_{t=1} (y_{v,t_v} - (\\hat{y}_{v,t_v})^{(20)}) ^2\\)"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "ITTGNN blog",
    "section": "",
    "text": "This blog provides information on ITTGNN.\n\nSome links related ITTGNN\n\nPytorchDeep learning Architectures\n\n\n\nPytorch Geometric Temporal: https://pytorch-geometric-temporal.readthedocs.io/en/latest/index.html\n\n\n\n\nGConvGRU\nGConvLSTM\nGCLSTM\nLRGCN\nDyGrEncoder\nEvolveGCNH\nEvolveGCNO\nTGCN\nDCRNN\nGNAR\n\n\n\n\n\n\n\n\n\n\n   \n     \n     \n       Order By\n       Default\n         \n          Date - Oldest\n        \n         \n          Date - Newest\n        \n         \n          Title\n        \n         \n          Author\n        \n     \n  \n    \n      \n      \n    \n\n\n\n\n\nDate\n\n\nTitle\n\n\nAuthor\n\n\n\n\n\n\nMay 31, 2099\n\n\nITSTGCN add Model\n\n\nSEOYEON CHOI\n\n\n\n\nMay 16, 2024\n\n\n논문 라이팅\n\n\nSEOYEON CHOI\n\n\n\n\nMar 12, 2024\n\n\nToy example / Algorithm\n\n\nSEOYEON CHOI\n\n\n\n\nFeb 29, 2024\n\n\n실험결과정리\n\n\nSEOYEON CHOI\n\n\n\n\nFeb 1, 2024\n\n\nSelf Consistency Toy ex\n\n\nSEOYEON CHOI\n\n\n\n\nJan 28, 2024\n\n\nGNAR-R\n\n\nSEOYEON CHOI\n\n\n\n\nJan 28, 2024\n\n\n시뮬 데이터 정리중\n\n\nSEOYEON CHOI\n\n\n\n\nJan 14, 2024\n\n\nToy Example Figure(Intro)\n\n\nSEOYEON CHOI\n\n\n\n\nJan 8, 2024\n\n\nResearch Sections of Experiments\n\n\nSEOYEON CHOI\n\n\n\n\nDec 29, 2023\n\n\nFigure for dashboard\n\n\nSEOYEON CHOI\n\n\n\n\nSep 20, 2023\n\n\n[IT-STGCN]논문리비전_수정\n\n\nSEOYEON CHOI\n\n\n\n\nAug 25, 2023\n\n\nBatch\n\n\nSEOYEON CHOI\n\n\n\n\nJul 20, 2023\n\n\nData management Figure for ITSTGCN\n\n\nSEOYEON CHOI\n\n\n\n\nJul 18, 2023\n\n\nEbayesThresh Toy ex\n\n\nSEOYEON CHOI\n\n\n\n\nJul 8, 2023\n\n\nToy example using GNAR\n\n\nSEOYEON CHOI\n\n\n\n\nJul 8, 2023\n\n\nToy example using GNAR\n\n\nSEOYEON CHOI\n\n\n\n\nJul 5, 2023\n\n\nData management for ITSTGCN\n\n\nSEOYEON CHOI\n\n\n\n\nJul 1, 2023\n\n\nEvolveGCNH_Simulation Tables_reshape\n\n\nSEOYEON CHOI\n\n\n\n\nJul 1, 2023\n\n\nEvolveGCNH_Simulation_reshape\n\n\nSEOYEON CHOI\n\n\n\n\nJun 28, 2023\n\n\nDYGRENCODER_Simulation Tables_reshape\n\n\nSEOYEON CHOI\n\n\n\n\nJun 28, 2023\n\n\nDYGRENCODER_Simulation_reshape\n\n\nSEOYEON CHOI\n\n\n\n\nJun 28, 2023\n\n\nDYGRENCODER_Simulation_reshape\n\n\nSEOYEON CHOI\n\n\n\n\nJun 25, 2023\n\n\nEvolveGCNO_Simulation Tables_reshape\n\n\nSEOYEON CHOI\n\n\n\n\nJun 25, 2023\n\n\nEvolveGCNO_Simulation_reshape\n\n\nSEOYEON CHOI\n\n\n\n\nJun 20, 2023\n\n\nTGCN_Simulation Tables_reshape\n\n\nSEOYEON CHOI\n\n\n\n\nJun 20, 2023\n\n\nTGCN_Simulation_reshape\n\n\nSEOYEON CHOI\n\n\n\n\nJun 13, 2023\n\n\nDCRNN_Simulation Tables_reshape\n\n\nSEOYEON CHOI\n\n\n\n\nJun 13, 2023\n\n\nDCRNN_Simulation_reshape\n\n\nSEOYEON CHOI\n\n\n\n\nJun 13, 2023\n\n\nLRGCN_Simulation Tables_reshape\n\n\nSEOYEON CHOI\n\n\n\n\nJun 13, 2023\n\n\nLRGCN_Simulation_reshape\n\n\nSEOYEON CHOI\n\n\n\n\nJun 6, 2023\n\n\nGCLSTM_Simulation Tables_reshape\n\n\nSEOYEON CHOI\n\n\n\n\nJun 6, 2023\n\n\nGCLSTM_Simulation_reshape\n\n\nSEOYEON CHOI\n\n\n\n\nMay 30, 2023\n\n\nGConvLSTM_Simulation Tables_reshape\n\n\nSEOYEON CHOI\n\n\n\n\nMay 30, 2023\n\n\nGConvLSTM_Simulation_reshape\n\n\nSEOYEON CHOI\n\n\n\n\nMay 27, 2023\n\n\nAdding the RecurrentGCN models\n\n\nSEOYEON CHOI\n\n\n\n\nMay 25, 2023\n\n\nGConvGRU_Simulation Boxplot_reshape\n\n\nSEOYEON CHOI\n\n\n\n\nMay 25, 2023\n\n\nGConvGRU_Simulation Tables_reshape\n\n\nSEOYEON CHOI\n\n\n\n\nMay 17, 2023\n\n\nGConvGRU and GNAR_Simulation Tables_reshape\n\n\nSEOYEON CHOI\n\n\n\n\nMay 11, 2023\n\n\nPyG Geometric Temporal CPU vs GPU\n\n\nSEOYEON CHOI\n\n\n\n\nMay 11, 2023\n\n\nPyG Geometric Temporal Examples\n\n\nSEOYEON CHOI\n\n\n\n\nMay 6, 2023\n\n\nITSTGCN Article Refernece\n\n\nSEOYEON CHOI\n\n\n\n\nMay 4, 2023\n\n\nQuestions of PyTorch Geometric Temporal\n\n\nSEOYEON CHOI\n\n\n\n\nApr 29, 2023\n\n\nPadalme GSO_st\n\n\nSEOYEON CHOI\n\n\n\n\nApr 27, 2023\n\n\nSimulation Tables\n\n\nSEOYEON CHOI\n\n\n\n\nApr 27, 2023\n\n\nToy Example Note\n\n\nGUEBIN CHOI\n\n\n\n\nApr 25, 2023\n\n\nNote_weight amatrix\n\n\nGUEBIN CHOI\n\n\n\n\nApr 6, 2023\n\n\nMETRLADatasetLoader-Tutorial\n\n\nSEOYEON CHOI\n\n\n\n\nApr 5, 2023\n\n\nSimulation\n\n\nSEOYEON CHOI\n\n\n\n\nApr 5, 2023\n\n\nSimulation\n\n\nSEOYEON CHOI\n\n\n\n\nApr 5, 2023\n\n\nSimulation_reshape\n\n\nSEOYEON CHOI\n\n\n\n\nMar 22, 2023\n\n\nSimualtionPlanner-Tutorial\n\n\nSEOYEON CHOI\n\n\n\n\nMar 20, 2023\n\n\ndata load, data save as pickle\n\n\nSEOYEON CHOI\n\n\n\n\nMar 18, 2023\n\n\nSimualtionPlanner-Tutorial\n\n\nSEOYEON CHOI\n\n\n\n\nMar 17, 2023\n\n\nITSTGCN-Tutorial\n\n\nSEOYEON CHOI\n\n\n\n\nMar 3, 2023\n\n\nSY 1st ITSTGCN\n\n\nSEOYEON CHOI\n\n\n\n\nFeb 15, 2023\n\n\n1st ITSTGCN\n\n\nGUEBIN CHOI\n\n\n\n\nFeb 15, 2023\n\n\n2nd ITSTGCN\n\n\nGUEBIN CHOI\n\n\n\n\nFeb 7, 2023\n\n\nClass of Method(WikiMath) lag 1\n\n\nSEOYEON CHOI\n\n\n\n\nFeb 6, 2023\n\n\nClass of Method(GNAR) lag 1 80% Missing repeat\n\n\nSEOYEON CHOI\n\n\n\n\nFeb 6, 2023\n\n\nClass of Method(GNAR) lag 2\n\n\nSEOYEON CHOI\n\n\n\n\nJan 28, 2023\n\n\nClass of Method(GNAR) lag 1\n\n\nSEOYEON CHOI\n\n\n\n\nJan 28, 2023\n\n\nClass of Method(WikiMath) lag 4\n\n\nSEOYEON CHOI\n\n\n\n\nJan 26, 2023\n\n\nClass of Method\n\n\nGuebin Choi\n\n\n\n\nJan 21, 2023\n\n\nClass of Method\n\n\nSEOYEON CHOI\n\n\n\n\nJan 20, 2023\n\n\n1st ST-GCN Example dividing train and test\n\n\nSEOYEON CHOI\n\n\n\n\nJan 17, 2023\n\n\n2nd ST-GCN Example dividing train and test\n\n\nSEOYEON CHOI\n\n\n\n\nJan 11, 2023\n\n\nGCN Algorithm Example 1\n\n\nSEOYEON CHOI\n\n\n\n\nJan 5, 2023\n\n\nGNAR data\n\n\nSEOYEON CHOI\n\n\n\n\nDec 29, 2022\n\n\n[IT-STGCN] STGCN 튜토리얼\n\n\n신록예찬, SEOYEON CHOI\n\n\n\n\nDec 28, 2022\n\n\nSimulation of geometric-temporal\n\n\nSEOYEON CHOI\n\n\n\n\nDec 21, 2022\n\n\nPyTorch ST-GCN Dataset\n\n\nSEOYEON CHOI\n\n\n\n\nDec 5, 2022\n\n\nTORCH_GEOMETRIC.NN\n\n\nSEOYEON CHOI\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/1_studies/2023-09-20-논문리비전_수정.out.html",
    "href": "posts/1_studies/2023-09-20-논문리비전_수정.out.html",
    "title": "[IT-STGCN]논문리비전_수정",
    "section": "",
    "text": "1. intro\n- 초록색은 나쁘지 않음. 하지만 아래의 내용을 보완하는게 좋음.\n\n분야의 예시로 신경과학, 환경데이터, 교통자료가 있는데 우리가 실제로 분석한 자료들이 사용된 논문을 찾아보며 예시를 들것 (Chickenpox, …) 사용하지 않더라도 예시를 들것.\n이러한 자료를 분석하는것이 왜 어려운지 설명할 것. 즉 단순히 시계열로 해석하거나 공간자료로 해석하면 어떠한 문제가 있는지 간단히 서술할 것. (1~2문장) 레퍼런스 찾을것. (torch_geometric_temporal 의 도입부분 활용)\n\n기존\nIn recent years, the field of spatiotemporal datasets has emerged, enabling the simultaneous con- sideration of both the time and space dimensions. The examples include neuroscience(Atluri et al., 2016), environmental data(Thompson et al., 2014), traffic dynamics(Castro et al., 2013), and more. Specifically, traffic dynamics is a prevalent spatiotemporal dataset and is crucial because examining traffic data from both spatial and temporal perspectives can lead to advancements in traffic control. The incorporation of both spatial and temporal aspects enables a comprehensive understanding of complex phenomena, making spatiotemporal datasets invaluable for various applications and en- hancing the accuracy of predictive models.\n참고\nPyTorch Geometric Temporal: Spatiotemporal Signal Processing with Neural Machine Learning Models\n\nAt the same time the existing geometric deep learning frameworks operate on graphs which have a fixed topology and it is also assumed that the node features and labels are static. Besides limiting assumptions about the input data, these off-the-shelf libraries are not designed to operate on spatiotemporal data.\n\nSpatio-Temporal Graph Convolutional Networks: A Deep Learning Framework for Traffic Forecasting\n\nClassic statistical and machine learning models are two major representatives of data-driven methods. In time- series analysis, autoregressive integrated moving average (ARIMA) and its variants are one of the most consolidated approaches based on classical statistics [Ahmed and Cook, 1979; Williams and Hoel, 2003]. However, this type of model is limited by the stationary assumption of time sequences and fails to take the spatio-temporal correlation into account. Therefore, these approaches have constrained representabil- ity of highly nonlinear traffic flow. Recently, classic statistical models have been vigorously challenged by machine learning methods on traffic prediction tasks.\nDue to the high nonlinearity and complexity of traffic flow, tradi- tional methods cannot satisfy the requirements of mid-and-long term prediction tasks and often ne- glect spatial and temporal dependencies.\n\n수정\nIn recent years, the field of spatiotemporal datasets has emerged, enabling the simultaneous consider- ation of both the time and space dimensions. The examples include health data(Rozemberczki et al., 2021b), customer data(Rozemberczki et al., 2021a), energy data(Rozemberczki et al., 2021a), neu- roscience(Atluri et al., 2016), environmental data(Thompson et al., 2014), traffic dynamics(Castro et al., 2013), and more. Specifically, traffic dynamics is a prevalent spatiotemporal dataset and is crucial because examining traffic data from both spatial and temporal perspectives can lead to ad- vancements in traffic control. Classic time-series statistical methods to analyze those kind of data already exist, but they are limited by certain conditions, such as assumptions about the data. Specif- ically, these classic methods cannot account for spatiotemporal correlations and are not designed to work with spatiotemporal data(Yu et al., 2017; Rozemberczki et al., 2021a). In result, when we analize spatiotemporal data to use enough information, we can improve accuracy during us- ing appropriate geometric deep learning frameworks.\n\n- 붉은부분\n\n의도는 좋으나 sparse data 는 올바르지 않은 표현임. missing, irregulary observed data 등으로 설명할 것.\n이러한 자료가 왜 발생하는지 설명할 것. (이부분은 레퍼런스 필요) 이러한 자료를 처리하는 것이 어려운 이유를 설명할 것.[1]\n우리의 아이디어는 “호모지니우스하지 않은 그래프 -&gt; 호모지니우스화 시킴” 인데 이러한 방식은 이상한방식이 아님. Yu et al. (2017) and Guo et al. (2019) Bai et al. (2020), Li et al. (2019), Zhao et al. (2019) 이 우리와 비슷한 연구를 했음.\n\n-기존\nHowever, when dealing with spatiotemporal datasets, sparse data is a common occurrence, which is unpredictable. For example, the sensor data from machines representing a spatiotemporal dataset may contain missing values due to unexpected events like sensor malfunction or temporal factors such as distance or time delay. It is a simple way to use interpolation methods like linear, nearest, etc. However, these methods can occasionally be imprecise in producing estimates. Moreover, in a method of learning spatiotemporal data Yu et al. (2017) and Guo et al. (2019) try to learn data after making it to be complete, i.e., allocate to other values from missing data with linear interpolation. Graph Convolution Network(GCN) is also a needed interpolation method before learning. Furthermore, Bai et al. (2020), Li et al. (2019), Zhao et al. (2019) tried to fill missing values by linear interpolation.\n- 참고\nTraffic Speed Prediction with Missing Data based on TGCN\n\nIn addition, there usually contains missing values in the collected data of traffic sensors due to the electronics unit failure. As is shown in Fig.1, There exist a lot of missing values during 22:00-24:00. This can decrease the prediction accuracy of aforementioned prediction models.\nFor the proposed model, if the input time series contains missing values, the model will produce failure because of the missing values can not be computed during the training process.\n\nMissing Data: Our View of the State of the Ar\n\nWhy do missing data create such difficulty in scientific research? Because most data analysis procedures were not designed for them. Missingness is usually a nuisance, not the main focus of inquiry, but handling it in a principled manner raises conceptual difficulties and computational challenges\n\nLSTM-based traffic flow prediction with missing data\n\nNevertheless, due to missing data, irregular sampling, and varying length, the data remain difficult to explore with high efficiency. In a traffic environment, this problem becomes even worse because the traffic sensors are often controlled manually.\n\nGraph neural networks: A review of methods and applications\n\nHomogeneous/Heterogeneous Graphs. Nodes and edges in ho- mogeneous graphs have same types, while nodes and edges have different types in heterogeneous graphs. Types for nodes and edges play important roles in heterogeneous graphs and should be further considered.\n\nT-GCN: A Temporal Graph Convolutional Network for Traffic Prediction\n\nSince the Los-loop dataset contained some missing data, we used the linear interpolation method to fill missing values.\n\nSpatio-Temporal Graph Convolutional Networks: A Deep Learning Framework for Traffic Forecasting\n\nThe linear interpolation method is used to fill missing values after data cleaning. In addition, data input are normalized by Z-Score method.\n\nAdaptive Graph Convolutional Recurrent Network for Traffic Forecasting\n\nData Preprocess: The missing values in the datasets are filled by linear interpolation. Then, both datasets are aggregated into 5-minute windows, resulting in 288 data points per day.\n\n- 수정\nDealing with spatiotemporal datasets often presents a common challenge, which is the frequent occurrence of irregularly observed data. For instance, as highlighted by (Ge et al., 2019), traffic sensor data commonly suffers from missing observations due to electronic unit failures, which can significantly impact prediction accuracy. The difficulty in handling irregular data arises for several reasons. First, many traditional data analysis procedures were designed for datasets with complete observations Schafer & Graham (2002). Second, when dealing with time-series datasets containing missing data, attempting to learn from such data can lead to challenges as it may result in the failure to capture certain time points Ge et al. (2019); Tian et al. (2018). That’s the reason why it’s important to transform incomplete data into complete data before conducting any learning or analysis.\nBefore move on to introduce our purpose, we need a definition of graph signal. To describe the geometric structures of data domain, graphs are well known as generic data representation forms(Shuman et al., 2013). So in this paper, we interpret data as Gt = (Vt,Et), V means ver- tics and E means edges. On specific Gt, it has a finite collection of samples and we call it as a graph signal(Shuman et al., 2013). Now, we would like to show the purpose of this paper that is mak- ing complete data when we approach the irregularly data. To satisfy this condition, we recognize the data not the heterogeneous graph, but the homogeneous graph by interpolation. Homogeneous graphs have same types of nodes and edges, and hetero geneous graphs have different types of them(Zhou et al., 2020). In a method of learning spatiotemporal data, Bai et al. (2020); Zhao et al. (2019); Yu et al. (2017); Guo et al. (2019) try to learn data after making it to be complete, i.e., allocate to other values from missing data with linear interpolation. In our proposed method, it is crucial to rightly estimate the underlying function when training spatiotemporal dataset because the functions define the expected pattern of the data. And that pattern would affect to read the trend of datasets. However it can be hard to estimate when it has many percentage of missing date.\n\n- 아래식은 틀렸음. 이건 회귀모형이 아님.. GNAR의 notation을 사용하여 모형을 다시표현해볼것..\n\n\n\nimage.png\n\n\n\n이부분이 아주 클리어 해야함\n사용하는 대부분의 Notations들이 정리되어야함.\nintro에 쓰는 것이 부담스러우면 제외해도 무방\n뒤에 self consistence estimator에 사용할 Notation을 함께 고려\n\n- 빨간부분 삭제후 다시 작성 (혹은 공부할 것)\n\n\n\nimage.png\n\n\n- 초록색부분은 나쁘지 않음\n\n\n\nimage.png\n\n\n기존\nAfter interpolation to learn dataset, we can write a model as \\[y_i =f(x_i)+ε_i,\\] f(xi) represents the underlying function, and εi is thought to follow a normal distribution. In this paper, we try to train yi as eliminate sparse strong signal of εi to get lower mean square error between test data and predicted data. In other word, we study to remain εi without points which can consider heavier tails. In our proposed method, it is crucial to rightly estimate the underlying function f when training spatiotemporal dataset because the functions define the expected pattern of the data. And that pattern would affect to read the trend of datasets. However it can be hard to estimate f when it has many percentage of missing date.\n수정\n삭제함\n\n\n\n2. Related works\n- 2.1과 2.2를 왜 리뷰하는지 설명이 필요함\n- 2.1에서 왜 Convolution Operator에 집중하는지 설명이 필요\n- 2.2에서 왜 Dynamic graphs에 집중하는지 설명이 필요\n- 전체적으로 이름은 related works인데 뭐가 related 되어있길래 이런것들을 소개하는지 클리어하지 않음. (솔직히 저도 저 방법들이 우리랑 뭔 관련있는지 잘모르겠어요)\n- 기존\nRELATED WORK\n2.1 PROPAGATION MODULE\nTo start build the model with the simple graph structure, we can use computational modules which are the propagation module, the sampling module, and the pooling module. Especially, the prop- agation module is a commonly used computational module. It utilizes convolution and recurrent operators to aggregate information about neighbors. The skip operation is a rule of gathering infor- mation from past representations and mitigating the over-smoothing problem. It can be divided into two types: convolutional and recurrent operator(Zhou et al., 2020) and we focus on colvolutional operator. 2Under review as a conference paper at ICLR 2024\nConvolutional Operator The convolutional operator can be considered a combination of spectral and spatial methods. First, there are a few classic models, which are spectral approaches: Spec- tral Network, ChebNet, and Adaptive Graph Convolution Network(AGCN). The spectral network is proposed by Bruna et al. (2013), which is defined as the characteristics of convolutions in the Fourier domain, which are determined by the eigendecomposition of the graph Laplacian. ChebNet, suggested by Defferrard et al. (2016), employed the K-localized convolution to construct a convolu- tional neural network that could avoid calculating eigenvectors of Laplacian. AGCN(Li et al., 2018) follows the relationship of the spatial aspect, at the same time, uses the residual graph Laplacian, and Li et al. (2018) called it an Adaptive graph. Next are the spatial approaches. The concept of Neural Frames Per Second (Neural FPS) is introduced by Duvenaud et al. (2015). They utilize dif- ferent weight matrices based on nodes with different degrees, but this approach may not be scalable to handle large-scale data. There is a model called Patchy-san proposed by Niepert et al. (2016). In the first step of this model, they select k numbers of neighbors of nodes. After normalizing around k neighbors, the model functions as a receptive field. The Diffusion-Convolutional Neural Net- works(DCNNs) of Atwood & Towsley (2016) are also considered the neighbor between nodes and can be used in classification by changing edges and adjacency matrix. DCNN uses the metrics of transition to get the neighborhood for nodes. The Dual Graph Convolutional Networks (DGCN) pro- posed by Zhuang & Ma (2018) consider local and global consistency. Gao et al. (2018) proposed the Learnable Graph Convolutional Networks (LGCN), which is based on the Learnable Graph Convo- lutional Layer, and the layer transforms the graph into a 1-D format, taking into account the number of nodes for definition.\n2.2 GRAPH TYPE AND SCALE\nIt is important to consider there is not the only simple type graphs. So, we can approach to face variant grape types for real world data which is complex. The graphs’ classification categories can be directed/undirected, Homogeneous/heterogeneous, and static/dynamic graphsZhou et al. (2020). The directed graph can be called when edges of graph are connected, and the undirected graph means the opposite. The directed graph is better than the undirected graph because the first one has more information than the second one. The homogeneous graph has the same types of nodes and edges; however, the heterogeneous graph has different types. That means that information on nodes and edges is important when we analyze the heterogeneous graph. We can call a dynamic graph if the input features or graph topology change. It is reasonable that time points should be considered carefully there rather than a static graph. Zhou et al. (2020) also propose a classification of graphs based on their scale and type, which includes directed, heterogeneous, dynamic, hypergraph, signed, and large graphs.\nDynamic graphs Among them, we focus on the dynamic graph. Spatial and temporal informa- tion is collected on DCRNN(Diffusion Convolution Recurrent Neural Network)(Li et al., 2017) and STGCN(Spatio-temporal graph convolutional networks)(Yu et al., 2017). In detail, DCRNN gets the spatial data by GNN and then transfer the output to sequence-to-sequence or the sequence model such as RNN to consider temporal dependency and STGCN stacks multiple statio-temporal con- volutional blocks which are consisted one spatil graphconvolutional layer and two temporal gate convolutional layers. On the other hand, Structure-RNN(Jain et al., 2016) and ST-GCN(Yan et al., 2018) simultaneously capture spatial and temporal messages. To enable the application of traditional GNNs on the extended graphs, both Structural-RNN and ST-GCN expand the static graph structure by incorporating temporal connections. Structual-RNN adds edges between consecutive time steps, representing nodes and edges with nodeRNNs and edgeRNNs in a bipartite graph. ST-GCN involves constructing spatiotemporal graphs by stacking graph frames from each time step. However, Pareja et al. (2020) argue that using node features in learning can impact the model’s performance and propose EvolceGCN, a method designed for dynamic graphs.\n- 참고\n\nsnapshot이 homogeneous가 아닌데 missing 부분을 채워 넣어 homogeneuos graph 로 해석하고 분석\n\n위에서 언급한 저자들 입력\nSpatio-Temporal Graph Convolutional Networks: A Deep Learning Framework for Traffic Forecasting(Bing Yu, Haoteng Yin, Zhanxing Zhu)\n\nThe linear interpolation method is used to fill missing values after data cleaning. In addition, data input are normalized by Z-Score method.\n\nGraph Markov network for traffic forecasting with missing data\n\nWe denote the completed state by, in which all missing values are filled based on historical data\n\n\n아예 full로 데이터가 존재한다고 가정하고 homogenous graph 로 보고 제시된 방법론\n\nScalable Spatiotemporal Graph Neural Networks(Andrea Cini, Ivan Marisca, Filippo Maria Bianchi, Cesare Alippi)\n\nThe first dataset contains data coming from the Irish Commission for Energy Reg- ulation Smart Metering Project (CER-E; Commission for Energy Regulation 2016), which has been previously used for benchmarking spatiotemporal imputation methods (Cini, Marisca, and Alippi 2022);however, differently from previ- ous works, we consider the full sensor network consisting of 6435 smart meters measuring energy consumption ev- ery 30 minutes at both residential and commercial/industrial premises.\n\n\n처음부터 heterogeneous graph를 input data로 가정하며 만들어진 방법론\n\nLearning Deep Representation from Big and Heterogeneous Data for Traffic Accident Inference(Quanjun Chen, Xuan Song, Harutoshi Yamada, Ryosuke Shibasaki)\n\nBy mining big and hetero- geneous data, we aim to understand and develop a general model to estimate traffic accident risk. With the input of real- time GPS data, our model can simulate traffic accident risk on a large scale.\nWe extract hierarchical feature representation of meshed human mobility data from Stack denoise Autoencoder (SdAE), for a more efficient and precise prediction of risk levels in supervised learning.\n\nISTD-GCN: Iterative Spatial-Temporal Diffusion Graph Convolutional Network for Traffic Speed Forecasting(Yi Xie, Yun Xiong, Yangyong Zhu)\n\nTherefore, we can model such heterogeneous spatial-temporal structures as a homogeneous process ofdiffusion\n\n- 수정\nAs we mentioned, irregular spatiotemporal data is often encountered in the real world. It is well- known that neural networks are better suited for regular data. Therefore, many attempts have been made to transform data with different structures into the same structure through snapshots. If we interpret this as a graph, we can divide it into homogeneous graphs and heterogeneous graphs. Ho- mogeneous graphs have the same types of nodes and edges, while heterogeneous graphs do not(Zhou et al., 2020). There are Numerous methods to address the challenge of dealing with this issue. For instance, to fill missing values, Bai et al. (2020); Yu et al. (2017); Guo et al. (2019) employ linear in- terpolation, while Cui et al. (2020) utilize historical data. All of them tried to convert heterogeneous graph into homogeneous graph. However, if we create regular data using interpolation methods, the result may have low accuracy. Additionally, Cini et al. (2023) assume that the input data is originally complete, which is equivalent to interpreting the data as a homogeneous graph from the beginning. Furthermore, Chen et al. (2016); Xie et al. (2020) proposed a general model that treats input data as a heterogeneous graph, assuming a lack of supported sensing data. It might be efficient to handle data with a heterogeneous structure in each snapshot. But the real data often represents homogeneous graph and the missing values transforms it into a heterogeneous graph, that means the structures of every snapshot are not different.\n\n\n3. Backgrounds\n- 좋아요\n- 자잘한건 제가 수정하면 될 듯합니다.\n\n\n4.\n- 내용을 좀 더 팬시하게 쓸 필요가 있어보임\n- 아래부분을 정리하여 알고리즘화 해야함.\n\n\n\nimage.png\n\n\n- 기존\n\n\n5. Experiments\n- 아직 덜 읽어봄\n- 데이터 설명은 Appendix에, 실험결과와 Fig는 본문에 있는게 좋음\n[1] 보통 결측없이 모두 관측한상태에서는 모형이 잘 동작함, 대부분의 spatio temporal data는 각각의 스냅샷마다 동일한 그래프구조를 가진다는 가정을 사용함. 스냅샷마다 그래프구조가 다른 경우를 가정하는 모형도 있음. 그러한 모형의 예시는 A,B,C,…. 등이 있음. 하지만 이러한 연구는 애초에 데이터가 스냅샷마다 non-호모지니우스하게 생겼으면 효율적일 수 있으나, 실제true model은 스냅샷마다 그래프구조가 동일하다고 여겨지지만 결측치로 인하여 스냅샷마다 호모지니우스가 깨지는 경우는 효율적이지 않을 수 있음. 우리는 이 부분에 초점을 맞추었음. 우리의 아이디어는 호모지니우스 하지 않은 그래프를 A,B,C, 등을 이용하여 그대로 처리하는것 보다 missing을 처리하여 호모지니우스하게 강제로 만들고 그 자료를 분석하자는 아이디어임."
  },
  {
    "objectID": "posts/1_studies/2024-05-16-scholar_writing.html#self-consistent-estimator",
    "href": "posts/1_studies/2024-05-16-scholar_writing.html#self-consistent-estimator",
    "title": "논문 라이팅",
    "section": "Self-Consistent Estimator",
    "text": "Self-Consistent Estimator\nTo deal with incomplete data, we introduce the concept of \"self-consistency\" which is proposed by \\citet{efron1967two} who originally used it to address censored data. Then, \\citet{hastie1989principal} provided a definition of principal curves as smooth curves that maintain self-consistency across a distribution or dataset. Especially, the term of \"self-consistency\" is presented for regression function estimator by \\citet{flury1996self, lee2007self}. According to \\citet{lee2007self}, we can estimate $\\hat{f}_{obs}$ given underlying function of observed data with this specific self-consistent equation:\n\n$E(\\hat{f}_{com} |x_{obs}, f = \\hat{f}_{obs}) = \\hat{f}_{obs} \\cdots (1)$\n\n$\\hat{f}_{com}$ is an estimate of $f$ via $x_{com}$ which is assumed as complete data and consisted of $\\{ x_{obs}, x_{mis}\\}$ where $x_{obs}$ is observed data and $x_{mis}$ is missing value which is not available. Additionally, \\citet{lee2007self} obtained the \"optimal\" estimate $\\hat{f}_{com}$ for their regression function $f$, facilitating the derivation of their \"best\" incomplete data estimator $\\hat{f}_{obs}$ using the corresponding complete data procedure. Given the independence of this equation from estimation, it suggests potential applicability to our dataset assuming the presence of missing values. Under this condition, we extend the self-consistent equation with following one:\n\n$E ( \\hat{s}_{t,com} | \\{ f \\{ n \\} : n \\in O \\}, \\{ s_t \\} = \\{ \\hat{s}_{t, obs} \\})= \\hat{s}_{t, obs}, t = 1, 2, \\dots T \\cdots (2)$\n\nWhitin this context, $\\hat{s}_{com}$ represents an estimation of $s_t$ derived from complete data, while $\\hat{s}_{t,obs}$ is an estimation derived from observed data. We utilized $\\{ \\tilde{f} (n): n \\in M\\}$ as missing data $\\{ f(n): n \\in M\\}$ is not available. After calcurating an estimated complete dataset $\\{ \\hat{f} (n)\\} = \\{ f(n) : n \\in O \\} \\cup \\{ \\tilde{f}(n) : n \\in M\\}$, it can be explained as the corresponding decomposition followed:\n\n$\\hat{f}(n) = \\sum^T_{t=1} \\hat{s}_t(n), t=1,2,\\dots , T\\cdots (3)$\n\nThe simplest way to obtain $\\hat{f}_{obs}$ is to update $\\{ f^{(i)}(n): n \\in M\\}$ and decompose $\\hat{f}^{(i)}(n) = \\sum^K_{t=1} \\hat{s}^{(i)}_k(n)$ iteratively. Furthermore, $f^{(i)}(n)$ is satisfied the condition of self-consistency if $f^{(i)}(n) = f^{(i+1)}(n)$, and it was varyfied by \\citet{cox1984analysis, flury1996self}. \n\nNote that we define the $i$ as epoch and use linear implication method for part of missing data in our experiment.\n\n\\subsection{Ebayesthresh}\nThen we can say that if $F(\\omega)$ can properly estimated and $F_s(\\omega)$ can be properly extracted from $F(\\omega)$, the deterministic term(underlying function) of $x_t$ can be obtained. We employ the empirical Bayes thresholding bscause we would estimate $F_s(\\omega)$ as a periodogram of $x_t$ and extract $F_s(\\omega)$ from $F(\\omega)$ \\cite{johnstone2005ebayesthresh}. We assume throughout that the observations. \n\n$$X_i \\sim N(\\mu_i,1)$$ \n\nWithin a Bayesian context, the notion of sparsity is naturally modeled by a suitable prior distribution for the parameters $\\mu_i$. We model the $\\mu_i$ as having independent prior distributions each given by the mixture \n$$f_{\\tt{prior}}(\\mu)=(1-w)\\delta_0(\\mu)+w\\gamma(\\mu).$$ \nHere the function \\(\\gamma\\) is usually chosen as Laplace density with scale parameter \\(a &gt; 0\\) \\(\\gamma(u)=\\frac{a}{2}e^{-a|u|}.\\) The empirical Bayes approach estimates each \\(\\mu_i\\) by its posterior median."
  },
  {
    "objectID": "posts/1_studies/2024-05-16-scholar_writing.html#ebayse-threshold",
    "href": "posts/1_studies/2024-05-16-scholar_writing.html#ebayse-threshold",
    "title": "논문 라이팅",
    "section": "Ebayse Threshold",
    "text": "Ebayse Threshold\nFirst, we give information about \\({\\cal G} = \\{ V, E, {\\bf f} \\}\\), where \\(| V|\\) is the number of nodes, and we denote $ E$ as edges of the graph, which contains a connection of nodes. We consider an adjacency matrix \\(\\bf{W} \\in \\mathbf{R}^{T \\times T}\\) by assuming that the nodes connect as it is a time-series domain and getting graph Laplacian. And then, we get \\({\\bf D}\\), which is a diagonal degree matrix, and normalize it. It is available to decompose the graph Laplacian \\({\\bf \\tilde L}={\\bf V}{\\bf \\Lambda}{\\bf V}^\\top\\). Now, we get Graph Fourier transforms of \\(\\bf f\\) and calculate \\({\\bf V}^\\top{\\bf f}\\). It allows us to obtain a periodogram of it. We want to get \\(f_{\\tt trimed}\\) so we used Empirical Bayes Thresholding by to estimate \\(f_{\\tt threshed}\\) as known as step function from \\(f\\) values. We can impute missing values on their index to calculate \\(f_{\\tt trimed}\\)."
  },
  {
    "objectID": "posts/1_studies/2024-05-16-scholar_writing.html#overview-of-method",
    "href": "posts/1_studies/2024-05-16-scholar_writing.html#overview-of-method",
    "title": "논문 라이팅",
    "section": "Overview of Method",
    "text": "Overview of Method\n\\begin{algorithm}\n\\caption{ITSTGNN algorithm}\n\\begin{algorithmic}[1]\n    \\INPUT Graph ${\\cal G}$\n    \\ENSURE ddd\n    \\STATE dd\n    \\WHILE{d}\n        \\STATE ss\n    \\ENDWHILE\n\\end{algorithmic}\n\\end{algorithm}\nWhen the graph signal \\({\\bf f}\\) is given on \\({\\cal G} = (V, E)\\) on spatio temporal data, The detailed steps of our proposed method are summarized as follows:"
  },
  {
    "objectID": "posts/1_studies/2024-05-16-scholar_writing.html#conditions",
    "href": "posts/1_studies/2024-05-16-scholar_writing.html#conditions",
    "title": "논문 라이팅",
    "section": "Conditions",
    "text": "Conditions\nIn this section, we evaluated the performance of our proposed method in different architectures by changing the missing proportion of several datasets. Real-world datasets often include a substantial number of missing values, and as the rate of missing values rises, it becomes progressively challenging for the data to follow trends. Therefore, we aim to conduct those experiments with gradually higher rates of missing values. Furthermore, we introduced incomplete data by choosing missing data randomly and in blocks to simulate real-world scenarios. The experiments were conducted under three parts:\n\n\\begin{itemize}\n\n    \\item{Baseline}: We conducted with the original observed complete data(Table \\ref{tab:datainfo}).\n    \\item{Randomly Missing}: The proportion of missing values which were selected completly at random was various and that values in every node.\n    \\item{Block Missing}: We assumed that some nodes experiences missing values during a specific interval.\n\n\\end{itemize}"
  },
  {
    "objectID": "posts/1_studies/2024-05-16-scholar_writing.html#models-description",
    "href": "posts/1_studies/2024-05-16-scholar_writing.html#models-description",
    "title": "논문 라이팅",
    "section": "Models Description",
    "text": "Models Description\nWe included nine recurrent graph convolutions temporal graph neural networks methods\\cite{rozemberczki2021pytorch}, which incorporates deep learning and parametric learning approaches for processing spatio temporal signals. We also used GNAR(Generalised Network AutoRegressive) proposed by \\cite{knight2019generalised}. GNAR is known as a Graph Deviation Network based on the AR(Auto Regressive) model, a combination of a learning structure and a Graph Neural Network, and it predicts the new value for using the past one\\citep{knight2019generalised}. That is the reason why the forecasting values of GNAR converge to zero. As a result, we employed ten types of different methods in this paper and present the explanations of each model like Table \\ref{tab:modelexp} on \\textbf{Appendix}. We utilized Mean Square Error (MSE) as the evaluation metric to assess the forecasting accuracy of the datasets. The results are presented as Mean $\\pm$ Standard Deviation (SD)."
  },
  {
    "objectID": "posts/1_studies/2024-05-16-scholar_writing.html#datasets",
    "href": "posts/1_studies/2024-05-16-scholar_writing.html#datasets",
    "title": "논문 라이팅",
    "section": "Datasets",
    "text": "Datasets\nWe carried out our experiments using several datasets having stability, which we can get on PyTorch Geometric temporal from \\citet{rozemberczki2021pytorch} and call as Static Graph Temporal Signal(Table \\ref{tab:datainfo}). Also, the dataset was split into training and testing subsets, allocating 80\\% for training and 20\\% for testing purposes."
  },
  {
    "objectID": "posts/1_studies/2024-05-16-scholar_writing.html#randomly-missing-values",
    "href": "posts/1_studies/2024-05-16-scholar_writing.html#randomly-missing-values",
    "title": "논문 라이팅",
    "section": "Randomly Missing values",
    "text": "Randomly Missing values\n\\begin{ex} \n    Figure \\ref{fig:exfig1} illustrates the outcomes for the $\\tt{Chickenpox}$ dataset organized based on varying levels of missing data, with the GConvLSTM employed. As the missing data rates increase, both Classic(STGNN) and Proposed models(IT-STGNN) exhibit a tendency for the mean squared error (MSE) to rise. Particularly noteworthy is the comparison between the trendlines of Classic and Proposed: as the rate of missing values increases, the MSE of the Proposed method tends to increase more gradually. In contrast, the Classic model displays a more rapid increase in MSE. This comparison suggests that as the ratio of missing values grows, our proposed methods tend to predict better compared to the Classic models. It becomes evident that as the percentage of missing data becomes higher, our proposed method performs relatively well.\n\\end{ex}\n\n\\begin{figure}\n    \\centering\n    \\includegraphics[width=1\\linewidth]{figure/ex1.png}\n    \\caption{ata.}\n    \\label{fig:exfig1}\n\\end{figure}\n\n\\begin{ex} \n    Figure \\ref{fig:exfig2} gives information about an overview of experiment outcomes across five datasets at nine archetectures, showcasing the impact of varying rates of randomly distributed missing values. It is clear that most results has the incremental rise in MSE as the outcome of randomly generated missing values increases. Specifically, Classic(STGNN) methods were hugely on the rise compared to Proposed methods(IT-STGNN).\n\\end{ex}\n\n\\begin{figure}\n    \\centering\n    \\includegraphics[width=1\\linewidth]{figure/ex2.png}\n    \\caption{ata.}\n    \\label{fig:exfig2}\n\\end{figure}\n\n\\textbf{MSE ranking}\n\\begin{ex} \n    MSE rankings were established independently of missing value rates, with higher MSE values positioned towards the right(Figure \\ref{fig:exfig3}). Observations across datasets consistently highlighted the proposed methods' lower MSE performance compared to the classic methods. Additionally, across the Windmillsmall dataset, all proposed methods reliably showed lower MSE values than the classic methods\n\\end{ex}\n\n\\begin{figure}\n    \\centering\n    \\includegraphics[width=1\\linewidth]{figure/ex3.png}\n    \\caption{ata.}\n    \\label{fig:exfig3}\n\\end{figure}\n\n\\textbf{Ratio of datasets' description}\n\n\\begin{ex} \n    Figure \\ref{fig:exfig4} illustrated the ratio($\\frac{T}{V}$) of datasets' information based on Table \\ref{tab:datainfo}($y$-axis) and MSE difference between Proposed methods and Classic methods($x$-axis). According to Figure \\ref{fig:exfig4}, as the propotion($\\frac{T}{V}$) grows, the MSE difference is also going up. It leads to a meaning that our proposed methods would outperform at the specific dataset condition which has much time($T$) data. \n\\end{ex}\n\n\\begin{figure}\n    \\centering\n    \\includegraphics[width=1\\linewidth]{figure/ex4.png}\n    \\caption{ata.}\n    \\label{fig:exfig4}\n\\end{figure}\n\n\\subsection{Block missing values}\n\n\\begin{ex} \n    We highlighted the results when datasets has block missing values(Figure \\ref{fig:exfig5}). There was a MSE difference slightly between Proposed methods and Classic methods respectively. Even thought All of the resules had not dramatic differences, MSE of our proposed methods was lower than Classic one generally. Additionally, it would be valuable research to experiment assuming higher block missing rates in the future.\n\\end{ex}\n\n\\begin{figure}\n    \\centering\n    \\includegraphics[width=1\\linewidth]{figure/ex5.png}\n    \\caption{ata.}\n    \\label{fig:exfig5}\n\\end{figure}"
  },
  {
    "objectID": "posts/1_studies/2024-05-16-scholar_writing.html#conditions-we-set",
    "href": "posts/1_studies/2024-05-16-scholar_writing.html#conditions-we-set",
    "title": "논문 라이팅",
    "section": "Conditions we set",
    "text": "Conditions we set\n\n% \\begin{table*} % % % %\n\\end{table}"
  },
  {
    "objectID": "posts/1_studies/2024-05-16-scholar_writing.html#pedalme",
    "href": "posts/1_studies/2024-05-16-scholar_writing.html#pedalme",
    "title": "논문 라이팅",
    "section": "Pedalme",
    "text": "Pedalme\n\nWikimath\n\\begin{table}[H]\n\\centering\n\\small\n    \\begin{threeparttable}[H]\n    \\label{wikimath_GFT_table}\n        \\begin{tabular}{lcccc}\n            \\toprule\n            &    \\multicolumn{2}{c}{  \\textbf{Random(80\\%)}  }          & \\multicolumn{2}{c}{ \\textbf{The same missing(51.2\\%)} } \\\\\n                       & Classic            &      Proposed       &  Classic          & Proposed\\tnote{1}  \\\\\\midrule\n            GConvGRU     & 0.932$\\pm$0.043 & 0.687$\\pm$0.021 & 0.726$\\pm$0.015  &  \\textbf{0.533$\\pm$0.003}     \\\\\n            GConvLSTM    & 1.423$\\pm$0.121 & 0.920$\\pm$0.069 & 0.963$\\pm$0.098  &  \\textbf{0.653$\\pm$0.033}    \\\\\n            GCLSTM       & 1.407$\\pm$0.117 & 0.815$\\pm$0.058 & 0.824$\\pm$0.052  &  \\textbf{0.622$\\pm$0.011}    \\\\\n            LRGCN        & 1.105$\\pm$0.099 & 0.769$\\pm$0.045 & 0.810$\\pm$0.064  &  \\textbf{0.624$\\pm$0.019}   \\\\\n            DyGrEncoder  & 0.770$\\pm$0.045 & 0.606$\\pm$0.017 & 0.626$\\pm$0.027  &  \\textbf{0.561$\\pm$0.031}   \\\\\n            EvolveGCNO   & 0.915$\\pm$0.063 & 0.877$\\pm$0.045 & 0.753$\\pm$0.026  &  \\textbf{0.745$\\pm$0.017}    \\\\\n            EvolveGCNH   & 0.863$\\pm$0.038 & 0.780$\\pm$0.027 & 0.818$\\pm$0.031  &  0.794$\\pm$0.031    \\\\\n            TGCN         & 0.827$\\pm$0.030 & 0.771$\\pm$0.020 & 0.782$\\pm$0.030  &  \\textbf{0.750$\\pm$0.039}    \\\\\n            DCRNN        & 0.846$\\pm$0.031 & 0.672$\\pm$0.007 & 0.665$\\pm$0.015  &  \\textbf{0.592$\\pm$0.005}    \\\\\n            \\bottomrule\n        \\end{tabular}\n    \\begin{tablenotes}\n    \\item[1] Joint Time and Graph Stationarity, which is considered to be jointly stationary in both the vertex and the time domain\n    \\end{tablenotes}\n    \\caption{\n    The performance of the $\\tt{Wikimath}$ dataset with the Graph Shift Operator (GSO) was compared under two different situations: one that considers only time stationarity and another that includes time and graph stationarity, assuming the same index missing data at the same time points for each node.\n    }\n    \\end{threeparttable}\n\\label{tb:gsotwo}\n\\end{table}"
  },
  {
    "objectID": "posts/3_result/3_figure/2023-12-29-fig_dashboard.html",
    "href": "posts/3_result/3_figure/2023-12-29-fig_dashboard.html",
    "title": "Figure for dashboard",
    "section": "",
    "text": "import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n\n#---#\nimport plotly.express as px\nimport plotly.graph_objects as go\nimport plotly.io as pio\n\npd.options.plotting.backend = \"plotly\"\npio.templates.default = \"plotly_white\"\n\n\nimport pickle"
  },
  {
    "objectID": "posts/3_result/3_figure/2023-12-29-fig_dashboard.html#fivevts",
    "href": "posts/3_result/3_figure/2023-12-29-fig_dashboard.html#fivevts",
    "title": "Figure for dashboard",
    "section": "FiveVTS",
    "text": "FiveVTS\n\nfivenodes = pd.concat([\n    df[(df['dataset']=='fivenodes') & (df['mtype']=='rand') & (df['inter_method']=='linear') & (df['nof_filters']==12) & \n       (df['lags']==2) & (df['epoch']==50) & (df['model']=='GConvGRU') & (df['mrate'].isin([0.3, 0.5, 0.6, 0.7, 0.8]))],\n    df[(df['dataset']=='fivenodes') & (df['mtype']=='rand') & (df['inter_method']=='linear') & (df['nof_filters']==12) & \n       (df['lags']==2) & (df['epoch']==50) & (df['model']=='GConvLSTM') & (df['mrate'].isin([0.3, 0.5, 0.6, 0.7, 0.8]))],\n    df[(df['dataset']=='fivenodes') & (df['mtype']=='rand') & (df['inter_method']=='linear') & (df['nof_filters']==4) & \n       (df['lags']==2) & (df['epoch']==50) & (df['model']=='GCLSTM') & (df['mrate'].isin([0.3, 0.5, 0.6, 0.7, 0.8]))],\n    df[(df['dataset']=='fivenodes') & (df['mtype']=='rand') & (df['inter_method']=='linear') & (df['nof_filters']==4) & \n       (df['lags']==2) & (df['epoch']==50) & (df['model']=='LRGCN') & (df['mrate'].isin([0.3, 0.5, 0.6, 0.7, 0.8]))],\n    df[(df['dataset']=='fivenodes') & (df['mtype']=='rand') & (df['inter_method']=='linear') & (df['nof_filters']==12) & \n       (df['lags']==2) & (df['epoch']==50) & (df['model']=='DyGrEncoder') & (df['mrate'].isin([0.3, 0.5, 0.6, 0.7, 0.8]))],\n    df[(df['dataset']=='fivenodes') & (df['mtype']=='rand') & (df['inter_method']=='linear') & (df['lags']==2) & \n       (df['epoch']==50) & (df['model']=='EvolveGCNH') & (df['mrate'].isin([0.3, 0.5, 0.6, 0.7, 0.8]))],\n    df[(df['dataset']=='fivenodes') & (df['mtype']=='rand') & (df['inter_method']=='linear') & (df['lags']==2) & \n       (df['epoch']==50) & (df['model']=='EvolveGCNO') & (df['mrate'].isin([0.3, 0.5, 0.6, 0.7, 0.8]))],\n    df[(df['dataset']=='fivenodes') & (df['mtype']=='rand') & (df['inter_method']=='linear') & (df['nof_filters']==12) & \n       (df['lags']==2) & (df['epoch']==50) & (df['model']=='TGCN') & (df['mrate'].isin([0.3, 0.5, 0.6, 0.7, 0.8]))],\n    df[(df['dataset']=='fivenodes') & (df['mtype']=='rand') & (df['inter_method']=='linear') & (df['nof_filters']==2) & \n       (df['lags']==2) & (df['epoch']==50) & (df['model']=='DCRNN') & (df['mrate'].isin([0.3, 0.5, 0.6, 0.7, 0.8]))]\n])\n\nfivenodes['model'] = pd.Categorical(fivenodes['model'], categories=[\"GConvGRU\", \"GConvLSTM\", \"GCLSTM\", \"LRGCN\", \"DyGrEncoder\", \"EvolveGCNH\", \"EvolveGCNO\", \"TGCN\", \"DCRNN\"])\nfivenodes['method'] = pd.Categorical(fivenodes['method'], categories=['STGCN', 'IT-STGCN'])\n\nfivenodes['mrate'] = fivenodes['mrate'].astype(str)\nfivenodes = fivenodes.sort_values(by=['model','mrate'])\n\nfig = px.box(fivenodes,x='mrate',y='mse',color='method',width=70, log_y=True,facet_col='model',facet_col_wrap=3)\n\nfig.layout['xaxis']['title']['text']=''\nfig.layout['xaxis2']['title']['text']=''\nfig.layout['xaxis3']['title']['text']=''\nfig.layout['yaxis']['title']['text']=''\nfig.layout['yaxis4']['title']['text']='MSE(log scale)'\nfig.layout['yaxis7']['title']['text']=''\n\n\nfig.layout.xaxis4.showticklabels=True\nfig.layout.xaxis5.showticklabels=True\nfig.layout.xaxis6.showticklabels=True\nfig.layout.xaxis7.showticklabels=True\nfig.layout.xaxis8.showticklabels=True\nfig.layout.xaxis9.showticklabels=True\n\nfig.layout.yaxis2.showticklabels=True\nfig.layout.yaxis3.showticklabels=True\nfig.layout.yaxis5.showticklabels=True\nfig.layout.yaxis6.showticklabels=True\nfig.layout.yaxis8.showticklabels=True\nfig.layout.yaxis9.showticklabels=True\n\n\nfig.update_layout(template=\"seaborn\")\n\nfor i in range(0, 9):\n    fig.data[i]['marker']['color'] = 'blue'\n    fig.data[i]['name'] = 'Classic'\nfor i in range(9, 17):    \n    fig.data[i]['marker']['color'] = 'red'\n    fig.data[i]['name'] = 'Proposed'\n\nfig.update_layout(legend=dict(x=1, y=1, traceorder='normal', orientation='v'))\nfig.update_layout(title_text=\"FiveVTS\")\n\nfig.update_layout(height=1200, width=2000)\nfig.for_each_annotation(lambda a: a.update(text=a.text.split(\"=\")[-1]))    \nfig.update_layout(legend=dict(\n    yanchor=\"top\",\n    y=0.99,\n    xanchor=\"left\",\n    x=0.01\n))\n\nwith open('fivenodes_fig.pkl', 'wb') as file:\n    pickle.dump(fig, file)\n\n\nwith open('fivenodes_fig.pkl', 'rb') as file:\n    fivenodes_fig = pickle.load(file)\n\nfivenodes_fig"
  },
  {
    "objectID": "posts/3_result/3_figure/2023-12-29-fig_dashboard.html#chickenpox",
    "href": "posts/3_result/3_figure/2023-12-29-fig_dashboard.html#chickenpox",
    "title": "Figure for dashboard",
    "section": "Chickenpox",
    "text": "Chickenpox\n\nchickenpox = pd.concat([\n    df.query(\"dataset=='chickenpox' & mtype=='rand' & inter_method == 'linear' & nof_filters==16 & lags==4 & epoch==50 & model=='GConvGRU' & (mrate == 0.3 | mrate == 0.5 | mrate == 0.6 | mrate == 0.8)\"),\n    df.query(\"dataset=='chickenpox' & mtype=='rand' & inter_method == 'linear' & nof_filters==32 & lags==4 & epoch==50 & model=='GConvLSTM' & (mrate == 0.3 | mrate == 0.5 | mrate == 0.6 | mrate == 0.8)\"),\n    df.query(\"dataset=='chickenpox' & mtype=='rand' & inter_method == 'linear' & nof_filters==16 & lags==4 & epoch==50 & model=='GCLSTM' & (mrate == 0.3 | mrate == 0.5 | mrate == 0.6 | mrate == 0.8)\"),\n    df.query(\"dataset=='chickenpox' & mtype=='rand' & inter_method == 'linear' & nof_filters==8 & lags==4 & epoch==50 & model=='LRGCN' & (mrate == 0.3 | mrate == 0.5 | mrate == 0.6 | mrate == 0.8)\"),\n    df.query(\"dataset=='chickenpox' & mtype=='rand' & inter_method == 'linear' & nof_filters==12 & lags==4 & epoch==50 & model=='DyGrEncoder' & (mrate == 0.3 | mrate == 0.5 | mrate == 0.6 | mrate == 0.8)\"),\n    df.query(\"dataset=='chickenpox' & mtype=='rand' & inter_method == 'linear' & lags==4 & epoch==50 & model=='EvolveGCNH' & (mrate == 0.3 | mrate == 0.5 | mrate == 0.6 | mrate == 0.8)\"),\n    df.query(\"dataset=='chickenpox' & mtype=='rand' & inter_method == 'linear' & lags==4 & epoch==50 & model=='EvolveGCNO' & (mrate == 0.3 | mrate == 0.5 | mrate == 0.6 | mrate == 0.8)\"),\n    df.query(\"dataset=='chickenpox' & mtype=='rand' & inter_method == 'linear' & nof_filters==12 & lags==4 & epoch==50 & model=='TGCN' & (mrate == 0.3 | mrate == 0.5 | mrate == 0.6 | mrate == 0.8)\"),\n    df.query(\"dataset=='chickenpox' & mtype=='rand' & inter_method == 'linear' & nof_filters==16 & lags==4 & epoch==50 & model=='DCRNN' & (mrate == 0.3 | mrate == 0.5 | mrate == 0.6 | mrate == 0.8)\")\n])\n\nchickenpox['model'] = pd.Categorical(chickenpox['model'], categories=[\"GConvGRU\",\"GConvLSTM\",\"GCLSTM\",\"LRGCN\",\"DyGrEncoder\",\"EvolveGCNH\",\"EvolveGCNO\",\"TGCN\",\"DCRNN\"])\nchickenpox['method'] = pd.Categorical(chickenpox['method'], categories=['STGCN','IT-STGCN'])\n\nchickenpox['mrate'] = chickenpox['mrate'].astype(str)\nchickenpox = chickenpox.sort_values(by=['model','mrate'])\n\nfig = px.box(chickenpox,x='mrate',y='mse',color='method',width=70, log_y=True,facet_col='model',facet_col_wrap=3)\n\nfig.layout['xaxis']['title']['text']=''\nfig.layout['xaxis2']['title']['text']=''\nfig.layout['xaxis3']['title']['text']=''\nfig.layout['yaxis']['title']['text']=''\nfig.layout['yaxis4']['title']['text']='MSE(log scale)'\nfig.layout['yaxis7']['title']['text']=''\n\nfig.layout.xaxis4.showticklabels=True\nfig.layout.xaxis5.showticklabels=True\nfig.layout.xaxis6.showticklabels=True\nfig.layout.xaxis7.showticklabels=True\nfig.layout.xaxis8.showticklabels=True\nfig.layout.xaxis9.showticklabels=True\n\nfig.layout.yaxis2.showticklabels=True\nfig.layout.yaxis3.showticklabels=True\nfig.layout.yaxis5.showticklabels=True\nfig.layout.yaxis6.showticklabels=True\nfig.layout.yaxis8.showticklabels=True\nfig.layout.yaxis9.showticklabels=True\n\nfor i in range(0, 9):\n    fig.data[i]['marker']['color'] = 'blue'\n    fig.data[i]['name'] = 'Classic'\nfor i in range(9, 17):    \n    fig.data[i]['marker']['color'] = 'red'\n    fig.data[i]['name'] = 'Proposed'\n\nfig.update_layout(legend=dict(x=1, y=1, traceorder='normal', orientation='v'))\nfig.update_layout(title_text=\"Chickenpox\")\nfig.update_layout(height=1200, width=2000)\nfig.for_each_annotation(lambda a: a.update(text=a.text.split(\"=\")[-1]))  \nfig.update_layout(template=\"seaborn\")  \nfig.update_layout(legend=dict(\n    yanchor=\"top\",\n    y=0.99,\n    xanchor=\"left\",\n    x=0.01\n))\n\n\nwith open('chickenpox_fig.pkl', 'wb') as file:\n    pickle.dump(fig, file)\n\n\nwith open('chickenpox_fig.pkl', 'rb') as file:\n    chickenpox_fig = pickle.load(file)\n\nchickenpox_fig"
  },
  {
    "objectID": "posts/3_result/3_figure/2023-12-29-fig_dashboard.html#pedalmedatasetloader",
    "href": "posts/3_result/3_figure/2023-12-29-fig_dashboard.html#pedalmedatasetloader",
    "title": "Figure for dashboard",
    "section": "PedalMeDatasetLoader",
    "text": "PedalMeDatasetLoader\n\npedalme = pd.concat([\n    df.query(\"dataset=='pedalme' & mtype=='rand' & inter_method == 'linear' & nof_filters==12 & lags==4 & epoch==50 & model=='GConvGRU' & (mrate == 0.3 | mrate == 0.5 | mrate == 0.6 | mrate == 0.8)\"),\n    df.query(\"dataset=='pedalme' & mtype=='rand' & inter_method == 'linear' & nof_filters==2 & lags==4 & epoch==50 & model=='GConvLSTM' & (mrate == 0.3 | mrate == 0.5 | mrate == 0.6 | mrate == 0.8)\"),\n    df.query(\"dataset=='pedalme' & mtype=='rand' & inter_method == 'linear' & nof_filters==4 & lags==4 & epoch==50 & model=='GCLSTM' & (mrate == 0.3 | mrate == 0.5 | mrate == 0.6 | mrate == 0.8)\"),\n    df.query(\"dataset=='pedalme' & mtype=='rand' & inter_method == 'linear' & nof_filters==8 & lags==4 & epoch==50 & model=='LRGCN' & (mrate == 0.3 | mrate == 0.5 | mrate == 0.6 | mrate == 0.8)\"),\n    df.query(\"dataset=='pedalme' & mtype=='rand' & inter_method == 'linear' & nof_filters==12 & lags==4 & epoch==50 & model=='DyGrEncoder' & (mrate == 0.3 | mrate == 0.5 | mrate == 0.6 | mrate == 0.8)\"),\n    df.query(\"dataset=='pedalme' & mtype=='rand' & inter_method == 'linear' & lags==4 & epoch==50 & model=='EvolveGCNH' & (mrate == 0.3 | mrate == 0.5 | mrate == 0.6 | mrate == 0.8)\"),\n    df.query(\"dataset=='pedalme' & mtype=='rand' & inter_method == 'linear' & lags==4 & epoch==50 & model=='EvolveGCNO' & (mrate == 0.3 | mrate == 0.5 | mrate == 0.6 | mrate == 0.8)\"),\n    df.query(\"dataset=='pedalme' & mtype=='rand' & inter_method == 'linear' & nof_filters==12 & lags==4 & epoch==50 & model=='TGCN' & (mrate == 0.3 | mrate == 0.5 | mrate == 0.6 | mrate == 0.8)\"),\n    df.query(\"dataset=='pedalme' & mtype=='rand' & inter_method == 'linear' & nof_filters==8 & lags==4 & epoch==50 & model=='DCRNN' & (mrate == 0.3 | mrate == 0.5 | mrate == 0.6 | mrate == 0.8)\")\n], ignore_index=True)\n\npedalme['model'] = pd.Categorical(pedalme['model'], categories=[\"GConvGRU\", \"GConvLSTM\", \"GCLSTM\", \"LRGCN\", \"DyGrEncoder\", \"EvolveGCNH\", \"EvolveGCNO\", \"TGCN\", \"DCRNN\"], ordered=True)\npedalme['method'] = pd.Categorical(pedalme['method'], categories=['STGCN', 'IT-STGCN'], ordered=True)\n\npedalme['mrate'] = pedalme['mrate'].astype(str)\npedalme = pedalme.sort_values(by=['model','mrate'])\n\nfig = px.box(pedalme,x='mrate',y='mse',color='method',width=70, log_y=True,facet_col='model',facet_col_wrap=3)\n\nfig.layout['xaxis']['title']['text']=''\nfig.layout['xaxis2']['title']['text']=''\nfig.layout['xaxis3']['title']['text']=''\nfig.layout['yaxis']['title']['text']=''\nfig.layout['yaxis4']['title']['text']='MSE(log scale)'\nfig.layout['yaxis7']['title']['text']=''\n\n\nfig.layout.xaxis4.showticklabels=True\nfig.layout.xaxis5.showticklabels=True\nfig.layout.xaxis6.showticklabels=True\nfig.layout.xaxis7.showticklabels=True\nfig.layout.xaxis8.showticklabels=True\nfig.layout.xaxis9.showticklabels=True\n\nfig.layout.yaxis2.showticklabels=True\nfig.layout.yaxis3.showticklabels=True\nfig.layout.yaxis5.showticklabels=True\nfig.layout.yaxis6.showticklabels=True\nfig.layout.yaxis8.showticklabels=True\nfig.layout.yaxis9.showticklabels=True\n\nfor i in range(0, 9):\n    fig.data[i]['marker']['color'] = 'blue'\n    fig.data[i]['name'] = 'Classic'\nfor i in range(9, 17):    \n    fig.data[i]['marker']['color'] = 'red'\n    fig.data[i]['name'] = 'Proposed'\n\nfig.update_layout(legend=dict(x=1, y=1, traceorder='normal', orientation='v'))\nfig.update_layout(title_text=\"Pedalme\")\nfig.update_layout(height=1200, width=2000)\nfig.for_each_annotation(lambda a: a.update(text=a.text.split(\"=\")[-1]))  \nfig.update_layout(template=\"seaborn\")  \nfig.update_layout(legend=dict(\n    yanchor=\"top\",\n    y=0.99,\n    xanchor=\"left\",\n    x=0.01\n))\n\nwith open('pedalme_fig.pkl', 'wb') as file:\n    pickle.dump(fig, file)\n\n\nwith open('pedalme_fig.pkl', 'rb') as file:\n    pedalme_fig = pickle.load(file)\n\npedalme_fig"
  },
  {
    "objectID": "posts/3_result/3_figure/2023-12-29-fig_dashboard.html#wikimathsdatasetloader",
    "href": "posts/3_result/3_figure/2023-12-29-fig_dashboard.html#wikimathsdatasetloader",
    "title": "Figure for dashboard",
    "section": "WikiMathsDatasetLoader",
    "text": "WikiMathsDatasetLoader\n\nwikimath = pd.concat([\n    df.query(\"dataset=='wikimath' & mtype=='rand' & inter_method == 'linear' & nof_filters==12 & lags==8 & epoch==50 & model=='GConvGRU' & (mrate == 0.3 | mrate == 0.5 | mrate == 0.6 | mrate == 0.8)\"),\n    df.query(\"dataset=='wikimath' & mtype=='rand' & inter_method == 'linear' & nof_filters==64 & lags==8 & epoch==50 & model=='GConvLSTM' & (mrate == 0.3 | mrate == 0.5 | mrate == 0.6 | mrate == 0.8)\"),\n    df.query(\"dataset=='wikimath' & mtype=='rand' & inter_method == 'linear' & nof_filters==64 & lags==8 & epoch==50 & model=='GCLSTM' & (mrate == 0.3 | mrate == 0.5 | mrate == 0.6 | mrate == 0.8)\"),\n    df.query(\"dataset=='wikimath' & mtype=='rand' & inter_method == 'linear' & nof_filters==32 & lags==8 & epoch==50 & model=='LRGCN' & (mrate == 0.3 | mrate == 0.5 | mrate == 0.6 | mrate == 0.8)\"),\n    df.query(\"dataset=='wikimath' & mtype=='rand' & inter_method == 'linear' & nof_filters==12 & lags==8 & epoch==50 & model=='DyGrEncoder' & (mrate == 0.3 | mrate == 0.5 | mrate == 0.6 | mrate == 0.8)\"),\n    df.query(\"dataset=='wikimath' & mtype=='rand' & inter_method == 'linear' & lags==8 & epoch==50 & model=='EvolveGCNH' & (mrate == 0.3 | mrate == 0.5 | mrate == 0.6 | mrate == 0.8)\"),\n    df.query(\"dataset=='wikimath' & mtype=='rand' & inter_method == 'linear' & lags==8 & epoch==50 & model=='EvolveGCNO' & (mrate == 0.3 | mrate == 0.5 | mrate == 0.6 | mrate == 0.8)\"),\n    df.query(\"dataset=='wikimath' & mtype=='rand' & inter_method == 'linear' & nof_filters==12 & lags==8 & epoch==50 & model=='TGCN' & (mrate == 0.3 | mrate == 0.5 | mrate == 0.6 | mrate == 0.8)\"),\n    df.query(\"dataset=='wikimath' & mtype=='rand' & inter_method == 'linear' & nof_filters==12 & lags==8 & epoch==50 & model=='DCRNN' & (mrate == 0.3 | mrate == 0.5 | mrate == 0.6 | mrate == 0.8)\")\n], ignore_index=True)\n\nwikimath['model'] = pd.Categorical(wikimath['model'], categories=[\"GConvGRU\", \"GConvLSTM\", \"GCLSTM\", \"LRGCN\", \"DyGrEncoder\", \"EvolveGCNH\", \"EvolveGCNO\", \"TGCN\", \"DCRNN\"], ordered=True)\nwikimath['method'] = pd.Categorical(wikimath['method'], categories=['STGCN', 'IT-STGCN'], ordered=True)\n\nwikimath['mrate'] = wikimath['mrate'].astype(str)\nwikimath = wikimath.sort_values(by=['model','mrate'])\n\nfig = px.box(wikimath,x='mrate',y='mse',color='method',width=70, log_y=True,facet_col='model',facet_col_wrap=3)\n\nfig.layout['xaxis']['title']['text']=''\nfig.layout['xaxis2']['title']['text']=''\nfig.layout['xaxis3']['title']['text']=''\nfig.layout['yaxis']['title']['text']=''\nfig.layout['yaxis4']['title']['text']='MSE(log scale)'\nfig.layout['yaxis7']['title']['text']=''\n\nfig.layout.xaxis4.showticklabels=True\nfig.layout.xaxis5.showticklabels=True\nfig.layout.xaxis6.showticklabels=True\nfig.layout.xaxis7.showticklabels=True\nfig.layout.xaxis8.showticklabels=True\nfig.layout.xaxis9.showticklabels=True\n\nfig.layout.yaxis2.showticklabels=True\nfig.layout.yaxis3.showticklabels=True\nfig.layout.yaxis5.showticklabels=True\nfig.layout.yaxis6.showticklabels=True\nfig.layout.yaxis8.showticklabels=True\nfig.layout.yaxis9.showticklabels=True\n\nfor i in range(0, 9):\n    fig.data[i]['marker']['color'] = 'blue'\n    fig.data[i]['name'] = 'Classic'\nfor i in range(9, 17):    \n    fig.data[i]['marker']['color'] = 'red'\n    fig.data[i]['name'] = 'Proposed'\n\nfig.update_layout(legend=dict(x=1, y=1, traceorder='normal', orientation='v'))\nfig.update_layout(title_text=\"Wikimath\")\nfig.update_layout(height=1200, width=2000)\nfig.for_each_annotation(lambda a: a.update(text=a.text.split(\"=\")[-1]))  \nfig.update_layout(template=\"seaborn\")  \nfig.update_layout(legend=dict(\n    yanchor=\"top\",\n    y=0.99,\n    xanchor=\"left\",\n    x=0.01\n))\n\nwith open('wikimath_fig.pkl', 'wb') as file:\n    pickle.dump(fig, file)\n\n\nwith open('wikimath_fig.pkl', 'rb') as file:\n    wikimath_fig = pickle.load(file)\n\nwikimath_fig"
  },
  {
    "objectID": "posts/3_result/3_figure/2023-12-29-fig_dashboard.html#windmillsmall",
    "href": "posts/3_result/3_figure/2023-12-29-fig_dashboard.html#windmillsmall",
    "title": "Figure for dashboard",
    "section": "Windmillsmall",
    "text": "Windmillsmall\n\nwindmillsmall = pd.concat([\n    df.query(\"dataset=='windmillsmall' & mtype=='rand' & inter_method == 'linear' & nof_filters==12 & lags==8 & epoch==50 & model=='GConvGRU' & (mrate == 0.3 | mrate == 0.5 | mrate == 0.6 | mrate == 0.7)\"),\n    df.query(\"dataset=='windmillsmall' & mtype=='rand' & inter_method == 'linear' & nof_filters==16 & lags==8 & epoch==50 & model=='GConvLSTM' & (mrate == 0.3 | mrate == 0.5 | mrate == 0.6 | mrate == 0.7)\"),\n    df.query(\"dataset=='windmillsmall' & mtype=='rand' & inter_method == 'linear' & nof_filters==16 & lags==8 & epoch==50 & model=='GCLSTM' & (mrate == 0.3 | mrate == 0.5 | mrate == 0.6 | mrate == 0.7)\"),\n    df.query(\"dataset=='windmillsmall' & mtype=='rand' & inter_method == 'linear' & nof_filters==12 & lags==8 & epoch==50 & model=='LRGCN' & (mrate == 0.3 | mrate == 0.5 | mrate == 0.6 | mrate == 0.7)\"),\n    df.query(\"dataset=='windmillsmall' & mtype=='rand' & inter_method == 'linear' & nof_filters==12 & lags==8 & epoch==50 & model=='DyGrEncoder' & (mrate == 0.3 | mrate == 0.5 | mrate == 0.6 | mrate == 0.7)\"),\n    df.query(\"dataset=='windmillsmall' & mtype=='rand' & inter_method == 'linear' & lags==8 & epoch==50 & model=='EvolveGCNH' & (mrate == 0.3 | mrate == 0.5 | mrate == 0.6 | mrate == 0.7)\"),\n    df.query(\"dataset=='windmillsmall' & mtype=='rand' & inter_method == 'linear' & lags==8 & epoch==50 & model=='EvolveGCNO' & (mrate == 0.3 | mrate == 0.5 | mrate == 0.6 | mrate == 0.7)\"),\n    df.query(\"dataset=='windmillsmall' & mtype=='rand' & inter_method == 'linear' & nof_filters==12 & lags==8 & epoch==50 & model=='TGCN' & (mrate == 0.3 | mrate == 0.5 | mrate == 0.6 | mrate == 0.7)\"),\n    df.query(\"dataset=='windmillsmall' & mtype=='rand' & inter_method == 'linear' & nof_filters==4 & lags==8 & epoch==50 & model=='DCRNN' & (mrate == 0.3 | mrate == 0.5 | mrate == 0.6 | mrate == 0.7)\")\n], ignore_index=True)\n\nwindmillsmall['model'] = pd.Categorical(windmillsmall['model'], categories=[\"GConvGRU\", \"GConvLSTM\", \"GCLSTM\", \"LRGCN\", \"DyGrEncoder\", \"EvolveGCNH\", \"EvolveGCNO\", \"TGCN\", \"DCRNN\"], ordered=True)\nwindmillsmall['method'] = pd.Categorical(windmillsmall['method'], categories=['STGCN', 'IT-STGCN'], ordered=True)\n\nwindmillsmall['mrate'] = windmillsmall['mrate'].astype(str)\nwindmillsmall = windmillsmall.sort_values(by=['model','mrate'])\n\nfig = px.box(windmillsmall,x='mrate',y='mse',color='method',width=70, log_y=True,facet_col='model',facet_col_wrap=3)\n\nfig.layout['xaxis']['title']['text']=''\nfig.layout['xaxis2']['title']['text']=''\nfig.layout['xaxis3']['title']['text']=''\nfig.layout['yaxis']['title']['text']=''\nfig.layout['yaxis4']['title']['text']='MSE(log scale)'\nfig.layout['yaxis7']['title']['text']=''\n\nfig.layout.xaxis4.showticklabels=True\nfig.layout.xaxis5.showticklabels=True\nfig.layout.xaxis6.showticklabels=True\nfig.layout.xaxis7.showticklabels=True\nfig.layout.xaxis8.showticklabels=True\nfig.layout.xaxis9.showticklabels=True\n\nfig.layout.yaxis2.showticklabels=True\nfig.layout.yaxis3.showticklabels=True\nfig.layout.yaxis5.showticklabels=True\nfig.layout.yaxis6.showticklabels=True\nfig.layout.yaxis8.showticklabels=True\nfig.layout.yaxis9.showticklabels=True\n\n\nfor i in range(0, 9):\n    fig.data[i]['marker']['color'] = 'blue'\n    fig.data[i]['name'] = 'Classic'\nfor i in range(9, 17):    \n    fig.data[i]['marker']['color'] = 'red'\n    fig.data[i]['name'] = 'Proposed'\n\nfig.update_layout(legend=dict(x=1, y=1, traceorder='normal', orientation='v'))\nfig.update_layout(title_text=\"Windmillsmall\")\nfig.update_layout(height=1200, width=2000)\nfig.for_each_annotation(lambda a: a.update(text=a.text.split(\"=\")[-1]))  \nfig.update_layout(template=\"seaborn\")  \nfig.update_layout(legend=dict(\n    yanchor=\"top\",\n    y=0.99,\n    xanchor=\"left\",\n    x=0.005\n))\n\n\nwith open('windmillsmall_fig.pkl', 'wb') as file:\n    pickle.dump(fig, file)\n\n\nwith open('windmillsmall_fig.pkl', 'rb') as file:\n    windmillsmall_fig = pickle.load(file)\n\nwindmillsmall_fig"
  },
  {
    "objectID": "posts/3_result/3_figure/2023-12-29-fig_dashboard.html#montevideobus",
    "href": "posts/3_result/3_figure/2023-12-29-fig_dashboard.html#montevideobus",
    "title": "Figure for dashboard",
    "section": "Montevideobus",
    "text": "Montevideobus\n\nmonte = pd.concat([\n    df.query(\"dataset=='monte' & mtype=='rand' & inter_method == 'nearest' & nof_filters==12 & lags==4 & epoch==50 & model=='GConvGRU' & (mrate == 0.3 | mrate == 0.5 | mrate == 0.7 | mrate == 0.8)\"),\n    df.query(\"dataset=='monte' & mtype=='rand' & inter_method == 'nearest' & nof_filters==12 & lags==4 & epoch==50 & model=='GConvLSTM' & (mrate == 0.3 | mrate == 0.5 | mrate == 0.7 | mrate == 0.8)\"),\n    df.query(\"dataset=='monte' & mtype=='rand' & inter_method == 'nearest' & nof_filters==12 & lags==4 & epoch==50 & model=='GCLSTM' & (mrate == 0.3 | mrate == 0.5 | mrate == 0.7 | mrate == 0.8)\"),\n    df.query(\"dataset=='monte' & mtype=='rand' & inter_method == 'nearest' & nof_filters==2 & lags==4 & epoch==50 & model=='LRGCN' & (mrate == 0.3 | mrate == 0.5 | mrate == 0.7 | mrate == 0.8)\"),\n    df.query(\"dataset=='monte' & mtype=='rand' & inter_method == 'nearest' & nof_filters==12 & lags==4 & epoch==50 & model=='DyGrEncoder' & (mrate == 0.3 | mrate == 0.5 | mrate == 0.7 | mrate == 0.8)\"),\n    df.query(\"dataset=='monte' & mtype=='rand' & inter_method == 'nearest' & lags==4 & epoch==50 & model=='EvolveGCNH' & (mrate == 0.3 | mrate == 0.5 | mrate == 0.7 | mrate == 0.8)\"),\n    df.query(\"dataset=='monte' & mtype=='rand' & inter_method == 'nearest' & lags==4 & epoch==50 & model=='EvolveGCNO' & (mrate == 0.3 | mrate == 0.5 | mrate == 0.7 | mrate == 0.8)\"),\n    df.query(\"dataset=='monte' & mtype=='rand' & inter_method == 'nearest' & nof_filters==8 & lags==4 & epoch==50 & model=='TGCN' & (mrate == 0.3 | mrate == 0.5 | mrate == 0.7 | mrate == 0.8)\"),\n    df.query(\"dataset=='monte' & mtype=='rand' & inter_method == 'nearest' & nof_filters==12 & lags==4 & epoch==50 & model=='DCRNN' & (mrate == 0.3 | mrate == 0.5 | mrate == 0.7 | mrate == 0.8)\")\n], ignore_index=True)\n\nmonte['model'] = pd.Categorical(monte['model'], categories=[\"GConvGRU\", \"GConvLSTM\", \"GCLSTM\", \"LRGCN\", \"DyGrEncoder\", \"EvolveGCNH\", \"EvolveGCNO\", \"TGCN\", \"DCRNN\"], ordered=True)\nmonte['method'] = pd.Categorical(monte['method'], categories=['STGCN', 'IT-STGCN'], ordered=True)\n\nmonte['mrate'] = monte['mrate'].astype(str)\nmonte = monte.sort_values(by=['model','mrate'])\n\nfig = px.box(monte,x='mrate',y='mse',color='method',width=70, log_y=True,facet_col='model',facet_col_wrap=3)\n\nfig.layout['xaxis']['title']['text']=''\nfig.layout['xaxis2']['title']['text']=''\nfig.layout['xaxis3']['title']['text']=''\nfig.layout['yaxis']['title']['text']=''\nfig.layout['yaxis4']['title']['text']='MSE(log scale)'\nfig.layout['yaxis7']['title']['text']=''\n\n\nfig.layout.xaxis4.showticklabels=True\nfig.layout.xaxis5.showticklabels=True\nfig.layout.xaxis6.showticklabels=True\nfig.layout.xaxis7.showticklabels=True\nfig.layout.xaxis8.showticklabels=True\nfig.layout.xaxis9.showticklabels=True\n\nfig.layout.yaxis2.showticklabels=True\nfig.layout.yaxis3.showticklabels=True\nfig.layout.yaxis5.showticklabels=True\nfig.layout.yaxis6.showticklabels=True\nfig.layout.yaxis8.showticklabels=True\nfig.layout.yaxis9.showticklabels=True\n\n\nfor i in range(0, 9):\n    fig.data[i]['marker']['color'] = 'blue'\n    fig.data[i]['name'] = 'Classic'\nfor i in range(9, 17):    \n    fig.data[i]['marker']['color'] = 'red'\n    fig.data[i]['name'] = 'Proposed'\n\nfig.update_layout(legend=dict(x=1, y=1, traceorder='normal', orientation='v'))\nfig.update_layout(title_text=\"MontevideoBus\")\nfig.update_layout(height=1200, width=2000)\nfig.for_each_annotation(lambda a: a.update(text=a.text.split(\"=\")[-1]))  \nfig.update_layout(template=\"seaborn\")  \nfig.update_layout(legend=dict(\n    yanchor=\"top\",\n    y=0.99,\n    xanchor=\"left\",\n    x=0.01\n))\n\n\n\nwith open('monte_fig.pkl', 'wb') as file:\n    pickle.dump(fig, file)\n\n\nwith open('monte_fig.pkl', 'rb') as file:\n    monte_fig = pickle.load(file)\n\nmonte_fig"
  },
  {
    "objectID": "posts/3_result/3_table/2023-07-05-ITSTGCN_data_management.html",
    "href": "posts/3_result/3_table/2023-07-05-ITSTGCN_data_management.html",
    "title": "Data management for ITSTGCN",
    "section": "",
    "text": "순환형 구조를 가진 모델(Models with recurrent structures):\n논문에서 제안하는 방법은 GConvGRU (Chebyshev Graph Convolutional Gated Recurrent Unit Cell)와 GConvLSTM (Graph Convolutional Recurrent Network)이라는 두 가지 모델을 소개합니다. GConvGRU는 그래프 데이터에 대한 시간적인 의존성을 캡처하는데 사용되며, GConvLSTM은 그래프와 시퀀스 데이터를 동시에 처리하는데 활용됩니다.\nGConvGRU는 Chebyshev 그래프 합성곱과 Gated Recurrent Unit (GRU)을 결합하여 그래프 데이터의 시간적 의존성을 모델링합니다. 이 모델은 그래프 내 노드들 간의 연결과 그래프의 구조를 고려하여 시계열 데이터를 예측하고 분석하는데 유용합니다.\n“GC-LSTM: 그래프 합성곱 임베딩 LSTM을 이용한 동적 링크 예측”이라는 논문은 동적인 그래프에서 링크 예측 작업에 그래프 합성곱 네트워크(GCN)와 Long Short-Term Memory(LSTM) 셀을 결합한 새로운 모델을 제안합니다.\n논문에서는 그래프 구조가 시간에 따라 변화하는 동적인 그래프에서 노드들 사이의 링크 존재 여부를 예측하는 문제를 다룹니다. 기존의 링크 예측 방법들은 동적인 그래프의 변화를 처리하는데 어려움이 있습니다.\n제안하는 GC-LSTM 모델은 그래프 합성곱을 사용하여 노드의 특징을 임베딩하고 그래프 구조를 파악합니다. 동시에 LSTM 셀을 활용하여 시간적 의존성과 순차적 패턴을 모델링합니다.\nGC-LSTM 모델은 동적인 그래프 데이터를 시간 순서대로 처리하여 변화하는 그래프 구조를 적절히 반영하고 미래의 링크를 정확하게 예측할 수 있습니다. 실험과 평가를 통해 GC-LSTM 모델이 동적인 링크 예측 작업에서 다른 최신 기법들보다 우수한 성능을 보여주며, 동적인 그래프 구조를 다루는데 유용한 접근 방법임을 입증합니다."
  },
  {
    "objectID": "posts/3_result/3_table/2023-07-05-ITSTGCN_data_management.html#baseline",
    "href": "posts/3_result/3_table/2023-07-05-ITSTGCN_data_management.html#baseline",
    "title": "Data management for ITSTGCN",
    "section": "Baseline",
    "text": "Baseline\n\npd.merge(df.query(\"dataset=='fivenodes' and mtype!='rand' and mtype!='block' and method !='GNAR'\").groupby(['model','nof_filters','lags','epoch'])['mse'].mean().reset_index(),\n         df.query(\"dataset=='fivenodes' and mtype!='rand' and mtype!='block' and method !='GNAR'\").groupby(['model','nof_filters','lags','epoch'])['mse'].std().reset_index(),\n         on=['model','nof_filters','lags','epoch']).rename(columns={'mse_x':'mean','mse_y':'std'}).round(3)\n\n\n\n\n\n\n\n\nmodel\nnof_filters\nlags\nepoch\nmean\nstd\n\n\n\n\n0\nDCRNN\n2.0\n2\n50.0\n1.229\n0.041\n\n\n1\nDyGrEncoder\n12.0\n2\n50.0\n1.114\n0.037\n\n\n2\nEvolveGCNH\n12.0\n2\n50.0\n1.175\n0.068\n\n\n3\nEvolveGCNO\n12.0\n2\n50.0\n1.168\n0.065\n\n\n4\nGCLSTM\n4.0\n2\n50.0\n1.209\n0.023\n\n\n5\nGConvGRU\n12.0\n2\n50.0\n0.732\n0.005\n\n\n6\nGConvLSTM\n12.0\n2\n50.0\n1.131\n0.041\n\n\n7\nLRGCN\n4.0\n2\n50.0\n1.212\n0.024\n\n\n8\nTGCN\n12.0\n2\n50.0\n1.085\n0.016"
  },
  {
    "objectID": "posts/3_result/3_table/2023-07-05-ITSTGCN_data_management.html#random",
    "href": "posts/3_result/3_table/2023-07-05-ITSTGCN_data_management.html#random",
    "title": "Data management for ITSTGCN",
    "section": "Random",
    "text": "Random\n\npd.merge(df.query(\"dataset=='fivenodes' and mtype=='rand'\").groupby(['model','mrate','nof_filters','inter_method','method','lags','epoch'])['mse'].mean().reset_index(),\n         df.query(\"dataset=='fivenodes' and mtype=='rand'\").groupby(['model','mrate','nof_filters','inter_method','method','lags','epoch'])['mse'].std().reset_index(),\n         on=['model','inter_method','method','nof_filters','mrate','lags','epoch']).rename(columns={'mse_x':'mean','mse_y':'std'}).round(3).query(\"mrate==0.7 and inter_method=='linear'\")\n\n\n\n\n\n\n\n\nmodel\nmrate\nnof_filters\ninter_method\nmethod\nlags\nepoch\nmean\nstd\n\n\n\n\n12\nDCRNN\n0.7\n2.0\nlinear\nIT-STGCN\n2\n50.0\n1.247\n0.044\n\n\n13\nDCRNN\n0.7\n2.0\nlinear\nSTGCN\n2\n50.0\n1.271\n0.066\n\n\n32\nDyGrEncoder\n0.7\n12.0\nlinear\nIT-STGCN\n2\n50.0\n1.252\n0.060\n\n\n33\nDyGrEncoder\n0.7\n12.0\nlinear\nSTGCN\n2\n50.0\n1.548\n0.158\n\n\n52\nEvolveGCNH\n0.7\n12.0\nlinear\nIT-STGCN\n2\n50.0\n1.188\n0.049\n\n\n53\nEvolveGCNH\n0.7\n12.0\nlinear\nSTGCN\n2\n50.0\n1.228\n0.064\n\n\n72\nEvolveGCNO\n0.7\n12.0\nlinear\nIT-STGCN\n2\n50.0\n1.162\n0.052\n\n\n73\nEvolveGCNO\n0.7\n12.0\nlinear\nSTGCN\n2\n50.0\n1.198\n0.045\n\n\n92\nGCLSTM\n0.7\n4.0\nlinear\nIT-STGCN\n2\n50.0\n1.228\n0.034\n\n\n93\nGCLSTM\n0.7\n4.0\nlinear\nSTGCN\n2\n50.0\n1.245\n0.033\n\n\n112\nGConvGRU\n0.7\n12.0\nlinear\nIT-STGCN\n2\n50.0\n1.180\n0.060\n\n\n113\nGConvGRU\n0.7\n12.0\nlinear\nSTGCN\n2\n50.0\n1.858\n0.139\n\n\n132\nGConvLSTM\n0.7\n12.0\nlinear\nIT-STGCN\n2\n50.0\n1.287\n0.075\n\n\n133\nGConvLSTM\n0.7\n12.0\nlinear\nSTGCN\n2\n50.0\n1.472\n0.125\n\n\n152\nLRGCN\n0.7\n4.0\nlinear\nIT-STGCN\n2\n50.0\n1.244\n0.041\n\n\n153\nLRGCN\n0.7\n4.0\nlinear\nSTGCN\n2\n50.0\n1.261\n0.047\n\n\n172\nTGCN\n0.7\n12.0\nlinear\nIT-STGCN\n2\n50.0\n1.110\n0.037\n\n\n173\nTGCN\n0.7\n12.0\nlinear\nSTGCN\n2\n50.0\n1.184\n0.057"
  },
  {
    "objectID": "posts/3_result/3_table/2023-07-05-ITSTGCN_data_management.html#block",
    "href": "posts/3_result/3_table/2023-07-05-ITSTGCN_data_management.html#block",
    "title": "Data management for ITSTGCN",
    "section": "Block",
    "text": "Block\n\npd.merge(df.query(\"dataset=='fivenodes' and mtype=='block' and inter_method=='linear'\").groupby(['model','mrate','nof_filters','inter_method','method','epoch'])['mse'].mean().reset_index(),\n         df.query(\"dataset=='fivenodes' and mtype=='block' and inter_method=='linear'\").groupby(['model','mrate','nof_filters','inter_method','method','epoch'])['mse'].std().reset_index(),\n         on=['model','inter_method','method','nof_filters','mrate','epoch']).rename(columns={'mse_x':'mean','mse_y':'std'}).round(3)\n\n\n\n\n\n\n\n\nmodel\nmrate\nnof_filters\ninter_method\nmethod\nepoch\nmean\nstd\n\n\n\n\n0\nDCRNN\n0.125\n2.0\nlinear\nIT-STGCN\n50.0\n1.232\n0.033\n\n\n1\nDCRNN\n0.125\n2.0\nlinear\nSTGCN\n50.0\n1.260\n0.051\n\n\n2\nDyGrEncoder\n0.125\n12.0\nlinear\nIT-STGCN\n50.0\n1.124\n0.035\n\n\n3\nDyGrEncoder\n0.125\n12.0\nlinear\nSTGCN\n50.0\n1.173\n0.037\n\n\n4\nEvolveGCNH\n0.125\n12.0\nlinear\nIT-STGCN\n50.0\n1.181\n0.055\n\n\n5\nEvolveGCNH\n0.125\n12.0\nlinear\nSTGCN\n50.0\n1.197\n0.076\n\n\n6\nEvolveGCNO\n0.125\n12.0\nlinear\nIT-STGCN\n50.0\n1.162\n0.040\n\n\n7\nEvolveGCNO\n0.125\n12.0\nlinear\nSTGCN\n50.0\n1.176\n0.056\n\n\n8\nGCLSTM\n0.125\n4.0\nlinear\nIT-STGCN\n50.0\n1.219\n0.025\n\n\n9\nGCLSTM\n0.125\n4.0\nlinear\nSTGCN\n50.0\n1.244\n0.033\n\n\n10\nGConvGRU\n0.125\n12.0\nlinear\nIT-STGCN\n50.0\n1.165\n0.043\n\n\n11\nGConvGRU\n0.125\n12.0\nlinear\nSTGCN\n50.0\n1.210\n0.039\n\n\n12\nGConvLSTM\n0.125\n12.0\nlinear\nIT-STGCN\n50.0\n1.140\n0.038\n\n\n13\nGConvLSTM\n0.125\n12.0\nlinear\nSTGCN\n50.0\n1.172\n0.055\n\n\n14\nLRGCN\n0.125\n4.0\nlinear\nIT-STGCN\n50.0\n1.220\n0.020\n\n\n15\nLRGCN\n0.125\n4.0\nlinear\nSTGCN\n50.0\n1.251\n0.037\n\n\n16\nTGCN\n0.125\n12.0\nlinear\nIT-STGCN\n50.0\n1.090\n0.015\n\n\n17\nTGCN\n0.125\n12.0\nlinear\nSTGCN\n50.0\n1.107\n0.020"
  },
  {
    "objectID": "posts/3_result/3_table/2023-07-05-ITSTGCN_data_management.html#baseline-1",
    "href": "posts/3_result/3_table/2023-07-05-ITSTGCN_data_management.html#baseline-1",
    "title": "Data management for ITSTGCN",
    "section": "Baseline",
    "text": "Baseline\n\npd.merge(df.query(\"dataset=='chickenpox' and mtype!='rand' and mtype!='block' and method !='GNAR' and lags==4\").groupby(['model','nof_filters','lags','epoch'])['mse'].mean().reset_index(),\n         df.query(\"dataset=='chickenpox' and mtype!='rand' and mtype!='block' and method !='GNAR'\").groupby(['model','nof_filters','lags','epoch'])['mse'].std().reset_index(),\n         on=['model','nof_filters','lags','epoch']).rename(columns={'mse_x':'mean','mse_y':'std'}).round(3)\n\n\n\n\n\n\n\n\nmodel\nnof_filters\nlags\nepoch\nmean\nstd\n\n\n\n\n0\nDCRNN\n16.0\n4\n50.0\n0.727\n0.009\n\n\n1\nDyGrEncoder\n12.0\n4\n50.0\n0.906\n0.051\n\n\n2\nEvolveGCNH\n32.0\n4\n50.0\n1.000\n0.020\n\n\n3\nEvolveGCNO\n32.0\n4\n50.0\n0.986\n0.018\n\n\n4\nGCLSTM\n16.0\n4\n50.0\n0.885\n0.051\n\n\n5\nGConvGRU\n16.0\n4\n50.0\n0.752\n0.013\n\n\n6\nGConvLSTM\n32.0\n4\n50.0\n0.959\n0.088\n\n\n7\nLRGCN\n8.0\n4\n50.0\n0.868\n0.047\n\n\n8\nTGCN\n12.0\n4\n50.0\n1.090\n0.042"
  },
  {
    "objectID": "posts/3_result/3_table/2023-07-05-ITSTGCN_data_management.html#random-1",
    "href": "posts/3_result/3_table/2023-07-05-ITSTGCN_data_management.html#random-1",
    "title": "Data management for ITSTGCN",
    "section": "Random",
    "text": "Random\n\npd.merge(df.query(\"dataset=='chickenpox' and mtype=='rand' and mrate==0.8\").groupby(['model','mrate','inter_method','nof_filters','method'])['mse'].mean().reset_index(),\n         df.query(\"dataset=='chickenpox' and mtype=='rand' and mrate==0.8\").groupby(['model','mrate','inter_method','nof_filters','method'])['mse'].std().reset_index(),\n         on=['model','method','inter_method','mrate','nof_filters']).rename(columns={'mse_x':'mean','mse_y':'std'})\n\n\n\n\n\n\n\n\nmodel\nmrate\ninter_method\nnof_filters\nmethod\nmean\nstd\n\n\n\n\n0\nDCRNN\n0.8\nlinear\n16.0\nIT-STGCN\n1.467027\n0.075759\n\n\n1\nDCRNN\n0.8\nlinear\n16.0\nSTGCN\n2.287427\n0.073821\n\n\n2\nDyGrEncoder\n0.8\nlinear\n12.0\nIT-STGCN\n1.398729\n0.063095\n\n\n3\nDyGrEncoder\n0.8\nlinear\n12.0\nSTGCN\n2.126992\n0.240039\n\n\n4\nEvolveGCNH\n0.8\nlinear\n32.0\nIT-STGCN\n1.140095\n0.041585\n\n\n5\nEvolveGCNH\n0.8\nlinear\n32.0\nSTGCN\n1.202927\n0.060884\n\n\n6\nEvolveGCNO\n0.8\nlinear\n32.0\nIT-STGCN\n1.161391\n0.053718\n\n\n7\nEvolveGCNO\n0.8\nlinear\n32.0\nSTGCN\n1.233670\n0.095648\n\n\n8\nGCLSTM\n0.8\nlinear\n16.0\nIT-STGCN\n1.370550\n0.072418\n\n\n9\nGCLSTM\n0.8\nlinear\n16.0\nSTGCN\n2.172284\n0.185516\n\n\n10\nGConvGRU\n0.8\nlinear\n16.0\nIT-STGCN\n1.585524\n0.199285\n\n\n11\nGConvGRU\n0.8\nlinear\n16.0\nSTGCN\n2.528949\n0.292085\n\n\n12\nGConvLSTM\n0.8\nlinear\n32.0\nIT-STGCN\n1.432865\n0.080189\n\n\n13\nGConvLSTM\n0.8\nlinear\n32.0\nSTGCN\n2.521938\n0.111349\n\n\n14\nLRGCN\n0.8\nlinear\n8.0\nIT-STGCN\n1.333524\n0.071124\n\n\n15\nLRGCN\n0.8\nlinear\n8.0\nSTGCN\n1.632052\n0.156044\n\n\n16\nTGCN\n0.8\nlinear\n12.0\nIT-STGCN\n1.183393\n0.027691\n\n\n17\nTGCN\n0.8\nlinear\n12.0\nSTGCN\n1.465696\n0.064056"
  },
  {
    "objectID": "posts/3_result/3_table/2023-07-05-ITSTGCN_data_management.html#block-1",
    "href": "posts/3_result/3_table/2023-07-05-ITSTGCN_data_management.html#block-1",
    "title": "Data management for ITSTGCN",
    "section": "Block",
    "text": "Block\n\npd.merge(df.query(\"dataset=='chickenpox' and mtype=='block' and inter_method=='linear'\").groupby(['model','inter_method','mrate','nof_filters','method'])['mse'].mean().reset_index(),\n         df.query(\"dataset=='chickenpox' and mtype=='block' and inter_method=='linear'\").groupby(['model','inter_method','mrate','nof_filters','method'])['mse'].std().reset_index(),\n         on=['model','method','inter_method','mrate','nof_filters']).rename(columns={'mse_x':'mean','mse_y':'std'}).round(3)\n\n\n\n\n\n\n\n\nmodel\ninter_method\nmrate\nnof_filters\nmethod\nmean\nstd\n\n\n\n\n0\nDCRNN\nlinear\n0.288\n16.0\nIT-STGCN\n0.740\n0.007\n\n\n1\nDCRNN\nlinear\n0.288\n16.0\nSTGCN\n0.812\n0.006\n\n\n2\nDyGrEncoder\nlinear\n0.288\n12.0\nIT-STGCN\n0.899\n0.035\n\n\n3\nDyGrEncoder\nlinear\n0.288\n12.0\nSTGCN\n0.912\n0.043\n\n\n4\nEvolveGCNH\nlinear\n0.288\n32.0\nIT-STGCN\n1.007\n0.021\n\n\n5\nEvolveGCNH\nlinear\n0.288\n32.0\nSTGCN\n1.027\n0.023\n\n\n6\nEvolveGCNO\nlinear\n0.288\n32.0\nIT-STGCN\n1.002\n0.015\n\n\n7\nEvolveGCNO\nlinear\n0.288\n32.0\nSTGCN\n1.028\n0.016\n\n\n8\nGCLSTM\nlinear\n0.288\n16.0\nIT-STGCN\n0.883\n0.045\n\n\n9\nGCLSTM\nlinear\n0.288\n16.0\nSTGCN\n0.890\n0.033\n\n\n10\nGConvGRU\nlinear\n0.288\n16.0\nIT-STGCN\n0.807\n0.016\n\n\n11\nGConvGRU\nlinear\n0.288\n16.0\nSTGCN\n0.828\n0.022\n\n\n12\nGConvLSTM\nlinear\n0.288\n32.0\nIT-STGCN\n0.911\n0.069\n\n\n13\nGConvLSTM\nlinear\n0.288\n32.0\nSTGCN\n0.900\n0.049\n\n\n14\nLRGCN\nlinear\n0.288\n8.0\nIT-STGCN\n0.888\n0.035\n\n\n15\nLRGCN\nlinear\n0.288\n8.0\nSTGCN\n0.911\n0.047\n\n\n16\nTGCN\nlinear\n0.288\n12.0\nIT-STGCN\n1.065\n0.031\n\n\n17\nTGCN\nlinear\n0.288\n12.0\nSTGCN\n1.082\n0.028"
  },
  {
    "objectID": "posts/3_result/3_table/2023-07-05-ITSTGCN_data_management.html#baseline-2",
    "href": "posts/3_result/3_table/2023-07-05-ITSTGCN_data_management.html#baseline-2",
    "title": "Data management for ITSTGCN",
    "section": "Baseline",
    "text": "Baseline\n\npd.merge(df.query(\"dataset=='pedalme' and mtype!='rand' and mtype!='block'\").groupby(['model','lags','nof_filters','epoch'])['mse'].mean().reset_index(),\n         df.query(\"dataset=='pedalme' and mtype!='rand' and mtype!='block'\").groupby(['model','lags','nof_filters','epoch'])['mse'].std().reset_index(),\n         on=['model','lags','nof_filters','epoch']).rename(columns={'mse_x':'mean','mse_y':'std'}).round(3).query(\"lags==4\")\n\n\n\n\n\n\n\n\nmodel\nlags\nnof_filters\nepoch\nmean\nstd\n\n\n\n\n0\nDCRNN\n4\n8.0\n50.0\n1.131\n0.015\n\n\n1\nDyGrEncoder\n4\n12.0\n50.0\n1.190\n0.047\n\n\n2\nEvolveGCNH\n4\n2.0\n50.0\n1.213\n0.057\n\n\n3\nEvolveGCNO\n4\n2.0\n50.0\n1.223\n0.051\n\n\n4\nGCLSTM\n4\n4.0\n50.0\n1.181\n0.040\n\n\n5\nGConvGRU\n4\n12.0\n50.0\n1.233\n0.107\n\n\n6\nGConvLSTM\n4\n2.0\n50.0\n1.214\n0.055\n\n\n7\nLRGCN\n4\n8.0\n50.0\n1.191\n0.054\n\n\n8\nTGCN\n4\n12.0\n50.0\n1.307\n0.075"
  },
  {
    "objectID": "posts/3_result/3_table/2023-07-05-ITSTGCN_data_management.html#random-2",
    "href": "posts/3_result/3_table/2023-07-05-ITSTGCN_data_management.html#random-2",
    "title": "Data management for ITSTGCN",
    "section": "Random",
    "text": "Random\n\npd.merge(df.query(\"dataset=='pedalme' and mtype=='rand' and inter_method=='nearest'\").groupby(['model','mrate','lags','nof_filters','inter_method','method'])['mse'].mean().reset_index(),\n         df.query(\"dataset=='pedalme' and mtype=='rand' and inter_method=='nearest'\").groupby(['model','mrate','lags','nof_filters','inter_method','method'])['mse'].std().reset_index(),\n         on=['model','method','nof_filters','mrate','lags','inter_method']).rename(columns={'mse_x':'mean','mse_y':'std'}).round(3).query(\"mrate == 0.6\")\n\n\n\n\n\n\n\n\nmodel\nmrate\nlags\nnof_filters\ninter_method\nmethod\nmean\nstd\n\n\n\n\n4\nDCRNN\n0.6\n4\n8.0\nnearest\nIT-STGCN\n1.303\n0.078\n\n\n5\nDCRNN\n0.6\n4\n8.0\nnearest\nSTGCN\n1.509\n0.068\n\n\n12\nDyGrEncoder\n0.6\n4\n12.0\nnearest\nIT-STGCN\n1.285\n0.051\n\n\n13\nDyGrEncoder\n0.6\n4\n12.0\nnearest\nSTGCN\n1.513\n0.083\n\n\n20\nEvolveGCNH\n0.6\n4\n2.0\nnearest\nIT-STGCN\n1.262\n0.091\n\n\n21\nEvolveGCNH\n0.6\n4\n2.0\nnearest\nSTGCN\n1.284\n0.066\n\n\n28\nEvolveGCNO\n0.6\n4\n2.0\nnearest\nIT-STGCN\n1.267\n0.067\n\n\n29\nEvolveGCNO\n0.6\n4\n2.0\nnearest\nSTGCN\n1.292\n0.075\n\n\n36\nGCLSTM\n0.6\n4\n4.0\nnearest\nIT-STGCN\n1.259\n0.042\n\n\n37\nGCLSTM\n0.6\n4\n4.0\nnearest\nSTGCN\n1.365\n0.064\n\n\n44\nGConvGRU\n0.6\n4\n12.0\nnearest\nIT-STGCN\n1.625\n0.324\n\n\n45\nGConvGRU\n0.6\n4\n12.0\nnearest\nSTGCN\n1.851\n0.254\n\n\n52\nGConvLSTM\n0.6\n4\n2.0\nnearest\nIT-STGCN\n1.248\n0.045\n\n\n53\nGConvLSTM\n0.6\n4\n2.0\nnearest\nSTGCN\n1.274\n0.078\n\n\n60\nLRGCN\n0.6\n4\n8.0\nnearest\nIT-STGCN\n1.286\n0.033\n\n\n61\nLRGCN\n0.6\n4\n8.0\nnearest\nSTGCN\n1.462\n0.084\n\n\n68\nTGCN\n0.6\n4\n12.0\nnearest\nIT-STGCN\n1.260\n0.072\n\n\n69\nTGCN\n0.6\n4\n12.0\nnearest\nSTGCN\n1.301\n0.090"
  },
  {
    "objectID": "posts/3_result/3_table/2023-07-05-ITSTGCN_data_management.html#block-2",
    "href": "posts/3_result/3_table/2023-07-05-ITSTGCN_data_management.html#block-2",
    "title": "Data management for ITSTGCN",
    "section": "Block",
    "text": "Block\n\npd.merge(df.query(\"dataset=='pedalme' and mtype=='block' and inter_method=='nearest' and lags==4\").groupby(['model','mrate','lags','inter_method','method'])['mse'].mean().reset_index(),\n         df.query(\"dataset=='pedalme' and mtype=='block' and inter_method=='nearest' and lags==4\").groupby(['model','mrate','lags','inter_method','method'])['mse'].std().reset_index(),\n         on=['model','method','mrate','lags','inter_method']).rename(columns={'mse_x':'mean','mse_y':'std'}).round(3)\n\n\n\n\n\n\n\n\nmodel\nmrate\nlags\ninter_method\nmethod\nmean\nstd\n\n\n\n\n0\nDCRNN\n0.286\n4\nnearest\nIT-STGCN\n1.150\n0.014\n\n\n1\nDCRNN\n0.286\n4\nnearest\nSTGCN\n1.304\n0.021\n\n\n2\nDyGrEncoder\n0.286\n4\nnearest\nIT-STGCN\n1.165\n0.032\n\n\n3\nDyGrEncoder\n0.286\n4\nnearest\nSTGCN\n1.269\n0.066\n\n\n4\nEvolveGCNH\n0.286\n4\nnearest\nIT-STGCN\n1.222\n0.040\n\n\n5\nEvolveGCNH\n0.286\n4\nnearest\nSTGCN\n1.265\n0.072\n\n\n6\nEvolveGCNO\n0.286\n4\nnearest\nIT-STGCN\n1.245\n0.045\n\n\n7\nEvolveGCNO\n0.286\n4\nnearest\nSTGCN\n1.246\n0.035\n\n\n8\nGCLSTM\n0.286\n4\nnearest\nIT-STGCN\n1.195\n0.029\n\n\n9\nGCLSTM\n0.286\n4\nnearest\nSTGCN\n1.248\n0.019\n\n\n10\nGConvGRU\n0.286\n4\nnearest\nIT-STGCN\n1.289\n0.115\n\n\n11\nGConvGRU\n0.286\n4\nnearest\nSTGCN\n1.270\n0.114\n\n\n12\nGConvLSTM\n0.286\n4\nnearest\nIT-STGCN\n1.222\n0.039\n\n\n13\nGConvLSTM\n0.286\n4\nnearest\nSTGCN\n1.237\n0.046\n\n\n14\nGNAR\n0.286\n4\nnearest\nGNAR\n1.303\n0.000\n\n\n15\nLRGCN\n0.286\n4\nnearest\nIT-STGCN\n1.165\n0.035\n\n\n16\nLRGCN\n0.286\n4\nnearest\nSTGCN\n1.263\n0.033\n\n\n17\nTGCN\n0.286\n4\nnearest\nIT-STGCN\n1.262\n0.066\n\n\n18\nTGCN\n0.286\n4\nnearest\nSTGCN\n1.232\n0.069"
  },
  {
    "objectID": "posts/3_result/3_table/2023-07-05-ITSTGCN_data_management.html#w_st",
    "href": "posts/3_result/3_table/2023-07-05-ITSTGCN_data_management.html#w_st",
    "title": "Data management for ITSTGCN",
    "section": "W_st",
    "text": "W_st\n\npd.merge(df2.query(\"dataset == 'pedalme' and mtype=='rand' and lags==4 and mrate==0.6 and inter_method=='nearest'\").groupby(['model','mrate','lags','inter_method','method','epoch'])['mse'].mean().reset_index(),\n         df2.query(\"dataset == 'pedalme' and mtype=='rand' and lags==4 and mrate==0.6 and inter_method=='nearest'\").groupby(['model','mrate','lags','inter_method','method','epoch'])['mse'].std().reset_index(),\n         on=['model','method','mrate','lags','inter_method','epoch']).rename(columns={'mse_x':'mean','mse_y':'std'}).round(3).query(\"method !='STGCN' and lags==4 and mrate == 0.6 and inter_method=='nearest'\")\n\n\n\n\n\n\n\n\nmodel\nmrate\nlags\ninter_method\nmethod\nepoch\nmean\nstd\n\n\n\n\n0\nDCRNN\n0.6\n4\nnearest\nIT-STGCN\n50.0\n1.208\n0.079\n\n\n2\nDyGrEncoder\n0.6\n4\nnearest\nIT-STGCN\n50.0\n1.305\n0.131\n\n\n4\nEvolveGCNH\n0.6\n4\nnearest\nIT-STGCN\n50.0\n1.246\n0.067\n\n\n6\nEvolveGCNO\n0.6\n4\nnearest\nIT-STGCN\n50.0\n1.248\n0.072\n\n\n8\nGCLSTM\n0.6\n4\nnearest\nIT-STGCN\n50.0\n1.231\n0.044\n\n\n10\nGConvGRU\n0.6\n4\nnearest\nIT-STGCN\n50.0\n1.410\n0.208\n\n\n12\nGConvLSTM\n0.6\n4\nnearest\nIT-STGCN\n50.0\n1.313\n0.205\n\n\n14\nLRGCN\n0.6\n4\nnearest\nIT-STGCN\n50.0\n1.331\n0.120\n\n\n16\nTGCN\n0.6\n4\nnearest\nIT-STGCN\n50.0\n1.338\n0.202"
  },
  {
    "objectID": "posts/3_result/3_table/2023-07-05-ITSTGCN_data_management.html#baseline-3",
    "href": "posts/3_result/3_table/2023-07-05-ITSTGCN_data_management.html#baseline-3",
    "title": "Data management for ITSTGCN",
    "section": "Baseline",
    "text": "Baseline\n\npd.merge(df.query(\"dataset=='wikimath' and mrate not in ['random','block'] and lags==8 and method !='GNAR'\").groupby(['model','lags','nof_filters','method','epoch'])['mse'].mean().reset_index(),\n         df.query(\"dataset=='wikimath' and mrate not in ['random','block'] and lags==8 and method !='GNAR'\").groupby(['model','lags','nof_filters','method','epoch'])['mse'].std().reset_index(),\n         on=['model','lags','nof_filters','method','epoch']).rename(columns={'mse_x':'mean','mse_y':'std'}).round(3)\n\n\n\n\n\n\n\n\nmodel\nlags\nnof_filters\nmethod\nepoch\nmean\nstd\n\n\n\n\n0\nDCRNN\n8\n12.0\nIT-STGCN\n50.0\n0.601\n0.033\n\n\n1\nDCRNN\n8\n12.0\nSTGCN\n50.0\n0.658\n0.095\n\n\n2\nDyGrEncoder\n8\n12.0\nIT-STGCN\n50.0\n0.573\n0.029\n\n\n3\nDyGrEncoder\n8\n12.0\nSTGCN\n50.0\n0.616\n0.082\n\n\n4\nEvolveGCNH\n8\n12.0\nIT-STGCN\n50.0\n0.800\n0.046\n\n\n5\nEvolveGCNH\n8\n12.0\nSTGCN\n50.0\n0.816\n0.062\n\n\n6\nEvolveGCNO\n8\n12.0\nIT-STGCN\n50.0\n0.746\n0.027\n\n\n7\nEvolveGCNO\n8\n12.0\nSTGCN\n50.0\n0.768\n0.052\n\n\n8\nGCLSTM\n8\n64.0\nIT-STGCN\n50.0\n0.667\n0.073\n\n\n9\nGCLSTM\n8\n64.0\nSTGCN\n50.0\n0.855\n0.277\n\n\n10\nGConvGRU\n8\n12.0\nIT-STGCN\n50.0\n0.549\n0.055\n\n\n11\nGConvGRU\n8\n12.0\nSTGCN\n50.0\n0.632\n0.139\n\n\n12\nGConvLSTM\n8\n64.0\nIT-STGCN\n50.0\n0.701\n0.112\n\n\n13\nGConvLSTM\n8\n64.0\nSTGCN\n50.0\n0.926\n0.286\n\n\n14\nLRGCN\n8\n32.0\nIT-STGCN\n50.0\n0.646\n0.062\n\n\n15\nLRGCN\n8\n32.0\nSTGCN\n50.0\n0.797\n0.186\n\n\n16\nTGCN\n8\n12.0\nIT-STGCN\n50.0\n0.748\n0.039\n\n\n17\nTGCN\n8\n12.0\nSTGCN\n50.0\n0.761\n0.047"
  },
  {
    "objectID": "posts/3_result/3_table/2023-07-05-ITSTGCN_data_management.html#random-3",
    "href": "posts/3_result/3_table/2023-07-05-ITSTGCN_data_management.html#random-3",
    "title": "Data management for ITSTGCN",
    "section": "Random",
    "text": "Random\n\npd.merge(df.query(\"dataset=='wikimath' and mtype=='rand' and inter_method=='linear' and method!='GNAR'\").groupby(['model','mrate','lags','inter_method','method'])['mse'].mean().reset_index(),\n         df.query(\"dataset=='wikimath' and mtype=='rand' and inter_method=='linear' and method!='GNAR'\").groupby(['model','mrate','lags','inter_method','method'])['mse'].std().reset_index(),\n         on=['model','method','mrate','inter_method','lags']).rename(columns={'mse_x':'mean','mse_y':'std'}).round(3).query(\"mrate == 0.8\")\n\n\n\n\n\n\n\n\nmodel\nmrate\nlags\ninter_method\nmethod\nmean\nstd\n\n\n\n\n6\nDCRNN\n0.8\n8\nlinear\nIT-STGCN\n0.672\n0.007\n\n\n7\nDCRNN\n0.8\n8\nlinear\nSTGCN\n0.846\n0.031\n\n\n14\nDyGrEncoder\n0.8\n8\nlinear\nIT-STGCN\n0.606\n0.017\n\n\n15\nDyGrEncoder\n0.8\n8\nlinear\nSTGCN\n0.770\n0.045\n\n\n22\nEvolveGCNH\n0.8\n8\nlinear\nIT-STGCN\n0.877\n0.045\n\n\n23\nEvolveGCNH\n0.8\n8\nlinear\nSTGCN\n0.915\n0.063\n\n\n30\nEvolveGCNO\n0.8\n8\nlinear\nIT-STGCN\n0.780\n0.027\n\n\n31\nEvolveGCNO\n0.8\n8\nlinear\nSTGCN\n0.863\n0.038\n\n\n38\nGCLSTM\n0.8\n8\nlinear\nIT-STGCN\n0.815\n0.058\n\n\n39\nGCLSTM\n0.8\n8\nlinear\nSTGCN\n1.407\n0.117\n\n\n46\nGConvGRU\n0.8\n8\nlinear\nIT-STGCN\n0.687\n0.021\n\n\n47\nGConvGRU\n0.8\n8\nlinear\nSTGCN\n0.932\n0.043\n\n\n54\nGConvLSTM\n0.8\n8\nlinear\nIT-STGCN\n0.920\n0.069\n\n\n55\nGConvLSTM\n0.8\n8\nlinear\nSTGCN\n1.423\n0.121\n\n\n62\nLRGCN\n0.8\n8\nlinear\nIT-STGCN\n0.769\n0.045\n\n\n63\nLRGCN\n0.8\n8\nlinear\nSTGCN\n1.105\n0.099\n\n\n70\nTGCN\n0.8\n8\nlinear\nIT-STGCN\n0.771\n0.020\n\n\n71\nTGCN\n0.8\n8\nlinear\nSTGCN\n0.827\n0.030"
  },
  {
    "objectID": "posts/3_result/3_table/2023-07-05-ITSTGCN_data_management.html#block-3",
    "href": "posts/3_result/3_table/2023-07-05-ITSTGCN_data_management.html#block-3",
    "title": "Data management for ITSTGCN",
    "section": "Block",
    "text": "Block\n\npd.merge(df.query(\"dataset=='wikimath' and mtype=='block'\").groupby(['model','mrate','lags','inter_method','method'])['mse'].mean().reset_index(),\n         df.query(\"dataset=='wikimath' and mtype=='block'\").groupby(['model','mrate','lags','inter_method','method'])['mse'].std().reset_index(),\n         on=['model','method','mrate','inter_method','lags']).rename(columns={'mse_x':'mean','mse_y':'std'}).round(3)\n\n\n\n\n\n\n\n\nmodel\nmrate\nlags\ninter_method\nmethod\nmean\nstd\n\n\n\n\n0\nDCRNN\n0.120\n8\nlinear\nIT-STGCN\n0.583\n0.006\n\n\n1\nDCRNN\n0.120\n8\nlinear\nSTGCN\n0.578\n0.005\n\n\n2\nDyGrEncoder\n0.120\n8\nlinear\nIT-STGCN\n0.563\n0.025\n\n\n3\nDyGrEncoder\n0.120\n8\nlinear\nSTGCN\n0.546\n0.016\n\n\n4\nEvolveGCNH\n0.120\n8\nlinear\nIT-STGCN\n0.776\n0.028\n\n\n5\nEvolveGCNH\n0.120\n8\nlinear\nSTGCN\n0.773\n0.021\n\n\n6\nEvolveGCNO\n0.120\n8\nlinear\nIT-STGCN\n0.732\n0.025\n\n\n7\nEvolveGCNO\n0.120\n8\nlinear\nSTGCN\n0.735\n0.022\n\n\n8\nGCLSTM\n0.120\n8\nlinear\nIT-STGCN\n0.640\n0.019\n\n\n9\nGCLSTM\n0.120\n8\nlinear\nSTGCN\n0.638\n0.013\n\n\n10\nGConvGRU\n0.004\n8\nlinear\nIT-STGCN\n0.529\n0.003\n\n\n11\nGConvGRU\n0.004\n8\nlinear\nSTGCN\n0.528\n0.003\n\n\n12\nGConvGRU\n0.096\n8\nlinear\nIT-STGCN\n0.529\n0.004\n\n\n13\nGConvGRU\n0.096\n8\nlinear\nSTGCN\n0.544\n0.011\n\n\n14\nGConvGRU\n0.120\n8\nlinear\nIT-STGCN\n0.523\n0.002\n\n\n15\nGConvGRU\n0.120\n8\nlinear\nSTGCN\n0.531\n0.002\n\n\n16\nGConvLSTM\n0.120\n8\nlinear\nIT-STGCN\n0.627\n0.014\n\n\n17\nGConvLSTM\n0.120\n8\nlinear\nSTGCN\n0.660\n0.034\n\n\n18\nGNAR\n0.120\n8\nlinear\nGNAR\n1.354\n0.000\n\n\n19\nLRGCN\n0.120\n8\nlinear\nIT-STGCN\n0.608\n0.012\n\n\n20\nLRGCN\n0.120\n8\nlinear\nSTGCN\n0.624\n0.024\n\n\n21\nTGCN\n0.120\n8\nlinear\nIT-STGCN\n0.748\n0.046\n\n\n22\nTGCN\n0.120\n8\nlinear\nSTGCN\n0.741\n0.046"
  },
  {
    "objectID": "posts/3_result/3_table/2023-07-05-ITSTGCN_data_management.html#missing-values-on-the-same-nodes",
    "href": "posts/3_result/3_table/2023-07-05-ITSTGCN_data_management.html#missing-values-on-the-same-nodes",
    "title": "Data management for ITSTGCN",
    "section": "missing values on the same nodes",
    "text": "missing values on the same nodes\n\npd.merge(df2.query(\"dataset=='wikimath' and lags==8 and inter_method=='linear'\").groupby(['model','mrate','lags','method','epoch'])['mse'].mean().reset_index(),\n        df2.query(\"dataset=='wikimath' and lags==8 and inter_method=='linear'\").groupby(['model','mrate','lags','method','epoch'])['mse'].std().reset_index(),\n         on=['model','method','mrate','lags','epoch']).rename(columns={'mse_x':'mean','mse_y':'std'}).round(3)\n\n\n\n\n\n\n\n\nmodel\nmrate\nlags\nmethod\nepoch\nmean\nstd\n\n\n\n\n0\nDCRNN\n0.512\n8\nIT-STGCN\n50.0\n0.592\n0.005\n\n\n1\nDCRNN\n0.512\n8\nSTGCN\n50.0\n0.665\n0.015\n\n\n2\nDyGrEncoder\n0.512\n8\nIT-STGCN\n50.0\n0.561\n0.031\n\n\n3\nDyGrEncoder\n0.512\n8\nSTGCN\n50.0\n0.626\n0.027\n\n\n4\nEvolveGCNH\n0.512\n8\nIT-STGCN\n50.0\n0.794\n0.031\n\n\n5\nEvolveGCNH\n0.512\n8\nSTGCN\n50.0\n0.818\n0.031\n\n\n6\nEvolveGCNO\n0.512\n8\nIT-STGCN\n50.0\n0.745\n0.017\n\n\n7\nEvolveGCNO\n0.512\n8\nSTGCN\n50.0\n0.753\n0.026\n\n\n8\nGCLSTM\n0.512\n8\nIT-STGCN\n50.0\n0.617\n0.011\n\n\n9\nGCLSTM\n0.512\n8\nSTGCN\n50.0\n0.823\n0.048\n\n\n10\nGConvGRU\n0.512\n8\nIT-STGCN\n50.0\n0.533\n0.003\n\n\n11\nGConvGRU\n0.512\n8\nSTGCN\n50.0\n0.726\n0.015\n\n\n12\nGConvLSTM\n0.512\n8\nIT-STGCN\n50.0\n0.653\n0.033\n\n\n13\nGConvLSTM\n0.512\n8\nSTGCN\n50.0\n0.963\n0.098\n\n\n14\nLRGCN\n0.512\n8\nIT-STGCN\n50.0\n0.624\n0.019\n\n\n15\nLRGCN\n0.512\n8\nSTGCN\n50.0\n0.810\n0.064\n\n\n16\nTGCN\n0.512\n8\nIT-STGCN\n50.0\n0.750\n0.039\n\n\n17\nTGCN\n0.512\n8\nSTGCN\n50.0\n0.782\n0.030"
  },
  {
    "objectID": "posts/3_result/3_table/2023-07-05-ITSTGCN_data_management.html#baseline-4",
    "href": "posts/3_result/3_table/2023-07-05-ITSTGCN_data_management.html#baseline-4",
    "title": "Data management for ITSTGCN",
    "section": "Baseline",
    "text": "Baseline\n\npd.merge(df.query(\"dataset=='windmillsmall' and mrate not in ['random','block'] and lags==8 and method !='GNAR'\").groupby(['model','lags','epoch'])['mse'].mean().reset_index(),\n         df.query(\"dataset=='windmillsmall' and mrate not in ['random','block'] and lags==8 and method !='GNAR'\").groupby(['model','lags','epoch'])['mse'].std().reset_index(),\n         on=['model','lags','epoch']).rename(columns={'mse_x':'mean','mse_y':'std'}).round(3)\n\n\n\n\n\n\n\n\nmodel\nlags\nepoch\nmean\nstd\n\n\n\n\n0\nDCRNN\n8\n50.0\n1.069\n0.136\n\n\n1\nDyGrEncoder\n8\n50.0\n1.130\n0.266\n\n\n2\nEvolveGCNH\n8\n50.0\n1.068\n0.141\n\n\n3\nEvolveGCNO\n8\n50.0\n1.097\n0.196\n\n\n4\nGCLSTM\n8\n50.0\n1.110\n0.220\n\n\n5\nGConvGRU\n8\n50.0\n1.146\n0.244\n\n\n6\nGConvLSTM\n8\n50.0\n1.122\n0.217\n\n\n7\nLRGCN\n8\n50.0\n1.091\n0.189\n\n\n8\nTGCN\n8\n50.0\n1.058\n0.116"
  },
  {
    "objectID": "posts/3_result/3_table/2023-07-05-ITSTGCN_data_management.html#random-4",
    "href": "posts/3_result/3_table/2023-07-05-ITSTGCN_data_management.html#random-4",
    "title": "Data management for ITSTGCN",
    "section": "Random",
    "text": "Random\n\npd.merge(df.query(\"dataset=='windmillsmall' and mtype=='rand' and method !='GNAR'\").groupby(['model','mrate','lags','inter_method','method'])['mse'].mean().reset_index(),\n         df.query(\"dataset=='windmillsmall' and mtype=='rand' and method !='GNAR'\").groupby(['model','mrate','lags','inter_method','method'])['mse'].std().reset_index(),\n         on=['model','method','mrate','inter_method','lags']).rename(columns={'mse_x':'mean','mse_y':'std'}).round(3)\n\n\n\n\n\n\n\n\nmodel\nmrate\nlags\ninter_method\nmethod\nmean\nstd\n\n\n\n\n0\nDCRNN\n0.7\n8\nlinear\nIT-STGCN\n1.117\n0.034\n\n\n1\nDCRNN\n0.7\n8\nlinear\nSTGCN\n1.348\n0.057\n\n\n2\nDyGrEncoder\n0.7\n8\nlinear\nIT-STGCN\n1.127\n0.009\n\n\n3\nDyGrEncoder\n0.7\n8\nlinear\nSTGCN\n1.709\n0.074\n\n\n4\nEvolveGCNH\n0.7\n8\nlinear\nIT-STGCN\n1.129\n0.035\n\n\n5\nEvolveGCNH\n0.7\n8\nlinear\nSTGCN\n1.330\n0.137\n\n\n6\nEvolveGCNO\n0.7\n8\nlinear\nIT-STGCN\n1.149\n0.026\n\n\n7\nEvolveGCNO\n0.7\n8\nlinear\nSTGCN\n1.495\n0.137\n\n\n8\nGCLSTM\n0.7\n8\nlinear\nIT-STGCN\n1.116\n0.021\n\n\n9\nGCLSTM\n0.7\n8\nlinear\nSTGCN\n1.573\n0.105\n\n\n10\nGConvGRU\n0.7\n8\nlinear\nIT-STGCN\n1.194\n0.042\n\n\n11\nGConvGRU\n0.7\n8\nlinear\nSTGCN\n1.662\n0.073\n\n\n12\nGConvLSTM\n0.7\n8\nlinear\nIT-STGCN\n1.142\n0.021\n\n\n13\nGConvLSTM\n0.7\n8\nlinear\nSTGCN\n1.599\n0.057\n\n\n14\nLRGCN\n0.7\n8\nlinear\nIT-STGCN\n1.110\n0.012\n\n\n15\nLRGCN\n0.7\n8\nlinear\nSTGCN\n1.492\n0.087\n\n\n16\nTGCN\n0.7\n8\nlinear\nIT-STGCN\n1.071\n0.010\n\n\n17\nTGCN\n0.7\n8\nlinear\nSTGCN\n1.305\n0.039"
  },
  {
    "objectID": "posts/3_result/3_table/2023-07-05-ITSTGCN_data_management.html#block-4",
    "href": "posts/3_result/3_table/2023-07-05-ITSTGCN_data_management.html#block-4",
    "title": "Data management for ITSTGCN",
    "section": "Block",
    "text": "Block\n\npd.merge(df.query(\"dataset=='windmillsmall' and mtype=='block' and method !='GNAR'\").groupby(['model','mrate','nof_filters','lags','method'])['mse'].mean().reset_index(),\n         df.query(\"dataset=='windmillsmall' and mtype=='block' and method !='GNAR'\").groupby(['model','mrate','nof_filters','lags','method'])['mse'].std().reset_index(),\n         on=['model','method','nof_filters','mrate','lags']).rename(columns={'mse_x':'mean','mse_y':'std'}).round(3)\n\n\n\n\n\n\n\n\nmodel\nmrate\nnof_filters\nlags\nmethod\nmean\nstd\n\n\n\n\n0\nDCRNN\n0.081\n4.0\n8\nIT-STGCN\n0.983\n0.002\n\n\n1\nDCRNN\n0.081\n4.0\n8\nSTGCN\n0.994\n0.005\n\n\n2\nDyGrEncoder\n0.081\n12.0\n8\nIT-STGCN\n0.985\n0.005\n\n\n3\nDyGrEncoder\n0.081\n12.0\n8\nSTGCN\n0.985\n0.003\n\n\n4\nEvolveGCNH\n0.081\n12.0\n8\nIT-STGCN\n0.986\n0.003\n\n\n5\nEvolveGCNH\n0.081\n12.0\n8\nSTGCN\n0.993\n0.003\n\n\n6\nEvolveGCNO\n0.081\n12.0\n8\nIT-STGCN\n0.983\n0.002\n\n\n7\nEvolveGCNO\n0.081\n12.0\n8\nSTGCN\n0.990\n0.002\n\n\n8\nGCLSTM\n0.081\n16.0\n8\nIT-STGCN\n0.985\n0.003\n\n\n9\nGCLSTM\n0.081\n16.0\n8\nSTGCN\n0.985\n0.002\n\n\n10\nGConvGRU\n0.081\n12.0\n8\nIT-STGCN\n1.007\n0.005\n\n\n11\nGConvGRU\n0.081\n12.0\n8\nSTGCN\n1.008\n0.006\n\n\n12\nGConvLSTM\n0.081\n16.0\n8\nIT-STGCN\n0.997\n0.022\n\n\n13\nGConvLSTM\n0.081\n16.0\n8\nSTGCN\n0.989\n0.009\n\n\n14\nLRGCN\n0.081\n12.0\n8\nIT-STGCN\n0.985\n0.002\n\n\n15\nLRGCN\n0.081\n12.0\n8\nSTGCN\n0.985\n0.003\n\n\n16\nTGCN\n0.081\n12.0\n8\nIT-STGCN\n0.992\n0.015\n\n\n17\nTGCN\n0.081\n12.0\n8\nSTGCN\n0.999\n0.013"
  },
  {
    "objectID": "posts/3_result/3_table/2023-07-05-ITSTGCN_data_management.html#baseline-5",
    "href": "posts/3_result/3_table/2023-07-05-ITSTGCN_data_management.html#baseline-5",
    "title": "Data management for ITSTGCN",
    "section": "Baseline",
    "text": "Baseline\n\npd.merge(df.query(\"dataset=='monte' and mrate not in ['random','block'] and lags==4 and method !='GNAR'\").groupby(['model','lags','epoch'])['mse'].mean().reset_index(),\n         df.query(\"dataset=='monte' and mrate not in ['random','block'] and lags==4 and method !='GNAR'\").groupby(['model','lags','epoch'])['mse'].std().reset_index(),\n         on=['model','lags','epoch']).rename(columns={'mse_x':'mean','mse_y':'std'}).round(3)\n\n\n\n\n\n\n\n\nmodel\nlags\nepoch\nmean\nstd\n\n\n\n\n0\nDCRNN\n4\n50.0\n1.034\n0.112\n\n\n1\nDyGrEncoder\n4\n50.0\n1.153\n0.185\n\n\n2\nEvolveGCNH\n4\n50.0\n1.594\n0.457\n\n\n3\nEvolveGCNO\n4\n50.0\n1.763\n0.570\n\n\n4\nGCLSTM\n4\n50.0\n1.017\n0.072\n\n\n5\nGConvGRU\n4\n50.0\n1.064\n0.192\n\n\n6\nGConvLSTM\n4\n50.0\n1.013\n0.085\n\n\n7\nLRGCN\n4\n50.0\n0.981\n0.022\n\n\n8\nTGCN\n4\n50.0\n1.046\n0.085"
  },
  {
    "objectID": "posts/3_result/3_table/2023-07-05-ITSTGCN_data_management.html#random-5",
    "href": "posts/3_result/3_table/2023-07-05-ITSTGCN_data_management.html#random-5",
    "title": "Data management for ITSTGCN",
    "section": "Random",
    "text": "Random\n\npd.merge(df.query(\"dataset=='monte' and mtype=='rand' and inter_method=='nearest' and mrate==0.8\").groupby(['model','mrate','nof_filters','lags','inter_method','method'])['mse'].mean().reset_index(),\n         df.query(\"dataset=='monte' and mtype=='rand' and inter_method=='nearest' and mrate==0.8\").groupby(['model','mrate','nof_filters','lags','inter_method','method'])['mse'].std().reset_index(),\n         on=['model','mrate','nof_filters','inter_method','method','mrate','lags']).rename(columns={'mse_x':'mean','mse_y':'std'}).round(3)\n\n\n\n\n\n\n\n\nmodel\nmrate\nnof_filters\nlags\ninter_method\nmethod\nmean\nstd\n\n\n\n\n0\nDCRNN\n0.8\n12.0\n4\nnearest\nIT-STGCN\n1.111\n0.036\n\n\n1\nDCRNN\n0.8\n12.0\n4\nnearest\nSTGCN\n1.225\n0.073\n\n\n2\nDyGrEncoder\n0.8\n12.0\n4\nnearest\nIT-STGCN\n1.216\n0.118\n\n\n3\nDyGrEncoder\n0.8\n12.0\n4\nnearest\nSTGCN\n1.358\n0.149\n\n\n4\nEvolveGCNH\n0.8\n12.0\n4\nnearest\nIT-STGCN\n1.845\n0.504\n\n\n5\nEvolveGCNH\n0.8\n12.0\n4\nnearest\nSTGCN\n2.158\n0.545\n\n\n6\nEvolveGCNO\n0.8\n12.0\n4\nnearest\nIT-STGCN\n2.263\n0.476\n\n\n7\nEvolveGCNO\n0.8\n12.0\n4\nnearest\nSTGCN\n2.623\n0.693\n\n\n8\nGCLSTM\n0.8\n12.0\n4\nnearest\nIT-STGCN\n1.032\n0.028\n\n\n9\nGCLSTM\n0.8\n12.0\n4\nnearest\nSTGCN\n1.140\n0.061\n\n\n10\nGConvGRU\n0.8\n12.0\n4\nnearest\nIT-STGCN\n1.096\n0.019\n\n\n11\nGConvGRU\n0.8\n12.0\n4\nnearest\nSTGCN\n1.516\n0.040\n\n\n12\nGConvLSTM\n0.8\n12.0\n4\nnearest\nIT-STGCN\n1.156\n0.062\n\n\n13\nGConvLSTM\n0.8\n12.0\n4\nnearest\nSTGCN\n1.134\n0.069\n\n\n14\nLRGCN\n0.8\n2.0\n4\nnearest\nIT-STGCN\n0.982\n0.013\n\n\n15\nLRGCN\n0.8\n2.0\n4\nnearest\nSTGCN\n0.989\n0.029\n\n\n16\nTGCN\n0.8\n8.0\n4\nnearest\nIT-STGCN\n1.073\n0.024\n\n\n17\nTGCN\n0.8\n8.0\n4\nnearest\nSTGCN\n1.218\n0.086"
  },
  {
    "objectID": "posts/3_result/3_table/2023-07-05-ITSTGCN_data_management.html#block-5",
    "href": "posts/3_result/3_table/2023-07-05-ITSTGCN_data_management.html#block-5",
    "title": "Data management for ITSTGCN",
    "section": "Block",
    "text": "Block\n\npd.merge(df.query(\"dataset=='monte' and mtype=='block' and inter_method=='nearest'\").groupby(['model','mrate','lags','inter_method','method'])['mse'].mean().reset_index(),\n         df.query(\"dataset=='monte' and mtype=='block' and inter_method=='nearest'\").groupby(['model','mrate','lags','inter_method','method'])['mse'].std().reset_index(),\n         on=['model','method','mrate','inter_method','lags']).rename(columns={'mse_x':'mean','mse_y':'std'}).round(3)\n\n\n\n\n\n\n\n\nmodel\nmrate\nlags\ninter_method\nmethod\nmean\nstd\n\n\n\n\n0\nDCRNN\n0.149\n4\nnearest\nIT-STGCN\n0.940\n0.001\n\n\n1\nDCRNN\n0.149\n4\nnearest\nSTGCN\n0.956\n0.003\n\n\n2\nDyGrEncoder\n0.149\n4\nnearest\nIT-STGCN\n1.005\n0.046\n\n\n3\nDyGrEncoder\n0.149\n4\nnearest\nSTGCN\n1.030\n0.044\n\n\n4\nEvolveGCNH\n0.149\n4\nnearest\nIT-STGCN\n1.392\n0.110\n\n\n5\nEvolveGCNH\n0.149\n4\nnearest\nSTGCN\n1.612\n0.216\n\n\n6\nEvolveGCNO\n0.149\n4\nnearest\nIT-STGCN\n1.345\n0.110\n\n\n7\nEvolveGCNO\n0.149\n4\nnearest\nSTGCN\n1.766\n0.123\n\n\n8\nGCLSTM\n0.149\n4\nnearest\nIT-STGCN\n0.959\n0.008\n\n\n9\nGCLSTM\n0.149\n4\nnearest\nSTGCN\n0.956\n0.005\n\n\n10\nGConvGRU\n0.149\n4\nnearest\nIT-STGCN\n0.932\n0.002\n\n\n11\nGConvGRU\n0.149\n4\nnearest\nSTGCN\n0.935\n0.004\n\n\n12\nGConvLSTM\n0.149\n4\nnearest\nIT-STGCN\n0.949\n0.008\n\n\n13\nGConvLSTM\n0.149\n4\nnearest\nSTGCN\n0.950\n0.005\n\n\n14\nGNAR\n0.149\n4\nnearest\nGNAR\n1.062\n0.000\n\n\n15\nLRGCN\n0.149\n4\nnearest\nIT-STGCN\n0.978\n0.024\n\n\n16\nLRGCN\n0.149\n4\nnearest\nSTGCN\n0.977\n0.020\n\n\n17\nTGCN\n0.149\n4\nnearest\nIT-STGCN\n0.984\n0.007\n\n\n18\nTGCN\n0.149\n4\nnearest\nSTGCN\n0.985\n0.005"
  },
  {
    "objectID": "posts/2_research/2022-12-21-ST-GCN_Dataset.html",
    "href": "posts/2_research/2022-12-21-ST-GCN_Dataset.html",
    "title": "PyTorch ST-GCN Dataset",
    "section": "",
    "text": "PyTorch Geometric Temporal Dataset\nhttps://pytorch-geometric-temporal.readthedocs.io/en/latest/modules/dataset.html#module-torch_geometric_temporal.dataset.chickenpox\n논문\n|Dataset|Signal|Graph|Frequency|𝑇|ㅣ𝑉ㅣ | |:–:|:–:||:–:||:–:| |Chickenpox Hungary|Temporal|Static|Weekly|522|20| |Windmill Large|Temporal|Static|Hourly|17,472|319| |Windmill Medium|Temporal|Static|Hourly|17,472|26| |Windmill Small|Temporal|Static|Hourly|17,472|11| |Pedal Me Deliveries|Temporal|Static|Weekly|36|15| |Wikipedia Math|Temporal|Static|Daily|731|1,068| |Twitter Tennis RG|Static|Dynamic|Hourly|120|1000| |Twitter Tennis UO|Static|Dynamic|Hourly|112|1000| |Covid19 England|Temporal|Dynamic|Daily|61|129| |Montevideo Buses|Temporal|Static|Hourly|744|675| |MTM-1 Hand Motions|Temporal|Static|1/24 Seconds|14,469|21|\nimport networkx as nx\nimport matplotlib.pyplot as plt\nimport numpy as np\nfrom tqdm import tqdm\nfrom torch_geometric_temporal.signal import temporal_signal_split"
  },
  {
    "objectID": "posts/2_research/2022-12-21-ST-GCN_Dataset.html#chickenpoxdatasetloader",
    "href": "posts/2_research/2022-12-21-ST-GCN_Dataset.html#chickenpoxdatasetloader",
    "title": "PyTorch ST-GCN Dataset",
    "section": "ChickenpoxDatasetLoader",
    "text": "ChickenpoxDatasetLoader\nChickenpox Hungary\n\nA spatiotemporal dataset about the officially reported cases of chickenpox in Hungary. The nodes are counties and edges describe direct neighbourhood relationships. The dataset covers the weeks between 2005 and 2015 without missingness.\n\n데이터정리\n\nT = 519\nN = 20 # number of nodes\nE = 102 # edges\n\\(f(v,t)\\)의 차원? (1,)\n시간에 따라서 Number of nodes가 변하는지? False\n시간에 따라서 Number of nodes가 변하는지? False\nX: (20,4) (N,4), \\(f(v,t_0),f(v,t_1),f(v,t_2),f(v,t_3)\\)\ny: (20,) (N,), \\(f(v,t_4)\\)\n예제코드적용가능여부: Yes\n\n- Nodes : 20\n\nvertices are counties\n\n-Edges : 102\n\nedges are neighbourhoods\n\n- Time : 519\n\nbetween 2004 and 2014\nper weeks\n\n\nfrom torch_geometric_temporal.dataset import  ChickenpoxDatasetLoader\nfrom torch_geometric_temporal.signal import temporal_signal_split\n\nloader = ChickenpoxDatasetLoader()\n\ndataset = loader.get_dataset(lags=1)\n\ntrain_dataset, test_dataset = temporal_signal_split(dataset, train_ratio=1)\n\n\ndata=[]\nfor time, snapshot in enumerate(train_dataset):\n    data.append([time,snapshot])\n\n\ntime\n\n519\n\n\n\n(data[0][1]).x.type,(data[0][1]).edge_index.type,(data[0][1]).edge_attr.type,(data[0][1]).y.type\n\n(&lt;function Tensor.type&gt;,\n &lt;function Tensor.type&gt;,\n &lt;function Tensor.type&gt;,\n &lt;function Tensor.type&gt;)\n\n\n\nmax((data[4][1]).x[0])\n\ntensor(2.1339)\n\n\n\nG = nx.Graph()\n\n\nnode_list = torch.tensor(range(20)).tolist()\n\n\nG.add_nodes_from(node_list)\n\n\ndata[-1]\n\n[519, Data(x=[20, 1], edge_index=[2, 102], edge_attr=[102], y=[20])]\n\n\n\nlen(data[0][1].edge_index[0])\n\n102\n\n\n\nedge_list=[]\nfor i in range(519):\n    for j in range(len(data[0][1].edge_index[0])):\n        edge_list.append([data[i][1].edge_index[0][j].tolist(),data[i][1].edge_index[1][j].tolist()])\n\n\nG.add_edges_from(edge_list)\n\n\nG.number_of_nodes(),G.number_of_edges()\n\n(20, 61)\n\n\n\nnx.draw(G,with_labels=True,font_weight='bold',node_color='green',node_size=350,font_color='white',width=1)\n\n\n\n\n\n\n\n\n\ntime별 같은 edge 정보를 가지고 있나 확인\n\nnp.where(data[0][1].edge_index != data[10][1].edge_index)\n\n(array([], dtype=int64), array([], dtype=int64))\n\n\n\n\nfrom torch_geometric_temporal.dataset import  ChickenpoxDatasetLoader\nloader = ChickenpoxDatasetLoader()\n\ndataset = loader.get_dataset(lags=4)\n\ntrain_dataset, test_dataset = temporal_signal_split(dataset, train_ratio=0.5)\n\n\ndata=[]\nfor time, snapshot in enumerate(train_dataset):\n    data.append([time,snapshot])\n\n\n(data[0][1]).x[0], (data[0][1]).y[0]\n\n(tensor([-0.0011,  0.0286,  0.3547,  0.2954]), tensor(0.7106))\n\n\n\\(t=0\\)에서 \\(X\\)와 \\(y\\)를 정리하면 아래와 같음.\n\nX:= \\(x_0,x_1,x_2,x_3\\)\ny:= \\(x_4\\)\n\n\n(data[1][1]).x[0]\n\ntensor([0.0286, 0.3547, 0.2954, 0.7106])\n\n\n\nX:= \\(x_1,x_2,x_3,x_4\\)\ny:= \\(x_5\\)\n\n\n(data[2][1]).x[0]\n\ntensor([ 0.3547,  0.2954,  0.7106, -0.6831])\n\n\n\nX:=\\(x_2,x_3,x_4,x_5\\)\ny:=\\(x_6\\)\n\n하나의 노드에 길이가 \\(T\\)인 시계열이 맵핑되어 있음. (노드는 총 20개)\n각 노드마다 아래와 같은 과정으로 예측이 됨\n\n\\((x_0,x_1,x_2,x_3) \\to (x_4)\\)\n\\((x_1,x_2,x_3,x_4) \\to (x_5)\\)\n\n\\(f(v,t), v \\in \\{v_1,\\dots,v_{20}\\}, t=1,2,\\dots,519\\)\n\\[{\\bf X}_{t=1} = \\begin{bmatrix}\nf(v_1,t=1) & f(v_1,t=2) & f(v_1,t=3)&  f(v_1,t=4) \\\\\nf(v_2,t=1) & f(v_2,t=2) & f(v_2,t=3)&  f(v_2,t=4) \\\\\n\\dots & \\dots  & \\dots & \\dots  \\\\\nf(v_{20},t=1) & f(v_{20},t=2) & f(v_{20},t=3)&  f(v_{20},t=4)\n\\end{bmatrix}\\]\n\\[{\\bf y}_{t=1} = \\begin{bmatrix}\nf(v_1,t=5) \\\\\nf(v_2,t=5) \\\\\n\\dots  \\\\\nf(v_{20},t=5)\n\\end{bmatrix}\\]\n\nLearn\n\nmodel = RecurrentGCN(node_features=4, filters=32)\n\noptimizer = torch.optim.Adam(model.parameters(), lr=0.01)\n\nmodel.train()\n\nfor epoch in tqdm(range(50)):\n    for time, snapshot in enumerate(train_dataset):\n        y_hat = model(snapshot.x, snapshot.edge_index, snapshot.edge_attr)\n        cost = torch.mean((y_hat-snapshot.y)**2)\n        cost.backward()\n        optimizer.step()\n        optimizer.zero_grad()\n\n100%|██████████| 50/50 [00:52&lt;00:00,  1.05s/it]\n\n\n\nfor time, snapshot in enumerate(train_dataset):\n    _x = snapshot.x \n    _edge_index = snapshot.edge_index \n    _edge_attr = snapshot.edge_attr\n    _y = snapshot.y\n    break\n\n\n_x.shape\n\ntorch.Size([20, 4])\n\n\n\n_edge_index.shape\n\ntorch.Size([2, 102])\n\n\n\n_edge_attr.shape\n\ntorch.Size([102])\n\n\n\n_y.shape\n\ntorch.Size([20])\n\n\n\n_x.shape\n\ntorch.Size([20, 4])\n\n\nx\n\nVertex features are lagged weekly counts of the chickenpox cases (we included 4 lags). y\nThe target is the weekly number of cases for the upcoming week\n\n\n_x\n\ntensor([[-1.0814e-03,  2.8571e-02,  3.5474e-01,  2.9544e-01],\n        [-7.1114e-01, -5.9843e-01,  1.9051e-01,  1.0922e+00],\n        [-3.2281e+00, -2.2910e-01,  1.6103e+00, -1.5487e+00],\n        [ 6.4750e-01, -2.2117e+00, -9.6858e-01,  1.1862e+00],\n        [-1.7302e-01, -9.4717e-01,  1.0347e+00, -6.3751e-01],\n        [ 3.6345e-01, -7.5468e-01,  2.9768e-01, -1.6273e-01],\n        [-3.4174e+00,  1.7031e+00, -1.6434e+00,  1.7434e+00],\n        [-1.9641e+00,  5.5208e-01,  1.1811e+00,  6.7002e-01],\n        [-2.2133e+00,  3.0492e+00, -2.3839e+00,  1.8545e+00],\n        [-3.3141e-01,  9.5218e-01, -3.7281e-01, -8.2971e-02],\n        [-1.8380e+00, -5.8728e-01, -3.5514e-02, -7.2298e-02],\n        [-3.4669e-01, -1.9827e-01,  3.9540e-01, -2.4774e-01],\n        [ 1.4219e+00, -1.3266e+00,  5.2338e-01, -1.6374e-01],\n        [-7.7044e-01,  3.2872e-01, -1.0400e+00,  3.4945e-01],\n        [-7.8061e-01, -6.5022e-01,  1.4361e+00, -1.2864e-01],\n        [-1.0993e+00,  1.2732e-01,  5.3621e-01,  1.9023e-01],\n        [ 2.4583e+00, -1.7811e+00,  5.0732e-02, -9.4371e-01],\n        [ 1.0945e+00, -1.5922e+00,  1.3818e-01,  1.1855e+00],\n        [-7.0875e-01, -2.2460e-01, -7.0875e-01,  1.5630e+00],\n        [-1.8228e+00,  7.8633e-01, -5.6172e-01,  1.2647e+00]])\n\n\n\n_y\n\ntensor([ 0.7106, -0.0725,  2.6099,  1.7870,  0.8024, -0.2614, -0.8370,  1.9674,\n        -0.4212,  0.1655,  1.2519,  2.3743,  0.7877,  0.4531, -0.1721, -0.0614,\n         1.0452,  0.3203, -1.3791,  0.0036])"
  },
  {
    "objectID": "posts/2_research/2022-12-21-ST-GCN_Dataset.html#pedalmedatasetloader",
    "href": "posts/2_research/2022-12-21-ST-GCN_Dataset.html#pedalmedatasetloader",
    "title": "PyTorch ST-GCN Dataset",
    "section": "PedalMeDatasetLoader",
    "text": "PedalMeDatasetLoader\nPedal Me Deliveries\n\nA dataset about the number of weekly bicycle package deliveries by Pedal Me in London during 2020 and 2021. Nodes in the graph represent geographical units and edges are proximity based mutual adjacency relationships.\n\n데이터정리\n\nT = 33\nV = 지역의 집합\nN = 15 # number of nodes\nE = 225 # edges\n\\(f(v,t)\\)의 차원? (1,) # number of deliveries\n시간에 따라서 N이 변하는지? False\n시간에 따라서 E가 변하는지? False\nX: (15,4) (N,4), \\(f(v,t_0),f(v,t_1),f(v,t_2),f(v,t_3)\\)\ny: (15,) (N,), \\(f(v,t_4)\\)\n예제코드적용가능여부: Yes\n\n- Nodes : 15\n\nvertices are localities\n\n-Edges : 225\n\nedges are spatial_connections\n\n- Time : 33\n\nbetween 2020 and 2021\nper weeks\n\n\nfrom torch_geometric_temporal.dataset import  PedalMeDatasetLoader\nfrom torch_geometric_temporal.signal import temporal_signal_split\n\nloader = PedalMeDatasetLoader()\n\ndataset = loader.get_dataset(lags=1)\n\ntrain_dataset, test_dataset = temporal_signal_split(dataset, train_ratio=1)\n\n\ndata=[]\nfor time, snapshot in enumerate(train_dataset):\n    data.append([time,snapshot])\n\n\ntime\n\n33\n\n\n\n(data[0][1]).x.shape,(data[0][1]).edge_index.shape,(data[0][1]).edge_attr.shape\n\n(torch.Size([15, 1]), torch.Size([2, 225]), torch.Size([225]))\n\n\n\nG = nx.Graph()\n\n\nnode_list = torch.tensor(range(15)).tolist()\n\n\nG.add_nodes_from(node_list)\n\n\ndata[-1]\n\n[33, Data(x=[15, 1], edge_index=[2, 225], edge_attr=[225], y=[15])]\n\n\n\nedge_list=[]\nfor i in range(33):\n    for j in range(len(data[0][1].edge_index[0])):\n        edge_list.append([data[i][1].edge_index[0][j].tolist(),data[i][1].edge_index[1][j].tolist()])\n\n\nG.add_edges_from(edge_list)\n\n\nG.number_of_nodes(),G.number_of_edges()\n\n(15, 120)\n\n\n\nnx.draw(G,with_labels=True,font_weight='bold',node_color='green',node_size=350,font_color='white',width=1)\n\n\n\n\n\n\n\n\n\ntime별 같은 edge 정보를 가지고 있나 확인\n\nnp.where(data[0][1].edge_index != data[10][1].edge_index)\n\n(array([], dtype=int64), array([], dtype=int64))\n\n\n\n\nfrom torch_geometric_temporal.dataset import  PedalMeDatasetLoader\nloader = PedalMeDatasetLoader()\n\ndataset = loader.get_dataset(lags=4)\n\ntrain_dataset, test_dataset = temporal_signal_split(dataset, train_ratio=0.5)\n\n\ndata=[]\nfor time, snapshot in enumerate(train_dataset):\n    data.append([time,snapshot])\n\n\n(data[0][1]).x[0], (data[0][1]).y[0]\n\n(tensor([ 3.0574, -0.0477, -0.3076,  0.2437]), tensor(-0.2710))\n\n\n\\(t=0\\)에서 \\(X\\)와 \\(y\\)를 정리하면 아래와 같음.\n\nX:= \\(x_0,x_1,x_2,x_3\\)\ny:= \\(x_4\\)\n\n\n(data[1][1]).x[0], (data[1][1]).y[0]\n\n(tensor([-0.0477, -0.3076,  0.2437, -0.2710]), tensor(0.2490))\n\n\n\nX:= \\(x_1,x_2,x_3,x_4\\)\ny:= \\(x_5\\)\n\n\n(data[2][1]).x[0], (data[2][1]).y[0]\n\n(tensor([-0.3076,  0.2437, -0.2710,  0.2490]), tensor(-0.0357))\n\n\n\nX:=\\(x_2,x_3,x_4,x_5\\)\ny:=\\(x_6\\)\n\n하나의 노드에 길이가 \\(T\\)인 시계열이 맵핑되어 있음. (노드는 총 15개)\n각 노드마다 아래와 같은 과정으로 예측이 됨\n\n\\((x_0,x_1,x_2,x_3) \\to (x_4)\\)\n\\((x_1,x_2,x_3,x_4) \\to (x_5)\\)\n\n\\(f(v,t), v \\in \\{v_1,\\dots,v_{15}\\}, t=1,2,\\dots,33\\)\n\\[{\\bf X}_{t=1} = \\begin{bmatrix}\nf(v_1,t=1) & f(v_1,t=2) & f(v_1,t=3)&  f(v_1,t=4) \\\\\nf(v_2,t=1) & f(v_2,t=2) & f(v_2,t=3)&  f(v_2,t=4) \\\\\n\\dots & \\dots  & \\dots & \\dots  \\\\\nf(v_{20},t=1) & f(v_{20},t=2) & f(v_{20},t=3)&  f(v_{20},t=4)\n\\end{bmatrix}\\]\n\\[{\\bf y}_{t=1} = \\begin{bmatrix}\nf(v_1,t=5) \\\\\nf(v_2,t=5) \\\\\n\\dots  \\\\\nf(v_{20},t=5)\n\\end{bmatrix}\\]\n\nLearn\n\nmodel = RecurrentGCN(node_features=4, filters=32)\n\noptimizer = torch.optim.Adam(model.parameters(), lr=0.01)\n\nmodel.train()\n\nfor epoch in tqdm(range(50)):\n    for time, snapshot in enumerate(train_dataset):\n        y_hat = model(snapshot.x, snapshot.edge_index, snapshot.edge_attr)\n        cost = torch.mean((y_hat-snapshot.y)**2)\n        cost.backward()\n        optimizer.step()\n        optimizer.zero_grad()\n\n100%|██████████| 50/50 [00:03&lt;00:00, 16.04it/s]\n\n\n\nfor time, snapshot in enumerate(train_dataset):\n    _x = snapshot.x \n    _edge_index = snapshot.edge_index \n    _edge_attr = snapshot.edge_attr\n    _y = snapshot.y\n    break\n\n\n_x.shape\n\ntorch.Size([15, 4])\n\n\n\n_edge_index.shape\n\ntorch.Size([2, 225])\n\n\n\n_edge_attr.shape\n\ntorch.Size([225])\n\n\n\n_y.shape\n\ntorch.Size([15])\n\n\n\n_x.shape\n\ntorch.Size([15, 4])\n\n\nx\n\nVertex features are lagged weekly counts of the delivery demands (we included 4 lags).\n주마다 배달 수요의 수가 얼마나 될지 percentage로, t-4시점까지?\n\ny\n\nThe target is the weekly number of deliveries the upcoming week. Our dataset consist of more than 30 snapshots (weeks).\n그 다음주에 배달의 수가 몇 퍼센트로 발생할지?\n\n\n_x[0:3]\n\ntensor([[ 3.0574, -0.0477, -0.3076,  0.2437],\n        [ 3.2126,  0.1240,  0.0764,  0.5582],\n        [ 1.9071, -0.8883,  1.5280, -0.7184]])\n\n\n\n_y\n\ntensor([-0.2710,  0.0888,  0.4733,  0.0907, -0.3129,  0.1184,  0.5886, -0.6571,\n         0.2647,  0.2338,  0.1720,  0.5720, -0.9568, -0.4138, -0.5271])"
  },
  {
    "objectID": "posts/2_research/2022-12-21-ST-GCN_Dataset.html#wikimathsdatasetloader",
    "href": "posts/2_research/2022-12-21-ST-GCN_Dataset.html#wikimathsdatasetloader",
    "title": "PyTorch ST-GCN Dataset",
    "section": "WikiMathsDatasetLoader",
    "text": "WikiMathsDatasetLoader\nWikipedia Math\n\nContains Wikipedia pages about popular mathematics topics and edges describe the links from one page to another. Features describe the number of daily visits between 2019 and 2021 March.\n\n데이터정리\n\nT = 722\nV = 위키피디아 페이지\nN = 1068 # number of nodes\nE = 27079 # edges\n\\(f(v,t)\\)의 차원? (1,) # 해당페이지를 유저가 방문한 횟수\n시간에 따라서 N이 변하는지? False\n시간에 따라서 E가 변하는지? False\nX: (1068,8) (N,8), \\(f(v,t_0),f(v,t_1),f(v,t_2),f(v,t_3),f(v,t_4),f(v,t_5),f(v,t_6),f(v,t_7)\\)\ny: (1068,) (N,), \\(f(v,t_8)\\)\n예제코드적용가능여부: Yes\n\n- Nodes : 1068\n\nvertices are Wikipedia pages\n\n-Edges : 27079\n\nedges are links between them\n\n- Time : 722\n\nWikipedia pages between March 16th 2019 and March 15th 2021\nper weeks\n\n\nfrom torch_geometric_temporal.dataset import  WikiMathsDatasetLoader\nfrom torch_geometric_temporal.signal import temporal_signal_split\n\nloader = WikiMathsDatasetLoader()\n\ndataset = loader.get_dataset()\n\ntrain_dataset, test_dataset = temporal_signal_split(dataset, train_ratio=1)\n\n\ndata=[]\nfor time, snapshot in enumerate(train_dataset):\n    data.append([time,snapshot])\n\n\ntime\n\n722\n\n\n\n(data[0][1]).x.shape,(data[0][1]).edge_index.shape,(data[0][1]).edge_attr.shape\n\n(torch.Size([1068, 8]), torch.Size([2, 27079]), torch.Size([27079]))\n\n\n\n(data[10][1]).x\n\ntensor([[ 0.4972,  0.6838,  0.7211,  ..., -0.8513,  0.1881,  1.3820],\n        [ 0.5457,  0.6016,  0.7071,  ..., -0.4599, -0.6089, -0.0626],\n        [ 0.6305,  1.1404,  0.8779,  ..., -0.5370,  0.7422,  0.3862],\n        ...,\n        [ 0.8699,  0.5451,  1.9254,  ..., -0.8351,  0.3828,  0.3828],\n        [ 0.2451,  0.9629,  1.0526,  ..., -0.9213,  0.8731, -0.1138],\n        [ 0.0200, -0.0871,  0.2342,  ..., -0.4712,  0.0717,  0.2859]])\n\n\n\nG = nx.Graph()\n\n\nnode_list = torch.tensor(range(1068)).tolist()\n\n\nG.add_nodes_from(node_list)\n\n\nedge_list=[]\nfor i in range(722):\n    for j in range(len(data[0][1].edge_index[0])):\n        edge_list.append([data[i][1].edge_index[0][j].tolist(),data[i][1].edge_index[1][j].tolist()])\n\n\nG.add_edges_from(edge_list)\n\n\nG.number_of_nodes(),G.number_of_edges()\n\n(1068, 27079)\n\n\n\nnx.draw(G,node_color='green',node_size=100,width=1)\n\n\n\n\n\n\n\n\n\ntime별 같은 edge 정보를 가지고 있나 확인\n\nnp.where(data[0][1].edge_index != data[10][1].edge_index)\n\n(array([], dtype=int64), array([], dtype=int64))\n\n\n\nnp.where(data[11][1].edge_index != data[10][1].edge_index)\n\n(array([], dtype=int64), array([], dtype=int64))\n\n\n\nnp.where(data[11][1].edge_index != data[20][1].edge_index)\n\n(array([], dtype=int64), array([], dtype=int64))\n\n\n\nhttps://www.kaggle.com/code/mapologo/loading-wikipedia-math-essentials\n\nfrom torch_geometric_temporal.dataset import  WikiMathsDatasetLoader\nloader = WikiMathsDatasetLoader()\n\ndataset = loader.get_dataset(lags=8)\n\ntrain_dataset, test_dataset = temporal_signal_split(dataset, train_ratio=0.5)\n\n\ndata=[]\nfor time, snapshot in enumerate(train_dataset):\n    data.append([time,snapshot])\n\n\n(data[0][1]).x[0], (data[0][1]).y[0]\n\n(tensor([-0.4323, -0.4739,  0.2659,  0.4844,  0.5367,  0.6412,  0.2179, -0.7617]),\n tensor(-0.4067))\n\n\n\\(t=0\\)에서 \\(X\\)와 \\(y\\)를 정리하면 아래와 같음.\n\nX:= \\(x_0,x_1,x_2,x_3,,x_4,x_5,x_6,x_7\\)\ny:= \\(x_9\\)\n\n\n(data[1][1]).x[0],(data[1][1]).y[0]\n\n(tensor([-0.4739,  0.2659,  0.4844,  0.5367,  0.6412,  0.2179, -0.7617, -0.4067]),\n tensor(0.3064))\n\n\n\nX:= \\(x_1,x_2,x_3,x_4,x_5,x_6,x_7,x_8\\)\ny:= \\(x_9\\)\n\n\n(data[2][1]).x[0],(data[2][1]).y[0]\n\n(tensor([ 0.2659,  0.4844,  0.5367,  0.6412,  0.2179, -0.7617, -0.4067,  0.3064]),\n tensor(0.4972))\n\n\n\nX:=\\(x_2,x_3,x_4,x_5,x_6,x_7,x_8,x_9\\)\ny:=\\(x_{10}\\)\n\n하나의 노드에 길이가 \\(T\\)인 시계열이 맵핑되어 있음. (노드는 총 1068개)\n각 노드마다 아래와 같은 과정으로 예측이 됨\n\n\\((x_0,x_1,x_2,x_3,x_4,x_5,x_6,x_7) \\to (x_8)\\)\n\\((x_1,x_2,x_3,x_4,x_5,x_6,x_7,x_8) \\to (x_9)\\)\n\n\\(f(v,t), v \\in \\{v_1,\\dots,v_{1068}\\}, t=1,2,\\dots,722\\)\n\\[{\\bf X}_{t=1} = \\begin{bmatrix}\nf(v_1,t=1) & f(v_1,t=2) & f(v_1,t=3)&  f(v_1,t=4) \\\\\nf(v_2,t=1) & f(v_2,t=2) & f(v_2,t=3)&  f(v_2,t=4) \\\\\n\\dots & \\dots  & \\dots & \\dots  \\\\\nf(v_{20},t=1) & f(v_{20},t=2) & f(v_{20},t=3)&  f(v_{20},t=4)\n\\end{bmatrix}\\]\n\\[{\\bf y}_{t=1} = \\begin{bmatrix}\nf(v_1,t=5) \\\\\nf(v_2,t=5) \\\\\n\\dots  \\\\\nf(v_{20},t=5)\n\\end{bmatrix}\\]\n\nLearn\n\nmodel = RecurrentGCN(node_features=8, filters=32)\n\noptimizer = torch.optim.Adam(model.parameters(), lr=0.01)\n\nmodel.train()\n\nfor epoch in tqdm(range(50)):\n    for time, snapshot in enumerate(train_dataset):\n        y_hat = model(snapshot.x, snapshot.edge_index, snapshot.edge_attr)\n        cost = torch.mean((y_hat-snapshot.y)**2)\n        cost.backward()\n        optimizer.step()\n        optimizer.zero_grad()\n\n100%|██████████| 50/50 [09:28&lt;00:00, 11.37s/it]\n\n\n\nfor time, snapshot in enumerate(train_dataset):\n    _x = snapshot.x \n    _edge_index = snapshot.edge_index \n    _edge_attr = snapshot.edge_attr\n    _y = snapshot.y\n    break\n\n\n_x.shape\n\ntorch.Size([1068, 8])\n\n\n\n_edge_index.shape\n\ntorch.Size([2, 27079])\n\n\n어떤 페이지에 refer가 되었는지\n\n_edge_index[0][:5],_edge_index[1][:5]\n\n(tensor([0, 0, 0, 0, 0]), tensor([1, 2, 3, 4, 5]))\n\n\n\n_edge_attr.shape\n\ntorch.Size([27079])\n\n\n\n_edge_attr[:5]\n\ntensor([1., 4., 2., 2., 5.])\n\n\n\nWeights represent the number of links found at the source Wikipedia page linking to the target Wikipedia page.\n\n가중치는 엣지별 한 페이지에 refer되었는지, 몇 번 되었나 수 나옴\n\n_y.shape\n\ntorch.Size([1068])\n\n\n\n_x.shape\n\ntorch.Size([1068, 8])\n\n\nx\n\nlag 를 몇으로 지정하느냐에 따라 다르게 추출\n\ny\n\nThe target is the daily user visits to the Wikipedia pages between March 16th 2019 and March 15th 2021 which results in 731 periods.\n매일 위키피디아 해당 페이지에 몇 명의 유저가 방문하는지!\n음수가 왜 나오지..\n\n\n_x[0:3]\n\ntensor([[-0.4323, -0.4739,  0.2659,  0.4844,  0.5367,  0.6412,  0.2179, -0.7617],\n        [-0.4041, -0.4165, -0.0751,  0.1484,  0.4153,  0.4464, -0.3916, -0.8137],\n        [-0.3892,  0.0634,  0.5913,  0.5370,  0.4646,  0.2776, -0.0724, -0.8116]])\n\n\n\n_y[:3]\n\ntensor([-0.4067, -0.1620, -0.4043])\n\n\n\ny_hat[:3].data\n\ntensor([[-0.0648],\n        [ 0.0314],\n        [-1.0724]])"
  },
  {
    "objectID": "posts/2_research/2022-12-21-ST-GCN_Dataset.html#windmilloutputlargedatasetloader",
    "href": "posts/2_research/2022-12-21-ST-GCN_Dataset.html#windmilloutputlargedatasetloader",
    "title": "PyTorch ST-GCN Dataset",
    "section": "WindmillOutputLargeDatasetLoader",
    "text": "WindmillOutputLargeDatasetLoader\nHourly energy output of windmills from a European country for more than 2 years. Vertices represent 319 windmills and weighted edges describe the strength of relationships. The target variable allows for regression tasks.\n데이터정리\n\nT = 17470\nV = 풍력발전소\nN = 319 # number of nodes\nE = 101761 = N^2 # edges\n\\(f(v,t)\\)의 차원? (1,) # Hourly energy output\n시간에 따라서 N이 변하는지? False\n시간에 따라서 E가 변하는지? False\nX: (319,4) (N,4), \\(f(v,t_0),f(v,t_1),f(v,t_2),f(v,t_3)\\)\ny: (319,) (N,), \\(f(v,t_4)\\)\n예제코드적용가능여부: Yes\n\n- Nodes : 319\n\nvertices represent 319 windmills\n\n-Edges : 101761\n\nweighted edges describe the strength of relationships.\n\n- Time : 17470\n\nmore than 2 years\n\n\nfrom torch_geometric_temporal.dataset import  WindmillOutputLargeDatasetLoader\nfrom torch_geometric_temporal.signal import temporal_signal_split\n\nloader = WindmillOutputLargeDatasetLoader()\n\ndataset = loader.get_dataset(lags=1)\n\ntrain_dataset, test_dataset = temporal_signal_split(dataset, train_ratio=1)\n\n\ndata=[]\nfor time, snapshot in enumerate(train_dataset):\n    data.append([time,snapshot])\n\n\ntime\n\n17470\n\n\n\n(data[0][1]).x.shape,(data[0][1]).edge_index.shape,(data[0][1]).edge_attr.shape\n\n(torch.Size([319, 1]), torch.Size([2, 101761]), torch.Size([101761]))\n\n\n\nG = nx.Graph()\n\n\nnode_list = torch.tensor(range(319)).tolist()\n\n\nG.add_nodes_from(node_list)\n\n\ndata[-1]\n\n[17470, Data(x=[319, 1], edge_index=[2, 101761], edge_attr=[101761], y=[319])]\n\n\ntime이 너무 많아서 일부만 시각화함!!\n\nedge_list=[]\nfor i in range(1000):\n    for j in range(len(data[0][1].edge_index[0])):\n        edge_list.append([data[i][1].edge_index[0][j].tolist(),data[i][1].edge_index[1][j].tolist()])\n\n\nG.add_edges_from(edge_list)\n\n\nG.number_of_nodes(),G.number_of_edges()\n\n(319, 51040)\n\n\n\nnx.draw(G,with_labels=True,font_weight='bold',node_color='green',node_size=350,font_color='white',width=1)\n\n\n\n\n\n\n\n\n\ntime별 같은 edge 정보를 가지고 있나 확인\n\nnp.where(data[0][1].edge_index != data[10][1].edge_index)\n\n(array([], dtype=int64), array([], dtype=int64))\n\n\n\n\nfrom torch_geometric_temporal.dataset import  WindmillOutputLargeDatasetLoader\nloader = WindmillOutputLargeDatasetLoader()\n\ndataset = loader.get_dataset(lags=4)\n\ntrain_dataset, test_dataset = temporal_signal_split(dataset, train_ratio=0.5)\n\n\ndata=[]\nfor time, snapshot in enumerate(train_dataset):\n    data.append([time,snapshot])\n\n\n(data[0][1]).x[0], (data[0][1]).y[0]\n\n(tensor([-0.5711, -0.7560,  2.6278, -0.8674]), tensor(-0.9877))\n\n\n\\(t=0\\)에서 \\(X\\)와 \\(y\\)를 정리하면 아래와 같음.\n\nX:= \\(x_0,x_1,x_2,x_3\\)\ny:= \\(x_4\\)\n\n\n(data[1][1]).x[0], (data[1][1]).y[0]\n\n(tensor([-0.7560,  2.6278, -0.8674, -0.9877]), tensor(-0.8583))\n\n\n\nX:= \\(x_1,x_2,x_3,x_4\\)\ny:= \\(x_5\\)\n\n\n(data[2][1]).x[0],(data[2][1]).y[0]\n\n(tensor([ 2.6278, -0.8674, -0.9877, -0.8583]), tensor(0.4282))\n\n\n\nX:=\\(x_2,x_3,x_4,x_5\\)\ny:=\\(x_6\\)\n\n하나의 노드에 길이가 \\(T\\)인 시계열이 맵핑되어 있음. (노드는 총 319개)\n각 노드마다 아래와 같은 과정으로 예측이 됨\n\n\\((x_0,x_1,x_2,x_3) \\to (x_4)\\)\n\\((x_1,x_2,x_3,x_4) \\to (x_5)\\)\n\n\\(f(v,t), v \\in \\{v_1,\\dots,v_{319}\\}, t=1,2,\\dots,17470\\)\n\\[{\\bf X}_{t=1} = \\begin{bmatrix}\nf(v_1,t=1) & f(v_1,t=2) & f(v_1,t=3)&  f(v_1,t=4) \\\\\nf(v_2,t=1) & f(v_2,t=2) & f(v_2,t=3)&  f(v_2,t=4) \\\\\n\\dots & \\dots  & \\dots & \\dots  \\\\\nf(v_{20},t=1) & f(v_{20},t=2) & f(v_{20},t=3)&  f(v_{20},t=4)\n\\end{bmatrix}\\]\n\\[{\\bf y}_{t=1} = \\begin{bmatrix}\nf(v_1,t=5) \\\\\nf(v_2,t=5) \\\\\n\\dots  \\\\\nf(v_{20},t=5)\n\\end{bmatrix}\\]\n\nLearn\n\nmodel = RecurrentGCN(node_features=4, filters=32)\n\noptimizer = torch.optim.Adam(model.parameters(), lr=0.01)\n\nmodel.train()\n\nfor epoch in tqdm(range(5)):\n    for time, snapshot in enumerate(train_dataset):\n        y_hat = model(snapshot.x, snapshot.edge_index, snapshot.edge_attr)\n        cost = torch.mean((y_hat-snapshot.y)**2)\n        cost.backward()\n        optimizer.step()\n        optimizer.zero_grad()\n\n100%|██████████| 5/5 [1:06:03&lt;00:00, 792.70s/it]\n\n\n\nfor time, snapshot in enumerate(train_dataset):\n    _x = snapshot.x \n    _edge_index = snapshot.edge_index \n    _edge_attr = snapshot.edge_attr\n    _y = snapshot.y\n    break\n\n\n_x.shape\n\ntorch.Size([319, 4])\n\n\n\n_edge_index.shape\n\ntorch.Size([2, 101761])\n\n\n\n_edge_attr.shape\n\ntorch.Size([101761])\n\n\n\n_y.shape\n\ntorch.Size([319])\n\n\n\n_x.shape\n\ntorch.Size([319, 4])\n\n\nx\n\n\n\ny\n\nThe target variable allows for regression tasks.\n\n\n_x[0:3]\n\ntensor([[-0.5711, -0.7560,  2.6278, -0.8674],\n        [-0.6936, -0.7264,  2.4113, -0.6052],\n        [-0.8666, -0.7785,  2.2759, -0.6759]])\n\n\n\n_y[0]\n\ntensor(-0.9877)"
  },
  {
    "objectID": "posts/2_research/2022-12-21-ST-GCN_Dataset.html#windmilloutputmediumdatasetloader",
    "href": "posts/2_research/2022-12-21-ST-GCN_Dataset.html#windmilloutputmediumdatasetloader",
    "title": "PyTorch ST-GCN Dataset",
    "section": "WindmillOutputMediumDatasetLoader",
    "text": "WindmillOutputMediumDatasetLoader\nHourly energy output of windmills from a European country for more than 2 years. Vertices represent 26 windmills and weighted edges describe the strength of relationships. The target variable allows for regression tasks.\n데이터정리\n\nT = 17470\nV = 풍력발전소\nN = 319 # number of nodes\nE = 101761 = N^2 # edges\n\\(f(v,t)\\)의 차원? (1,) # Hourly energy output\n시간에 따라서 N이 변하는지? False\n시간에 따라서 E가 변하는지? False\nX: (319,4) (N,4), \\(f(v,t_0),f(v,t_1),f(v,t_2),f(v,t_3)\\)\ny: (319,) (N,), \\(f(v,t_4)\\)\n예제코드적용가능여부: Yes\n\n- Nodes : 26\n\nvertices represent 26 windmills\n\n-Edges : 225\n\nweighted edges describe the strength of relationships\n\n- Time : 676\n\nmore than 2 years\n\n\nfrom torch_geometric_temporal.dataset import  WindmillOutputMediumDatasetLoader\nfrom torch_geometric_temporal.signal import temporal_signal_split\n\nloader = WindmillOutputMediumDatasetLoader()\n\ndataset = loader.get_dataset(lags=1)\n\ntrain_dataset, test_dataset = temporal_signal_split(dataset, train_ratio=1)\n\n\ndata=[]\nfor time, snapshot in enumerate(train_dataset):\n    data.append([time,snapshot])\n\n\ntime\n\n17470\n\n\n\n(data[0][1]).x.shape,(data[0][1]).edge_index.shape,(data[0][1]).edge_attr.shape\n\n(torch.Size([26, 1]), torch.Size([2, 676]), torch.Size([676]))\n\n\n\nG = nx.Graph()\n\n\nnode_list = torch.tensor(range(26)).tolist()\n\n\nG.add_nodes_from(node_list)\n\n\ndata[-1]\n\n[17470, Data(x=[26, 1], edge_index=[2, 676], edge_attr=[676], y=[26])]\n\n\n\nedge_list=[]\nfor i in range(17463):\n    for j in range(len(data[0][1].edge_index[0])):\n        edge_list.append([data[i][1].edge_index[0][j].tolist(),data[i][1].edge_index[1][j].tolist()])\n\n\nG.add_edges_from(edge_list)\n\n\nG.number_of_nodes(),G.number_of_edges()\n\n(26, 351)\n\n\n\nnx.draw(G,with_labels=True,font_weight='bold',node_color='green',node_size=350,font_color='white',width=1)\n\n\n\n\n\n\n\n\n\ntime별 같은 edge 정보를 가지고 있나 확인\n\nnp.where(data[0][1].edge_index != data[10][1].edge_index)\n\n(array([], dtype=int64), array([], dtype=int64))\n\n\n\n\nfrom torch_geometric_temporal.dataset import  WindmillOutputMediumDatasetLoader\nloader = WindmillOutputMediumDatasetLoader()\n\ndataset = loader.get_dataset(lags=4)\n\ntrain_dataset, test_dataset = temporal_signal_split(dataset, train_ratio=0.5)\n\n\ndata=[]\nfor time, snapshot in enumerate(train_dataset):\n    data.append([time,snapshot])\n\n\n(data[0][1]).x[0], (data[0][1]).y[0]\n\n(tensor([-0.2170, -0.2055, -0.1587, -0.1930]), tensor(-0.2149))\n\n\n\\(t=0\\)에서 \\(X\\)와 \\(y\\)를 정리하면 아래와 같음.\n\nX:= \\(x_0,x_1,x_2,x_3\\)\ny:= \\(x_4\\)\n\n\n(data[1][1]).x[0],(data[1][1]).y[0]\n\n(tensor([-0.2055, -0.1587, -0.1930, -0.2149]), tensor(-0.2336))\n\n\n\nX:= \\(x_1,x_2,x_3,x_4\\)\ny:= \\(x_5\\)\n\n\n(data[2][1]).x[0],(data[2][1]).y[0]\n\n(tensor([-0.1587, -0.1930, -0.2149, -0.2336]), tensor(-0.1785))\n\n\n\nX:=\\(x_2,x_3,x_4,x_5\\)\ny:=\\(x_6\\)\n\n하나의 노드에 길이가 \\(T\\)인 시계열이 맵핑되어 있음. (노드는 총 26개)\n각 노드마다 아래와 같은 과정으로 예측이 됨\n\n\\((x_0,x_1,x_2,x_3) \\to (x_4)\\)\n\\((x_1,x_2,x_3,x_4) \\to (x_5)\\)\n\n\\(f(v,t), v \\in \\{v_1,\\dots,v_{26}\\}, t=1,2,\\dots,177470\\)\n\\[{\\bf X}_{t=1} = \\begin{bmatrix}\nf(v_1,t=1) & f(v_1,t=2) & f(v_1,t=3)&  f(v_1,t=4) \\\\\nf(v_2,t=1) & f(v_2,t=2) & f(v_2,t=3)&  f(v_2,t=4) \\\\\n\\dots & \\dots  & \\dots & \\dots  \\\\\nf(v_{20},t=1) & f(v_{20},t=2) & f(v_{20},t=3)&  f(v_{20},t=4)\n\\end{bmatrix}\\]\n\\[{\\bf y}_{t=1} = \\begin{bmatrix}\nf(v_1,t=5) \\\\\nf(v_2,t=5) \\\\\n\\dots  \\\\\nf(v_{20},t=5)\n\\end{bmatrix}\\]\n\nLearn\n\nmodel = RecurrentGCN(node_features=4, filters=32)\n\noptimizer = torch.optim.Adam(model.parameters(), lr=0.01)\n\nmodel.train()\n\nfor epoch in tqdm(range(5)):\n    for time, snapshot in enumerate(train_dataset):\n        y_hat = model(snapshot.x, snapshot.edge_index, snapshot.edge_attr)\n        cost = torch.mean((y_hat-snapshot.y)**2)\n        cost.backward()\n        optimizer.step()\n        optimizer.zero_grad()\n\n100%|██████████| 5/5 [03:23&lt;00:00, 40.73s/it]\n\n\n\nfor time, snapshot in enumerate(train_dataset):\n    _x = snapshot.x \n    _edge_index = snapshot.edge_index \n    _edge_attr = snapshot.edge_attr\n    _y = snapshot.y\n    break\n\n\n_x.shape\n\ntorch.Size([26, 4])\n\n\n\n_edge_index.shape\n\ntorch.Size([2, 676])\n\n\n\n_edge_attr.shape\n\ntorch.Size([676])\n\n\n\n_y.shape\n\ntorch.Size([26])\n\n\n\n_x.shape\n\ntorch.Size([26, 4])\n\n\nx\n\n\n\ny\n\nThe target variable allows for regression tasks.\n\n\n_x[0:3]\n\ntensor([[-0.2170, -0.2055, -0.1587, -0.1930],\n        [-0.1682, -0.2708, -0.1051,  1.1786],\n        [ 1.1540, -0.6707, -0.8291, -0.6823]])\n\n\n\n_y[0]\n\ntensor(-0.2149)"
  },
  {
    "objectID": "posts/2_research/2022-12-21-ST-GCN_Dataset.html#windmilloutputsmalldatasetloader",
    "href": "posts/2_research/2022-12-21-ST-GCN_Dataset.html#windmilloutputsmalldatasetloader",
    "title": "PyTorch ST-GCN Dataset",
    "section": "WindmillOutputSmallDatasetLoader",
    "text": "WindmillOutputSmallDatasetLoader\nHourly energy output of windmills from a European country for more than 2 years. Vertices represent 11 windmills and weighted edges describe the strength of relationships. The target variable allows for regression tasks.\n데이터정리\n\nT = 17470\nV = 풍력발전소\nN = 11 # number of nodes\nE = 121 = N^2 # edges\n\\(f(v,t)\\)의 차원? (1,) # Hourly energy output\n시간에 따라서 N이 변하는지? False\n시간에 따라서 E가 변하는지? False\nX: (11,4) (N,4), \\(f(v,t_0),f(v,t_1),f(v,t_2),f(v,t_3)\\)\ny: (11,) (N,), \\(f(v,t_4)\\)\n예제코드적용가능여부: Yes\n\n- Nodes : 11\n\nvertices represent 11 windmills\n\n-Edges : 121\n\nweighted edges describe the strength of relationships\n\n- Time : 17470\n\nmore than 2 years\n\n\nfrom torch_geometric_temporal.dataset import WindmillOutputSmallDatasetLoader\nfrom torch_geometric_temporal.signal import temporal_signal_split\n\nloader = WindmillOutputSmallDatasetLoader()\n\ndataset = loader.get_dataset()\n\ntrain_dataset, test_dataset = temporal_signal_split(dataset, train_ratio=1)\n\n\ndata=[]\nfor time, snapshot in enumerate(train_dataset):\n    data.append([time,snapshot])\n\n\ntime\n\n17463\n\n\n\n(data[0][1]).x.shape,(data[0][1]).edge_index.shape,(data[0][1]).edge_attr.shape\n\n(torch.Size([11, 8]), torch.Size([2, 121]), torch.Size([121]))\n\n\n\ndata[-1]\n\n[17463, Data(x=[11, 8], edge_index=[2, 121], edge_attr=[121], y=[11])]\n\n\n\nG = nx.Graph()\n\n\nnode_list = torch.tensor(range(11)).tolist()\n\n\nG.add_nodes_from(node_list)\n\n\nedge_list=[]\nfor i in range(17463):\n    for j in range(len(data[0][1].edge_index[0])):\n        edge_list.append([data[i][1].edge_index[0][j].tolist(),data[i][1].edge_index[1][j].tolist()])\n\n\nG.add_edges_from(edge_list)\n\n\nG.number_of_nodes(),G.number_of_edges()\n\n(11, 66)\n\n\n\nnx.draw(G,with_labels=True,font_weight='bold',node_color='green',node_size=350,font_color='white',width=1)\n\n\n\n\n\n\n\n\n\ntime별 같은 edge 정보를 가지고 있나 확인\n\nnp.where(data[0][1].edge_index != data[10][1].edge_index)\n\n(array([], dtype=int64), array([], dtype=int64))\n\n\n\n\nfrom torch_geometric_temporal.dataset import WindmillOutputSmallDatasetLoader\nloader = WindmillOutputSmallDatasetLoader()\n\ndataset = loader.get_dataset(lags=4)\n\ntrain_dataset, test_dataset = temporal_signal_split(dataset, train_ratio=0.5)\n\n\ndata=[]\nfor time, snapshot in enumerate(train_dataset):\n    data.append([time,snapshot])\n\n\n(data[0][1]).x[0], (data[0][1]).y[0]\n\n(tensor([ 0.8199, -0.4972,  0.4923, -0.8299]), tensor(-0.6885))\n\n\n\\(t=0\\)에서 \\(X\\)와 \\(y\\)를 정리하면 아래와 같음.\n\nX:= \\(x_0,x_1,x_2,x_3\\)\ny:= \\(x_4\\)\n\n\n(data[1][1]).x[0],(data[1][1]).y[0]\n\n(tensor([-0.4972,  0.4923, -0.8299, -0.6885]), tensor(0.7092))\n\n\n\nX:= \\(x_1,x_2,x_3,x_4\\)\ny:= \\(x_5\\)\n\n\n(data[2][1]).x[0],(data[2][1]).y[0]\n\n(tensor([ 0.4923, -0.8299, -0.6885,  0.7092]), tensor(-0.9356))\n\n\n\nX:=\\(x_2,x_3,x_4,x_5\\)\ny:=\\(x_6\\)\n\n하나의 노드에 길이가 \\(T\\)인 시계열이 맵핑되어 있음. (노드는 총 11개)\n각 노드마다 아래와 같은 과정으로 예측이 됨\n\n\\((x_0,x_1,x_2,x_3) \\to (x_4)\\)\n\\((x_1,x_2,x_3,x_4) \\to (x_5)\\)\n\n\\(f(v,t), v \\in \\{v_1,\\dots,v_{11}\\}, t=1,2,\\dots,17463\\)\n\\[{\\bf X}_{t=1} = \\begin{bmatrix}\nf(v_1,t=1) & f(v_1,t=2) & f(v_1,t=3)&  f(v_1,t=4) \\\\\nf(v_2,t=1) & f(v_2,t=2) & f(v_2,t=3)&  f(v_2,t=4) \\\\\n\\dots & \\dots  & \\dots & \\dots  \\\\\nf(v_{20},t=1) & f(v_{20},t=2) & f(v_{20},t=3)&  f(v_{20},t=4)\n\\end{bmatrix}\\]\n\\[{\\bf y}_{t=1} = \\begin{bmatrix}\nf(v_1,t=5) \\\\\nf(v_2,t=5) \\\\\n\\dots  \\\\\nf(v_{20},t=5)\n\\end{bmatrix}\\]\n\nLearn\n\nmodel = RecurrentGCN(node_features=4, filters=32)\n\noptimizer = torch.optim.Adam(model.parameters(), lr=0.01)\n\nmodel.train()\n\nfor epoch in tqdm(range(5)):\n    for time, snapshot in enumerate(train_dataset):\n        y_hat = model(snapshot.x, snapshot.edge_index, snapshot.edge_attr)\n        cost = torch.mean((y_hat-snapshot.y)**2)\n        cost.backward()\n        optimizer.step()\n        optimizer.zero_grad()\n\n100%|██████████| 5/5 [02:55&lt;00:00, 35.01s/it]\n\n\n\nfor time, snapshot in enumerate(train_dataset):\n    _x = snapshot.x \n    _edge_index = snapshot.edge_index \n    _edge_attr = snapshot.edge_attr\n    _y = snapshot.y\n    break\n\n\n_x.shape\n\ntorch.Size([11, 4])\n\n\n\n_edge_index.shape\n\ntorch.Size([2, 121])\n\n\n\n_edge_attr.shape\n\ntorch.Size([121])\n\n\n\n_y.shape\n\ntorch.Size([11])\n\n\n\n_x.shape\n\ntorch.Size([11, 4])\n\n\nx\n\n\n\ny\n\nThe target variable allows for regression tasks.\n\n\n_x[0:3]\n\ntensor([[ 0.8199, -0.4972,  0.4923, -0.8299],\n        [ 1.1377, -0.3742,  0.3668, -0.8333],\n        [ 0.9979, -0.5643,  0.4070, -0.8918]])\n\n\n\n_y\n\ntensor([-0.6885, -0.6594, -0.6303, -0.6983, -0.5416, -0.6186, -0.6031, -0.7580,\n        -0.6659, -0.5948, -0.5088])"
  },
  {
    "objectID": "posts/2_research/2022-12-21-ST-GCN_Dataset.html#metrladatasetloader_real-world-traffic-dataset",
    "href": "posts/2_research/2022-12-21-ST-GCN_Dataset.html#metrladatasetloader_real-world-traffic-dataset",
    "title": "PyTorch ST-GCN Dataset",
    "section": "METRLADatasetLoader_real world traffic dataset",
    "text": "METRLADatasetLoader_real world traffic dataset\nA traffic forecasting dataset based on Los Angeles Metropolitan traffic conditions. The dataset contains traffic readings collected from 207 loop detectors on highways in Los Angeles County in aggregated 5 minute intervals for 4 months between March 2012 to June 2012.\n데이터정리\n\nT = 33\nV = 구역\nN = 207 # number of nodes\nE = 225\n\\(f(v,t)\\)의 차원? (3,) # Hourly energy output\n시간에 따라서 N이 변하는지? False\n시간에 따라서 E가 변하는지? False\nX: (207,4) (N,2,12), \\(x_0,x_1,x_2,x_3,x_4,x_5,x_6,x_7,x_8,x_9,x_{10},x_{11},z_0,z_1,z_2,z_3,z_4,z_5,z_6,z_7,z_8,z_9,z_{10},z_{11}\\)\ny: (207,) (N,), \\((x_{12})\\)\n예제코드적용가능여부: No\n\nhttps://arxiv.org/pdf/1707.01926.pdf\n- Nodes : 207\n\nvertices are localities\n\n-Edges : 225\n\nedges are spatial_connections\n\n- Time : 33\n\nbetween 2020 and 2021\nper weeks\n\n\nfrom torch_geometric_temporal.dataset import METRLADatasetLoader\nfrom torch_geometric_temporal.signal import temporal_signal_split\n\nloader = METRLADatasetLoader()\n\ndataset = loader.get_dataset()\n\ntrain_dataset, test_dataset = temporal_signal_split(dataset, train_ratio=1)\n\n\ndata=[]\nfor time, snapshot in enumerate(train_dataset):\n    data.append([time,snapshot])\n\n\ntime\n\n34248\n\n\n\n(data[0][1]).x.shape,(data[0][1]).edge_index.shape,(data[0][1]).edge_attr.shape\n\n(torch.Size([207, 2, 12]), torch.Size([2, 1722]), torch.Size([1722]))\n\n\n\ndata[-1]\n\n[34248,\n Data(x=[207, 2, 12], edge_index=[2, 1722], edge_attr=[1722], y=[207, 12])]\n\n\n\nG = nx.Graph()\n\n\nnode_list = torch.tensor(range(20)).tolist()\n\n\nG.add_nodes_from(node_list)\n\n\nedge_list=[]\nfor i in range(1000):\n    for j in range(len(data[0][1].edge_index[0])):\n        edge_list.append([data[i][1].edge_index[0][j].tolist(),data[i][1].edge_index[1][j].tolist()])\n\n\nG.add_edges_from(edge_list)\n\n\nG.number_of_nodes(),G.number_of_edges()\n\n(207, 1520)\n\n\n\nnx.draw(G,with_labels=True,font_weight='bold',node_color='green',node_size=350,font_color='white',width=1)\n\n\n\n\n\n\n\n\n\ntime별 같은 edge 정보를 가지고 있나 확인\n\nnp.where(data[0][1].edge_index != data[10][1].edge_index)\n\n(array([], dtype=int64), array([], dtype=int64))\n\n\n\n논문 내용 중\n\n\n\nimage.png\n\n\n\nfrom torch_geometric_temporal.dataset import METRLADatasetLoader\nloader = METRLADatasetLoader()\n\ndataset = loader.get_dataset()\n\ntrain_dataset, test_dataset = temporal_signal_split(dataset, train_ratio=0.5)\n\n\n\n\n\n\n\nNote\n\n\n\nlags option 없어서 error 뜸 : get_dataset() got an unexpected keyword argument ‘lags’\n\n\n\ndata=[]\nfor time, snapshot in enumerate(train_dataset):\n    data.append([time,snapshot])\n\n\n(data[0][1]).x[0], (data[0][1]).y[0]\n\n(tensor([[ 0.5332,  0.4486,  0.5146, -2.6522, -2.6522,  0.1847,  0.6383,  0.4961,\n           0.7497,  0.4899,  0.5751,  0.4280],\n         [-1.7292, -1.7171, -1.7051, -1.6930, -1.6810, -1.6689, -1.6569, -1.6448,\n          -1.6328, -1.6207, -1.6087, -1.5966]]),\n tensor([0.3724, 0.2452, 0.4961, 0.6521, 0.1126, 0.5311, 0.5091, 0.4713, 0.4218,\n         0.3909, 0.4761, 0.5641]))\n\n\n\\(t=0\\)에서 \\(X,Z\\)와 \\(y\\)를 정리하면 아래와 같음.\n\nX:= \\(x_0,x_1,x_2,x_3,x_4,x_5,x_6,x_7,x_8,x_9,x_{10},x_{11}\\)\nZ:= \\(z_0,z_1,z_2,z_3,z_4,z_5,z_6,z_7,z_8,z_9,z_{10},z_{11}\\)\ny:= \\(x_{12}\\)\n\n\n(data[1][1]).x[0],(data[1][1]).y[0]\n\n(tensor([[ 0.4486,  0.5146, -2.6522, -2.6522,  0.1847,  0.6383,  0.4961,  0.7497,\n           0.4899,  0.5751,  0.4280,  0.3724],\n         [-1.7171, -1.7051, -1.6930, -1.6810, -1.6689, -1.6569, -1.6448, -1.6328,\n          -1.6207, -1.6087, -1.5966, -1.5846]]),\n tensor([ 0.2452,  0.4961,  0.6521,  0.1126,  0.5311,  0.5091,  0.4713,  0.4218,\n          0.3909,  0.4761,  0.5641, -0.0022]))\n\n\n\nX:= \\(x_1,x_2,x_3,x_4,x_5,x_6,x_7,x_8,x_9,x_{10},x_{11},x_{12}\\)\nZ:= \\(z_1,z_2,z_3,z_4,z_5,z_6,z_7,z_8,z_9,z_{10},z_{11},z_{12}\\)\ny:= \\(x_{13}\\)\n\n\n(data[2][1]).x[0],(data[2][1]).y[0]\n\n(tensor([[ 0.5146, -2.6522, -2.6522,  0.1847,  0.6383,  0.4961,  0.7497,  0.4899,\n           0.5751,  0.4280,  0.3724,  0.2452],\n         [-1.7051, -1.6930, -1.6810, -1.6689, -1.6569, -1.6448, -1.6328, -1.6207,\n          -1.6087, -1.5966, -1.5846, -1.5725]]),\n tensor([ 0.4961,  0.6521,  0.1126,  0.5311,  0.5091,  0.4713,  0.4218,  0.3909,\n          0.4761,  0.5641, -0.0022,  0.4218]))\n\n\n\nX:= \\(x_2,x_3,x_4,x_5,x_6,x_7,x_8,x_9,x_{10},x_{11},x_{12},x_{13}\\)\nZ:= \\(z_2,z_3,z_4,z_5,z_6,z_7,z_8,z_9,z_{10},z_{11},z_{12},z_{13}\\)\ny:= \\(x_{14}\\)\n\n하나의 노드에 길이가 \\(T\\)인 시계열이 맵핑되어 있음. (노드는 총 207개)\n각 노드마다 아래와 같은 과정으로 예측이 됨\n\n\\(x_0,x_1,x_2,x_3,x_4,x_5,x_6,x_7,x_8,x_9,x_{10},x_{11},z_0,z_1,z_2,z_3,z_4,z_5,z_6,z_7,z_8,z_9,z_{10},z_{11} \\to (x_{12})\\)\n\\(x_1,x_2,x_3,x_4,x_5,x_6,x_7,x_8,x_9,x_{10},x_{11},x_{12},z_1,z_2,z_3,z_4,z_5,z_6,z_7,z_8,z_9,z_{10},z_{11},z_{12} \\to (x_{13})\\)\n\n\\(f(v,t), v \\in \\{v_1,\\dots,v_{207}\\}, t=1,2,\\dots,34248\\)\n\\[{\\bf X}_{t=1} = \\begin{bmatrix}\nf(v_1,t=1) & f(v_1,t=2) & f(v_1,t=3)&  f(v_1,t=4) \\\\\nf(v_2,t=1) & f(v_2,t=2) & f(v_2,t=3)&  f(v_2,t=4) \\\\\n\\dots & \\dots  & \\dots & \\dots  \\\\\nf(v_{20},t=1) & f(v_{20},t=2) & f(v_{20},t=3)&  f(v_{20},t=4)\n\\end{bmatrix}\\]\n\\[{\\bf y}_{t=1} = \\begin{bmatrix}\nf(v_1,t=5) \\\\\nf(v_2,t=5) \\\\\n\\dots  \\\\\nf(v_{20},t=5)\n\\end{bmatrix}\\]\n\nLearn\n\nmodel = RecurrentGCN(node_features=1, filters=32)\n\noptimizer = torch.optim.Adam(model.parameters(), lr=0.01)\n\nmodel.train()\n\nfor epoch in tqdm(range(50)):\n    for time, snapshot in enumerate(train_dataset):\n        y_hat = model(snapshot.x, snapshot.edge_index, snapshot.edge_attr)\n        cost = torch.mean((y_hat-snapshot.y)**2)\n        cost.backward()\n        optimizer.step()\n        optimizer.zero_grad()\n\n\nfor time, snapshot in enumerate(train_dataset):\n    _x = snapshot.x \n    _edge_index = snapshot.edge_index \n    _edge_attr = snapshot.edge_attr\n    _y = snapshot.y\n    break\n\n\n_x.shape\n\ntorch.Size([207, 2, 12])\n\n\n\nnode 207개, traffic sensor 2개\n\n\n_edge_index.shape\n\ntorch.Size([2, 1722])\n\n\n\n_edge_attr.shape\n\ntorch.Size([1722])\n\n\n\n_y.shape\n\ntorch.Size([207, 12])\n\n\n\n_x.shape\n\ntorch.Size([207, 2, 12])\n\n\ny\n\ntraffic speed\n\n\n_x[0]\n\ntensor([[ 0.5332,  0.4486,  0.5146, -2.6522, -2.6522,  0.1847,  0.6383,  0.4961,\n          0.7497,  0.4899,  0.5751,  0.4280],\n        [-1.7292, -1.7171, -1.7051, -1.6930, -1.6810, -1.6689, -1.6569, -1.6448,\n         -1.6328, -1.6207, -1.6087, -1.5966]])\n\n\n\n_y[0]\n\ntensor([0.3724, 0.2452, 0.4961, 0.6521, 0.1126, 0.5311, 0.5091, 0.4713, 0.4218,\n        0.3909, 0.4761, 0.5641])"
  },
  {
    "objectID": "posts/2_research/2022-12-21-ST-GCN_Dataset.html#pemsbaydatasetloader",
    "href": "posts/2_research/2022-12-21-ST-GCN_Dataset.html#pemsbaydatasetloader",
    "title": "PyTorch ST-GCN Dataset",
    "section": "PemsBayDatasetLoader",
    "text": "PemsBayDatasetLoader\nhttps://onlinelibrary.wiley.com/doi/pdf/10.1111/tgis.12644\nA traffic forecasting dataset as described in Diffusion Convolution Layer Paper.\nThis traffic dataset is collected by California Transportation Agencies (CalTrans) Performance Measurement System (PeMS). It is represented by a network of 325 traffic sensors in the Bay Area with 6 months of traffic readings ranging from Jan 1st 2017 to May 31th 2017 in 5 minute intervals.\n데이터정리\n\nT = 17470\nV = 풍력발전소\nN = 325 # number of nodes\nE = 101761 = N^2 # edges\n\\(f(v,t)\\)의 차원? (1,) # Hourly energy output\n시간에 따라서 N이 변하는지? False\n시간에 따라서 E가 변하는지? False\nX: (325,2,12) (N,2,12),\n\n\\(x_0,x_1,x_2,x_3,x_4,x_5,x_6,x_7,x_8,x_9,x_{10},x_{11}\\)\n\\(z_0,z_1,z_2,z_3,z_4,z_5,z_6,z_7,z_8,z_9,z_{10},z_{11}\\)\n\ny: (325,) (N,2,12),\n\n\\(x_{13},x_{14},x_{15},x_{16},x_{17},x_{18},x_{19},x_{20},x_{21},x_{22},x_{23},x_{24}\\)\n\\(z_{13},z_{14},z_{15},z_{16},z_{17},z_{18},z_{19},z_{20},z_{21},z_{22},z_{23},z_{24}\\)\n\n예제코드적용가능여부: No\n\n- Nodes : 325\n\nvertices are sensors\n\n-Edges : 2694\n\nweighted edges are between seonsor paris measured by the road nretwork distance\n\n- Time : 52081\n\n6 months of traffic readings ranging from Jan 1st 2017 to May 31th 2017 in 5 minute intervals\n\n\nfrom torch_geometric_temporal.dataset import PemsBayDatasetLoader\nfrom torch_geometric_temporal.signal import temporal_signal_split\n\nloader = PemsBayDatasetLoader()\n\ndataset = loader.get_dataset()\n\ntrain_dataset, test_dataset = temporal_signal_split(dataset, train_ratio=1)\n\n\ndata=[]\nfor time, snapshot in enumerate(train_dataset):\n    data.append([time,snapshot])\n\n\ntime\n\n52081\n\n\n\n(data[0][1]).x.shape,(data[0][1]).edge_index.shape,(data[0][1]).edge_attr.shape\n\n(torch.Size([325, 2, 12]), torch.Size([2, 2694]), torch.Size([2694]))\n\n\n\nG = nx.Graph()\n\n\nnode_list = torch.tensor(range(325)).tolist()\n\n\ndata[-1]\n\n[52081,\n Data(x=[325, 2, 12], edge_index=[2, 2694], edge_attr=[2694], y=[325, 2, 12])]\n\n\n\nG.add_nodes_from(node_list)\n\n\nedge_list=[]\nfor i in range(1000):\n    for j in range(len(data[0][1].edge_index[0])):\n        edge_list.append([data[i][1].edge_index[0][j].tolist(),data[i][1].edge_index[1][j].tolist()])\n\n\nG.add_edges_from(edge_list)\n\n\nG.number_of_nodes(),G.number_of_edges()\n\n(325, 2404)\n\n\n\nnx.draw(G,node_color='green',node_size=50,font_color='white',width=1)\n\n\n\n\n\n\n\n\n\ntime별 같은 edge 정보를 가지고 있나 확인\n\nnp.where(data[0][1].edge_index != data[10][1].edge_index)\n\n(array([], dtype=int64), array([], dtype=int64))\n\n\n\n\nfrom torch_geometric_temporal.dataset import PemsBayDatasetLoader\nloader = PemsBayDatasetLoader()\n\ndataset = loader.get_dataset()\n\ntrain_dataset, test_dataset = temporal_signal_split(dataset, train_ratio=0.5)\n\n\ndata=[]\nfor time, snapshot in enumerate(train_dataset):\n    data.append([time,snapshot])\n\n\n(data[0][1]).x[0], (data[0][1]).y[0]\n\n(tensor([[ 0.9821,  0.9928,  1.0251,  1.0574,  1.0466,  1.0681,  0.9821,  1.0251,\n           1.0143,  0.9928,  0.9498,  0.9821],\n         [-1.6127, -1.6005, -1.5883, -1.5762, -1.5640, -1.5518, -1.5397, -1.5275,\n          -1.5153, -1.5032, -1.4910, -1.4788]]),\n tensor([[ 1.0143,  0.9821,  0.9821,  1.0036,  1.0143,  0.9605,  0.9498,  1.0251,\n           0.9928,  0.9928,  0.9498,  0.9928],\n         [-1.4667, -1.4545, -1.4423, -1.4302, -1.4180, -1.4058, -1.3937, -1.3815,\n          -1.3694, -1.3572, -1.3450, -1.3329]]))\n\n\n\\(t=0\\)에서 \\(X,Z\\)와 \\(y,s\\)를 정리하면 아래와 같음.\n\nX:= \\(x_0,x_1,x_2,x_3,x_4,x_5,x_6,x_7,x_8,x_9,x_{10},x_{11}\\)\nZ:= \\(z_0,z_1,z_2,z_3,z_4,z_5,z_6,z_7,z_8,z_9,z_{10},z_{11}\\)\ny:= \\(x_{12},x_{13},x_{14},x_{15},x_{16},x_{17},x_{18},x_{19},x_{20},x_{21},x_{22},x_{23}\\)\ns:= \\(z_{12},z_{13},z_{14},z_{15},z_{16},z_{17},z_{18},z_{19},z_{20},z_{21},z_{22},z_{23}\\)\n\n\n(data[1][1]).x[0],(data[1][1]).y[0]\n\n(tensor([[ 0.9928,  1.0251,  1.0574,  1.0466,  1.0681,  0.9821,  1.0251,  1.0143,\n           0.9928,  0.9498,  0.9821,  1.0143],\n         [-1.6005, -1.5883, -1.5762, -1.5640, -1.5518, -1.5397, -1.5275, -1.5153,\n          -1.5032, -1.4910, -1.4788, -1.4667]]),\n tensor([[ 0.9821,  0.9821,  1.0036,  1.0143,  0.9605,  0.9498,  1.0251,  0.9928,\n           0.9928,  0.9498,  0.9928,  0.9821],\n         [-1.4545, -1.4423, -1.4302, -1.4180, -1.4058, -1.3937, -1.3815, -1.3694,\n          -1.3572, -1.3450, -1.3329, -1.3207]]))\n\n\n\nX:= \\(x_1,x_2,x_3,x_4,x_5,x_6,x_7,x_8,x_9,x_{10},x_{11},x_{12}\\)\nZ:= \\(z_1,z_2,z_3,z_4,z_5,z_6,z_7,z_8,z_9,z_{10},z_{11},z_{12}\\)\ny:= \\(x_{13},x_{14},x_{15},x_{16},x_{17},x_{18},x_{19},x_{20},x_{21},x_{22},x_{23},x_{24}\\)\ns:= \\(z_{13},z_{14},z_{15},z_{16},z_{17},z_{18},z_{19},z_{20},z_{21},z_{22},z_{23},z_{24}\\)\n\n\n(data[2][1]).x[0],(data[2][1]).y[0]\n\n(tensor([[ 1.0251,  1.0574,  1.0466,  1.0681,  0.9821,  1.0251,  1.0143,  0.9928,\n           0.9498,  0.9821,  1.0143,  0.9821],\n         [-1.5883, -1.5762, -1.5640, -1.5518, -1.5397, -1.5275, -1.5153, -1.5032,\n          -1.4910, -1.4788, -1.4667, -1.4545]]),\n tensor([[ 0.9821,  1.0036,  1.0143,  0.9605,  0.9498,  1.0251,  0.9928,  0.9928,\n           0.9498,  0.9928,  0.9821,  1.0143],\n         [-1.4423, -1.4302, -1.4180, -1.4058, -1.3937, -1.3815, -1.3694, -1.3572,\n          -1.3450, -1.3329, -1.3207, -1.3085]]))\n\n\n\nX:= \\(x_2,x_3,x_4,x_5,x_6,x_7,x_8,x_9,x_{10},x_{11},x_{12},x_{13}\\)\nZ:= \\(z_2,z_3,z_4,z_5,z_6,z_7,z_8,z_9,z_{10},z_{11},z_{12},z_{13}\\)\ny:= \\(x_{14},x_{15},x_{16},x_{17},x_{18},x_{19},x_{20},x_{21},x_{22},x_{23},x_{24},x_{25}\\)\ns:= \\(z_{14},z_{15},z_{16},z_{17},z_{18},z_{19},z_{20},z_{21},z_{22},z_{23},z_{24},z_{25}\\)\n\n하나의 노드에 길이가 \\(T\\)인 시계열이 맵핑되어 있음. (노드는 총 325개)\n각 노드마다 아래와 같은 과정으로 예측이 됨\n\n\\(x_0,x_1,x_2,x_3,x_4,x_5,x_6,x_7,x_8,x_9,x_{10},x_{11} \\to x_{12},x_{13},x_{14},x_{15},x_{16},x_{17},x_{18},x_{19},x_{20},x_{21},x_{22},x_{23}\\)\n\\(z_0,z_1,z_2,z_3,z_4,z_5,z_6,z_7,z_8,z_9,z_{10},z_{11} \\to z_{12},z_{13},z_{14},z_{15},z_{16},z_{17},z_{18},z_{19},z_{20},z_{21},z_{22},z_{23}\\)\n\\(x_1,x_2,x_3,x_4,x_5,x_6,x_7,x_8,x_9,x_{10},x_{11},x_{12} \\to x_{13},x_{14},x_{15},x_{16},x_{17},x_{18},x_{19},x_{20},x_{21},x_{22},x_{23},x_{24}\\)\n\\(z_1,z_2,z_3,z_4,z_5,z_6,z_7,z_8,z_9,z_{10},z_{11},z_{12} \\to z_{13},z_{14},z_{15},z_{16},z_{17},z_{18},z_{19},z_{20},z_{21},z_{22},z_{23},z_{24}\\)\n\n\\(f(v,t), v \\in \\{v_1,\\dots,v_{325}\\}, t=1,2,\\dots,52081\\)\n\\[{\\bf X}_{t=1} = \\begin{bmatrix}\nf(v_1,t=1) & f(v_1,t=2) & f(v_1,t=3)&  f(v_1,t=4) \\\\\nf(v_2,t=1) & f(v_2,t=2) & f(v_2,t=3)&  f(v_2,t=4) \\\\\n\\dots & \\dots  & \\dots & \\dots  \\\\\nf(v_{20},t=1) & f(v_{20},t=2) & f(v_{20},t=3)&  f(v_{20},t=4)\n\\end{bmatrix}\\]\n\\[{\\bf y}_{t=1} = \\begin{bmatrix}\nf(v_1,t=5) \\\\\nf(v_2,t=5) \\\\\n\\dots  \\\\\nf(v_{20},t=5)\n\\end{bmatrix}\\]\n\nLearn\n\nmodel = RecurrentGCN(node_features=4, filters=32)\n\noptimizer = torch.optim.Adam(model.parameters(), lr=0.01)\n\nmodel.train()\n\nfor epoch in tqdm(range(50)):\n    for time, snapshot in enumerate(train_dataset):\n        y_hat = model(snapshot.x, snapshot.edge_index, snapshot.edge_attr)\n        cost = torch.mean((y_hat-snapshot.y)**2)\n        cost.backward()\n        optimizer.step()\n        optimizer.zero_grad()\n\n\nfor time, snapshot in enumerate(train_dataset):\n    _x = snapshot.x \n    _edge_index = snapshot.edge_index \n    _edge_attr = snapshot.edge_attr\n    _y = snapshot.y\n    break\n\n\n_x.shape\n\ntorch.Size([325, 2, 12])\n\n\n\n_edge_index.shape\n\ntorch.Size([2, 2694])\n\n\n\n_edge_attr.shape\n\ntorch.Size([2694])\n\n\n\n_y.shape\n\ntorch.Size([325, 2, 12])\n\n\n\n_x.shape\n\ntorch.Size([325, 2, 12])\n\n\nx\n\n.!\n\ny\n\ncapturing temporal dependencies..?\n\nedges connect sensors\nFor instance, the traffic conditions on one road on Wednesday at 3:00 p.m. are similar to the traffic conditions on Thursday at the same time.\n\n_x[0:3]\n\ntensor([[[ 0.9821,  0.9928,  1.0251,  1.0574,  1.0466,  1.0681,  0.9821,\n           1.0251,  1.0143,  0.9928,  0.9498,  0.9821],\n         [-1.6127, -1.6005, -1.5883, -1.5762, -1.5640, -1.5518, -1.5397,\n          -1.5275, -1.5153, -1.5032, -1.4910, -1.4788]],\n\n        [[ 0.6054,  0.5839,  0.6592,  0.6269,  0.6808,  0.6377,  0.6700,\n           0.6054,  0.6162,  0.6162,  0.5839,  0.5947],\n         [-1.6127, -1.6005, -1.5883, -1.5762, -1.5640, -1.5518, -1.5397,\n          -1.5275, -1.5153, -1.5032, -1.4910, -1.4788]],\n\n        [[ 0.9390,  0.9175,  0.8960,  0.9175,  0.9067,  0.9175,  0.9175,\n           0.8852,  0.9283,  0.8960,  0.9067,  0.8960],\n         [-1.6127, -1.6005, -1.5883, -1.5762, -1.5640, -1.5518, -1.5397,\n          -1.5275, -1.5153, -1.5032, -1.4910, -1.4788]]])\n\n\n\n_y[0]\n\ntensor([[ 1.0143,  0.9821,  0.9821,  1.0036,  1.0143,  0.9605,  0.9498,  1.0251,\n          0.9928,  0.9928,  0.9498,  0.9928],\n        [-1.4667, -1.4545, -1.4423, -1.4302, -1.4180, -1.4058, -1.3937, -1.3815,\n         -1.3694, -1.3572, -1.3450, -1.3329]])"
  },
  {
    "objectID": "posts/2_research/2022-12-21-ST-GCN_Dataset.html#englandcoviddatasetloader",
    "href": "posts/2_research/2022-12-21-ST-GCN_Dataset.html#englandcoviddatasetloader",
    "title": "PyTorch ST-GCN Dataset",
    "section": "EnglandCovidDatasetLoader",
    "text": "EnglandCovidDatasetLoader\nCovid19 England\n\nA dataset about mass mobility between regions in England and the number of confirmed COVID-19 cases from March to May 2020 [38]. Each day contains a different mobility graph and node features corresponding to the number of cases in the previous days. Mobility stems from Facebook Data For Good 1 and cases from gov.uk 2\n\nhttps://arxiv.org/pdf/2009.08388.pdf\n데이터정리\n\nT = 52\nV = 지역\nN = 129 # number of nodes\nE = 2158\n\\(f(v,t)\\)의 차원? (1,) # 코로나확진자수\n시간에 따라서 Number of nodes가 변하는지? False\n시간에 따라서 Number of nodes가 변하는지? False\nX: (20,4) (N,4), \\(f(v,t_0),f(v,t_1),f(v,t_2),f(v,t_3)\\)\ny: (20,) (N,), \\(f(v,t_4)\\)\n예제코드적용가능여부: Yes\n\n- Nodes : 129\n\nvertices are correspond to the number of COVID-19 cases in the region in the past window days.\n\n-Edges : 2158\n\nthe spatial edges capture county-to-county movement at a specific date, and a county is connected to a number of past instances of itself with temporal edges.\n\n- Time : 52\n\nfrom 3 March to 12 of May\n\n\nfrom torch_geometric_temporal.dataset import EnglandCovidDatasetLoader\nfrom torch_geometric_temporal.signal import temporal_signal_split\n\nloader = EnglandCovidDatasetLoader()\n\ndataset = loader.get_dataset()\n\ntrain_dataset, test_dataset = temporal_signal_split(dataset, train_ratio=1)\n\n\ndata=[]\nfor time, snapshot in enumerate(train_dataset):\n    data.append([time,snapshot])\n\n\ntime\n\n52\n\n\n\n(data[0][1]).x.shape,(data[0][1]).edge_index.shape,(data[0][1]).edge_attr.shape\n\n(torch.Size([129, 8]), torch.Size([2, 2158]), torch.Size([2158]))\n\n\n\nG = nx.Graph()\n\n\nnode_list = torch.tensor(range(129)).tolist()\n\n\nG.add_nodes_from(node_list)\n\n\ndata[-1]\n\n[52, Data(x=[129, 8], edge_index=[2, 1424], edge_attr=[1424], y=[129])]\n\n\n\nlen(data[0][1].edge_index[0])\n\n2158\n\n\n\nedge_list=[]\nfor i in range(52):\n    for j in range(100):\n        edge_list.append([data[i][1].edge_index[0][j].tolist(),data[i][1].edge_index[1][j].tolist()])\n\n\nG.add_edges_from(edge_list)\n\n\nG.number_of_nodes(),G.number_of_edges()\n\n(129, 1230)\n\n\n\nnx.draw(G,with_labels=True,font_weight='bold',node_color='green',node_size=350,font_color='white',width=1)\n\n\n\n\n\n\n\n\n\ntime별 같은 edge 정보를 가지고 있나 확인\n\nnp.where(data[2][1].edge_index !=data[2][1].edge_index)\n\n(array([], dtype=int64), array([], dtype=int64))\n\n\n\n\nfrom torch_geometric_temporal.dataset import EnglandCovidDatasetLoader\nloader = EnglandCovidDatasetLoader()\n\ndataset = loader.get_dataset(lags=4)\n\ntrain_dataset, test_dataset = temporal_signal_split(dataset, train_ratio=0.5)\n\n\ndata=[]\nfor time, snapshot in enumerate(train_dataset):\n    data.append([time,snapshot])\n\n\n(data[0][1]).x[0], (data[0][1]).y[0]\n\n(tensor([-1.4697, -1.9283, -1.6990, -1.8137]), tensor(-1.8137))\n\n\n\\(t=0\\)에서 \\(X\\)와 \\(y\\)를 정리하면 아래와 같음.\n\nX:= \\(x_0,x_1,x_2,x_3\\)\ny:= \\(x_4\\)\n\n\n(data[1][1]).x[0],(data[1][1]).y[0]\n\n(tensor([-1.9283, -1.6990, -1.8137, -1.8137]), tensor(-0.8965))\n\n\n\nX:= \\(x_1,x_2,x_3,x_4\\)\ny:= \\(x_5\\)\n\n\n(data[2][1]).x[0],(data[2][1]).y[0]\n\n(tensor([-1.6990, -1.8137, -1.8137, -0.8965]), tensor(-1.1258))\n\n\n\nX:=\\(x_2,x_3,x_4,x_5\\)\ny:=\\(x_6\\)\n\n하나의 노드에 길이가 \\(T\\)인 시계열이 맵핑되어 있음. (노드는 총 129개)\n각 노드마다 아래와 같은 과정으로 예측이 됨\n\n\\((x_0,x_1,x_2,x_3) \\to (x_4)\\)\n\\((x_1,x_2,x_3,x_4) \\to (x_5)\\)\n\n\\(f(v,t), v \\in \\{v_1,\\dots,v_{129}\\}, t=1,2,\\dots,52\\)\n\\[{\\bf X}_{t=1} = \\begin{bmatrix}\nf(v_1,t=1) & f(v_1,t=2) & f(v_1,t=3)&  f(v_1,t=4) \\\\\nf(v_2,t=1) & f(v_2,t=2) & f(v_2,t=3)&  f(v_2,t=4) \\\\\n\\dots & \\dots  & \\dots & \\dots  \\\\\nf(v_{20},t=1) & f(v_{20},t=2) & f(v_{20},t=3)&  f(v_{20},t=4)\n\\end{bmatrix}\\]\n\\[{\\bf y}_{t=1} = \\begin{bmatrix}\nf(v_1,t=5) \\\\\nf(v_2,t=5) \\\\\n\\dots  \\\\\nf(v_{20},t=5)\n\\end{bmatrix}\\]\n\nLearn\n\nmodel = RecurrentGCN(node_features=4, filters=32)\n\noptimizer = torch.optim.Adam(model.parameters(), lr=0.01)\n\nmodel.train()\n\nfor epoch in tqdm(range(50)):\n    for time, snapshot in enumerate(train_dataset):\n        y_hat = model(snapshot.x, snapshot.edge_index, snapshot.edge_attr)\n        cost = torch.mean((y_hat-snapshot.y)**2)\n        cost.backward()\n        optimizer.step()\n        optimizer.zero_grad()\n\n100%|██████████| 50/50 [00:07&lt;00:00,  6.30it/s]\n\n\n\nfor time, snapshot in enumerate(train_dataset):\n    _x = snapshot.x \n    _edge_index = snapshot.edge_index \n    _edge_attr = snapshot.edge_attr\n    _y = snapshot.y\n    break\n\n\n_x.shape\n\ntorch.Size([129, 4])\n\n\n\n_edge_index.shape\n\ntorch.Size([2, 2158])\n\n\n\n_edge_attr.shape\n\ntorch.Size([2158])\n\n\n\n_y.shape\n\ntorch.Size([129])\n\n\n\ny_hat.shape\n\ntorch.Size([129, 1])\n\n\n\n_x.shape\n\ntorch.Size([129, 4])\n\n\nx\n\n\n\ny\n\n\n\nThe node features correspond to the number of COVID-19 cases in the region in the past window days.\nThe task is to predict the number of cases in each node after 1 day\n\n_x[0:3]\n\ntensor([[-1.4697, -1.9283, -1.6990, -1.8137],\n        [-1.2510, -1.1812, -1.3208, -1.1812],\n        [-1.0934, -1.0934, -1.0934, -1.0934]])\n\n\n\n_y[:3]\n\ntensor([-1.8137, -1.3208, -1.0934])"
  },
  {
    "objectID": "posts/2_research/2022-12-21-ST-GCN_Dataset.html#montevideobusdatasetloader",
    "href": "posts/2_research/2022-12-21-ST-GCN_Dataset.html#montevideobusdatasetloader",
    "title": "PyTorch ST-GCN Dataset",
    "section": "MontevideoBusDatasetLoader",
    "text": "MontevideoBusDatasetLoader\nhttps://www.fing.edu.uy/~renzom/msc/uploads/msc-thesis.pdf\nMontevideo Buses\n\nA dataset about the hourly passenger inflow at bus stop level for eleven bus lines from the city of Montevideo. Nodes are bus stops and edges represent connections between the stops; the dataset covers a whole month of traffic patterns.\n\n데이터정리\n\nT = 739\nV = 버스정류장\nN = 675 # number of nodes\nE = 101761 = N^2 # edges\n\\(f(v,t)\\)의 차원? (1,) # passenger inflow\n시간에 따라서 Number of nodes가 변하는지? False\n시간에 따라서 Number of nodes가 변하는지? False\nX: (675,4) (N,4), \\(f(v,t_0),f(v,t_1),f(v,t_2),f(v,t_3)\\)\ny: (675,,) (N,), \\(f(v,t_4)\\)\n예제코드적용가능여부: Yes\n\n- Nodes : 675\n\nvertices are bus stops\n\n-Edges : 690\n\nedges are links between bus stops when a bus line connects them and the weight represent the road distance\n\n- Time : 739\n\nhourly inflow passenger data at bus stop level for 11 bus lines during October 2020 from Montevideo city (Uruguay).\n\n\nfrom torch_geometric_temporal.dataset import MontevideoBusDatasetLoader\nfrom torch_geometric_temporal.signal import temporal_signal_split\n\nloader = MontevideoBusDatasetLoader()\n\ndataset = loader.get_dataset()\n\ntrain_dataset, test_dataset = temporal_signal_split(dataset, train_ratio=1)\n\n\ndata=[]\nfor time, snapshot in enumerate(train_dataset):\n    data.append([time,snapshot])\n\n\ntime\n\n739\n\n\n\n(data[0][1]).x.shape,(data[0][1]).edge_index.shape,(data[0][1]).edge_attr.shape\n\n(torch.Size([675, 4]), torch.Size([2, 690]), torch.Size([690]))\n\n\n\nG = nx.Graph()\n\n\nnode_list = torch.tensor(range(675)).tolist()\n\n\nG.add_nodes_from(node_list)\n\n\nedge_list=[]\nfor i in range(739):\n    for j in range(len(data[0][1].edge_index[0])):\n        edge_list.append([data[i][1].edge_index[0][j].tolist(),data[i][1].edge_index[1][j].tolist()])\n\n\nG.add_edges_from(edge_list)\n\n\nG.number_of_nodes(),G.number_of_edges()\n\n(675, 690)\n\n\n\nnx.draw(G,node_color='green',node_size=50,font_color='white',width=1)\n\n\n\n\n\n\n\n\n\ntime별 같은 edge 정보를 가지고 있나 확인\n\nnp.where(data[0][1].edge_index != data[10][1].edge_index)\n\n(array([], dtype=int64), array([], dtype=int64))\n\n\n\n\nfrom torch_geometric_temporal.dataset import MontevideoBusDatasetLoader\nloader = MontevideoBusDatasetLoader()\n\ndataset = loader.get_dataset(lags=4)\n\ntrain_dataset, test_dataset = temporal_signal_split(dataset, train_ratio=0.5)\n\n\ndata=[]\nfor time, snapshot in enumerate(train_dataset):\n    data.append([time,snapshot])\n\n\n(data[0][1]).x[0], (data[0][1]).y[0]\n\n(tensor([-0.4200, -0.4200, -0.4200, -0.4200]), tensor(-0.4200))\n\n\n\\(t=0\\)에서 \\(X\\)와 \\(y\\)를 정리하면 아래와 같음.\n\nX:= \\(x_0,x_1,x_2,x_3\\)\ny:= \\(x_4\\)\n\n\n(data[1][1]).x[0],(data[1][1]).y[0]\n\n(tensor([-0.4200, -0.4200, -0.4200, -0.4200]), tensor(-0.4200))\n\n\n\nX:= \\(x_1,x_2,x_3,x_4\\)\ny:= \\(x_5\\)\n\n\n(data[2][1]).x[0],(data[2][1]).y[0]\n\n(tensor([-0.4200, -0.4200, -0.4200, -0.4200]), tensor(-0.4200))\n\n\n\nX:=\\(x_2,x_3,x_4,x_5\\)\ny:=\\(x_6\\)\n\n하나의 노드에 길이가 \\(T\\)인 시계열이 맵핑되어 있음. (노드는 총 675개)\n각 노드마다 아래와 같은 과정으로 예측이 됨\n\n\\((x_0,x_1,x_2,x_3) \\to (x_4)\\)\n\\((x_1,x_2,x_3,x_4) \\to (x_5)\\)\n\n\\(f(v,t), v \\in \\{v_1,\\dots,v_{675}\\}, t=1,2,\\dots,739\\)\n\\[{\\bf X}_{t=1} = \\begin{bmatrix}\nf(v_1,t=1) & f(v_1,t=2) & f(v_1,t=3)&  f(v_1,t=4) \\\\\nf(v_2,t=1) & f(v_2,t=2) & f(v_2,t=3)&  f(v_2,t=4) \\\\\n\\dots & \\dots  & \\dots & \\dots  \\\\\nf(v_{20},t=1) & f(v_{20},t=2) & f(v_{20},t=3)&  f(v_{20},t=4)\n\\end{bmatrix}\\]\n\\[{\\bf y}_{t=1} = \\begin{bmatrix}\nf(v_1,t=5) \\\\\nf(v_2,t=5) \\\\\n\\dots  \\\\\nf(v_{20},t=5)\n\\end{bmatrix}\\]\n\nLearn\n\nmodel = RecurrentGCN(node_features=4, filters=32)\n\noptimizer = torch.optim.Adam(model.parameters(), lr=0.01)\n\nmodel.train()\n\nfor epoch in tqdm(range(50)):\n    for time, snapshot in enumerate(train_dataset):\n        y_hat = model(snapshot.x, snapshot.edge_index, snapshot.edge_attr)\n        cost = torch.mean((y_hat-snapshot.y)**2)\n        cost.backward()\n        optimizer.step()\n        optimizer.zero_grad()\n\n100%|██████████| 50/50 [01:51&lt;00:00,  2.23s/it]\n\n\n\nfor time, snapshot in enumerate(train_dataset):\n    _x = snapshot.x \n    _edge_index = snapshot.edge_index \n    _edge_attr = snapshot.edge_attr\n    _y = snapshot.y\n    break\n\n\n_x.shape\n\ntorch.Size([675, 4])\n\n\n\n_edge_index.shape\n\ntorch.Size([2, 690])\n\n\n\n_edge_attr.shape\n\ntorch.Size([690])\n\n\n\n_y.shape\n\ntorch.Size([675])\n\n\n\n_x.shape\n\ntorch.Size([675, 4])\n\n\nx\n\n\n\ny\n\nThe target is the passenger inflow.\nThis is a curated dataset made from different data sources of the Metropolitan Transportation System (STM) of Montevide\n\n\n_x[0:3]\n\ntensor([[-0.4200, -0.4200, -0.4200, -0.4200],\n        [-0.0367, -0.0367, -0.0367, -0.0367],\n        [-0.2655, -0.2655, -0.2655, -0.2655]])\n\n\n\n_y[:3]\n\ntensor([-0.4200, -0.0367, -0.2655])"
  },
  {
    "objectID": "posts/2_research/2022-12-21-ST-GCN_Dataset.html#twittertennisdatasetloader",
    "href": "posts/2_research/2022-12-21-ST-GCN_Dataset.html#twittertennisdatasetloader",
    "title": "PyTorch ST-GCN Dataset",
    "section": "TwitterTennisDatasetLoader",
    "text": "TwitterTennisDatasetLoader\nhttps://appliednetsci.springeropen.com/articles/10.1007/s41109-018-0080-5?ref=https://githubhelp.com\nTwitter Tennis RG and UO\n\nTwitter mention graphs of major tennis tournaments from 2017. Each snapshot contains the graph of popular player or sport news accounts and mentions between them [5, 6]. Node labels encode the number of mentions received and vertex features are structural properties\n\n데이터정리\n\nT = 52081\nV = 트위터계정\n\nN = 1000 # number of nodes\nE = 119 = N^2 # edges\n\\(f(v,t)\\)의 차원? (1,) # passenger inflow\n시간에 따라서 N이 변하는지? ??\n시간에 따라서 E가 변하는지? True\nX: ?\ny: ?\n예제코드적용가능여부: No\n\n- Nodes : 1000\n\nvertices are Twitter accounts\n\n-Edges : 119\n\nedges are mentions between them\n\n- Time : 52081\n\nTwitter mention graphs related to major tennis tournaments from 2017\n\n\nfrom torch_geometric_temporal.dataset import TwitterTennisDatasetLoader\nfrom torch_geometric_temporal.signal import temporal_signal_split\n\nloader = TwitterTennisDatasetLoader()\n\ndataset = loader.get_dataset()\n\ntrain_dataset, test_dataset = temporal_signal_split(dataset, train_ratio=1)\n\n\ndata=[]\nfor time, snapshot in enumerate(train_dataset):\n    data.append([time,snapshot])\n\n\ntime\n\n119\n\n\n\n(data[0][1]).x.shape,(data[0][1]).edge_index.shape,(data[0][1]).edge_attr.shape\n\n(torch.Size([1000, 16]), torch.Size([2, 89]), torch.Size([89]))\n\n\n\ndata[0][1].x[0]\n\ntensor([0., 0., 0., 0., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n\n\n\ndata[0][1].edge_index[0]\n\ntensor([ 42, 909, 909, 909, 233, 233, 450, 256, 256, 256, 256, 256, 434, 434,\n        434, 233, 233, 233, 233, 233, 233, 233,   9,   9, 355,  84,  84,  84,\n         84, 140, 140, 140, 140,   0, 140, 238, 238, 238, 649, 875, 875, 234,\n         73,  73, 341, 341, 341, 341, 341, 417, 293, 991,  74, 581, 282, 162,\n        144, 383, 383, 135, 135, 910, 910, 910, 910, 910,  87,  87,  87,  87,\n          9,   9, 934, 934, 162, 225,  42, 911, 911, 911, 911, 911, 911, 911,\n        911, 498, 498,  64, 435])\n\n\n\ndata[0][1].edge_attr\n\ntensor([2., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 2., 1., 1., 1., 1., 1.,\n        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 2., 2., 1., 1., 1., 1., 1., 1.,\n        1., 1., 1., 1., 3., 2., 1., 1., 1., 1., 2., 2., 2., 1., 1., 1., 3.])\n\n\n\nG = nx.Graph()\n\n\nnode_list = torch.tensor(range(1000)).tolist()\n\n\nG.add_nodes_from(node_list)\n\n\nedge_list=[]\nfor i in range(119):\n    for j in range(40):\n        edge_list.append([data[i][1].edge_index[0][j].tolist(),data[i][1].edge_index[1][j].tolist()])\n\n\nG.add_edges_from(edge_list)\n\n\nG.number_of_nodes(),G.number_of_edges()\n\n(1000, 2819)\n\n\n\nnx.draw(G,node_color='green',node_size=50,width=1)\n\n\n\n\n\n\n\n\n\ntime별 같은 edge 정보를 가지고 있나 확인\n\nlen(data[2][1].edge_index[0])\n\n67\n\n\n\nlen(data[0][1].edge_index[0])\n\n89\n\n\n다름..\n\n\nfrom torch_geometric_temporal.dataset import TwitterTennisDatasetLoader\nloader = TwitterTennisDatasetLoader()\n\ndataset = loader.get_dataset()\n\ntrain_dataset, test_dataset = temporal_signal_split(dataset, train_ratio=0.5)\n\n\ndata=[]\nfor time, snapshot in enumerate(train_dataset):\n    data.append([time,snapshot])\n\n\n(data[0][1]).x[0], (data[0][1]).y[0]\n\n(tensor([0., 0., 0., 0., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n tensor(4.8363))\n\n\n\n(data[1][1]).x[0],(data[1][1]).y[0]\n\n(tensor([0., 0., 0., 1., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n tensor(4.9200))\n\n\n\n(data[2][1]).x[0],(data[2][1]).y[0]\n\n(tensor([0., 0., 0., 0., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n tensor(6.5539))\n\n\n\n(data[3][1]).x[0],(data[3][1]).y[0]\n\n(tensor([0., 0., 0., 0., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n tensor(6.9651))\n\n\n\nfor time, snapshot in enumerate(train_dataset):\n    _x = snapshot.x \n    _edge_index = snapshot.edge_index \n    _edge_attr = snapshot.edge_attr\n    _y = snapshot.y\n    break\n\n\n_x.shape\n\ntorch.Size([1000, 16])\n\n\n\n_edge_index.shape\n\ntorch.Size([2, 89])\n\n\n\n_edge_attr.shape\n\ntorch.Size([89])\n\n\n\n_y.shape\n\ntorch.Size([1000])\n\n\n\n_x.shape\n\ntorch.Size([1000, 16])\n\n\nx\n\n\n\ny\n\n\n\n\n_x[0:3]\n\ntensor([[0., 0., 0., 0., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n        [0., 0., 1., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n        [0., 0., 1., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n\n\n\n_y[0]\n\ntensor(4.8363)"
  },
  {
    "objectID": "posts/2_research/2022-12-21-ST-GCN_Dataset.html#mtmdatasetloader",
    "href": "posts/2_research/2022-12-21-ST-GCN_Dataset.html#mtmdatasetloader",
    "title": "PyTorch ST-GCN Dataset",
    "section": "MTMDatasetLoader",
    "text": "MTMDatasetLoader\nMTM-1 Hand Motions\n\nA temporal dataset of MethodsTime Measurement-1 [36] motions, signalled as consecutive graph frames of 21 3D hand key points that were acquired via MediaPipe Hands [64] from original RGB-Video material. Node features encode the normalized xyz-coordinates of each finger joint and the vertices are connected according to the human hand structure.\n\n데이터정리\n\nT = 14452\nV = 손의 shape에 대응하는 dot\n\nN = 325 # number of nodes\nE = 19 = N^2 # edges\n\\(f(v,t)\\)의 차원? (Grasp, Release, Move, Reach, Poision, -1)\n시간에 따라서 N이 변하는지? ??\n시간에 따라서 E가 변하는지? ??\nX: ?\ny: ?\n예제코드적용가능여부: No\n\n- Nodes : 325\n\nvertices are are the finger joints of the human hand\n\n-Edges : 19\n\nedges are the bones connecting them\n\n- Time : 14452\n\nfrom torch_geometric_temporal.dataset import MTMDatasetLoader\nfrom torch_geometric_temporal.signal import temporal_signal_split\n\nloader = MTMDatasetLoader()\n\ndataset = loader.get_dataset()\n\ntrain_dataset, test_dataset = temporal_signal_split(dataset, train_ratio=1)\n\n\ndata=[]\nfor time, snapshot in enumerate(train_dataset):\n    data.append([time,snapshot])\n\n\ntime\n\n14452\n\n\n\n(data[0][1]).x.shape,(data[0][1]).edge_index.shape,(data[0][1]).edge_attr.shape\n\n(torch.Size([3, 21, 16]), torch.Size([2, 19]), torch.Size([19]))\n\n\n\nG = nx.Graph()\n\n\nnode_list = torch.tensor(range(21)).tolist()\n\n\nG.add_nodes_from(node_list)\n\n\nedge_list=[]\nfor i in range(14452):\n    for j in range(len(data[0][1].edge_index[0])):\n        edge_list.append([data[i][1].edge_index[0][j].tolist(),data[i][1].edge_index[1][j].tolist()])\n\n\nG.add_edges_from(edge_list)\n\n\nG.number_of_nodes(),G.number_of_edges()\n\n(21, 19)\n\n\n\nnx.draw(G,with_labels=True,font_weight='bold',node_color='green',node_size=350,font_color='white',width=1)\n\n\n\n\n\n\n\n\n\ntime별 같은 edge 정보를 가지고 있나 확인\n\nnp.where(data[0][1].edge_index != data[12][1].edge_index)\n\n(array([], dtype=int64), array([], dtype=int64))\n\n\n\n\nfrom torch_geometric_temporal.dataset import MTMDatasetLoader\nloader = MTMDatasetLoader()\n\ndataset = loader.get_dataset()\n\ntrain_dataset, test_dataset = temporal_signal_split(dataset, train_ratio=0.5)\n\n\ndata=[]\nfor time, snapshot in enumerate(train_dataset):\n    data.append([time,snapshot])\n\n\n(data[0][1]).x[0], (data[0][1]).y[0]\n\n(tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]]),\n tensor([0., 0., 1., 0., 0., 0.]))\n\n\n\n(data[1][1]).x[0],(data[1][1]).y[0]\n\n(tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]]),\n tensor([0., 0., 1., 0., 0., 0.]))\n\n\n\nfor time, snapshot in enumerate(train_dataset):\n    _x = snapshot.x \n    _edge_index = snapshot.edge_index \n    _edge_attr = snapshot.edge_attr\n    _y = snapshot.y\n    break\n\n\n_x.shape\n\ntorch.Size([3, 21, 16])\n\n\n\n_edge_index.shape\n\ntorch.Size([2, 19])\n\n\n\n_edge_attr.shape\n\ntorch.Size([19])\n\n\n\n_y.shape\n\ntorch.Size([16, 6])\n\n\n\n_x.shape\n\ntorch.Size([3, 21, 16])\n\n\nx\n\nThe data x is returned in shape (3, 21, T),\n\ny\n\nThe targets are manually labeled for each frame, according to one of the five MTM-1 motions (classes ): Grasp, Release, Move, Reach, Position plus a negative class for frames without graph signals (no hand present).\nthe target is returned one-hot-encoded in shape (T, 6).\n\n\n_x[0]\n\ntensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n\n\n\n_y[0]\n\ntensor([0., 0., 1., 0., 0., 0.])"
  },
  {
    "objectID": "posts/2_research/2024-02-01-Self Consistency toy ex.html",
    "href": "posts/2_research/2024-02-01-Self Consistency toy ex.html",
    "title": "Self Consistency Toy ex",
    "section": "",
    "text": "Ref: Self Consistency: A General Recipe for Wavelet Estimation With Irregularly-spaced and/or Incomplete Data\n\\[\\mathbb{E}(\\hat{f}_{com} | f = \\hat{f}_{obs}) = \\hat{f}_{obs}\\]"
  },
  {
    "objectID": "posts/2_research/2024-02-01-Self Consistency toy ex.html#result",
    "href": "posts/2_research/2024-02-01-Self Consistency toy ex.html#result",
    "title": "Self Consistency Toy ex",
    "section": "Result",
    "text": "Result\n\nmiss_num_1 = [5,7,8]\n\n\nwith plt.style.context('seaborn-white'):\n    fig, ax = plt.subplots(figsize=(20,10))\n    # fig.suptitle('Figure',fontsize=40)\n    plt.tight_layout()\n    \n    ax.plot(x,y_miss,'o',label='Incomplete Data',markersize=8)\n    ax.plot(x,y,'--',color='blue')\n    ax.plot(x,y_true,label='True',color='black',alpha=0.5)\n    ax.plot(miss_num_1,y_miss_impu[miss_num],'o',color='C3',markersize=8)\n    ax.plot(miss_num_1,y_miss_impu_zero[miss_num],'*',color='C3',markersize=8)\n    ax.plot(miss_num_1,y_miss_impu_one[miss_num],'*',color='C3',markersize=8)\n    ax.plot(miss_num_1,y_miss_impu_two[miss_num],'*',color='C3',markersize=8)\n    ax.plot(miss_num_1,y_miss_impu_tre[miss_num],'*',color='C3',markersize=8)\n    ax.plot(miss_num_1,y_miss_impu_fth[miss_num],'*',color='C3',markersize=8)\n    ax.plot(miss_num_1,y_miss_impu_fif[miss_num],'o',color='C3',markersize=8)\n    ax.legend(fontsize=20,loc='upper left',facecolor='white', frameon=True)\n    \n    # ax.annotate('1st value',xy=(miss_num[0]+0.1,y_miss_impu[miss_num][0]),fontsize=20)\n    # ax.annotate('1st value',xy=(miss_num[1]+0.1,y_miss_impu[miss_num][1]),fontsize=20)\n    # ax.annotate('1st value',xy=(miss_num[2]+0.1,y_miss_impu[miss_num][2]),fontsize=20)\n    # ax.annotate('5th iteration',xy=(miss_num[0]+0.1,y_miss_impu_fif[miss_num][0]),fontsize=20)\n    # ax.annotate('5th iteration',xy=(miss_num[1]+0.1,y_miss_impu_fif[miss_num][1]),fontsize=20)\n    # ax.annotate('5th iteration',xy=(miss_num[2]+0.1,y_miss_impu_fif[miss_num][2]),fontsize=20)\n    \n    # ax.arrow(miss_num[0]+0.4, y_miss_impu[miss_num][0]+0.6, 0, y_miss_impu_fif[miss_num][0]-1.5,linestyle= 'dashed', head_width=0.1, head_length=0.3, fc='black',ec='black',alpha=0.5)\n    # ax.arrow(miss_num[1]+0.4, y_miss_impu[miss_num][1]+0.6, 0, y_miss_impu_fif[miss_num][1]-1.5,linestyle= 'dashed', head_width=0.1, head_length=0.3, fc='black',ec='black',alpha=0.5)\n    # ax.arrow(miss_num[2]+0.4, y_miss_impu[miss_num][2]+0.6, 0, y_miss_impu_fif[miss_num][2]-1.5,linestyle= 'dashed', head_width=0.1, head_length=0.3, fc='black',ec='black',alpha=0.5)\n\n    # ax.plot(miss_num, y_miss_impu[miss_num], 'o', markersize=30, markerfacecolor='none', markeredgecolor='red',markeredgewidth=1,color='C4')\n    \n    ax.tick_params(axis='y', labelsize=20)\n    ax.tick_params(axis='x', labelsize=20)\n# plt.savefig('Self_consistency_Toy.png')\n\n\n\n\n\n\n\n\n\nwith plt.style.context('seaborn-white'):\n    fig, ax = plt.subplots(figsize=(20,10))\n    # fig.suptitle('Figure',fontsize=40)\n    plt.tight_layout()\n    \n    ax.plot(x,y_true,label='True',color='black',alpha=0.5)\n    ax.plot(x,y_miss,'o',label='$x_{obs}$',markersize=8)\n    # ax.plot(x,y,'--',color='blue')\n    \n    # ax.plot(x,y_iter_zero)\n    # ax.plot(x,y_iter_one)\n    # ax.plot(x,y_iter_two)\n    # ax.plot(x,y_iter_tre)\n    ax.plot(x,y_iter_fth,color='blue',alpha=0.5,label='$\\hat{f}_{obs}$')\n    # ax.plot(x,y_iter_fif)\n    ax.plot(miss_num_1,y_miss_impu[miss_num],'o',color='C3',markersize=8)\n    ax.plot(miss_num_1,y_miss_impu_zero[miss_num],'*',color='C3',markersize=8)\n    ax.plot(miss_num_1,y_miss_impu_one[miss_num],'*',color='C3',markersize=8)\n    ax.plot(miss_num_1,y_miss_impu_two[miss_num],'*',color='C3',markersize=8)\n    ax.plot(miss_num_1,y_miss_impu_tre[miss_num],'*',color='C3',markersize=8)\n    ax.plot(miss_num_1,y_miss_impu_fth[miss_num],'o',color='C3',markersize=8)\n    # ax.plot(miss_num,y_miss_impu_fif[miss_num],'o',color='C3',markersize=8)\n    \n    ax.legend(fontsize=20,loc='upper left',facecolor='white', frameon=True)\n    \n    ax.annotate('1st value',xy=(miss_num_1[0]+0.1,y_miss_impu[miss_num][0]),fontsize=20)\n    ax.annotate('1st value',xy=(miss_num_1[1]+0.1,y_miss_impu[miss_num][1]),fontsize=20)\n    ax.annotate('1st value',xy=(miss_num_1[2]+0.1,y_miss_impu[miss_num][2]),fontsize=20)\n    ax.annotate('5th iteration',xy=(miss_num_1[0]+0.1,y_miss_impu_fth[miss_num][0]-0.3),fontsize=20)\n    ax.annotate('5th iteration',xy=(miss_num_1[1]+0.1,y_miss_impu_fth[miss_num][1]-0.3),fontsize=20)\n    ax.annotate('5th iteration',xy=(miss_num_1[2]+0.1,y_miss_impu_fth[miss_num][2]-0.3),fontsize=20)\n    \n    ax.arrow(miss_num_1[0]+0.4, y_miss_impu[miss_num][0]+0.6, 0, y_miss_impu_fth[miss_num][0]-1.5,linestyle= 'dashed', head_width=0.1, head_length=0.3, fc='black',ec='black',alpha=0.5)\n    ax.arrow(miss_num_1[1]+0.4, y_miss_impu[miss_num][1]+0.6, 0, y_miss_impu_fth[miss_num][1]-1.5,linestyle= 'dashed', head_width=0.1, head_length=0.3, fc='black',ec='black',alpha=0.5)\n    ax.arrow(miss_num_1[2]+0.4, y_miss_impu[miss_num][2]+0.6, 0, y_miss_impu_fth[miss_num][2]-1.5,linestyle= 'dashed', head_width=0.1, head_length=0.3, fc='black',ec='black',alpha=0.5)\n\n    # ax.plot(miss_num_1, y_miss_impu[miss_num], 'o', markersize=30, markerfacecolor='none', markeredgecolor='red',markeredgewidth=1,color='C4')\n    \n    ax.tick_params(axis='y', labelsize=20)\n    ax.tick_params(axis='x', labelsize=20)\n# plt.savefig('Self_consistency_Toy_0617.png')"
  },
  {
    "objectID": "posts/2_research/2024-02-01-Self Consistency toy ex.html#footnotes",
    "href": "posts/2_research/2024-02-01-Self Consistency toy ex.html#footnotes",
    "title": "Self Consistency Toy ex",
    "section": "Footnotes",
    "text": "Footnotes\n\n\n확률 과정 중 과거의 정보를 알고 있다면 미래의 기댓값이 현재값과 동일한 과정↩︎"
  },
  {
    "objectID": "posts/2_research/2023-05-17-itstgcntry)fail_version__Simulation_boxplot_reshape.html",
    "href": "posts/2_research/2023-05-17-itstgcntry)fail_version__Simulation_boxplot_reshape.html",
    "title": "Simulation_reshape",
    "section": "",
    "text": "Simulation Study"
  },
  {
    "objectID": "posts/2_research/2023-05-17-itstgcntry)fail_version__Simulation_boxplot_reshape.html#baseline",
    "href": "posts/2_research/2023-05-17-itstgcntry)fail_version__Simulation_boxplot_reshape.html#baseline",
    "title": "Simulation_reshape",
    "section": "Baseline",
    "text": "Baseline\n\ndata.query(\"method!='GNAR' and mrate ==0 \").plot.box(backend='plotly',x='epoch',color='method',y='mse',facet_col='RecurrentGCN',height=800)"
  },
  {
    "objectID": "posts/2_research/2023-05-17-itstgcntry)fail_version__Simulation_boxplot_reshape.html#random",
    "href": "posts/2_research/2023-05-17-itstgcntry)fail_version__Simulation_boxplot_reshape.html#random",
    "title": "Simulation_reshape",
    "section": "random",
    "text": "random\n\ndata.query(\"method!='GNAR' and mtype =='rand' \").plot.box(backend='plotly',x='mrate',color='method',y='mse',facet_col='RecurrentGCN',facet_row='inter_method',height=800)"
  },
  {
    "objectID": "posts/2_research/2023-05-17-itstgcntry)fail_version__Simulation_boxplot_reshape.html#block",
    "href": "posts/2_research/2023-05-17-itstgcntry)fail_version__Simulation_boxplot_reshape.html#block",
    "title": "Simulation_reshape",
    "section": "block",
    "text": "block\n\ndata.query(\"method!='GNAR' and mtype =='block' \").plot.box(backend='plotly',x='mrate',color='method',y='mse',facet_col='RecurrentGCN',facet_row='inter_method',height=600)"
  },
  {
    "objectID": "posts/2_research/2023-05-17-itstgcntry)fail_version__Simulation_boxplot_reshape.html#weight-matrix-time-node-고려한-결과",
    "href": "posts/2_research/2023-05-17-itstgcntry)fail_version__Simulation_boxplot_reshape.html#weight-matrix-time-node-고려한-결과",
    "title": "Simulation_reshape",
    "section": "weight matrix time, node 고려한 결과",
    "text": "weight matrix time, node 고려한 결과\n\n# df1 = pd.read_csv('./simulation_results/2023-04-30_13-00-12.csv')\n# df2 = pd.read_csv('./simulation_results/2023-04-30_13-31-32.csv')\n# df3 = pd.read_csv('./simulation_results/2023-04-30_14-01-49.csv')\n# df4 = pd.read_csv('./simulation_results/2023-04-30_14-31-56.csv')\n# df5 = pd.read_csv('./simulation_results/2023-04-30_15-02-23.csv')\n# df6 = pd.read_csv('./simulation_results/2023-04-30_15-33-03.csv')\n# df7 = pd.read_csv('./simulation_results/2023-04-30_16-07-43.csv')\n# df8 = pd.read_csv('./simulation_results/2023-04-30_16-41-35.csv')\n# df9 = pd.read_csv('./simulation_results/2023-04-30_17-14-51.csv')\n# df10 = pd.read_csv('./simulation_results/2023-04-30_17-49-34.csv')\n# df11 = pd.read_csv('./simulation_results/2023-04-30_18-21-29.csv')\n# df12 = pd.read_csv('./simulation_results/2023-04-30_18-50-24.csv')\n# df13 = pd.read_csv('./simulation_results/2023-04-30_20-33-28.csv')\n# df14 = pd.read_csv('./simulation_results/2023-05-04_16-40-05.csv')\n# df15 = pd.read_csv('./simulation_results/2023-05-04_17-34-00.csv')\n\n\ndata2 = pd.concat([df1,df2,df3,df4,df5,df6,df7,df8,df9,df10,df11,df12,df13,df14,df15],axis=0)\n\n\ndata2.to_csv('./simulation_results/Real_simulation_reshape/pedalme_Simulation_itstgcnsnd.csv',index=False)\n\n\ndata2 = pd.read_csv('./simulation_results/Real_simulation_reshape/pedalme_Simulation_itstgcnsnd.csv')\n\n\ndata2.query(\"mtype!='block'\").plot.box(backend='plotly',x='mrate',color='method',y='mse',facet_col='lags',facet_row='inter_method',height=1000)\n\n\ndata2.query(\"mtype=='block'\").plot.box(backend='plotly',x='mrate',color='method',y='mse',facet_col='lags',facet_row='inter_method',height=1200)"
  },
  {
    "objectID": "posts/2_research/2023-05-17-itstgcntry)fail_version__Simulation_boxplot_reshape.html#baseline-1",
    "href": "posts/2_research/2023-05-17-itstgcntry)fail_version__Simulation_boxplot_reshape.html#baseline-1",
    "title": "Simulation_reshape",
    "section": "Baseline",
    "text": "Baseline\n\ndata.query(\"mrate==0 and method!='GNAR'\").plot.box(backend='plotly',x='mrate',color='method',y='mse',facet_col='RecurrentGCN',facet_row='lags',height=600)"
  },
  {
    "objectID": "posts/2_research/2023-05-17-itstgcntry)fail_version__Simulation_boxplot_reshape.html#random-1",
    "href": "posts/2_research/2023-05-17-itstgcntry)fail_version__Simulation_boxplot_reshape.html#random-1",
    "title": "Simulation_reshape",
    "section": "random",
    "text": "random\n\ndata.query(\"mtype=='rand' and mrate !=0 and method!='GNAR'\").plot.box(backend='plotly',x='mrate',color='method',y='mse',facet_col='RecurrentGCN',facet_row='nof_filters',height=800)"
  },
  {
    "objectID": "posts/2_research/2023-05-17-itstgcntry)fail_version__Simulation_boxplot_reshape.html#block-1",
    "href": "posts/2_research/2023-05-17-itstgcntry)fail_version__Simulation_boxplot_reshape.html#block-1",
    "title": "Simulation_reshape",
    "section": "block",
    "text": "block\n\ndf1 = pd.read_csv('./simulation_results/2023-05-24_00-26-51.csv')\n# df2 = pd.read_csv('./simulation_results/2023-04-27_22-09-07.csv')\n# df3 = pd.read_csv('./simulation_results/2023-04-28_14-40-59.csv')\n# df4 = pd.read_csv('./simulation_results/2023-05-14_19-46-46.csv')\n# df5 = pd.read_csv('./simulation_results/2023-05-14_19-46-46.csv')\n\n\ndata = pd.concat([df1],axis=0)\n\n\ndata.to_csv('./simulation_results/Real_simulation_reshape/wikimath_block.csv',index=False)\n\n\ndata = pd.read_csv('./simulation_results/Real_simulation_reshape/wikimath_block.csv')\n\n\ndata.query(\"mtype=='block' and method!='GNAR'\").plot.box(backend='plotly',x='mrate',color='method',y='mse',facet_col='lags',facet_row='nof_filters',height=1200)"
  },
  {
    "objectID": "posts/2_research/2023-05-17-itstgcntry)fail_version__Simulation_boxplot_reshape.html#missing-values-on-the-same-nodes",
    "href": "posts/2_research/2023-05-17-itstgcntry)fail_version__Simulation_boxplot_reshape.html#missing-values-on-the-same-nodes",
    "title": "Simulation_reshape",
    "section": "missing values on the same nodes",
    "text": "missing values on the same nodes\n\n# 10%\n# df1 = pd.read_csv('./simulation_results/2023-04-29_03-57-07.csv') # STGCN IT-STGCN block\n# df2 = pd.read_csv('./simulation_results/2023-04-29_20-15-46.csv') # STGCN IT-STGCN\n# df3 = pd.read_csv('./simulation_results/2023-04-30_16-19-58.csv') # STGCN IT-STGCN\n# # 60% 확인하고 다시 돌리기\n# df4 = pd.read_csv('./simulation_results/2023-05-05_04-21-57.csv') # STGCN IT-STGCN 60%\n# df5 = pd.read_csv('./simulation_results/2023-05-06_11-34-46.csv') # STGCN IT-STGCN\n# df6 = pd.read_csv('./simulation_results/2023-05-06_23-43-35.csv') # STGCN IT-STGCN\n# df7 = pd.read_csv('./simulation_results/2023-05-07_14-06-44.csv') # STGCN IT-STGCN\n\n\ndata = pd.concat([df1,df2,df3,df4,df5,df6,df7],axis=0)\n\n\ndata.to_csv('./simulation_results/Real_simulation_reshape/wikimath_GSO_st.csv',index=False)\n\n\ndata = pd.read_csv('./simulation_results/Real_simulation_reshape/wikimath_GSO_st.csv')\n\n\ndata.query(\"method=='GNAR'\")['mse'].unique()\n\n\ndata.query(\"method!='GNAR' and mtype =='block' \").plot.box(backend='plotly',x='mrate',color='method',y='mse',facet_col='inter_method',facet_row='lags',height=800)"
  },
  {
    "objectID": "posts/2_research/2023-05-17-itstgcntry)fail_version__Simulation_boxplot_reshape.html#baseline-2",
    "href": "posts/2_research/2023-05-17-itstgcntry)fail_version__Simulation_boxplot_reshape.html#baseline-2",
    "title": "Simulation_reshape",
    "section": "Baseline",
    "text": "Baseline\n\ndata = pd.concat([df1,df2,df3,df4,df5,df7,df8,df9,df10,df11,df12,df13,df14,df15,df16,df17,df18,\n                 df19,df20,df21,df22,df23,df24,df25,df26,df27,df28,df29,df30,df31,df32,df33,df34,\n                 df35,df36,df37,df38,df39,df40,df41,df42,df43,df44,df45,df46,df47,df48,df49,df50,\n                 df51,df52,df53,df54,df55,df56,df57,df58,df59,df60,df61],axis=0)\n\n\ndata.to_csv('./simulation_results/Real_simulation_reshape/windmillsmall.csv',index=False)\n\n\ndata = pd.read_csv('./simulation_results/Real_simulation_reshape/windmillsmall.csv')\n\n\ndata.query(\"method=='GNAR' and mrate ==0\")['mse'].unique()\n\n\ndata.query(\"method!='GNAR' and mrate ==0 \").plot.box(backend='plotly',x='mrate',color='method',y='mse',facet_col='lags',height=800)"
  },
  {
    "objectID": "posts/2_research/2023-05-17-itstgcntry)fail_version__Simulation_boxplot_reshape.html#random-2",
    "href": "posts/2_research/2023-05-17-itstgcntry)fail_version__Simulation_boxplot_reshape.html#random-2",
    "title": "Simulation_reshape",
    "section": "random",
    "text": "random\n\ndata.query(\"method=='GNAR' and mrate !=0\")['mse'].unique()\n\n\ndata.query(\"method!='GNAR' and mtype =='rand' \").plot.box(backend='plotly',x='mrate',color='method',y='mse',facet_col='lags',facet_row='inter_method',height=800)"
  },
  {
    "objectID": "posts/2_research/2023-05-17-itstgcntry)fail_version__Simulation_boxplot_reshape.html#block-2",
    "href": "posts/2_research/2023-05-17-itstgcntry)fail_version__Simulation_boxplot_reshape.html#block-2",
    "title": "Simulation_reshape",
    "section": "block",
    "text": "block\n\n# df1 = pd.read_csv('./simulation_results/2023-04-24_02-48-08.csv') # STGCN IT-STGCN block\n# df2 = pd.read_csv('./simulation_results/2023-04-24_10-57-10.csv') # STGCN IT-STGCN\n# df3 = pd.read_csv('./simulation_results/2023-04-24_18-53-34.csv') # STGCN IT-STGCN\n# df4 = pd.read_csv('./simulation_results/2023-04-25_02-30-27.csv') # STGCN IT-STGCN\n# df5 = pd.read_csv('./simulation_results/2023-04-25_10-48-46.csv') # STGCN IT-STGCN\n# df6 = pd.read_csv('./simulation_results/2023-04-25_10-53-14.csv') # GNAR \n# df7 = pd.read_csv('./simulation_results/2023-04-25_18-40-53.csv') # STGCN IT-STGCN\n# df8 = pd.read_csv('./simulation_results/2023-04-25_23-30-08.csv') # STGCN IT-STGCN\n# df9 = pd.read_csv('./simulation_results/2023-04-26_04-15-00.csv') # STGCN IT-STGCN\n# df10 = pd.read_csv('./simulation_results/2023-04-27_07-59-36.csv') # STGCN IT-STGCN\n# df11 = pd.read_csv('./simulation_results/2023-04-27_15-29-00.csv') # STGCN IT-STGCN\n# df12 = pd.read_csv('./simulation_results/2023-04-27_23-37-18.csv') # STGCN IT-STGCN\n# df13 = pd.read_csv('./simulation_results/2023-04-28_08-21-54.csv') # STGCN IT-STGCN\n# df14 = pd.read_csv('./simulation_results/2023-04-28_16-06-55.csv') # STGCN IT-STGCN\n# df15 = pd.read_csv('./simulation_results/2023-04-28_21-19-37.csv') # STGCN IT-STGCN\n# df16 = pd.read_csv('./simulation_results/2023-04-29_03-07-03.csv') # STGCN IT-STGCN\n# df17 = pd.read_csv('./simulation_results/2023-04-29_09-00-42.csv') # STGCN IT-STGCN\n# df18 = pd.read_csv('./simulation_results/2023-04-29_19-07-49.csv') # STGCN IT-STGCN\n# df19 = pd.read_csv('./simulation_results/2023-04-30_05-14-07.csv') # STGCN IT-STGCN\n# df20 = pd.read_csv('./simulation_results/2023-04-30_15-23-16.csv') # STGCN IT-STGCN\n# df21 = pd.read_csv('./simulation_results/2023-05-01_00-16-37.csv') # STGCN IT-STGCN\n# df22 = pd.read_csv('./simulation_results/2023-05-01_07-41-52.csv') # STGCN IT-STGCN\n# df23 = pd.read_csv('./simulation_results/2023-05-01_16-21-41.csv') # STGCN IT-STGCN\n# df24 = pd.read_csv('./simulation_results/2023-05-01_23-38-23.csv') # STGCN IT-STGCN\n# df25 = pd.read_csv('./simulation_results/2023-05-02_13-51-13.csv') # STGCN IT-STGCN\n# df26 = pd.read_csv('./simulation_results/2023-05-02_21-43-26.csv') # STGCN IT-STGCN\n# df27 = pd.read_csv('./simulation_results/2023-05-03_06-04-32.csv') # STGCN IT-STGCN\n# df28 = pd.read_csv('./simulation_results/2023-05-03_13-43-11.csv') # STGCN IT-STGCN\n# df29 = pd.read_csv('./simulation_results/2023-05-03_21-58-04.csv') # STGCN IT-STGCN\n# df30 = pd.read_csv('./simulation_results/2023-05-04_04-39-00.csv') # STGCN IT-STGCN\n\n\ndata = pd.concat([df1,df2,df3,df4,df5,df6,df7,df8,df9,df10,df11,df12,df13,df14,df15,df16,\n                 df17,df18,df19,df20,df21,df22,df23,df24,df25,df26,df27,df28,df29,df30],axis=0)\n\n\ndata.to_csv('./simulation_results/Real_simulation_reshape/windmillsmall_block.csv',index=False)\n\n\ndata = pd.read_csv('./simulation_results/Real_simulation_reshape/windmillsmall_block.csv')\n\n\ndata.query(\"method=='GNAR'\")['mse'].unique()\n\n\ndata.query(\"method!='GNAR' and mtype =='block' \").plot.box(backend='plotly',x='mrate',color='method',y='mse',facet_col='lags',facet_row='inter_method',height=800)"
  },
  {
    "objectID": "posts/2_research/2023-05-17-itstgcntry)fail_version__Simulation_boxplot_reshape.html#baseline-3",
    "href": "posts/2_research/2023-05-17-itstgcntry)fail_version__Simulation_boxplot_reshape.html#baseline-3",
    "title": "Simulation_reshape",
    "section": "Baseline",
    "text": "Baseline\n\ndata.query(\"mrate==0 and method!='GNAR'\").plot.box(backend='plotly',x='mrate',color='method',y='mse',facet_col='lags',facet_row='nof_filters',height=600)"
  },
  {
    "objectID": "posts/2_research/2023-05-17-itstgcntry)fail_version__Simulation_boxplot_reshape.html#random-3",
    "href": "posts/2_research/2023-05-17-itstgcntry)fail_version__Simulation_boxplot_reshape.html#random-3",
    "title": "Simulation_reshape",
    "section": "random",
    "text": "random\n\ndata.query(\"mtype=='rand' and mrate !=0 and method!='GNAR' and mrate!=0.3 and mrate!=0.4\").plot.box(backend='plotly',x='mrate',color='method',y='mse',facet_col='RecurrentGCN',facet_row='inter_method',height=800)"
  },
  {
    "objectID": "posts/2_research/2023-05-17-itstgcntry)fail_version__Simulation_boxplot_reshape.html#block-3",
    "href": "posts/2_research/2023-05-17-itstgcntry)fail_version__Simulation_boxplot_reshape.html#block-3",
    "title": "Simulation_reshape",
    "section": "block",
    "text": "block\n\n# df1 = pd.read_csv('./simulation_results/2023-05-04_21-03-21.csv')\n# df2 = pd.read_csv('./simulation_results/2023-05-05_12-10-44.csv')\n# df3 = pd.read_csv('./simulation_results/2023-05-06_12-42-22.csv')\n# df4 = pd.read_csv('./simulation_results/2023-05-06_15-40-47.csv')\n\n\ndata = pd.concat([df1,df2,df3,df4],axis=0)\n\n\ndata.to_csv('./simulation_results/Real_simulation_reshape/monte_block.csv',index=False)\n\n\ndata = pd.read_csv('./simulation_results/Real_simulation_reshape/monte_block.csv')\n\n\ndata.query(\"mtype=='block' and method=='GNAR'\")['mse'].mean()\n\n\ndata.query(\"mtype=='block' and method!='GNAR'\").plot.box(backend='plotly',x='mrate',color='method',y='mse',facet_col='lags',facet_row='nof_filters',height=1200)"
  },
  {
    "objectID": "posts/2_research/2023-04-06-METRLADatasetLoader.html",
    "href": "posts/2_research/2023-04-06-METRLADatasetLoader.html",
    "title": "METRLADatasetLoader-Tutorial",
    "section": "",
    "text": "METRLADatasetLoader\n\n\nimport torch\nfrom IPython.display import clear_output\npt_version = torch.__version__\nprint(pt_version)\n\n1.10.1\n\n\n\nimport numpy as np\nfrom torch_geometric_temporal.dataset import METRLADatasetLoader\nfrom torch_geometric_temporal.signal import StaticGraphTemporalSignal\n\nloader = METRLADatasetLoader()\ndataset = loader.get_dataset(num_timesteps_in=12, num_timesteps_out=12)\n\n#print(\"Dataset type:  \", dataset)\n#print(\"Number of samples / sequences: \",  len(set(dataset)))\n\n\nimport seaborn as sns\n# Visualize traffic over time\nsensor_number = 1\nhours = 24\nsensor_labels = [bucket.y[sensor_number][0].item() for bucket in list(dataset)[:hours]]\nsns.lineplot(data=sensor_labels)\n\n\n\n\n\n\n\n\n\nfrom torch_geometric_temporal.signal import temporal_signal_split\ntrain_dataset, test_dataset = temporal_signal_split(dataset, train_ratio=0.8)\n\n#print(\"Number of train buckets: \", len(set(train_dataset)))\n#print(\"Number of test buckets: \", len(set(test_dataset)))\n\n\nimport torch\nimport torch.nn.functional as F\nfrom torch_geometric_temporal.nn.recurrent import A3TGCN\n\nclass TemporalGNN(torch.nn.Module):\n    def __init__(self, node_features, periods):\n        super(TemporalGNN, self).__init__()\n        # Attention Temporal Graph Convolutional Cell\n        self.tgnn = A3TGCN(in_channels=node_features, \n                           out_channels=32, \n                           periods=periods)\n        # Equals single-shot prediction\n        self.linear = torch.nn.Linear(32, periods)\n\n    def forward(self, x, edge_index):\n        \"\"\"\n        x = Node features for T time steps\n        edge_index = Graph edge indices\n        \"\"\"\n        h = self.tgnn(x, edge_index)\n        h = F.relu(h)\n        h = self.linear(h)\n        return h\n\nTemporalGNN(node_features=2, periods=12)\n\nTemporalGNN(\n  (tgnn): A3TGCN(\n    (_base_tgcn): TGCN(\n      (conv_z): GCNConv(2, 32)\n      (linear_z): Linear(in_features=64, out_features=32, bias=True)\n      (conv_r): GCNConv(2, 32)\n      (linear_r): Linear(in_features=64, out_features=32, bias=True)\n      (conv_h): GCNConv(2, 32)\n      (linear_h): Linear(in_features=64, out_features=32, bias=True)\n    )\n  )\n  (linear): Linear(in_features=32, out_features=12, bias=True)\n)\n\n\n\n# GPU support\ndevice = torch.device('cpu') # cuda\nsubset = 2000\n\n# Create model and optimizers\nmodel = TemporalGNN(node_features=2, periods=12).to(device)\noptimizer = torch.optim.Adam(model.parameters(), lr=0.01)\nmodel.train()\n\nprint(\"Running training...\")\nfor epoch in range(10): \n    loss = 0\n    step = 0\n    for snapshot in train_dataset:\n        snapshot = snapshot.to(device)\n        # Get model predictions\n        y_hat = model(snapshot.x, snapshot.edge_index)\n        # Mean squared error\n        loss = loss + torch.mean((y_hat-snapshot.y)**2) \n        step += 1\n        if step &gt; subset:\n          break\n\n    loss = loss / (step + 1)\n    loss.backward()\n    optimizer.step()\n    optimizer.zero_grad()\n    print(\"Epoch {} train MSE: {:.4f}\".format(epoch, loss.item()))\n\nRunning training...\nEpoch 0 train MSE: 0.7596\nEpoch 1 train MSE: 0.7398\nEpoch 2 train MSE: 0.7205\nEpoch 3 train MSE: 0.6996\nEpoch 4 train MSE: 0.6759\nEpoch 5 train MSE: 0.6495\nEpoch 6 train MSE: 0.6221\nEpoch 7 train MSE: 0.5963\nEpoch 8 train MSE: 0.5743\nEpoch 9 train MSE: 0.5573\n\n\n\nmodel.eval()\nloss = 0\nstep = 0\nhorizon = 288\n\n# Store for analysis\npredictions = []\nlabels = []\n\nfor snapshot in test_dataset:\n    snapshot = snapshot.to(device)\n    # Get predictions\n    y_hat = model(snapshot.x, snapshot.edge_index)\n    # Mean squared error\n    loss = loss + torch.mean((y_hat-snapshot.y)**2)\n    # Store for analysis below\n    labels.append(snapshot.y)\n    predictions.append(y_hat)\n    step += 1\n    if step &gt; horizon:\n          break\n\nloss = loss / (step+1)\nloss = loss.item()\nprint(\"Test MSE: {:.4f}\".format(loss))\n\nTest MSE: 0.6738"
  },
  {
    "objectID": "posts/2_research/2023-06-28-DYGRENCODER_simulation_table_reshape.html",
    "href": "posts/2_research/2023-06-28-DYGRENCODER_simulation_table_reshape.html",
    "title": "DYGRENCODER_Simulation Tables_reshape",
    "section": "",
    "text": "Simulation Tables"
  },
  {
    "objectID": "posts/2_research/2023-06-28-DYGRENCODER_simulation_table_reshape.html#baseline",
    "href": "posts/2_research/2023-06-28-DYGRENCODER_simulation_table_reshape.html#baseline",
    "title": "DYGRENCODER_Simulation Tables_reshape",
    "section": "Baseline",
    "text": "Baseline\n\npd.merge(data.query(\"dataset=='fivenodes' and mtype!='rand' and mtype!='block'\").groupby(['nof_filters','method','lags'])['mse'].mean().reset_index(),\n         data.query(\"dataset=='fivenodes' and mtype!='rand' and mtype!='block'\").groupby(['nof_filters','method','lags'])['mse'].std().reset_index(),\n         on=['method','nof_filters','lags']).rename(columns={'mse_x':'mean','mse_y':'std'}).round(3)\n\n\n\n\n\n\n\n\nnof_filters\nmethod\nlags\nmean\nstd\n\n\n\n\n0\n12\nIT-STGCN\n2\n1.113\n0.037\n\n\n1\n12\nSTGCN\n2\n1.115\n0.038"
  },
  {
    "objectID": "posts/2_research/2023-06-28-DYGRENCODER_simulation_table_reshape.html#random",
    "href": "posts/2_research/2023-06-28-DYGRENCODER_simulation_table_reshape.html#random",
    "title": "DYGRENCODER_Simulation Tables_reshape",
    "section": "Random",
    "text": "Random\n\npd.merge(data.query(\"dataset=='fivenodes' and mtype=='rand'\").groupby(['mrate','nof_filters','method','lags'])['mse'].mean().reset_index(),\n         data.query(\"dataset=='fivenodes' and mtype=='rand'\").groupby(['mrate','nof_filters','method','lags'])['mse'].std().reset_index(),\n         on=['method','nof_filters','mrate','lags']).rename(columns={'mse_x':'mean','mse_y':'std'}).round(3)\n\n\n\n\n\n\n\n\nmrate\nnof_filters\nmethod\nlags\nmean\nstd\n\n\n\n\n0\n0.7\n12\nIT-STGCN\n2\n1.258\n0.069\n\n\n1\n0.7\n12\nSTGCN\n2\n1.494\n0.133\n\n\n2\n0.8\n12\nIT-STGCN\n2\n1.322\n0.070\n\n\n3\n0.8\n12\nSTGCN\n2\n1.508\n0.137"
  },
  {
    "objectID": "posts/2_research/2023-06-28-DYGRENCODER_simulation_table_reshape.html#block",
    "href": "posts/2_research/2023-06-28-DYGRENCODER_simulation_table_reshape.html#block",
    "title": "DYGRENCODER_Simulation Tables_reshape",
    "section": "Block",
    "text": "Block\n\npd.merge(data.query(\"dataset=='fivenodes' and mtype=='block'\").groupby(['mrate','nof_filters','method'])['mse'].mean().reset_index(),\n         data.query(\"dataset=='fivenodes' and mtype=='block'\").groupby(['mrate','nof_filters','method'])['mse'].std().reset_index(),\n         on=['method','nof_filters','mrate']).rename(columns={'mse_x':'mean','mse_y':'std'}).round(3)\n\n\n\n\n\n\n\n\nmrate\nnof_filters\nmethod\nmean\nstd\n\n\n\n\n0\n0.125\n12\nIT-STGCN\n1.126\n0.033\n\n\n1\n0.125\n12\nSTGCN\n1.154\n0.040"
  },
  {
    "objectID": "posts/2_research/2023-06-28-DYGRENCODER_simulation_table_reshape.html#baseline-1",
    "href": "posts/2_research/2023-06-28-DYGRENCODER_simulation_table_reshape.html#baseline-1",
    "title": "DYGRENCODER_Simulation Tables_reshape",
    "section": "Baseline",
    "text": "Baseline\n\npd.merge(data.query(\"dataset=='chickenpox' and mtype!='rand' and mtype!='block'\").groupby(['nof_filters','method'])['mse'].mean().reset_index(),\n         data.query(\"dataset=='chickenpox' and mtype!='rand' and mtype!='block'\").groupby(['nof_filters','method'])['mse'].std().reset_index(),\n         on=['method','nof_filters']).rename(columns={'mse_x':'mean','mse_y':'std'}).round(3)\n\n\n\n\n\n\n\n\nnof_filters\nmethod\nmean\nstd\n\n\n\n\n0\n12\nIT-STGCN\n0.910\n0.042\n\n\n1\n12\nSTGCN\n0.902\n0.058"
  },
  {
    "objectID": "posts/2_research/2023-06-28-DYGRENCODER_simulation_table_reshape.html#random-1",
    "href": "posts/2_research/2023-06-28-DYGRENCODER_simulation_table_reshape.html#random-1",
    "title": "DYGRENCODER_Simulation Tables_reshape",
    "section": "Random",
    "text": "Random\n\npd.merge(data.query(\"dataset=='chickenpox' and mtype=='rand'\").groupby(['mrate','inter_method','nof_filters','method'])['mse'].mean().reset_index(),\n         data.query(\"dataset=='chickenpox' and mtype=='rand'\").groupby(['mrate','inter_method','nof_filters','method'])['mse'].std().reset_index(),\n         on=['method','inter_method','mrate','nof_filters']).rename(columns={'mse_x':'mean','mse_y':'std'}).round(3)\n\n\n\n\n\n\n\n\nmrate\ninter_method\nnof_filters\nmethod\nmean\nstd\n\n\n\n\n0\n0.3\nlinear\n12\nIT-STGCN\n0.868\n0.028\n\n\n1\n0.3\nlinear\n12\nSTGCN\n1.080\n0.037\n\n\n2\n0.8\nlinear\n12\nIT-STGCN\n1.399\n0.063\n\n\n3\n0.8\nlinear\n12\nSTGCN\n2.127\n0.240"
  },
  {
    "objectID": "posts/2_research/2023-06-28-DYGRENCODER_simulation_table_reshape.html#block-1",
    "href": "posts/2_research/2023-06-28-DYGRENCODER_simulation_table_reshape.html#block-1",
    "title": "DYGRENCODER_Simulation Tables_reshape",
    "section": "Block",
    "text": "Block\n\npd.merge(data.query(\"dataset=='chickenpox' and mtype=='block'\").groupby(['inter_method','mrate','nof_filters','method'])['mse'].mean().reset_index(),\n         data.query(\"dataset=='chickenpox' and mtype=='block'\").groupby(['inter_method','mrate','nof_filters','method'])['mse'].std().reset_index(),\n         on=['method','inter_method','mrate','nof_filters']).rename(columns={'mse_x':'mean','mse_y':'std'})\n\n\n\n\n\n\n\n\ninter_method\nmrate\nnof_filters\nmethod\nmean\nstd\n\n\n\n\n0\nlinear\n0.28777\n12\nIT-STGCN\n0.898825\n0.034600\n\n\n1\nlinear\n0.28777\n12\nSTGCN\n0.912353\n0.043433\n\n\n2\nnearest\n0.28777\n12\nIT-STGCN\n0.908500\n0.043167\n\n\n3\nnearest\n0.28777\n12\nSTGCN\n0.930462\n0.035268"
  },
  {
    "objectID": "posts/2_research/2023-06-28-DYGRENCODER_simulation_table_reshape.html#baseline-2",
    "href": "posts/2_research/2023-06-28-DYGRENCODER_simulation_table_reshape.html#baseline-2",
    "title": "DYGRENCODER_Simulation Tables_reshape",
    "section": "Baseline",
    "text": "Baseline\n\npd.merge(data.query(\"dataset=='pedalme' and mtype!='rand' and mtype!='block'\").groupby(['lags','nof_filters','method'])['mse'].mean().reset_index(),\n         data.query(\"dataset=='pedalme' and mtype!='rand' and mtype!='block'\").groupby(['lags','nof_filters','method'])['mse'].std().reset_index(),\n         on=['method','lags','nof_filters']).rename(columns={'mse_x':'mean','mse_y':'std'}).round(3).query(\"lags==4\")\n\n\n\n\n\n\n\n\nlags\nnof_filters\nmethod\nmean\nstd\n\n\n\n\n0\n4\n12\nIT-STGCN\n1.197\n0.052\n\n\n1\n4\n12\nSTGCN\n1.182\n0.041"
  },
  {
    "objectID": "posts/2_research/2023-06-28-DYGRENCODER_simulation_table_reshape.html#random-2",
    "href": "posts/2_research/2023-06-28-DYGRENCODER_simulation_table_reshape.html#random-2",
    "title": "DYGRENCODER_Simulation Tables_reshape",
    "section": "Random",
    "text": "Random\n\npd.merge(data.query(\"dataset=='pedalme' and mtype=='rand'\").groupby(['mrate','lags','inter_method','method'])['mse'].mean().reset_index(),\n         data.query(\"dataset=='pedalme' and mtype=='rand'\").groupby(['mrate','lags','inter_method','method'])['mse'].std().reset_index(),\n         on=['method','mrate','lags','inter_method']).rename(columns={'mse_x':'mean','mse_y':'std'}).round(3)\n\n\n\n\n\n\n\n\nmrate\nlags\ninter_method\nmethod\nmean\nstd\n\n\n\n\n0\n0.3\n4\nlinear\nIT-STGCN\n1.207\n0.046\n\n\n1\n0.3\n4\nlinear\nSTGCN\n1.279\n0.061\n\n\n2\n0.3\n4\nnearest\nIT-STGCN\n1.205\n0.075\n\n\n3\n0.3\n4\nnearest\nSTGCN\n1.289\n0.096\n\n\n4\n0.6\n4\nlinear\nIT-STGCN\n1.294\n0.056\n\n\n5\n0.6\n4\nlinear\nSTGCN\n1.526\n0.078\n\n\n6\n0.6\n4\nnearest\nIT-STGCN\n1.285\n0.051\n\n\n7\n0.6\n4\nnearest\nSTGCN\n1.513\n0.083"
  },
  {
    "objectID": "posts/2_research/2023-06-28-DYGRENCODER_simulation_table_reshape.html#block-2",
    "href": "posts/2_research/2023-06-28-DYGRENCODER_simulation_table_reshape.html#block-2",
    "title": "DYGRENCODER_Simulation Tables_reshape",
    "section": "Block",
    "text": "Block\n\npd.merge(data.query(\"dataset=='pedalme' and mtype=='block'\").groupby(['mrate','lags','inter_method','method'])['mse'].mean().reset_index(),\n         data.query(\"dataset=='pedalme' and mtype=='block'\").groupby(['mrate','lags','inter_method','method'])['mse'].std().reset_index(),\n         on=['method','mrate','lags','inter_method']).rename(columns={'mse_x':'mean','mse_y':'std'}).round(3).query(\"lags==4\")\n\n\n\n\n\n\n\n\nmrate\nlags\ninter_method\nmethod\nmean\nstd\n\n\n\n\n0\n0.286\n4\nlinear\nIT-STGCN\n1.167\n0.040\n\n\n1\n0.286\n4\nlinear\nSTGCN\n1.222\n0.054\n\n\n2\n0.286\n4\nnearest\nIT-STGCN\n1.165\n0.032\n\n\n3\n0.286\n4\nnearest\nSTGCN\n1.269\n0.066"
  },
  {
    "objectID": "posts/2_research/2023-06-28-DYGRENCODER_simulation_table_reshape.html#w_st",
    "href": "posts/2_research/2023-06-28-DYGRENCODER_simulation_table_reshape.html#w_st",
    "title": "DYGRENCODER_Simulation Tables_reshape",
    "section": "W_st",
    "text": "W_st\n\npd.merge(data_pedal2.query(\"mtype=='rand'\").groupby(['mrate','lags','inter_method','method'])['mse'].mean().reset_index(),\n         data_pedal2.query(\"mtype=='rand'\").groupby(['mrate','lags','inter_method','method'])['mse'].std().reset_index(),\n         on=['method','mrate','lags','inter_method']).rename(columns={'mse_x':'mean','mse_y':'std'}).round(3).query(\"lags==4\")\n\n\n\n\n\n\n\n\nmrate\nlags\ninter_method\nmethod\nmean\nstd\n\n\n\n\n0\n0.3\n4\nlinear\nIT-STGCN\n1.222\n0.083\n\n\n1\n0.3\n4\nlinear\nSTGCN\n1.276\n0.058\n\n\n2\n0.3\n4\nnearest\nIT-STGCN\n1.208\n0.091\n\n\n3\n0.3\n4\nnearest\nSTGCN\n1.281\n0.068\n\n\n4\n0.6\n4\nlinear\nIT-STGCN\n1.287\n0.095\n\n\n5\n0.6\n4\nlinear\nSTGCN\n1.497\n0.077\n\n\n6\n0.6\n4\nnearest\nIT-STGCN\n1.305\n0.131\n\n\n7\n0.6\n4\nnearest\nSTGCN\n1.513\n0.073\n\n\n\n\n\n\n\n\npd.merge(data_pedal2.query(\"mtype=='block'\").groupby(['mrate','lags','inter_method','method'])['mse'].mean().reset_index(),\n         data_pedal2.query(\"mtype=='block'\").groupby(['mrate','lags','inter_method','method'])['mse'].std().reset_index(),\n         on=['method','mrate','lags','inter_method']).rename(columns={'mse_x':'mean','mse_y':'std'}).round(3).query(\"lags==4\")\n\n\n\n\n\n\n\n\nmrate\nlags\ninter_method\nmethod\nmean\nstd\n\n\n\n\n0\n0.286\n4\nlinear\nIT-STGCN\n1.196\n0.055\n\n\n1\n0.286\n4\nlinear\nSTGCN\n1.224\n0.037\n\n\n2\n0.286\n4\nnearest\nIT-STGCN\n1.204\n0.063\n\n\n3\n0.286\n4\nnearest\nSTGCN\n1.246\n0.043"
  },
  {
    "objectID": "posts/2_research/2023-06-28-DYGRENCODER_simulation_table_reshape.html#baseline-3",
    "href": "posts/2_research/2023-06-28-DYGRENCODER_simulation_table_reshape.html#baseline-3",
    "title": "DYGRENCODER_Simulation Tables_reshape",
    "section": "Baseline",
    "text": "Baseline\n\npd.merge(data.query(\"dataset=='wikimath' and mrate==0\").groupby(['lags','nof_filters','method'])['mse'].mean().reset_index(),\n         data.query(\"dataset=='wikimath' and mrate==0\").groupby(['lags','nof_filters','method'])['mse'].std().reset_index(),\n         on=['lags','nof_filters','method']).rename(columns={'mse_x':'mean','mse_y':'std'}).round(3)\n\n\n\n\n\n\n\n\nlags\nnof_filters\nmethod\nmean\nstd\n\n\n\n\n0\n8\n12\nIT-STGCN\n0.563\n0.030\n\n\n1\n8\n12\nSTGCN\n0.560\n0.029"
  },
  {
    "objectID": "posts/2_research/2023-06-28-DYGRENCODER_simulation_table_reshape.html#random-3",
    "href": "posts/2_research/2023-06-28-DYGRENCODER_simulation_table_reshape.html#random-3",
    "title": "DYGRENCODER_Simulation Tables_reshape",
    "section": "Random",
    "text": "Random\n\npd.merge(data.query(\"dataset=='wikimath' and mtype=='rand'\").groupby(['mrate','lags','method'])['mse'].mean().reset_index(),\n         data.query(\"dataset=='wikimath' and mtype=='rand'\").groupby(['mrate','lags','method'])['mse'].std().reset_index(),\n         on=['method','mrate','lags']).rename(columns={'mse_x':'mean','mse_y':'std'}).round(3)\n\n\n\n\n\n\n\n\nmrate\nlags\nmethod\nmean\nstd\n\n\n\n\n0\n0.3\n8\nIT-STGCN\n0.578\n0.031\n\n\n1\n0.3\n8\nSTGCN\n0.562\n0.016\n\n\n2\n0.5\n8\nIT-STGCN\n0.565\n0.024\n\n\n3\n0.5\n8\nSTGCN\n0.614\n0.024\n\n\n4\n0.6\n8\nIT-STGCN\n0.566\n0.021\n\n\n5\n0.6\n8\nSTGCN\n0.644\n0.032\n\n\n6\n0.8\n8\nIT-STGCN\n0.606\n0.017\n\n\n7\n0.8\n8\nSTGCN\n0.770\n0.045"
  },
  {
    "objectID": "posts/2_research/2023-06-28-DYGRENCODER_simulation_table_reshape.html#block-3",
    "href": "posts/2_research/2023-06-28-DYGRENCODER_simulation_table_reshape.html#block-3",
    "title": "DYGRENCODER_Simulation Tables_reshape",
    "section": "Block",
    "text": "Block\n\npd.merge(data.query(\"dataset=='wikimath' and mtype=='block'\").groupby(['mrate','lags','method'])['mse'].mean().reset_index(),\n         data.query(\"dataset=='wikimath' and mtype=='block'\").groupby(['mrate','lags','method'])['mse'].std().reset_index(),\n         on=['method','mrate','lags']).rename(columns={'mse_x':'mean','mse_y':'std'})\n\n\n\n\n\n\n\n\nmrate\nlags\nmethod\nmean\nstd\n\n\n\n\n0\n0.119837\n8\nIT-STGCN\n0.563498\n0.024983\n\n\n1\n0.119837\n8\nSTGCN\n0.546224\n0.015992"
  },
  {
    "objectID": "posts/2_research/2023-06-28-DYGRENCODER_simulation_table_reshape.html#missing-values-on-the-same-nodes",
    "href": "posts/2_research/2023-06-28-DYGRENCODER_simulation_table_reshape.html#missing-values-on-the-same-nodes",
    "title": "DYGRENCODER_Simulation Tables_reshape",
    "section": "missing values on the same nodes",
    "text": "missing values on the same nodes\n\npd.merge(data_wiki_GSO.groupby(['mrate','lags','method'])['mse'].mean().reset_index(),\n        data_wiki_GSO.groupby(['mrate','lags','method'])['mse'].std().reset_index(),\n         on=['method','mrate','lags']).rename(columns={'mse_x':'mean','mse_y':'std'}).round(3)\n\n\n\n\n\n\n\n\nmrate\nlags\nmethod\nmean\nstd\n\n\n\n\n0\n0.512\n8\nIT-STGCN\n0.561\n0.031\n\n\n1\n0.512\n8\nSTGCN\n0.626\n0.027"
  },
  {
    "objectID": "posts/2_research/2023-06-28-DYGRENCODER_simulation_table_reshape.html#baseline-4",
    "href": "posts/2_research/2023-06-28-DYGRENCODER_simulation_table_reshape.html#baseline-4",
    "title": "DYGRENCODER_Simulation Tables_reshape",
    "section": "Baseline",
    "text": "Baseline\n\npd.merge(data.query(\"dataset=='windmillsmall' and mrate==0\").groupby(['lags','method'])['mse'].mean().reset_index(),\n         data.query(\"dataset=='windmillsmall' and mrate==0\").groupby(['lags','method'])['mse'].std().reset_index(),\n         on=['method','lags']).rename(columns={'mse_x':'mean','mse_y':'std'}).round(3)\n\n\n\n\n\n\n\n\nlags\nmethod\nmean\nstd\n\n\n\n\n0\n8\nIT-STGCN\n0.989\n0.010\n\n\n1\n8\nSTGCN\n0.987\n0.005"
  },
  {
    "objectID": "posts/2_research/2023-06-28-DYGRENCODER_simulation_table_reshape.html#random-4",
    "href": "posts/2_research/2023-06-28-DYGRENCODER_simulation_table_reshape.html#random-4",
    "title": "DYGRENCODER_Simulation Tables_reshape",
    "section": "Random",
    "text": "Random\n\npd.merge(data.query(\"dataset=='windmillsmall' and mtype=='rand'\").groupby(['mrate','lags','method'])['mse'].mean().reset_index(),\n         data.query(\"dataset=='windmillsmall' and mtype=='rand'\").groupby(['mrate','lags','method'])['mse'].std().reset_index(),\n         on=['method','mrate','lags']).rename(columns={'mse_x':'mean','mse_y':'std'}).round(3)\n\n\n\n\n\n\n\n\nmrate\nlags\nmethod\nmean\nstd\n\n\n\n\n0\n0.7\n8\nIT-STGCN\n1.127\n0.009\n\n\n1\n0.7\n8\nSTGCN\n1.709\n0.074"
  },
  {
    "objectID": "posts/2_research/2023-06-28-DYGRENCODER_simulation_table_reshape.html#block-4",
    "href": "posts/2_research/2023-06-28-DYGRENCODER_simulation_table_reshape.html#block-4",
    "title": "DYGRENCODER_Simulation Tables_reshape",
    "section": "Block",
    "text": "Block\n\npd.merge(data.query(\"dataset=='windmillsmall' and mtype=='block'\").groupby(['mrate','lags','method'])['mse'].mean().reset_index(),\n         data.query(\"dataset=='windmillsmall' and mtype=='block'\").groupby(['mrate','lags','method'])['mse'].std().reset_index(),\n         on=['method','mrate','lags']).rename(columns={'mse_x':'mean','mse_y':'std'}).round(3)\n\n\n\n\n\n\n\n\nmrate\nlags\nmethod\nmean\nstd\n\n\n\n\n0\n0.081\n8\nIT-STGCN\n0.985\n0.005\n\n\n1\n0.081\n8\nSTGCN\n0.985\n0.003"
  },
  {
    "objectID": "posts/2_research/2023-06-28-DYGRENCODER_simulation_table_reshape.html#baseline-5",
    "href": "posts/2_research/2023-06-28-DYGRENCODER_simulation_table_reshape.html#baseline-5",
    "title": "DYGRENCODER_Simulation Tables_reshape",
    "section": "Baseline",
    "text": "Baseline\n\npd.merge(data.query(\"dataset=='monte' and mrate==0\").groupby(['lags','method'])['mse'].mean().reset_index(),\n         data.query(\"dataset=='monte' and mrate==0\").groupby(['lags','method'])['mse'].std().reset_index(),\n         on=['method','lags']).rename(columns={'mse_x':'mean','mse_y':'std'}).round(3)\n\n\n\n\n\n\n\n\nlags\nmethod\nmean\nstd\n\n\n\n\n0\n4\nIT-STGCN\n1.008\n0.044\n\n\n1\n4\nSTGCN\n0.983\n0.011"
  },
  {
    "objectID": "posts/2_research/2023-06-28-DYGRENCODER_simulation_table_reshape.html#random-5",
    "href": "posts/2_research/2023-06-28-DYGRENCODER_simulation_table_reshape.html#random-5",
    "title": "DYGRENCODER_Simulation Tables_reshape",
    "section": "Random",
    "text": "Random\n\npd.merge(data.query(\"dataset=='monte' and mtype=='rand'\").groupby(['mrate','lags','inter_method','method'])['mse'].mean().reset_index(),\n         data.query(\"dataset=='monte' and mtype=='rand'\").groupby(['mrate','lags','inter_method','method'])['mse'].std().reset_index(),\n         on=['mrate','inter_method','method','mrate','lags']).rename(columns={'mse_x':'mean','mse_y':'std'})\n\n\n\n\n\n\n\n\nmrate\nlags\ninter_method\nmethod\nmean\nstd\n\n\n\n\n0\n0.3\n4\nnearest\nIT-STGCN\n1.026822\n0.058198\n\n\n1\n0.3\n4\nnearest\nSTGCN\n1.173290\n0.108070\n\n\n2\n0.5\n4\nnearest\nIT-STGCN\n1.080011\n0.068523\n\n\n3\n0.5\n4\nnearest\nSTGCN\n1.285871\n0.139744\n\n\n4\n0.7\n4\nnearest\nIT-STGCN\n1.197729\n0.087667\n\n\n5\n0.7\n4\nnearest\nSTGCN\n1.467420\n0.228006\n\n\n6\n0.8\n4\nnearest\nIT-STGCN\n1.215692\n0.117963\n\n\n7\n0.8\n4\nnearest\nSTGCN\n1.357897\n0.148740"
  },
  {
    "objectID": "posts/2_research/2023-06-28-DYGRENCODER_simulation_table_reshape.html#block-5",
    "href": "posts/2_research/2023-06-28-DYGRENCODER_simulation_table_reshape.html#block-5",
    "title": "DYGRENCODER_Simulation Tables_reshape",
    "section": "Block",
    "text": "Block\n\npd.merge(data.query(\"dataset=='monte' and mtype=='block'\").groupby(['mrate','lags','inter_method','method'])['mse'].mean().reset_index(),\n         data.query(\"dataset=='monte' and mtype=='block'\").groupby(['mrate','lags','inter_method','method'])['mse'].std().reset_index(),\n         on=['method','mrate','inter_method','lags']).rename(columns={'mse_x':'mean','mse_y':'std'})\n\n\n\n\n\n\n\n\nmrate\nlags\ninter_method\nmethod\nmean\nstd\n\n\n\n\n0\n0.149142\n4\nnearest\nIT-STGCN\n1.005486\n0.046063\n\n\n1\n0.149142\n4\nnearest\nSTGCN\n1.030326\n0.044023"
  },
  {
    "objectID": "posts/2_research/2023-07-01-EvolveGCNH_Simulation_boxplot_reshape.html",
    "href": "posts/2_research/2023-07-01-EvolveGCNH_Simulation_boxplot_reshape.html",
    "title": "EvolveGCNH_Simulation_reshape",
    "section": "",
    "text": "Simulation Study"
  },
  {
    "objectID": "posts/2_research/2023-07-01-EvolveGCNH_Simulation_boxplot_reshape.html#baseline",
    "href": "posts/2_research/2023-07-01-EvolveGCNH_Simulation_boxplot_reshape.html#baseline",
    "title": "EvolveGCNH_Simulation_reshape",
    "section": "Baseline",
    "text": "Baseline\n\ndata.query(\"method!='GNAR' and mrate==0\").plot.box(backend='plotly',x='mrate',color='method',y='mse',facet_col='lags',facet_row='nof_filters',height=600)"
  },
  {
    "objectID": "posts/2_research/2023-07-01-EvolveGCNH_Simulation_boxplot_reshape.html#random",
    "href": "posts/2_research/2023-07-01-EvolveGCNH_Simulation_boxplot_reshape.html#random",
    "title": "EvolveGCNH_Simulation_reshape",
    "section": "Random",
    "text": "Random\n\ndata.query(\"method!='GNAR' and mtype =='rand'\").plot.box(backend='plotly',x='mrate',color='method',y='mse',facet_col='inter_method',facet_row='nof_filters',height=600)"
  },
  {
    "objectID": "posts/2_research/2023-07-01-EvolveGCNH_Simulation_boxplot_reshape.html#block",
    "href": "posts/2_research/2023-07-01-EvolveGCNH_Simulation_boxplot_reshape.html#block",
    "title": "EvolveGCNH_Simulation_reshape",
    "section": "Block",
    "text": "Block\n\ndata.query(\"method!='GNAR' and mtype =='block' \").plot.box(backend='plotly',x='mrate',color='method',y='mse',facet_col='inter_method',facet_row='nof_filters',height=600)"
  },
  {
    "objectID": "posts/2_research/2023-07-01-EvolveGCNH_Simulation_boxplot_reshape.html#baseline-1",
    "href": "posts/2_research/2023-07-01-EvolveGCNH_Simulation_boxplot_reshape.html#baseline-1",
    "title": "EvolveGCNH_Simulation_reshape",
    "section": "Baseline",
    "text": "Baseline\n\ndata.query(\"method!='GNAR' and mrate ==0 \").plot.box(backend='plotly',x='mrate',color='method',y='mse',facet_col='nof_filters',height=600)"
  },
  {
    "objectID": "posts/2_research/2023-07-01-EvolveGCNH_Simulation_boxplot_reshape.html#random-1",
    "href": "posts/2_research/2023-07-01-EvolveGCNH_Simulation_boxplot_reshape.html#random-1",
    "title": "EvolveGCNH_Simulation_reshape",
    "section": "Random",
    "text": "Random\n\ndata.query(\"method!='GNAR' and mtype =='rand'\").plot.box(backend='plotly',x='mrate',color='method',y='mse',facet_row='inter_method',height=800)"
  },
  {
    "objectID": "posts/2_research/2023-07-01-EvolveGCNH_Simulation_boxplot_reshape.html#block-1",
    "href": "posts/2_research/2023-07-01-EvolveGCNH_Simulation_boxplot_reshape.html#block-1",
    "title": "EvolveGCNH_Simulation_reshape",
    "section": "Block",
    "text": "Block\n\ndata.query(\"method!='GNAR' and mtype =='block'\").plot.box(backend='plotly',x='mrate',color='method',y='mse',facet_col='inter_method',facet_row='lags',height=800)"
  },
  {
    "objectID": "posts/2_research/2023-07-01-EvolveGCNH_Simulation_boxplot_reshape.html#baseline-2",
    "href": "posts/2_research/2023-07-01-EvolveGCNH_Simulation_boxplot_reshape.html#baseline-2",
    "title": "EvolveGCNH_Simulation_reshape",
    "section": "Baseline",
    "text": "Baseline\n\ndata.query(\"mrate ==0 and lags!=2\").plot.box(backend='plotly',x='epoch',color='method',y='mse',facet_col='nof_filters',facet_row='lags',height=400)"
  },
  {
    "objectID": "posts/2_research/2023-07-01-EvolveGCNH_Simulation_boxplot_reshape.html#random-2",
    "href": "posts/2_research/2023-07-01-EvolveGCNH_Simulation_boxplot_reshape.html#random-2",
    "title": "EvolveGCNH_Simulation_reshape",
    "section": "random",
    "text": "random\n\ndata.query(\"method!='GNAR' and mtype =='rand' and lags!=2\").plot.box(backend='plotly',x='mrate',color='method',y='mse',facet_row='inter_method',height=800)"
  },
  {
    "objectID": "posts/2_research/2023-07-01-EvolveGCNH_Simulation_boxplot_reshape.html#block-2",
    "href": "posts/2_research/2023-07-01-EvolveGCNH_Simulation_boxplot_reshape.html#block-2",
    "title": "EvolveGCNH_Simulation_reshape",
    "section": "block",
    "text": "block\n\ndata.query(\"method!='GNAR' and mtype =='block' and lags!=2 \").plot.box(backend='plotly',x='mrate',color='method',y='mse',facet_col='nof_filters',facet_row='inter_method',height=800)"
  },
  {
    "objectID": "posts/2_research/2023-07-01-EvolveGCNH_Simulation_boxplot_reshape.html#weight-matrix-time-node-고려한-결과",
    "href": "posts/2_research/2023-07-01-EvolveGCNH_Simulation_boxplot_reshape.html#weight-matrix-time-node-고려한-결과",
    "title": "EvolveGCNH_Simulation_reshape",
    "section": "weight matrix time, node 고려한 결과",
    "text": "weight matrix time, node 고려한 결과\n\ndf1 = pd.read_csv('./simulation_results/2023-07-02_07-01-12.csv')\ndf2 = pd.read_csv('./simulation_results/2023-07-02_07-19-21.csv')\n\n\ndata2 = pd.concat([df1,df2],axis=0)\n\n\ndata2.to_csv('./simulation_results/Real_simulation_reshape/EvolveGCNH_pedalme_Simulation_itstgcnsnd.csv',index=False)\n\n\ndata2 = pd.read_csv('./simulation_results/Real_simulation_reshape/EvolveGCNH_pedalme_Simulation_itstgcnsnd.csv')\n\n\ndata2.query(\"mtype=='rand'\").plot.box(backend='plotly',x='mrate',color='method',y='mse',facet_col='inter_method',facet_row='nof_filters',height=1000)\n\n                                                \n\n\n\ndata2.query(\"mtype=='block'\").plot.box(backend='plotly',x='mrate',color='method',y='mse',facet_col='inter_method',facet_row='nof_filters',height=1000)"
  },
  {
    "objectID": "posts/2_research/2023-07-01-EvolveGCNH_Simulation_boxplot_reshape.html#baseline-3",
    "href": "posts/2_research/2023-07-01-EvolveGCNH_Simulation_boxplot_reshape.html#baseline-3",
    "title": "EvolveGCNH_Simulation_reshape",
    "section": "Baseline",
    "text": "Baseline\n\ndata.query(\"mrate==0 and method!='GNAR'\").plot.box(backend='plotly',x='mrate',color='method',y='mse',height=600)"
  },
  {
    "objectID": "posts/2_research/2023-07-01-EvolveGCNH_Simulation_boxplot_reshape.html#random-3",
    "href": "posts/2_research/2023-07-01-EvolveGCNH_Simulation_boxplot_reshape.html#random-3",
    "title": "EvolveGCNH_Simulation_reshape",
    "section": "random",
    "text": "random\n\ndata.query(\"mtype=='rand' and mrate !=0 and method!='GNAR'\").plot.box(backend='plotly',x='mrate',color='method',y='mse',facet_col='lags',facet_row='nof_filters',height=800)"
  },
  {
    "objectID": "posts/2_research/2023-07-01-EvolveGCNH_Simulation_boxplot_reshape.html#block-3",
    "href": "posts/2_research/2023-07-01-EvolveGCNH_Simulation_boxplot_reshape.html#block-3",
    "title": "EvolveGCNH_Simulation_reshape",
    "section": "block",
    "text": "block\n\ndata.query(\"mtype=='block' and method!='GNAR'\").plot.box(backend='plotly',x='mrate',color='method',y='mse',facet_col='lags',facet_row='nof_filters',height=1200)"
  },
  {
    "objectID": "posts/2_research/2023-07-01-EvolveGCNH_Simulation_boxplot_reshape.html#missing-values-on-the-same-nodes",
    "href": "posts/2_research/2023-07-01-EvolveGCNH_Simulation_boxplot_reshape.html#missing-values-on-the-same-nodes",
    "title": "EvolveGCNH_Simulation_reshape",
    "section": "missing values on the same nodes",
    "text": "missing values on the same nodes\n\ndf1 = pd.read_csv('./simulation_results/2023-07-03_13-39-23.csv') # STGCN IT-STGCN block\ndf2 = pd.read_csv('./simulation_results/2023-07-03_16-17-40.csv') # STGCN IT-STGCN\ndf3 = pd.read_csv('./simulation_results/2023-07-03_19-00-05.csv') \n\n\ndata = pd.concat([df1,df2,df3],axis=0)\n\n\ndata.to_csv('./simulation_results/Real_simulation_reshape/EvolveGCNH_wikimath_GSO_st.csv',index=False)\n\n\ndata = pd.read_csv('./simulation_results/Real_simulation_reshape/EvolveGCNH_wikimath_GSO_st.csv')\n\n\ndata.query(\"method!='GNAR' and mtype =='block' \").plot.box(backend='plotly',x='mrate',color='method',y='mse',facet_col='inter_method',facet_row='lags',height=800)"
  },
  {
    "objectID": "posts/2_research/2023-07-01-EvolveGCNH_Simulation_boxplot_reshape.html#baseline-4",
    "href": "posts/2_research/2023-07-01-EvolveGCNH_Simulation_boxplot_reshape.html#baseline-4",
    "title": "EvolveGCNH_Simulation_reshape",
    "section": "Baseline",
    "text": "Baseline\n\ndata.query(\"method!='GNAR' and mrate ==0 \").plot.box(backend='plotly',x='mrate',color='method',y='mse',facet_col='lags',height=800)"
  },
  {
    "objectID": "posts/2_research/2023-07-01-EvolveGCNH_Simulation_boxplot_reshape.html#random-4",
    "href": "posts/2_research/2023-07-01-EvolveGCNH_Simulation_boxplot_reshape.html#random-4",
    "title": "EvolveGCNH_Simulation_reshape",
    "section": "random",
    "text": "random\n\ndata.query(\"method!='GNAR' and mtype =='rand' \").plot.box(backend='plotly',x='mrate',color='method',y='mse',facet_col='lags',facet_row='inter_method',height=800)"
  },
  {
    "objectID": "posts/2_research/2023-07-01-EvolveGCNH_Simulation_boxplot_reshape.html#block-4",
    "href": "posts/2_research/2023-07-01-EvolveGCNH_Simulation_boxplot_reshape.html#block-4",
    "title": "EvolveGCNH_Simulation_reshape",
    "section": "block",
    "text": "block\n\ndata.query(\"method!='GNAR' and mtype =='block' \").plot.box(backend='plotly',x='mrate',color='method',y='mse',facet_col='lags',facet_row='inter_method',height=800)"
  },
  {
    "objectID": "posts/2_research/2023-07-01-EvolveGCNH_Simulation_boxplot_reshape.html#baseline-5",
    "href": "posts/2_research/2023-07-01-EvolveGCNH_Simulation_boxplot_reshape.html#baseline-5",
    "title": "EvolveGCNH_Simulation_reshape",
    "section": "Baseline",
    "text": "Baseline\n\ndata.query(\"mrate==0 and method!='GNAR'\").plot.box(backend='plotly',x='mrate',color='method',y='mse',facet_col='lags',facet_row='nof_filters',height=600)"
  },
  {
    "objectID": "posts/2_research/2023-07-01-EvolveGCNH_Simulation_boxplot_reshape.html#random-5",
    "href": "posts/2_research/2023-07-01-EvolveGCNH_Simulation_boxplot_reshape.html#random-5",
    "title": "EvolveGCNH_Simulation_reshape",
    "section": "random",
    "text": "random\n\ndata.query(\"mtype=='rand' and mrate !=0 and method!='GNAR'\").plot.box(backend='plotly',x='mrate',color='method',y='mse',facet_col='lags',facet_row='inter_method',height=800)"
  },
  {
    "objectID": "posts/2_research/2023-07-01-EvolveGCNH_Simulation_boxplot_reshape.html#block-5",
    "href": "posts/2_research/2023-07-01-EvolveGCNH_Simulation_boxplot_reshape.html#block-5",
    "title": "EvolveGCNH_Simulation_reshape",
    "section": "block",
    "text": "block\n\ndata.query(\"mtype=='block' and method!='GNAR'\").plot.box(backend='plotly',x='mrate',color='method',y='mse',facet_col='lags',facet_row='nof_filters',height=1200)"
  },
  {
    "objectID": "posts/2_research/2023-07-01-EvolveGCNH_simulation_table_reshape.html",
    "href": "posts/2_research/2023-07-01-EvolveGCNH_simulation_table_reshape.html",
    "title": "EvolveGCNH_Simulation Tables_reshape",
    "section": "",
    "text": "Simulation Tables"
  },
  {
    "objectID": "posts/2_research/2023-07-01-EvolveGCNH_simulation_table_reshape.html#baseline",
    "href": "posts/2_research/2023-07-01-EvolveGCNH_simulation_table_reshape.html#baseline",
    "title": "EvolveGCNH_Simulation Tables_reshape",
    "section": "Baseline",
    "text": "Baseline\n\npd.merge(data.query(\"dataset=='fivenodes' and mtype!='rand' and mtype!='block'\").groupby(['nof_filters','method','lags'])['mse'].mean().reset_index(),\n         data.query(\"dataset=='fivenodes' and mtype!='rand' and mtype!='block'\").groupby(['nof_filters','method','lags'])['mse'].std().reset_index(),\n         on=['method','nof_filters','lags']).rename(columns={'mse_x':'mean','mse_y':'std'}).round(3)\n\n\n\n\n\n\n\n\nnof_filters\nmethod\nlags\nmean\nstd\n\n\n\n\n0\n12\nIT-STGCN\n2\n1.174\n0.074\n\n\n1\n12\nSTGCN\n2\n1.175\n0.062"
  },
  {
    "objectID": "posts/2_research/2023-07-01-EvolveGCNH_simulation_table_reshape.html#random",
    "href": "posts/2_research/2023-07-01-EvolveGCNH_simulation_table_reshape.html#random",
    "title": "EvolveGCNH_Simulation Tables_reshape",
    "section": "Random",
    "text": "Random\n\npd.merge(data.query(\"dataset=='fivenodes' and mtype=='rand'\").groupby(['mrate','nof_filters','method','lags'])['mse'].mean().reset_index(),\n         data.query(\"dataset=='fivenodes' and mtype=='rand'\").groupby(['mrate','nof_filters','method','lags'])['mse'].std().reset_index(),\n         on=['method','nof_filters','mrate','lags']).rename(columns={'mse_x':'mean','mse_y':'std'}).round(3)\n\n\n\n\n\n\n\n\nmrate\nnof_filters\nmethod\nlags\nmean\nstd\n\n\n\n\n0\n0.7\n12\nIT-STGCN\n2\n1.197\n0.057\n\n\n1\n0.7\n12\nSTGCN\n2\n1.217\n0.064\n\n\n2\n0.8\n12\nIT-STGCN\n2\n1.210\n0.060\n\n\n3\n0.8\n12\nSTGCN\n2\n1.217\n0.060"
  },
  {
    "objectID": "posts/2_research/2023-07-01-EvolveGCNH_simulation_table_reshape.html#block",
    "href": "posts/2_research/2023-07-01-EvolveGCNH_simulation_table_reshape.html#block",
    "title": "EvolveGCNH_Simulation Tables_reshape",
    "section": "Block",
    "text": "Block\n\npd.merge(data.query(\"dataset=='fivenodes' and mtype=='block'\").groupby(['mrate','nof_filters','method'])['mse'].mean().reset_index(),\n         data.query(\"dataset=='fivenodes' and mtype=='block'\").groupby(['mrate','nof_filters','method'])['mse'].std().reset_index(),\n         on=['method','nof_filters','mrate']).rename(columns={'mse_x':'mean','mse_y':'std'}).round(3)\n\n\n\n\n\n\n\n\nmrate\nnof_filters\nmethod\nmean\nstd\n\n\n\n\n0\n0.125\n12\nIT-STGCN\n1.170\n0.055\n\n\n1\n0.125\n12\nSTGCN\n1.189\n0.060"
  },
  {
    "objectID": "posts/2_research/2023-07-01-EvolveGCNH_simulation_table_reshape.html#baseline-1",
    "href": "posts/2_research/2023-07-01-EvolveGCNH_simulation_table_reshape.html#baseline-1",
    "title": "EvolveGCNH_Simulation Tables_reshape",
    "section": "Baseline",
    "text": "Baseline\n\npd.merge(data.query(\"dataset=='chickenpox' and mtype!='rand' and mtype!='block'\").groupby(['nof_filters','method'])['mse'].mean().reset_index(),\n         data.query(\"dataset=='chickenpox' and mtype!='rand' and mtype!='block'\").groupby(['nof_filters','method'])['mse'].std().reset_index(),\n         on=['method','nof_filters']).rename(columns={'mse_x':'mean','mse_y':'std'}).round(3)\n\n\n\n\n\n\n\n\nnof_filters\nmethod\nmean\nstd\n\n\n\n\n0\n32\nIT-STGCN\n0.996\n0.019\n\n\n1\n32\nSTGCN\n1.004\n0.021"
  },
  {
    "objectID": "posts/2_research/2023-07-01-EvolveGCNH_simulation_table_reshape.html#random-1",
    "href": "posts/2_research/2023-07-01-EvolveGCNH_simulation_table_reshape.html#random-1",
    "title": "EvolveGCNH_Simulation Tables_reshape",
    "section": "Random",
    "text": "Random\n\npd.merge(data.query(\"dataset=='chickenpox' and mtype=='rand'\").groupby(['mrate','inter_method','nof_filters','method'])['mse'].mean().reset_index(),\n         data.query(\"dataset=='chickenpox' and mtype=='rand'\").groupby(['mrate','inter_method','nof_filters','method'])['mse'].std().reset_index(),\n         on=['method','inter_method','mrate','nof_filters']).rename(columns={'mse_x':'mean','mse_y':'std'}).round(3)\n\n\n\n\n\n\n\n\nmrate\ninter_method\nnof_filters\nmethod\nmean\nstd\n\n\n\n\n0\n0.3\nlinear\n32\nIT-STGCN\n1.011\n0.019\n\n\n1\n0.3\nlinear\n32\nSTGCN\n1.058\n0.015\n\n\n2\n0.8\nlinear\n32\nIT-STGCN\n1.140\n0.042\n\n\n3\n0.8\nlinear\n32\nSTGCN\n1.203\n0.061"
  },
  {
    "objectID": "posts/2_research/2023-07-01-EvolveGCNH_simulation_table_reshape.html#block-1",
    "href": "posts/2_research/2023-07-01-EvolveGCNH_simulation_table_reshape.html#block-1",
    "title": "EvolveGCNH_Simulation Tables_reshape",
    "section": "Block",
    "text": "Block\n\npd.merge(data.query(\"dataset=='chickenpox' and mtype=='block'\").groupby(['inter_method','mrate','nof_filters','method'])['mse'].mean().reset_index(),\n         data.query(\"dataset=='chickenpox' and mtype=='block'\").groupby(['inter_method','mrate','nof_filters','method'])['mse'].std().reset_index(),\n         on=['method','inter_method','mrate','nof_filters']).rename(columns={'mse_x':'mean','mse_y':'std'})\n\n\n\n\n\n\n\n\ninter_method\nmrate\nnof_filters\nmethod\nmean\nstd\n\n\n\n\n0\nlinear\n0.28777\n32\nIT-STGCN\n1.007400\n0.020847\n\n\n1\nlinear\n0.28777\n32\nSTGCN\n1.027340\n0.022523\n\n\n2\nnearest\n0.28777\n32\nIT-STGCN\n1.011141\n0.017937\n\n\n3\nnearest\n0.28777\n32\nSTGCN\n1.030143\n0.016657"
  },
  {
    "objectID": "posts/2_research/2023-07-01-EvolveGCNH_simulation_table_reshape.html#baseline-2",
    "href": "posts/2_research/2023-07-01-EvolveGCNH_simulation_table_reshape.html#baseline-2",
    "title": "EvolveGCNH_Simulation Tables_reshape",
    "section": "Baseline",
    "text": "Baseline\n\npd.merge(data.query(\"dataset=='pedalme' and mtype!='rand' and mtype!='block'\").groupby(['lags','nof_filters','method'])['mse'].mean().reset_index(),\n         data.query(\"dataset=='pedalme' and mtype!='rand' and mtype!='block'\").groupby(['lags','nof_filters','method'])['mse'].std().reset_index(),\n         on=['method','lags','nof_filters']).rename(columns={'mse_x':'mean','mse_y':'std'}).round(3).query(\"lags==4\")\n\n\n\n\n\n\n\n\nlags\nnof_filters\nmethod\nmean\nstd\n\n\n\n\n0\n4\n2\nIT-STGCN\n1.223\n0.053\n\n\n1\n4\n2\nSTGCN\n1.204\n0.060"
  },
  {
    "objectID": "posts/2_research/2023-07-01-EvolveGCNH_simulation_table_reshape.html#random-2",
    "href": "posts/2_research/2023-07-01-EvolveGCNH_simulation_table_reshape.html#random-2",
    "title": "EvolveGCNH_Simulation Tables_reshape",
    "section": "Random",
    "text": "Random\n\npd.merge(data.query(\"dataset=='pedalme' and mtype=='rand'\").groupby(['mrate','lags','inter_method','method'])['mse'].mean().reset_index(),\n         data.query(\"dataset=='pedalme' and mtype=='rand'\").groupby(['mrate','lags','inter_method','method'])['mse'].std().reset_index(),\n         on=['method','mrate','lags','inter_method']).rename(columns={'mse_x':'mean','mse_y':'std'}).round(3)\n\n\n\n\n\n\n\n\nmrate\nlags\ninter_method\nmethod\nmean\nstd\n\n\n\n\n0\n0.3\n4\nlinear\nIT-STGCN\n1.245\n0.069\n\n\n1\n0.3\n4\nlinear\nSTGCN\n1.273\n0.057\n\n\n2\n0.3\n4\nnearest\nIT-STGCN\n1.238\n0.046\n\n\n3\n0.3\n4\nnearest\nSTGCN\n1.244\n0.054\n\n\n4\n0.6\n4\nlinear\nIT-STGCN\n1.279\n0.057\n\n\n5\n0.6\n4\nlinear\nSTGCN\n1.301\n0.061\n\n\n6\n0.6\n4\nnearest\nIT-STGCN\n1.262\n0.091\n\n\n7\n0.6\n4\nnearest\nSTGCN\n1.284\n0.066"
  },
  {
    "objectID": "posts/2_research/2023-07-01-EvolveGCNH_simulation_table_reshape.html#block-2",
    "href": "posts/2_research/2023-07-01-EvolveGCNH_simulation_table_reshape.html#block-2",
    "title": "EvolveGCNH_Simulation Tables_reshape",
    "section": "Block",
    "text": "Block\n\npd.merge(data.query(\"dataset=='pedalme' and mtype=='block'\").groupby(['mrate','lags','inter_method','method'])['mse'].mean().reset_index(),\n         data.query(\"dataset=='pedalme' and mtype=='block'\").groupby(['mrate','lags','inter_method','method'])['mse'].std().reset_index(),\n         on=['method','mrate','lags','inter_method']).rename(columns={'mse_x':'mean','mse_y':'std'}).round(3).query(\"lags==4\")\n\n\n\n\n\n\n\n\nmrate\nlags\ninter_method\nmethod\nmean\nstd\n\n\n\n\n0\n0.286\n4\nlinear\nIT-STGCN\n1.259\n0.085\n\n\n1\n0.286\n4\nlinear\nSTGCN\n1.246\n0.073\n\n\n2\n0.286\n4\nnearest\nIT-STGCN\n1.222\n0.040\n\n\n3\n0.286\n4\nnearest\nSTGCN\n1.265\n0.072"
  },
  {
    "objectID": "posts/2_research/2023-07-01-EvolveGCNH_simulation_table_reshape.html#w_st",
    "href": "posts/2_research/2023-07-01-EvolveGCNH_simulation_table_reshape.html#w_st",
    "title": "EvolveGCNH_Simulation Tables_reshape",
    "section": "W_st",
    "text": "W_st\n\npd.merge(data_pedal2.query(\"mtype=='rand'\").groupby(['mrate','lags','inter_method','method'])['mse'].mean().reset_index(),\n         data_pedal2.query(\"mtype=='rand'\").groupby(['mrate','lags','inter_method','method'])['mse'].std().reset_index(),\n         on=['method','mrate','lags','inter_method']).rename(columns={'mse_x':'mean','mse_y':'std'}).round(3).query(\"lags==4\")\n\n\n\n\n\n\n\n\nmrate\nlags\ninter_method\nmethod\nmean\nstd\n\n\n\n\n0\n0.3\n4\nlinear\nIT-STGCN\n1.218\n0.058\n\n\n1\n0.3\n4\nlinear\nSTGCN\n1.237\n0.051\n\n\n2\n0.3\n4\nnearest\nIT-STGCN\n1.197\n0.068\n\n\n3\n0.3\n4\nnearest\nSTGCN\n1.237\n0.058\n\n\n4\n0.6\n4\nlinear\nIT-STGCN\n1.229\n0.070\n\n\n5\n0.6\n4\nlinear\nSTGCN\n1.278\n0.066\n\n\n6\n0.6\n4\nnearest\nIT-STGCN\n1.246\n0.067\n\n\n7\n0.6\n4\nnearest\nSTGCN\n1.291\n0.063\n\n\n\n\n\n\n\n\npd.merge(data_pedal2.query(\"mtype=='block'\").groupby(['mrate','lags','inter_method','method'])['mse'].mean().reset_index(),\n         data_pedal2.query(\"mtype=='block'\").groupby(['mrate','lags','inter_method','method'])['mse'].std().reset_index(),\n         on=['method','mrate','lags','inter_method']).rename(columns={'mse_x':'mean','mse_y':'std'}).round(3).query(\"lags==4\")\n\n\n\n\n\n\n\n\nmrate\nlags\ninter_method\nmethod\nmean\nstd\n\n\n\n\n0\n0.286\n4\nlinear\nIT-STGCN\n1.188\n0.042\n\n\n1\n0.286\n4\nlinear\nSTGCN\n1.230\n0.056\n\n\n2\n0.286\n4\nnearest\nIT-STGCN\n1.195\n0.037\n\n\n3\n0.286\n4\nnearest\nSTGCN\n1.240\n0.062"
  },
  {
    "objectID": "posts/2_research/2023-07-01-EvolveGCNH_simulation_table_reshape.html#baseline-3",
    "href": "posts/2_research/2023-07-01-EvolveGCNH_simulation_table_reshape.html#baseline-3",
    "title": "EvolveGCNH_Simulation Tables_reshape",
    "section": "Baseline",
    "text": "Baseline\n\npd.merge(data.query(\"dataset=='wikimath' and mrate==0\").groupby(['lags','nof_filters','method'])['mse'].mean().reset_index(),\n         data.query(\"dataset=='wikimath' and mrate==0\").groupby(['lags','nof_filters','method'])['mse'].std().reset_index(),\n         on=['lags','nof_filters','method']).rename(columns={'mse_x':'mean','mse_y':'std'}).round(3)\n\n\n\n\n\n\n\n\nlags\nnof_filters\nmethod\nmean\nstd\n\n\n\n\n0\n8\n12\nIT-STGCN\n0.784\n0.027\n\n\n1\n8\n12\nSTGCN\n0.771\n0.028"
  },
  {
    "objectID": "posts/2_research/2023-07-01-EvolveGCNH_simulation_table_reshape.html#random-3",
    "href": "posts/2_research/2023-07-01-EvolveGCNH_simulation_table_reshape.html#random-3",
    "title": "EvolveGCNH_Simulation Tables_reshape",
    "section": "Random",
    "text": "Random\n\npd.merge(data.query(\"dataset=='wikimath' and mtype=='rand'\").groupby(['mrate','lags','method'])['mse'].mean().reset_index(),\n         data.query(\"dataset=='wikimath' and mtype=='rand'\").groupby(['mrate','lags','method'])['mse'].std().reset_index(),\n         on=['method','mrate','lags']).rename(columns={'mse_x':'mean','mse_y':'std'}).round(3)\n\n\n\n\n\n\n\n\nmrate\nlags\nmethod\nmean\nstd\n\n\n\n\n0\n0.3\n8\nIT-STGCN\n0.775\n0.021\n\n\n1\n0.3\n8\nSTGCN\n0.787\n0.024\n\n\n2\n0.5\n8\nIT-STGCN\n0.791\n0.029\n\n\n3\n0.5\n8\nSTGCN\n0.804\n0.026\n\n\n4\n0.6\n8\nIT-STGCN\n0.795\n0.025\n\n\n5\n0.6\n8\nSTGCN\n0.843\n0.042\n\n\n6\n0.8\n8\nIT-STGCN\n0.877\n0.045\n\n\n7\n0.8\n8\nSTGCN\n0.915\n0.063"
  },
  {
    "objectID": "posts/2_research/2023-07-01-EvolveGCNH_simulation_table_reshape.html#block-3",
    "href": "posts/2_research/2023-07-01-EvolveGCNH_simulation_table_reshape.html#block-3",
    "title": "EvolveGCNH_Simulation Tables_reshape",
    "section": "Block",
    "text": "Block\n\npd.merge(data.query(\"dataset=='wikimath' and mtype=='block'\").groupby(['mrate','lags','method'])['mse'].mean().reset_index(),\n         data.query(\"dataset=='wikimath' and mtype=='block'\").groupby(['mrate','lags','method'])['mse'].std().reset_index(),\n         on=['method','mrate','lags']).rename(columns={'mse_x':'mean','mse_y':'std'})\n\n\n\n\n\n\n\n\nmrate\nlags\nmethod\nmean\nstd\n\n\n\n\n0\n0.119837\n8\nIT-STGCN\n0.775854\n0.027860\n\n\n1\n0.119837\n8\nSTGCN\n0.773374\n0.020599"
  },
  {
    "objectID": "posts/2_research/2023-07-01-EvolveGCNH_simulation_table_reshape.html#missing-values-on-the-same-nodes",
    "href": "posts/2_research/2023-07-01-EvolveGCNH_simulation_table_reshape.html#missing-values-on-the-same-nodes",
    "title": "EvolveGCNH_Simulation Tables_reshape",
    "section": "missing values on the same nodes",
    "text": "missing values on the same nodes\n\npd.merge(data_wiki_GSO.groupby(['mrate','lags','method'])['mse'].mean().reset_index(),\n        data_wiki_GSO.groupby(['mrate','lags','method'])['mse'].std().reset_index(),\n         on=['method','mrate','lags']).rename(columns={'mse_x':'mean','mse_y':'std'}).round(3)\n\n\n\n\n\n\n\n\nmrate\nlags\nmethod\nmean\nstd\n\n\n\n\n0\n0.512\n8\nIT-STGCN\n0.794\n0.031\n\n\n1\n0.512\n8\nSTGCN\n0.818\n0.031"
  },
  {
    "objectID": "posts/2_research/2023-07-01-EvolveGCNH_simulation_table_reshape.html#baseline-4",
    "href": "posts/2_research/2023-07-01-EvolveGCNH_simulation_table_reshape.html#baseline-4",
    "title": "EvolveGCNH_Simulation Tables_reshape",
    "section": "Baseline",
    "text": "Baseline\n\npd.merge(data.query(\"dataset=='windmillsmall' and mrate==0\").groupby(['lags','method'])['mse'].mean().reset_index(),\n         data.query(\"dataset=='windmillsmall' and mrate==0\").groupby(['lags','method'])['mse'].std().reset_index(),\n         on=['method','lags']).rename(columns={'mse_x':'mean','mse_y':'std'}).round(3)\n\n\n\n\n\n\n\n\nlags\nmethod\nmean\nstd\n\n\n\n\n0\n8\nIT-STGCN\n0.986\n0.002\n\n\n1\n8\nSTGCN\n0.986\n0.002"
  },
  {
    "objectID": "posts/2_research/2023-07-01-EvolveGCNH_simulation_table_reshape.html#random-4",
    "href": "posts/2_research/2023-07-01-EvolveGCNH_simulation_table_reshape.html#random-4",
    "title": "EvolveGCNH_Simulation Tables_reshape",
    "section": "Random",
    "text": "Random\n\npd.merge(data.query(\"dataset=='windmillsmall' and mtype=='rand'\").groupby(['mrate','lags','method'])['mse'].mean().reset_index(),\n         data.query(\"dataset=='windmillsmall' and mtype=='rand'\").groupby(['mrate','lags','method'])['mse'].std().reset_index(),\n         on=['method','mrate','lags']).rename(columns={'mse_x':'mean','mse_y':'std'}).round(3)\n\n\n\n\n\n\n\n\nmrate\nlags\nmethod\nmean\nstd\n\n\n\n\n0\n0.7\n8\nIT-STGCN\n1.129\n0.035\n\n\n1\n0.7\n8\nSTGCN\n1.330\n0.137"
  },
  {
    "objectID": "posts/2_research/2023-07-01-EvolveGCNH_simulation_table_reshape.html#block-4",
    "href": "posts/2_research/2023-07-01-EvolveGCNH_simulation_table_reshape.html#block-4",
    "title": "EvolveGCNH_Simulation Tables_reshape",
    "section": "Block",
    "text": "Block\n\npd.merge(data.query(\"dataset=='windmillsmall' and mtype=='block'\").groupby(['mrate','lags','method'])['mse'].mean().reset_index(),\n         data.query(\"dataset=='windmillsmall' and mtype=='block'\").groupby(['mrate','lags','method'])['mse'].std().reset_index(),\n         on=['method','mrate','lags']).rename(columns={'mse_x':'mean','mse_y':'std'}).round(3)\n\n\n\n\n\n\n\n\nmrate\nlags\nmethod\nmean\nstd\n\n\n\n\n0\n0.081\n8\nIT-STGCN\n0.986\n0.003\n\n\n1\n0.081\n8\nSTGCN\n0.993\n0.003"
  },
  {
    "objectID": "posts/2_research/2023-07-01-EvolveGCNH_simulation_table_reshape.html#baseline-5",
    "href": "posts/2_research/2023-07-01-EvolveGCNH_simulation_table_reshape.html#baseline-5",
    "title": "EvolveGCNH_Simulation Tables_reshape",
    "section": "Baseline",
    "text": "Baseline\n\npd.merge(data.query(\"dataset=='monte' and mrate==0\").groupby(['lags','method'])['mse'].mean().reset_index(),\n         data.query(\"dataset=='monte' and mrate==0\").groupby(['lags','method'])['mse'].std().reset_index(),\n         on=['method','lags']).rename(columns={'mse_x':'mean','mse_y':'std'}).round(3)\n\n\n\n\n\n\n\n\nlags\nmethod\nmean\nstd\n\n\n\n\n0\n4\nIT-STGCN\n1.358\n0.107\n\n\n1\n4\nSTGCN\n1.006\n0.018"
  },
  {
    "objectID": "posts/2_research/2023-07-01-EvolveGCNH_simulation_table_reshape.html#random-5",
    "href": "posts/2_research/2023-07-01-EvolveGCNH_simulation_table_reshape.html#random-5",
    "title": "EvolveGCNH_Simulation Tables_reshape",
    "section": "Random",
    "text": "Random\n\npd.merge(data.query(\"dataset=='monte' and mtype=='rand'\").groupby(['mrate','lags','inter_method','method'])['mse'].mean().reset_index(),\n         data.query(\"dataset=='monte' and mtype=='rand'\").groupby(['mrate','lags','inter_method','method'])['mse'].std().reset_index(),\n         on=['mrate','inter_method','method','mrate','lags']).rename(columns={'mse_x':'mean','mse_y':'std'})\n\n\n\n\n\n\n\n\nmrate\nlags\ninter_method\nmethod\nmean\nstd\n\n\n\n\n0\n0.3\n4\nnearest\nIT-STGCN\n1.398504\n0.135477\n\n\n1\n0.3\n4\nnearest\nSTGCN\n1.563461\n0.134200\n\n\n2\n0.5\n4\nnearest\nIT-STGCN\n1.441138\n0.110318\n\n\n3\n0.5\n4\nnearest\nSTGCN\n1.730591\n0.311464\n\n\n4\n0.7\n4\nnearest\nIT-STGCN\n1.700290\n0.328235\n\n\n5\n0.7\n4\nnearest\nSTGCN\n2.118390\n0.528367\n\n\n6\n0.8\n4\nnearest\nIT-STGCN\n1.845349\n0.504113\n\n\n7\n0.8\n4\nnearest\nSTGCN\n2.158357\n0.545361"
  },
  {
    "objectID": "posts/2_research/2023-07-01-EvolveGCNH_simulation_table_reshape.html#block-5",
    "href": "posts/2_research/2023-07-01-EvolveGCNH_simulation_table_reshape.html#block-5",
    "title": "EvolveGCNH_Simulation Tables_reshape",
    "section": "Block",
    "text": "Block\n\npd.merge(data.query(\"dataset=='monte' and mtype=='block'\").groupby(['mrate','lags','inter_method','method'])['mse'].mean().reset_index(),\n         data.query(\"dataset=='monte' and mtype=='block'\").groupby(['mrate','lags','inter_method','method'])['mse'].std().reset_index(),\n         on=['method','mrate','inter_method','lags']).rename(columns={'mse_x':'mean','mse_y':'std'})\n\n\n\n\n\n\n\n\nmrate\nlags\ninter_method\nmethod\nmean\nstd\n\n\n\n\n0\n0.149142\n4\nnearest\nIT-STGCN\n1.391881\n0.110351\n\n\n1\n0.149142\n4\nnearest\nSTGCN\n1.612466\n0.216224"
  },
  {
    "objectID": "posts/2_research/2023-05-06-article_refer.html",
    "href": "posts/2_research/2023-05-06-article_refer.html",
    "title": "ITSTGCN Article Refernece",
    "section": "",
    "text": "summerizing it\n\n\n\n\n\n\n\nNote\n\n\n\n\n글이 진행되는 순서로 작성함.\n\n\n\n\nIntroduction\nlittle1989analysis\n\nThe analysis of social science data with missing values\n\nLittle, Roderick JA and Rubin, Donald B\n\n\n위 논문은 구하지 못했고, 아래 논문에서 refer 건 것 보고 작성\nEvaluating the Influence of Missing Data on Classification Algorithms in Data Mining Applications by Luciano C. Blomberg\n내용\n\nFurthermore, distribution of missing data is another aspect that may influence the effectiveness of classification algorithms. Litle and Rubin [1987] presented three different mechanisms by which the missing data are distributed:\n\nli2018missing\n\nMissing value imputation for traffic-related time series data based on a multi-view learning method\n\nLi, Linchao and Zhang, Jian and Wang, Yonggang and Ran, Bin\n\n\n내용\n\nIn this circumstance, the difficulty of imputation is increased as we may not be able to find stable inputs for a model.\n\nSpatiotemporal Data\natluri2018spatio\n\nSpatio-temporal data mining: A survey of problems and methods\n\nAtluri, Gowtham and Karpatne, Anuj and Kumar, Vipin\n\n\n내용\n\n3.2. Data type 내용\n\nwang2020deep\n\nDeep learning for spatio-temporal data mining: A survey\n\nWang, Senzhang and Cao, Jiannong and Yu, Philip\n\n\n내용\n\n2.1. Spatio-Temporal Data Types 항목 내용에서 STData 종류\n\nyu2017spatio\n\nSpatio-temporal graph convolutional networks: A deep learning framework for traffic forecasting\n\nYu, Bing and Yin, Haoteng and Zhu, Zhanxing\n\n\n내용\n\nThe linear interpolation method is used to fill missing values after data cleaning.\n\nguo2019attention\n\nAttention based spatial-temporal graph convolutional networks for traffic flow forecasting\n\nGuo, Shengnan and Lin, Youfang and Feng, Ning and Song, Chao and Wan, Huaiyu\n\n\n내용\n\nThe missing values are filled by the linear interpolation.\n\nbai2020adaptive\n\nAdaptive graph convolutional recurrent network for traffic forecasting\n\nBai, Lei and Yao, Lina and Li, Can and Wang, Xianzhi and Wang, Can\n\n\n내용\n\nThe missing values in the datasets are filled by linear interpolation.\n\nli2019predicting\n\nPredicting path failure in time-evolving graphs\n\nLi, Jia and Han, Zhichao and Cheng, Hong and Su, Jiao and Wang, Pengyun and Zhang, Jianfeng and Pan, Lujia\n\n\n내용\n\nWe replace missing values with 0, and normalize the features to the range\n\nzhao2019t\n\nT-GCN: A Temporal Graph Convol- utional Network for Traffic Prediction\n\nZhao, Ling and Song, Yujiao and Zhang, Chao and Liu, Yu and Wang, Pu and Lin, Tao and Deng, Min and Li, Haifeng\n\n\n내용\n\nwe used the linear interpolation method to fill missing values.\n\nbaraldi2010introduction\n\nAn introduction to modern missing data analyses\n\nBaraldi, Amanda N and Enders, Craig K\n\n\n내용\n\nAn overview of traditional missing data techniques 항목\n\nshi2022air\n\nAir Quality Prediction Model Based on Spatio-temporal Graph Convolution Neural Networks\n\nShi, Weidi and Song, Anjun\n\n\n내용\n\nDue to weather reasons and corrosion or damage of some sensors, some data were lost and abnormal. Therefore, the data needs to be cleaned. We used the average imputation method to fill some lost data and deleted the data which lost too many values.\n\nbatista2003analysis\n\nAn analysis of four missing data treatment methods for supervised learning\n\nBatista, Gustavo EAPA and Monard, Maria Carolina\n\n\n내용\n\nIMPUTATION METHODS\nImputation methods involve replacing missing values with estimated ones based on information available in the data set. There are many options varying from naive methods, such as mean imputation, to some more robust methods based on relationships among attributes. A description of some widely used imputation methods follows:\n\nblomberg2013evaluating\n\nEvaluating the performance of regression algorithms on datasets with missing data\n\nBlomberg, Luciano Costa and Hemerich, Daiane and Ruiz, Duncan Dubugras Alcoba\n\n\n내용\n\nfor example, replaces the missing values with means or modes.\n\ndonders2006gentle\n\nReview: A gentle introduction to imputation of missing values\n\nDonders, A Rogier T and Van Der Heijden, Geert JMG and Stijnen, Theo and Moons, Karel GM\n\n\n내용\n\nthe indicator method and overall mean imputation, give biased results.\nThe mean of the standard errors is a measure for the uncertainty in the estimated associations caused by sampling the study subjects from a source popu- lation. Additionally, the standard deviation of the multiple estimated associations (e.g., regression coefficients) reflects the differences between the imputed data sets, i.e., the un- certainty in the estimated underlying distributions of the variables with missing values.\n\nbaraldi2010introduction\n\nAn introduction to modern missing data analyses\n\nBaraldi, Amanda N and Enders, Craig K\n\n\n내용\n\nTo illustrate the bias that can result from the use of traditional missing data methods, we use the artificial math performance data set found in Table 1.\n\n\n\nProposed Methods\nSelf-Consistent Estimator"
  },
  {
    "objectID": "posts/2_research/2023-07-08-toy_example_using_gnar.html",
    "href": "posts/2_research/2023-07-08-toy_example_using_gnar.html",
    "title": "Toy example using GNAR",
    "section": "",
    "text": "Import\n\nimport rpy2\nimport rpy2.robjects as ro \nfrom rpy2.robjects.vectors import FloatVector \nfrom rpy2.robjects.packages import importr\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport pandas as pd\nimport pickle\nimport torch\nimport itstgcn\nimport random\n\n/home/csy/anaconda3/envs/temp_csy/lib/python3.8/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n  from .autonotebook import tqdm as notebook_tqdm\n\n\n\ndef save_data(data_dict,fname):\n    with open(fname,'wb') as outfile:\n        pickle.dump(data_dict,outfile)\n\n\ndef load_data(fname):\n    with open(fname, 'rb') as outfile:\n        data_dict = pickle.load(outfile)\n    return data_dict\n\n\n\nGNAR Data copy\n\n%load_ext rpy2.ipython\n\n\n%%R\nlibrary(GNAR)\nlibrary(igraph)\n\n\nedge(list)\ndist(list)\n\n\n%%R\nplot(fiveNet, vertex.label = c(\"A\", \"B\", \"C\", \"D\", \"E\"))\n\n\n\n\n\n\n\n\n\n%%R\nas.matrix(fiveNet)\n\n     [,1] [,2] [,3] [,4] [,5]\n[1,]    0    0    0    1    1\n[2,]    0    0    1    1    0\n[3,]    0    1    0    1    0\n[4,]    1    1    1    0    0\n[5,]    1    0    0    0    0\n\n\n\n%%R\ndata(\"fiveNode\")\nanswer &lt;- GNARfit(vts = fiveVTS, net = fiveNet, alphaOrder = 2, betaOrder = c(1, 1))\nanswer\n\nModel: \nGNAR(2,[1,1]) \n\nCall:\nlm(formula = yvec ~ dmat + 0)\n\nCoefficients:\n dmatalpha1  dmatbeta1.1   dmatalpha2  dmatbeta2.1  \n    0.20624      0.50277      0.02124     -0.09523  \n\n\n\n\\[X_{i,t} = \\sum^p_{j=1}\\big( \\alpha_{i,j} X_{i,t-j} + \\sum^C_{c=1} \\sum^{s_j}_{r=1} \\beta_{j,r,c} \\sum_{1 \\in \\cal{N}^{(r)}_t (i)} \\omega^{(t)}_{i,q,c} X_{q,t-j} \\big) + u_{i,t}\\]\n\n\\(p \\in \\mathbb{N}\\) is the maximum time lag\n\\([s] = (s_1, \\dots , s_p)\\) and \\(s_j \\in \\mathbb{N}_0\\) is the maximum stage of neighbor dependence for time lag \\(j\\), with \\(\\mathbb{N}_0 = \\mathbb{N} \\cup \\{ 0\\}\\)\n\\(\\cal{N}^{(r)}_t (i)\\) is the \\(r\\)th stage neighbour set of node \\(i\\) at time \\(t\\)\n\\(\\omega^{(t)}_{i,q,c} \\in [0,1]\\) is the connection weight between node \\(i\\) and node \\(q\\) at time \\(t\\) if the path corresponds to covariate \\(c\\)\n\n\\[X_{A,t} = 0.206 X_{A,t−1}+0.503 (X_{E,t−1}+X_{D,t−1})/2+0.021 X_{A,t−2}−0.095(X_{E,t−2}+X_{D,t−2})/2+u_{A,t}\\]\n\\[X_{B,t} = 0.206 X_{B,t−1}+0.503 (X_{C,t−1}+X_{D,t−1})/2+0.021 X_{B,t−2}−0.095(X_{C,t−2}+X_{D,t−2})/2+u_{B,t}\\]\n\\[X_{C,t} = 0.206 X_{C,t−1}+0.503 (X_{B,t−1}+X_{D,t−1})/2+0.021 X_{C,t−2}−0.095(X_{B,t−2}+X_{D,t−2})/2+u_{C,t}\\]\n\\[X_{D,t} = 0.206 X_{D,t−1}+0.503 (X_{A,t−1}+X_{B,t−1}+X_{C,t−1})/3+0.021 X_{D,t−2}−0.095(X_{A,t−1}+X_{B,t−1}+X_{C,t−1})/3+u_{D,t}\\]\n\\[X_{E,t} = 0.206 X_{E,t−1}+0.503 (X_{A,t−1})+0.021 X_{E,t−2}−0.095(X_{A,t−1})+u_{E,t}\\]\n\nfrom statsmodels.tsa.arima.model import ARIMA\n\nhttps://communities.sas.com/t5/SAS-Tech-Tip/SAS-%ED%99%9C%EC%9A%A9-%EB%85%B8%ED%95%98%EC%9A%B0-%EC%8B%9C%EA%B3%84%EC%97%B4-AR-1-%EA%B3%BC-AR-2/ta-p/792106\n\na_ylag1 = np.random.normal(size=1)\na_ylag2 = np.random.normal(size=1)\n\n\nb_ylag1 = np.random.normal(size=1)\nb_ylag2 = np.random.normal(size=1)\n\n\n# alpha1 = 0.15\nalpha1 = 0.4\nalpha2 = 0.2\n# alpha2 = 0.01\n# beta1 = 0.503\nbeta1 = 0.01\nbeta2 = - 0.1\na_ar_values = []\na_ar_values_true = []\nb_ar_values = []\nb_ar_values_true = []\n\n\nfor i in range(500):\n    a_e = np.random.normal(size=1) * 0.2\n    b_e = np.random.normal(size=1)  * 0.2\n    \n    # a_y = alpha1 * a_ylag1 + alpha2 * a_ylag2 + a_e + beta1 * (e_ylag1 + d_ylag1) / 2 + beta2 * (e_ylag2 + d_ylag2) / 2\n    # b_y = alpha1 * b_ylag1 + alpha2 * b_ylag2 + b_e + beta1 * (c_ylag1 + d_ylag1) / 2 + beta2 * (c_ylag2 + d_ylag2) / 2\n    # c_y = alpha1 * c_ylag1 + alpha2 * c_ylag2 + c_e + beta1 * (b_ylag1 + d_ylag1) / 2 + beta2 * (b_ylag2 + d_ylag2) / 2\n    # d_y = alpha1 * d_ylag1 + alpha2 * d_ylag2 + d_e + beta1 * (a_ylag1 + b_ylag1 + c_ylag1) / 3 + beta2 * (a_ylag2 + b_ylag2 + c_ylag2) / 3\n    # e_y = alpha1 * e_ylag1 + alpha2 * e_ylag2 + e_e + beta1 * a_ylag1 + beta2 * a_ylag2\n    a_y_true = alpha1 * a_ylag1 + alpha2 * a_ylag2 + beta1 * (b_ylag1) + beta2 * (a_ylag2)\n    a_y = a_y_true + a_e\n    b_y_true = alpha1 * b_ylag1 + alpha2 * b_ylag2 + beta1 * (a_ylag1) + beta2 * (b_ylag2)\n    b_y = b_y_true + b_e\n    \n    a_ar_values_true.append(a_y_true[0])\n    a_ar_values.append(a_y[0])\n    b_ar_values_true.append(b_y_true[0])\n    b_ar_values.append(b_y[0])\n    \n    a_ylag2 = a_ylag1\n    b_ylag2 = b_ylag1\n    \n    a_ylag1 = a_y\n    b_ylag1 = b_y\n\n\nnp.sum(a_ar_values)\n\n0.6513402621576372\n\n\n\nnp.sum(b_ar_values)\n\n11.796436691361254\n\n\n\nplt.plot(a_ar_values_true)\n\n\n\n\n\n\n\n\n\nplt.plot(a_ar_values)\n\n\n\n\n\n\n\n\n\nplt.plot(b_ar_values_true)\n\n\n\n\n\n\n\n\n\nplt.plot(b_ar_values)\n\n\n\n\n\n\n\n\n\ndf = {'A' : a_ar_values, 'B' : b_ar_values}\n\n\n_node_ids = {'node1':0, 'node2':1}\n\n\n_edges = torch.tensor([[0,1],[1,0]]).tolist()\n\n\n_FX1 = np.stack([a_ar_values,b_ar_values],axis=1).tolist()\n\n\ndata_dict = {'edges':_edges, 'node_ids':_node_ids, 'FX':_FX1}\n\n\nsave_data(data_dict,'toy_ex_dataset.pkl')\n\n\n\nRandom\n\ndata_dict = load_data('toy_ex_dataset.pkl')\n\n\nloader = itstgcn.DatasetLoader(data_dict)\n\n\ndataset = loader.get_dataset(lags=2)\n\n\ntrain_dataset, test_dataset = itstgcn.temporal_signal_split(dataset, train_ratio=0.8)\n\n\nmindex_rand = itstgcn.rand_mindex(train_dataset,mrate=0.7)\n\n/home/csy/Dropbox/blog/posts/GCN/itstgcn/utils.py:71: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at /opt/conda/conda-bld/pytorch_1682343998658/work/torch/csrc/utils/tensor_new.cpp:245.)\n  lags = torch.tensor(train_dataset.features).shape[-1]\n\n\n\ntrain_dataset_miss_rand = itstgcn.miss(train_dataset,mindex_rand,mtype='rand')\n\n\ntrain_dataset_padded_rand = itstgcn.padding(train_dataset_miss_rand) # padding(train_dataset_miss,method='linear'와 같음)\n\n- 학습\n\nlrnr_rand = itstgcn.StgcnLearner(train_dataset_padded_rand)\n\n\nlrnr_rand.learn(filters=12,epoch=5)\n\n5/5\n\n\n\nlrnr_rand_it = itstgcn.ITStgcnLearner(train_dataset_padded_rand)\n\n\nlrnr_rand_it.learn(filters=12,epoch=5)\n\n5/5\n\n\n- 모형 평가 및 시각화\n\nevtor_rand = itstgcn.Evaluator(lrnr_rand,train_dataset_padded_rand,test_dataset)\n\n\nfig = evtor_rand.plot('--.',h=5,max_node=2,label='complete data',alpha=0.5) # max_nodes 는 1보다 커야함\nfig.set_figwidth(20)\nfig.set_figheight(10)\nfig.tight_layout()\nfig\n\n\n\n\n\n\n\n\n\nevtor_rand_it = itstgcn.Evaluator(lrnr_rand_it,train_dataset_padded_rand,test_dataset)\n\n\nfig = evtor_rand_it.plot('--.',h=5,max_node=2,label='complete data',alpha=0.5) # max_nodes 는 1보다 커야함\nfig.set_figwidth(20)\nfig.set_figheight(10)\nfig.tight_layout()\nfig\n\n\n\n\n\n\n\n\n\nevtor_rand.mse\n\n{'train': {'each_node': [0.014082524925470352, 0.015163550153374672],\n  'total': 0.014623038470745087},\n 'test': {'each_node': [0.07657670229673386, 0.06272050738334656],\n  'total': 0.0696486085653305},\n 'test(base)': {'each_node': [0.05262065306305885, 0.05082978680729866],\n  'total': 0.05172521993517876}}\n\n\n\nevtor_rand_it.mse\n\n{'train': {'each_node': [0.013057202100753784, 0.014180357567965984],\n  'total': 0.013618779368698597},\n 'test': {'each_node': [0.06001878157258034, 0.04732104390859604],\n  'total': 0.05366990715265274},\n 'test(base)': {'each_node': [0.05262065306305885, 0.05082978680729866],\n  'total': 0.05172521993517876}}\n\n\n\nwith plt.style.context('seaborn-white'):\n    fig, ((ax1, ax2), (ax3, ax4), (ax5,ax6)) = plt.subplots(3, 2,figsize=(40,20))\n    # fig.suptitle('Figure 1(node 1)',fontsize=40)\n    ax1.plot(a_ar_values_true,label='Ground Truth')\n    ax1.legend(fontsize=20,loc='lower left',facecolor='white', frameon=True)\n    \n    ax2.plot(np.array(data_dict['FX'])[:,0],'-',color='C3',label='Complete Data')\n    ax2.legend(fontsize=20,loc='lower left',facecolor='white', frameon=True)\n    ax2.tick_params(axis='y', labelsize=20)\n    ax2.tick_params(axis='x', labelsize=20)\n    \n    ax3.plot(np.array(data_dict['FX'])[:,0],'--',color='C5',alpha=0.5,label='Complete Data')\n    ax3.plot(torch.cat([torch.tensor(dataset.features)[:2,0,0],torch.tensor(train_dataset_miss_rand.targets).reshape(-1,2)[:,0]],dim=0),'--o',color='C3',label='Observed Data')\n    ax3.legend(fontsize=20,loc='lower left',facecolor='white', frameon=True)\n    ax3.tick_params(axis='y', labelsize=20)\n    ax3.tick_params(axis='x', labelsize=20)\n    \n    ax4.plot(np.array(data_dict['FX'])[:,0],'--',color='C5',alpha=0.5,label='Complete Data')\n    ax4.plot(torch.cat([torch.tensor(dataset.features)[:2,0,0],torch.tensor(train_dataset_padded_rand.targets).reshape(-1,2)[:,0]],dim=0),'--o',color='C3',alpha=0.8,label='Linear Interpolation')\n    ax4.legend(fontsize=20,loc='lower left',facecolor='white', frameon=True)\n    ax4.tick_params(axis='y', labelsize=20)\n    ax4.tick_params(axis='x', labelsize=20)\n    \n    ax5.plot(torch.tensor(data_dict['FX'])[:400,0],'--',color='C5',alpha=0.5,label='Complete Data')\n    ax5.plot(a_ar_values_true[:400],color='black',label='Ground Truth',lw=3)\n    ax5.plot(evtor_rand.fhat_tr[:,0],color='brown',lw=3,label='GConvGRU',alpha=0.5)\n    ax5.plot(evtor_rand_it.fhat_tr[:,0],color='blue',lw=3,label='IT-TGNN',alpha=0.5)\n    # ax4.plot(55, 0, 'o', markersize=100, markerfacecolor='none', markeredgecolor='red',markeredgewidth=3)\n    # ax4.plot(150, 0, 'o', markersize=80, markerfacecolor='none', markeredgecolor='red',markeredgewidth=3)\n    # ax4.plot(185, 0, 'o', markersize=80, markerfacecolor='none', markeredgecolor='red',markeredgewidth=3)\n    ax5.legend(fontsize=20,loc='lower left',facecolor='white', frameon=True)\n    ax5.tick_params(axis='y', labelsize=20)\n    ax5.tick_params(axis='x', labelsize=20)\n    \n    ax6.plot(torch.tensor(data_dict['FX'])[400:,0],'--',color='C5',alpha=0.5,label='Test')\n    ax6.plot(a_ar_values_true[400:],color='black',label='Ground Truth',lw=3)\n    ax6.plot(evtor_rand.fhat_test[:,0],color='brown',lw=3,label='GConvGRU',alpha=0.5)\n    ax6.plot(evtor_rand_it.fhat_test[:,0],color='blue',lw=3,label='IT-TGNN',alpha=0.5)\n    ax6.legend(fontsize=20,loc='lower left',facecolor='white', frameon=True)\n    ax6.tick_params(axis='y', labelsize=20)\n    ax6.tick_params(axis='x', labelsize=20)\n# # plt.savefig('try2_node1.png')\n\n\n\n\n\n\n\n\n\nwith plt.style.context('seaborn-white'):\n    fig, ((ax1, ax2), (ax3, ax4), (ax5,ax6)) = plt.subplots(3, 2,figsize=(40,20))\n    # fig.suptitle('Figure 1(node 1)',fontsize=40)\n    \n    ax1.plot(b_ar_values_true,label='Ground Truth')\n    ax1.legend(fontsize=20,loc='lower left',facecolor='white', frameon=True)\n    \n    ax2.plot(np.array(data_dict['FX'])[:,1],'-',color='C3',label='Complete Data')\n    ax2.legend(fontsize=20,loc='lower left',facecolor='white', frameon=True)\n    ax2.tick_params(axis='y', labelsize=20)\n    ax2.tick_params(axis='x', labelsize=20)\n    \n    ax3.plot(np.array(data_dict['FX'])[:,1],'--',color='C5',alpha=0.5,label='Complete Data')\n    ax3.plot(torch.cat([torch.tensor(dataset.features)[:2,1,0],torch.tensor(train_dataset_miss_rand.targets).reshape(-1,2)[:,1]],dim=0),'--o',color='C3',label='Observed Data')\n    ax3.legend(fontsize=20,loc='lower left',facecolor='white', frameon=True)\n    ax3.tick_params(axis='y', labelsize=20)\n    ax3.tick_params(axis='x', labelsize=20)\n    \n    ax4.plot(np.array(data_dict['FX'])[:,0],'--',color='C5',alpha=0.5,label='Complete Data')\n    ax4.plot(torch.cat([torch.tensor(dataset.features)[:2,1,0],torch.tensor(train_dataset_padded_rand.targets).reshape(-1,2)[:,1]],dim=0),'--o',color='C3',alpha=0.8,label='Linear Interpolation')\n    ax4.legend(fontsize=20,loc='lower left',facecolor='white', frameon=True)\n    ax4.tick_params(axis='y', labelsize=20)\n    ax4.tick_params(axis='x', labelsize=20)\n    \n    ax5.plot(torch.tensor(data_dict['FX'])[:399,1],'--',color='C5',alpha=0.5,label='Complete Data')\n    ax5.plot(b_ar_values_true[:400],color='black',label='Ground Truth',lw=3)\n    ax5.plot(evtor_rand.fhat_tr[:,1],color='brown',lw=3,label='GConvGRU',alpha=0.5)\n    ax5.plot(evtor_rand_it.fhat_tr[:,1],color='blue',lw=3,label='IT-TGNN',alpha=0.5)\n    # ax5.plot(55, 0, 'o', markersize=100, markerfacecolor='none', markeredgecolor='red',markeredgewidth=3)\n    # ax5.plot(150, 0, 'o', markersize=80, markerfacecolor='none', markeredgecolor='red',markeredgewidth=3)\n    # ax5.plot(185, 0, 'o', markersize=80, markerfacecolor='none', markeredgecolor='red',markeredgewidth=3)\n    ax5.legend(fontsize=20,loc='lower left',facecolor='white', frameon=True)\n    ax5.tick_params(axis='y', labelsize=20)\n    ax5.tick_params(axis='x', labelsize=20)\n    \n    ax6.plot(torch.tensor(data_dict['FX'])[400:,1],'--',color='C5',alpha=0.5,label='Test')\n    ax6.plot(b_ar_values_true[400:],color='black',label='Ground Truth',lw=3)\n    ax6.plot(evtor_rand.fhat_test[:,1],color='brown',lw=3,label='GConvGRU',alpha=0.5)\n    ax6.plot(evtor_rand_it.fhat_test[:,1],color='blue',lw=3,label='IT-TGNN',alpha=0.5)\n    ax6.legend(fontsize=20,loc='lower left',facecolor='white', frameon=True)\n    ax6.tick_params(axis='y', labelsize=20)\n    ax6.tick_params(axis='x', labelsize=20)\n# # plt.savefig('try2_node1.png')\n\n\n\n\n\n\n\n\n\n\nBlock\n\ndata_dict = load_data('toy_ex_dataset.pkl')\n\n\nloader = itstgcn.DatasetLoader(data_dict)\n\n\ndataset = loader.get_dataset(lags=2)\n\n\ntrain_dataset, test_dataset = itstgcn.temporal_signal_split(dataset, train_ratio=0.8)\n\n\nmindex_block = [list(range(80,180)),list(range(100,190))]\n\n\ntrain_dataset_miss_block = itstgcn.miss(train_dataset,mindex_block,mtype='block')\n\n\ntrain_dataset_padded_block = itstgcn.padding(train_dataset_miss_block) # padding(train_dataset_miss,method='linear'와 같음)\n\n- 학습\n\nlrnr_block = itstgcn.StgcnLearner(train_dataset_padded_block)\n\n\nlrnr_block.learn(filters=12,epoch=5)\n\n5/5\n\n\n\nlrnr_block_it = itstgcn.ITStgcnLearner(train_dataset_padded_block)\n\n\nlrnr_block_it.learn(filters=12,epoch=5)\n\n5/5\n\n\n- 모형 평가 및 시각화\n\nevtor_block = itstgcn.Evaluator(lrnr_block,train_dataset_padded_block,test_dataset)\n\n\nfig = evtor_block.plot('--.',h=5,max_node=2,label='complete data',alpha=0.5) # max_nodes 는 1보다 커야함\nfig.set_figwidth(20)\nfig.set_figheight(10)\nfig.tight_layout()\nfig\n\n\n\n\n\n\n\n\n\nevtor_block_it = itstgcn.Evaluator(lrnr_block_it,train_dataset_padded_block,test_dataset)\n\n\nfig = evtor_block_it.plot('--.',h=5,max_node=2,label='complete data',alpha=0.5) # max_nodes 는 1보다 커야함\nfig.set_figwidth(20)\nfig.set_figheight(10)\nfig.tight_layout()\nfig\n\n\n\n\n\n\n\n\n\nevtor_block.mse\n\n\nevtor_block_it.mse\n\n\nwith plt.style.context('seaborn-white'):\n    fig, ((ax1, ax2), (ax3, ax4), (ax5,ax6)) = plt.subplots(3, 2,figsize=(40,20))\n    # fig.suptitle('Figure 1(node 1)',fontsize=40)\n    \n    ax1.plot(a_ar_values_true,label='Ground Truth')\n    ax1.legend(fontsize=20,loc='lower left',facecolor='white', frameon=True)\n    \n    ax2.plot(np.array(data_dict['FX'])[:,0],'-',color='C3',label='Complete Data')\n    ax2.legend(fontsize=20,loc='lower left',facecolor='white', frameon=True)\n    ax2.tick_params(axis='y', labelsize=20)\n    ax2.tick_params(axis='x', labelsize=20)\n    \n    ax3.plot(np.array(data_dict['FX'])[:,0],'--',color='C5',alpha=0.5,label='Complete Data')\n    ax3.plot(torch.cat([torch.tensor(dataset.features)[:2,0,0],torch.tensor(train_dataset_miss_block.targets).reshape(-1,2)[:,0]],dim=0),'--o',color='C3',label='Observed Data')\n    ax3.legend(fontsize=20,loc='lower left',facecolor='white', frameon=True)\n    ax3.tick_params(axis='y', labelsize=20)\n    ax3.tick_params(axis='x', labelsize=20)\n    \n    ax4.plot(np.array(data_dict['FX'])[:,0],'--',color='C5',alpha=0.5,label='Complete Data')\n    ax4.plot(torch.cat([torch.tensor(dataset.features)[:2,0,0],torch.tensor(train_dataset_padded_block.targets).reshape(-1,2)[:,0]],dim=0),'--o',color='C3',alpha=0.8,label='Linear Interpolation')\n    ax4.legend(fontsize=20,loc='lower left',facecolor='white', frameon=True)\n    ax4.tick_params(axis='y', labelsize=20)\n    ax4.tick_params(axis='x', labelsize=20)\n    \n    ax5.plot(torch.tensor(data_dict['FX'])[:399,0],'--',color='C5',alpha=0.5,label='Complete Data')\n    ax5.plot(a_ar_values_true[:400],color='black',label='Ground Truth',lw=3)\n    ax5.plot(evtor_block.fhat_tr[:,0],color='brown',lw=3,label='GConvGRU',alpha=0.5)\n    ax5.plot(evtor_block_it.fhat_tr[:,0],color='blue',lw=3,label='IT-TGNN',alpha=0.5)\n    # ax5.plot(55, 0, 'o', markersize=100, markerfacecolor='none', markeredgecolor='red',markeredgewidth=3)\n    # ax5.plot(150, 0, 'o', markersize=80, markerfacecolor='none', markeredgecolor='red',markeredgewidth=3)\n    # ax5.plot(185, 0, 'o', markersize=80, markerfacecolor='none', markeredgecolor='red',markeredgewidth=3)\n    ax5.legend(fontsize=20,loc='lower left',facecolor='white', frameon=True)\n    ax5.tick_params(axis='y', labelsize=20)\n    ax5.tick_params(axis='x', labelsize=20)\n    \n    ax6.plot(torch.tensor(data_dict['FX'])[400:,0],'--',color='C5',alpha=0.5,label='Test')\n    ax6.plot(a_ar_values_true[400:],color='black',label='Ground Truth',lw=3)\n    ax6.plot(evtor_block.fhat_test[:,0],color='brown',lw=3,label='GConvGRU',alpha=0.5)\n    ax6.plot(evtor_block_it.fhat_test[:,0],color='blue',lw=3,label='IT-TGNN',alpha=0.5)\n    ax6.legend(fontsize=20,loc='lower left',facecolor='white', frameon=True)\n    ax6.tick_params(axis='y', labelsize=20)\n    ax6.tick_params(axis='x', labelsize=20)\n# # plt.savefig('try2_node1.png')\n\n\n\n\n\n\n\n\n\nwith plt.style.context('seaborn-white'):\n    fig, ((ax1, ax2), (ax3, ax4), (ax5,ax6)) = plt.subplots(3, 2,figsize=(40,20))\n    # fig.suptitle('Figure 1(node 1)',fontsize=40)\n    \n    ax1.plot(b_ar_values_true,label='TrGround Truthue')\n    ax1.legend(fontsize=20,loc='lower left',facecolor='white', frameon=True)\n    \n    ax2.plot(np.array(data_dict['FX'])[:,1],'-',color='C3',label='Complete Data')\n    ax2.legend(fontsize=20,loc='lower left',facecolor='white', frameon=True)\n    ax2.tick_params(axis='y', labelsize=20)\n    ax2.tick_params(axis='x', labelsize=20)\n    \n    ax3.plot(np.array(data_dict['FX'])[:,1],'--',color='C5',alpha=0.5,label='Complete Data')\n    ax3.plot(torch.cat([torch.tensor(dataset.features)[:2,1,0],torch.tensor(train_dataset_miss_block.targets).reshape(-1,2)[:,1]],dim=0),'--o',color='C3',label='Observed Data')\n    ax3.legend(fontsize=20,loc='lower left',facecolor='white', frameon=True)\n    ax3.tick_params(axis='y', labelsize=20)\n    ax3.tick_params(axis='x', labelsize=20)\n    \n    ax4.plot(np.array(data_dict['FX'])[:,0],'--',color='C5',alpha=0.5,label='Complete Data')\n    ax4.plot(torch.cat([torch.tensor(dataset.features)[:2,1,0],torch.tensor(train_dataset_padded_block.targets).reshape(-1,2)[:,1]],dim=0),'--o',color='C3',alpha=0.8,label='Linear Interpolation')\n    ax4.legend(fontsize=20,loc='lower left',facecolor='white', frameon=True)\n    ax4.tick_params(axis='y', labelsize=20)\n    ax4.tick_params(axis='x', labelsize=20)\n    \n    ax5.plot(torch.tensor(data_dict['FX'])[:399,1],'--',color='C5',alpha=0.5,label='Complete Data')\n    ax5.plot(b_ar_values_true[:400],color='black',label='Ground Truth',lw=3)\n    ax5.plot(evtor_block.fhat_tr[:,1],color='brown',lw=3,label='GConvGRU',alpha=0.5)\n    ax5.plot(evtor_block_it.fhat_tr[:,1],color='blue',lw=3,label='IT-TGNN',alpha=0.5)\n    # ax5.plot(55, 0, 'o', markersize=100, markerfacecolor='none', markeredgecolor='red',markeredgewidth=3)\n    # ax5.plot(150, 0, 'o', markersize=80, markerfacecolor='none', markeredgecolor='red',markeredgewidth=3)\n    # ax5.plot(185, 0, 'o', markersize=80, markerfacecolor='none', markeredgecolor='red',markeredgewidth=3)\n    ax5.legend(fontsize=20,loc='lower left',facecolor='white', frameon=True)\n    ax5.tick_params(axis='y', labelsize=20)\n    ax5.tick_params(axis='x', labelsize=20)\n    \n    ax6.plot(torch.tensor(data_dict['FX'])[400:,1],'--',color='C5',alpha=0.5,label='Test')\n    ax6.plot(b_ar_values_true[400:],color='black',label='Ground Truth',lw=3)\n    ax6.plot(evtor_block.fhat_test[:,1],color='brown',lw=3,label='GConvGRU',alpha=0.5)\n    ax6.plot(evtor_block_it.fhat_test[:,1],color='blue',lw=3,label='IT-TGNN',alpha=0.5)\n    ax6.legend(fontsize=20,loc='lower left',facecolor='white', frameon=True)\n    ax6.tick_params(axis='y', labelsize=20)\n    ax6.tick_params(axis='x', labelsize=20)\n# # plt.savefig('try2_node1.png')\n\n\n\n\n\n\n\n\n\n\nRandom & Block\n\ndata_dict = load_data('toy_ex_dataset.pkl')\n\n\nloader = itstgcn.DatasetLoader(data_dict)\n\n\ndataset = loader.get_dataset(lags=2)\n\n\ntrain_dataset, test_dataset = itstgcn.temporal_signal_split(dataset, train_ratio=0.8)\n\n\nmindex_rdbl = [random.sample(range(0, 400), int(400*0.7)),[np.array(list(range(150,220)))]]\n\n\ntrain_dataset_miss_rdbl = itstgcn.miss(train_dataset,mindex_rdbl,mtype='block')\n\n\ntrain_dataset_padded_rdbl = itstgcn.padding(train_dataset_miss_rdbl) # padding(train_dataset_miss,method='linear'와 같음)\n\n- 학습\n\nlrnr_rdbl = itstgcn.StgcnLearner(train_dataset_padded_rdbl)\n\n\nlrnr_rdbl.learn(filters=12,epoch=5)\n\n5/5\n\n\n\nlrnr_rdbl_it = itstgcn.ITStgcnLearner(train_dataset_padded_rdbl)\n\n\nlrnr_rdbl_it.learn(filters=12,epoch=5)\n\n5/5\n\n\n- 모형 평가 및 시각화\n\nevtor_rdbl = itstgcn.Evaluator(lrnr_rdbl,train_dataset_padded_rdbl,test_dataset)\n\n\nfig = evtor_rdbl.plot('--.',h=5,max_node=2,label='complete data',alpha=0.5) # max_nodes 는 1보다 커야함\nfig.set_figwidth(20)\nfig.set_figheight(10)\nfig.tight_layout()\nfig\n\n\n\n\n\n\n\n\n\nevtor_rdbl_it = itstgcn.Evaluator(lrnr_rdbl_it,train_dataset_padded_rdbl,test_dataset)\n\n\nfig = evtor_rdbl_it.plot('--.',h=5,max_node=2,label='complete data',alpha=0.5) # max_nodes 는 1보다 커야함\nfig.set_figwidth(20)\nfig.set_figheight(10)\nfig.tight_layout()\nfig\n\n\nevtor_rdbl.mse\n\n\nevtor_rdbl_it.mse\n\n\nwith plt.style.context('seaborn-white'):\n    fig, ((ax1, ax2), (ax3, ax4), (ax5,ax6)) = plt.subplots(3, 2,figsize=(40,20))\n    # fig.suptitle('Figure 1(node 1)',fontsize=40)\n    \n    ax1.plot(a_ar_values_true,label='Ground Truth')\n    ax1.legend(fontsize=20,loc='lower left',facecolor='white', frameon=True)\n    \n    ax2.plot(np.array(data_dict['FX'])[:,0],'-',color='C3',label='Complete Data')\n    ax2.legend(fontsize=20,loc='lower left',facecolor='white', frameon=True)\n    ax2.tick_params(axis='y', labelsize=20)\n    ax2.tick_params(axis='x', labelsize=20)\n    \n    ax3.plot(np.array(data_dict['FX'])[:,0],'--',color='C5',alpha=0.5,label='Complete Data')\n    ax3.plot(torch.cat([torch.tensor(dataset.features)[:2,0,0],torch.tensor(train_dataset_miss_rdbl.targets).reshape(-1,2)[:,0]],dim=0),'--o',color='C3',label='Observed Data')\n    ax3.legend(fontsize=20,loc='lower left',facecolor='white', frameon=True)\n    ax3.tick_params(axis='y', labelsize=20)\n    ax3.tick_params(axis='x', labelsize=20)\n    \n    ax4.plot(np.array(data_dict['FX'])[:,0],'--',color='C5',alpha=0.5,label='Complete Data')\n    ax4.plot(torch.cat([torch.tensor(dataset.features)[:2,0,0],torch.tensor(train_dataset_padded_rdbl.targets).reshape(-1,2)[:,0]],dim=0),'--o',color='C3',alpha=0.8,label='Linear Interpolation')\n    ax4.legend(fontsize=20,loc='lower left',facecolor='white', frameon=True)\n    ax4.tick_params(axis='y', labelsize=20)\n    ax4.tick_params(axis='x', labelsize=20)\n    \n    ax5.plot(torch.tensor(data_dict['FX'])[:399,0],'--',color='C5',alpha=0.5,label='Complete Data')\n    ax5.plot(a_ar_values_true[:400],color='black',label='Ground Truth',lw=3)\n    ax5.plot(evtor_rdbl.fhat_tr[:,0],color='brown',lw=3,label='GConvGRU',alpha=0.5)\n    ax5.plot(evtor_rdbl_it.fhat_tr[:,0],color='blue',lw=3,label='IT-TGNN',alpha=0.5)\n    # ax5.plot(55, 0, 'o', markersize=100, markerfacecolor='none', markeredgecolor='red',markeredgewidth=3)\n    # ax5.plot(150, 0, 'o', markersize=80, markerfacecolor='none', markeredgecolor='red',markeredgewidth=3)\n    # ax5.plot(185, 0, 'o', markersize=80, markerfacecolor='none', markeredgecolor='red',markeredgewidth=3)\n    ax5.legend(fontsize=20,loc='lower left',facecolor='white', frameon=True)\n    ax5.tick_params(axis='y', labelsize=20)\n    ax5.tick_params(axis='x', labelsize=20)\n    \n    ax6.plot(torch.tensor(data_dict['FX'])[400:,0],'--',color='C5',alpha=0.5,label='Test')\n    ax6.plot(a_ar_values_true[400:],color='black',label='Ground Truth',lw=3)\n    ax6.plot(evtor_rdbl.fhat_test[:,0],color='brown',lw=3,label='GConvGRU',alpha=0.5)\n    ax6.plot(evtor_rdbl_it.fhat_test[:,0],color='blue',lw=3,label='IT-TGNN',alpha=0.5)\n    ax6.legend(fontsize=20,loc='lower left',facecolor='white', frameon=True)\n    ax6.tick_params(axis='y', labelsize=20)\n    ax6.tick_params(axis='x', labelsize=20)\n# # plt.savefig('try2_node1.png')\n\n\nwith plt.style.context('seaborn-white'):\n    fig, ((ax1, ax2), (ax3, ax4), (ax5, ax6)) = plt.subplots(3, 2,figsize=(40,20))\n    # fig.suptitle('Figure 1(node 1)',fontsize=40)\n    \n    ax1.plot(b_ar_values_true,label='Ground Truth')\n    ax1.legend(fontsize=20,loc='lower left',facecolor='white', frameon=True)\n    \n    ax2.plot(np.array(data_dict['FX'])[:,1],'-',color='C3',label='Complete Data')\n    ax2.legend(fontsize=20,loc='lower left',facecolor='white', frameon=True)\n    ax2.tick_params(axis='y', labelsize=20)\n    ax2.tick_params(axis='x', labelsize=20)\n    \n    ax3.plot(np.array(data_dict['FX'])[:,1],'--',color='C5',alpha=0.5,label='Complete Data')\n    ax3.plot(torch.cat([torch.tensor(dataset.features)[:2,1,0],torch.tensor(train_dataset_miss_rand.targets).reshape(-1,2)[:,1]],dim=0),'--o',color='C3',label='Observed Data')\n    ax3.legend(fontsize=20,loc='lower left',facecolor='white', frameon=True)\n    ax3.tick_params(axis='y', labelsize=20)\n    ax3.tick_params(axis='x', labelsize=20)\n    \n    ax4.plot(np.array(data_dict['FX'])[:,0],'--',color='C5',alpha=0.5,label='Complete Data')\n    ax4.plot(torch.cat([torch.tensor(dataset.features)[:2,1,0],torch.tensor(train_dataset_padded_rdbl.targets).reshape(-1,2)[:,1]],dim=0),'--o',color='C3',alpha=0.8,label='Linear Interpolation')\n    ax4.legend(fontsize=20,loc='lower left',facecolor='white', frameon=True)\n    ax4.tick_params(axis='y', labelsize=20)\n    ax4.tick_params(axis='x', labelsize=20)\n    \n    ax5.plot(torch.tensor(data_dict['FX'])[:400,1],'--',color='C5',alpha=0.5,label='Complete Data')\n    ax5.plot(b_ar_values_true[:400],color='black',label='Ground Truth',lw=3)\n    ax5.plot(evtor_rdbl.fhat_tr[:,1],color='brown',lw=3,label='GConvGRU',alpha=0.5)\n    ax5.plot(evtor_rdbl_it.fhat_tr[:,1],color='blue',lw=3,label='IT-TGNN',alpha=0.5)\n    # ax5.plot(55, 0, 'o', markersize=100, markerfacecolor='none', markeredgecolor='red',markeredgewidth=3)\n    # ax5.plot(150, 0, 'o', markersize=80, markerfacecolor='none', markeredgecolor='red',markeredgewidth=3)\n    # ax5.plot(185, 0, 'o', markersize=80, markerfacecolor='none', markeredgecolor='red',markeredgewidth=3)\n    ax5.legend(fontsize=20,loc='lower left',facecolor='white', frameon=True)\n    ax5.tick_params(axis='y', labelsize=20)\n    ax5.tick_params(axis='x', labelsize=20)\n    \n    ax6.plot(torch.tensor(data_dict['FX'])[399:,1],'--',color='C5',alpha=0.5,label='Test')\n    ax6.plot(b_ar_values_true[400:],color='black',label='Ground Truth',lw=3)\n    ax6.plot(evtor_rdbl.fhat_test[:,1],color='brown',lw=3,label='GConvGRU',alpha=0.5)\n    ax6.plot(evtor_rdbl_it.fhat_test[:,1],color='blue',lw=3,label='IT-TGNN',alpha=0.5)\n    ax6.legend(fontsize=20,loc='lower left',facecolor='white', frameon=True)\n    ax6.tick_params(axis='y', labelsize=20)\n    ax6.tick_params(axis='x', labelsize=20)\n# # plt.savefig('try2_node1.png')"
  },
  {
    "objectID": "posts/2_research/2023-03-17-ITSTGCN-Tutorial.html",
    "href": "posts/2_research/2023-03-17-ITSTGCN-Tutorial.html",
    "title": "ITSTGCN-Tutorial",
    "section": "",
    "text": "edit\n\n\nimport\n\nimport itstgcn \nimport torch\n\n\n\n예제1: vanilla STGCN\n- 데이터\n\ndata_dict = itstgcn.load_data('./data/fivenodes.pkl')\n\n\nloader = itstgcn.DatasetLoader(data_dict)\ndataset = loader.get_dataset(lags=2)\ntrain_dataset, test_dataset = itstgcn.utils.temporal_signal_split(dataset, train_ratio=0.8)\n\n- 학습\n\nlrnr = itstgcn.StgcnLearner(train_dataset,dataset_name='five_nodes')\n\n/home/csy/Dropbox/blog/posts/GCN/itstgcn/learners.py:84: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at  /opt/conda/conda-bld/pytorch_1639180588308/work/torch/csrc/utils/tensor_new.cpp:201.)\n  self.lags = torch.tensor(train_dataset.features).shape[-1]\n\n\n\nlrnr.learn(filters=4,epoch=5)\n\n5/5\n\n\n- 적합값\n\n#lrnr(train_dataset) \n#lrnr(test_dataset)\n\n\n실행하면 X,y,yhat 출력\n\n- 모형 평가 및 시각화\n\nevtor = itstgcn.Evaluator(lrnr,train_dataset,test_dataset)\n\n\nfig = evtor.plot('--.',h=5,max_node=3,label='complete data',alpha=0.5) \nfig.set_figwidth(12)\nfig.tight_layout()\nfig\n\n\n\n\n\n\n\n\n\n\n예제2: padding missing values\n- 데이터\n\n_data = itstgcn.load_data('./data/fivenodes.pkl')\n_edges = torch.tensor(_data['edges']).nonzero().tolist()\n_FX = _data['f'].tolist()\n_node_ids = {'node1':0, 'node2':1, 'node3':2, 'node4':3, 'node5':4} \ndata_dict = {'edges':_edges, 'node_ids':_node_ids, 'FX':_FX}\n\n\nloader = itstgcn.DatasetLoader(data_dict)\ndataset = loader.get_dataset(lags=2)\ntrain_dataset, test_dataset = itstgcn.utils.temporal_signal_split(dataset, train_ratio=0.8)\n\n- 임의로 결측치 발생\n\nmindex = itstgcn.rand_mindex(train_dataset,mrate=0.5)\ntrain_dataset_miss = itstgcn.miss(train_dataset,mindex=mindex,mtype='rand')\n\n\nfig = itstgcn.plot(torch.tensor(train_dataset_miss.targets),'o')\nfig \n\n\n\n\n\n\n\n\n- 적절한 method로 결측치를 채움 (default 는 linear)\n\ntrain_dataset_padded = itstgcn.padding(train_dataset_miss) # padding(train_dataset_miss,method='linear'와 같음)\n\n\nfig = itstgcn.plot(torch.tensor(train_dataset_miss.targets),'o')\nitstgcn.plot_add(fig,torch.tensor(train_dataset_padded.targets),'--x',color='C1',alpha=0.5)\n\n\n\n\n\n\n\n\n다른 method로 결측치를 채울수도 있음. 사용할 수 있는 방법들은 아래에 정리되어 있음\n\nref: https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.interpolate.html\n\n\ntrain_dataset_padded = itstgcn.padding(train_dataset_miss,interpolation_method='nearest')\n\n\nfig = itstgcn.plot(torch.tensor(train_dataset_miss.targets),'o')\nitstgcn.plot_add(fig,torch.tensor(train_dataset_padded.targets),'--x',color='C1',alpha=0.5)\n\n\n\n\n\n\n\n\n\ntrain_dataset_padded = itstgcn.padding(train_dataset_miss,interpolation_method='quadratic')\n\n\nfig = itstgcn.plot(torch.tensor(train_dataset_miss.targets),'o')\nitstgcn.plot_add(fig,torch.tensor(train_dataset_padded.targets),'--x',color='C1',alpha=0.5)\n\n\n\n\n\n\n\n\n\ntrain_dataset_padded = itstgcn.padding(train_dataset_miss,interpolation_method='cubic')\n\n\nfig = itstgcn.utils.plot(torch.tensor(train_dataset_miss.targets),'o')\nitstgcn.utils.plot_add(fig,torch.tensor(train_dataset_padded.targets),'--x',color='C1',alpha=0.5)\n\n\n\n\n\n\n\n\n- 블락으로 결측치 발생\n\nmindex = [list(range(10,100)),[],list(range(50,80)),[],[]]\ntrain_dataset_miss = itstgcn.miss(train_dataset,mindex,mtype='block')\n\n\nfig = itstgcn.utils.plot(torch.tensor(train_dataset_miss.targets),'o')\nfig \n\n\n\n\n\n\n\n\n\n\n예제3: vanilla STGCN with random missing\n- data\n\n_data = itstgcn.load_data('./data/fivenodes.pkl')\n_edges = torch.tensor(_data['edges']).nonzero().tolist()\n_FX = _data['f'].tolist()\n_node_ids = {'node1':0, 'node2':1, 'node3':2, 'node4':3, 'node5':4} \ndata_dict = {'edges':_edges, 'node_ids':_node_ids, 'FX':_FX}\n\n\nloader = itstgcn.DatasetLoader(data_dict)\ndataset = loader.get_dataset(lags=2)\ntrain_dataset, test_dataset = itstgcn.utils.temporal_signal_split(dataset, train_ratio=0.8)\n\n\nmindex = itstgcn.rand_mindex(train_dataset,mrate=0.5)\ntrain_dataset_miss = itstgcn.miss(train_dataset,mindex,mtype='rand')\ntrain_dataset_padded = itstgcn.padding(train_dataset_miss) # padding(train_dataset_miss,method='linear'와 같음)\n\n- 학습\n\nlrnr = itstgcn.learners.StgcnLearner(train_dataset_padded)\n\n\nlrnr.learn(filters=4,epoch=5)\n\n5/5\n\n\n- 적합값\n\n#lrnr(train_dataset_padded) \n#lrnr(test_dataset)\n\n\n실행하면 X,y,yhat 출력\n\n- 모형 평가 및 시각화\n\nevtor = itstgcn.Evaluator(lrnr,train_dataset_padded,test_dataset)\n\n\nfig = evtor.plot('--.',h=5,max_node=5,label='complete data',alpha=0.5) # max_nodes 는 1보다 커야함\nfig.set_figwidth(12)\nfig.tight_layout()\nfig\n\n\n\n\n\n\n\n\n\n\n예제4: vanilla STGCN with block missing\n- data\n\n_data = itstgcn.load_data('./data/fivenodes.pkl')\n_edges = torch.tensor(_data['edges']).nonzero().tolist()\n_FX = _data['f'].tolist()\n_node_ids = {'node1':0, 'node2':1, 'node3':2, 'node4':3, 'node5':4} \ndata_dict = {'edges':_edges, 'node_ids':_node_ids, 'FX':_FX}\n\n\nloader = itstgcn.DatasetLoader(data_dict)\ndataset = loader.get_dataset(lags=2)\ntrain_dataset, test_dataset = itstgcn.utils.temporal_signal_split(dataset, train_ratio=0.8)\n\n\nmindex = [list(range(10,100)),[],list(range(50,80)),[],[]]\ntrain_dataset_miss = itstgcn.miss(train_dataset,mindex,mtype='block')\ntrain_dataset_padded = itstgcn.padding(train_dataset_miss) # padding(train_dataset_miss,method='linear'와 같음)\n\n- 학습\n\nlrnr = itstgcn.StgcnLearner(train_dataset_padded)\n\n\nlrnr.learn(filters=4,epoch=5)\n\n5/5\n\n\n- 적합값\n\n#lrnr(train_dataset_padded) \n#lrnr(test_dataset)\n\n\n실행하면 X,y,yhat 출력\n\n- 모형 평가 및 시각화\n\nevtor = itstgcn.Evaluator(lrnr,train_dataset_padded,test_dataset)\n\n\nfig = evtor.plot('--.',h=5,max_node=5,label='complete data',alpha=0.5) # max_nodes 는 1보다 커야함\nfig.set_figwidth(12)\nfig.tight_layout()\nfig\n\n\n\n\n\n\n\n\n\n\n예제5: threshold example (random)\n- data\n\n_data = itstgcn.load_data('./data/fivenodes.pkl')\n_edges = torch.tensor(_data['edges']).nonzero().tolist()\n_FX = _data['f'].tolist()\n_node_ids = {'node1':0, 'node2':1, 'node3':2, 'node4':3, 'node5':4} \ndata_dict = {'edges':_edges, 'node_ids':_node_ids, 'FX':_FX}\n\n\nloader = itstgcn.DatasetLoader(data_dict)\ndataset = loader.get_dataset(lags=2)\ntrain_dataset, test_dataset = itstgcn.utils.temporal_signal_split(dataset, train_ratio=0.8)\n\n- 결측치 발생 및 패딩\n\nmindex=itstgcn.rand_mindex(train_dataset,mrate=0.5)\ntrain_dataset_miss = itstgcn.miss(train_dataset,mindex,mtype='rand')\ntrain_dataset_padded = itstgcn.padding(train_dataset_miss)\n\n\nf_miss,_ = itstgcn.convert_train_dataset(train_dataset_miss)\nf_padded,_ = itstgcn.convert_train_dataset(train_dataset_padded)\n\n\nfig = itstgcn.utils.plot(f_miss,'o')\nitstgcn.utils.plot_add(fig,f_padded,'--x',alpha=0.5)\n\n\n\n\n\n\n\n\n- update by frequency thresholding\n\nfig = itstgcn.plot(f_miss,'o',alpha=0.5)\nitstgcn.plot_add(fig,f_padded,'--x',alpha=0.5)\nf_updated = itstgcn.update_from_freq_domain(f_padded,train_dataset_padded.mindex)\nitstgcn.plot_add(fig,f_updated,'-')\n\n\n\n\n\n\n\n\n\n\n예제6: threshold example (block)\n- data\n\n_data = itstgcn.load_data('./data/fivenodes.pkl')\n_edges = torch.tensor(_data['edges']).nonzero().tolist()\n_FX = _data['f'].tolist()\n_node_ids = {'node1':0, 'node2':1, 'node3':2, 'node4':3, 'node5':4} \ndata_dict = {'edges':_edges, 'node_ids':_node_ids, 'FX':_FX}\n\n\nloader = itstgcn.DatasetLoader(data_dict)\ndataset = loader.get_dataset(lags=2)\ntrain_dataset, test_dataset = itstgcn.temporal_signal_split(dataset, train_ratio=0.8)\n\n- 결측치 발생 및 패딩\n\nmindex=[list(range(10,100)),[],list(range(50,80)),[],[]]\ntrain_dataset_miss = itstgcn.miss(train_dataset,mindex,mtype='block')\ntrain_dataset_padded = itstgcn.padding(train_dataset_miss)\n\n\nf_miss,_ = itstgcn.convert_train_dataset(train_dataset_miss)\nf_padded,_ = itstgcn.convert_train_dataset(train_dataset_padded)\n\n\nfig = itstgcn.plot(f_miss,'o')\nitstgcn.plot_add(fig,f_padded,'--x',alpha=0.5)\n\n\n\n\n\n\n\n\n- update by frequency thresholding\n\nfig = itstgcn.plot(f_miss,'o',alpha=0.5)\nitstgcn.plot_add(fig,f_padded,'--x',alpha=0.5)\nf_updated = itstgcn.update_from_freq_domain(f_padded,train_dataset_padded.mindex)\nitstgcn.plot_add(fig,f_updated,'-')\n\n\n\n\n\n\n\n\n\n\n예제7: iterative thresholded STGCN (IT-STGCN) with random missing\n- data\n\n_data = itstgcn.load_data('./data/fivenodes.pkl')\n_edges = torch.tensor(_data['edges']).nonzero().tolist()\n_FX = _data['f'].tolist()\n_node_ids = {'node1':0, 'node2':1, 'node3':2, 'node4':3, 'node5':4} \ndata_dict = {'edges':_edges, 'node_ids':_node_ids, 'FX':_FX}\n\n\nloader = itstgcn.DatasetLoader(data_dict)\ndataset = loader.get_dataset(lags=2)\ntrain_dataset, test_dataset = itstgcn.temporal_signal_split(dataset, train_ratio=0.8)\n\n\nmindex = itstgcn.rand_mindex(train_dataset,mrate=0.5)\ntrain_dataset_miss = itstgcn.miss(train_dataset,mindex,mtype='rand')\ntrain_dataset_padded = itstgcn.padding(train_dataset_miss) # padding(train_dataset_miss,method='linear'와 같음)\n\n- 학습\n\nlrnr = itstgcn.ITStgcnLearner(train_dataset_padded)\n\n\nlrnr.learn(filters=4,epoch=5)\n\n5/5\n\n\n- 적합값\n\n#lrnr(train_dataset_padded) \n#lrnr(test_dataset)['yhat'].shape\n\n\n실행하면 X,y,yhat 출력\n\n- 모형 평가 및 시각화\n\nevtor = itstgcn.Evaluator(lrnr,train_dataset_padded,test_dataset)\n\n\nfig = evtor.plot('--.',h=5,max_node=3,label='complete data',alpha=0.5) # max_nodes 는 1보다 커야함\nfig.set_figwidth(12)\nfig.tight_layout()\nfig\n\n\n\n\n\n\n\n\n\n\n예제8: iterative thresholded STGCN (IT-STGCN) with block missing\n- data\n\n_data = itstgcn.load_data('./data/fivenodes.pkl')\n_edges = torch.tensor(_data['edges']).nonzero().tolist()\n_FX = _data['f'].tolist()\n_node_ids = {'node1':0, 'node2':1, 'node3':2, 'node4':3, 'node5':4} \ndata_dict = {'edges':_edges, 'node_ids':_node_ids, 'FX':_FX}\n\n\nloader = itstgcn.DatasetLoader(data_dict)\ndataset = loader.get_dataset(lags=2)\ntrain_dataset, test_dataset = itstgcn.temporal_signal_split(dataset, train_ratio=0.8)\n\n\nmindex = [list(range(10,100)),[],list(range(50,80)),[],[]]\ntrain_dataset_miss = itstgcn.miss(train_dataset,mindex,mtype='block')\ntrain_dataset_padded = itstgcn.padding(train_dataset_miss) # padding(train_dataset_miss,method='linear'와 같음)\n\n- 학습\n\nlrnr = itstgcn.ITStgcnLearner(train_dataset_padded)\n\n\nlrnr.learn(filters=4,epoch=5)\n\n5/5\n\n\n- 적합값\n\n#lrnr(train_dataset_padded) \n#lrnr(test_dataset)['yhat'].shape\n\n\n실행하면 X,y,yhat 출력\n\n- 모형 평가 및 시각화\n\nevtor = itstgcn.Evaluator(lrnr,train_dataset_padded,test_dataset)\n\n\nfig = evtor.plot('--.',h=5,max_node=3,label='complete data',alpha=0.5) # max_nodes 는 1보다 커야함\nfig.set_figwidth(12)\nfig.tight_layout()\nfig\n\n\n\n\n\n\n\n\n\n\n예제9: GNAR (random missing)\n\n_data = itstgcn.load_data('./data/fivenodes.pkl')\n_edges = torch.tensor(_data['edges']).nonzero().tolist()\n_FX = _data['f'].tolist()\n_node_ids = {'node1':0, 'node2':1, 'node3':2, 'node4':3, 'node5':4} \ndata_dict = {'edges':_edges, 'node_ids':_node_ids, 'FX':_FX}\n\n\nloader = itstgcn.DatasetLoader(data_dict)\n\n\nloader = itstgcn.DatasetLoader(data_dict)\ndataset = loader.get_dataset(lags=2)\ntrain_dataset, test_dataset = itstgcn.temporal_signal_split(dataset, train_ratio=0.8)\n\n\nmindex=itstgcn.rand_mindex(train_dataset,mrate=0.5)\ntrain_dataset_miss = itstgcn.miss(train_dataset,mindex,mtype='rand')\ntrain_dataset_padded = itstgcn.padding(train_dataset_miss) # padding(train_dataset_miss,method='linear'와 같음)\n\n- 학습\n\nlrnr = itstgcn.GNARLearner(train_dataset_padded)\n\n\nlrnr.learn()\n\nWARNING: diagonal entries present in original matrix, these will be removed\n\n\n- 적합값\n\n#lrnr(train_dataset_padded) \n#lrnr(test_dataset)\n\n\n실행하면 X,y,yhat 출력\n\n- 모형 평가 및 시각화\n\nevtor = itstgcn.Evaluator(lrnr,train_dataset_padded,test_dataset)\n\nWARNING: diagonal entries present in original matrix, these will be removed\nWARNING: diagonal entries present in original matrix, these will be removed\n\n\n\nfig = evtor.plot('--.',h=5,max_node=3,label='complete data',alpha=0.5) # max_nodes 는 1보다 커야함\nfig.set_figwidth(12)\nfig.tight_layout()\nfig\n\n\n\n\n\n\n\n\n\n\n예제10: GNAR (block missing)\n\n_data = itstgcn.load_data('./data/fivenodes.pkl')\n_edges = torch.tensor(_data['edges']).nonzero().tolist()\n_FX = _data['f'].tolist()\n_node_ids = {'node1':0, 'node2':1, 'node3':2, 'node4':3, 'node5':4} \ndata_dict = {'edges':_edges, 'node_ids':_node_ids, 'FX':_FX}\n\n\nloader = itstgcn.DatasetLoader(data_dict)\ndataset = loader.get_dataset(lags=2)\ntrain_dataset, test_dataset = itstgcn.temporal_signal_split(dataset, train_ratio=0.8)\n\n\nmindex=[list(range(10,100)),[],list(range(50,80)),[],[]]\ntrain_dataset_miss = itstgcn.miss(train_dataset,mindex,mtype='block')\ntrain_dataset_padded = itstgcn.padding(train_dataset_miss) # padding(train_dataset_miss,method='linear'와 같음)\n\n- 학습\n\nlrnr = itstgcn.GNARLearner(train_dataset_padded)\n\n\nlrnr.learn()\n\nWARNING: diagonal entries present in original matrix, these will be removed\n\n\n- 적합값\n\n#lrnr(train_dataset_padded) \n#lrnr(test_dataset)\n\n\n실행하면 X,y,yhat 출력\n\n- 모형 평가 및 시각화\n\nevtor = itstgcn.Evaluator(lrnr,train_dataset_padded,test_dataset)\n\nWARNING: diagonal entries present in original matrix, these will be removed\nWARNING: diagonal entries present in original matrix, these will be removed\n\n\n\nfig = evtor.plot('--.',h=5,max_node=3,label='complete data',alpha=0.5) # max_nodes 는 1보다 커야함\nfig.set_figwidth(12)\nfig.tight_layout()\nfig"
  },
  {
    "objectID": "posts/2_research/2023-01-18-Algorithm_traintest_2.html",
    "href": "posts/2_research/2023-01-18-Algorithm_traintest_2.html",
    "title": "2nd ST-GCN Example dividing train and test",
    "section": "",
    "text": "Try to divide train and test(ST-GCN WikiMathsDatasetLoader)"
  },
  {
    "objectID": "posts/2_research/2023-01-18-Algorithm_traintest_2.html#train",
    "href": "posts/2_research/2023-01-18-Algorithm_traintest_2.html#train",
    "title": "2nd ST-GCN Example dividing train and test",
    "section": "Train",
    "text": "Train\n\ndata_train=[]\nfor time, snapshot in enumerate(train_dataset):\n    data_train.append([time,snapshot])\n\n\ndata_train[0][1].x.shape,data_train[0][1].y.shape,data_train[0][1].edge_index.shape,data_train[0][1].edge_attr.shape\n\n(torch.Size([1068, 1]),\n torch.Size([1068]),\n torch.Size([2, 27079]),\n torch.Size([27079]))\n\n\n\ntime\n\n583\n\n\n\nT_train = time\nN = len(data[0][1].x)\n\n\nedge_index = data_train[0][1].edge_index\nedge_attr = data_train[0][1].edge_attr\n\n\nx_train = []\nfor i in range(time):\n    x_train.append(data_train[i][1].x)\n\n\ndata_tensor = torch.Tensor()\n# Iterate over the data points of the dataset\nfor i in x_train:\n    # Concatenate the data point to the tensor\n    data_tensor = torch.cat((data_tensor, i), dim=0)\nx_train = data_tensor.reshape(time,1068,-1)\nx_train.shape\n\ntorch.Size([583, 1068, 1])\n\n\n\ny_train = []\nfor i in range(time):\n    y_train.append(data_train[i][1].y)\n\n\ndata_tensor = torch.Tensor()\n# Iterate over the data points of the dataset\nfor i in y_train:\n    # Concatenate the data point to the tensor\n    data_tensor = torch.cat((data_tensor, i), dim=0)\ny_train = data_tensor.reshape(time,1068)\ny_train.shape\n\ntorch.Size([583, 1068])\n\n\n\nx_train.shape, y_train.shape\n\n(torch.Size([583, 1068, 1]), torch.Size([583, 1068]))"
  },
  {
    "objectID": "posts/2_research/2023-01-18-Algorithm_traintest_2.html#test",
    "href": "posts/2_research/2023-01-18-Algorithm_traintest_2.html#test",
    "title": "2nd ST-GCN Example dividing train and test",
    "section": "Test",
    "text": "Test\n\ndata_test=[]\nfor time, snapshot in enumerate(test_dataset):\n    data_test.append([time,snapshot])\n\n\ndata_test[0][1].x.shape,data_test[0][1].y.shape,data_test[0][1].edge_index.shape,data_test[0][1].edge_attr.shape\n\n(torch.Size([1068, 1]),\n torch.Size([1068]),\n torch.Size([2, 27079]),\n torch.Size([27079]))\n\n\n\ntime\n\n145\n\n\n\nT_test = time\n\n\nx_test = []\nfor i in range(time):\n    x_test.append(data_test[i][1].x)\n\n\ndata_tensor = torch.Tensor()\n# Iterate over the data points of the dataset\nfor i in x_test:\n    # Concatenate the data point to the tensor\n    data_tensor = torch.cat((data_tensor, i), dim=0)\nx_test = data_tensor.reshape(time,1068,-1)\nx_test.shape\n\ntorch.Size([145, 1068, 1])\n\n\n\ny_test = []\nfor i in range(time):\n    y_test.append(data_test[i][1].y)\n\n\ndata_tensor = torch.Tensor()\n# Iterate over the data points of the dataset\nfor i in y_test:\n    # Concatenate the data point to the tensor\n    data_tensor = torch.cat((data_tensor, i), dim=0)\ny_test = data_tensor.reshape(time,1068)\ny_test.shape\n\ntorch.Size([145, 1068])\n\n\n\nx_test.shape, y_test.shape\n\n(torch.Size([145, 1068, 1]), torch.Size([145, 1068]))"
  },
  {
    "objectID": "posts/2_research/2023-01-18-Algorithm_traintest_2.html#st-gcn",
    "href": "posts/2_research/2023-01-18-Algorithm_traintest_2.html#st-gcn",
    "title": "2nd ST-GCN Example dividing train and test",
    "section": "ST-GCN",
    "text": "ST-GCN\n\nmean_f_train = x_train_mean.reshape(T_train,N,1).float()\n\n\nmean_X = mean_f_train[:438,:,:]\nmean_y = mean_f_train[145:,:,:]\n\n\nmean_X.shape,mean_y.shape\n\n(torch.Size([438, 1068, 1]), torch.Size([438, 1068, 1]))\n\n\n\nmodel = RecurrentGCN(node_features=1, filters=4)\n\noptimizer = torch.optim.Adam(model.parameters(), lr=0.01)\n\nmodel.train()\n\nfor epoch in tqdm(range(50)):\n    for time, (xt,yt) in enumerate(zip(mean_X,mean_y)):\n        y_hat = model(xt, edge_index, edge_attr)\n        cost = torch.mean((y_hat-yt)**2)\n        cost.backward()\n        optimizer.step()\n        optimizer.zero_grad()\n\n100%|██████████| 50/50 [04:17&lt;00:00,  5.15s/it]\n\n\n\nmean_X_fore = mean_f_train[438:,:]\n\n\nmean_fhat = torch.stack([model(xt, edge_index, edge_attr) for xt in mean_X_fore]).detach().numpy()\n\n\nmean_X_fore.shape,x_test.shape\n\n(torch.Size([145, 1068, 1]), torch.Size([145, 1068, 1]))"
  },
  {
    "objectID": "posts/2_research/2023-01-18-Algorithm_traintest_2.html#st-gcn-1",
    "href": "posts/2_research/2023-01-18-Algorithm_traintest_2.html#st-gcn-1",
    "title": "2nd ST-GCN Example dividing train and test",
    "section": "ST-GCN",
    "text": "ST-GCN\n\nlinear_f_train = x_train_linear.clone()\n\n\nlinear_X = linear_f_train[:438,:,:]\nlinear_y = linear_f_train[145:,:,:]\n\n\nmodel = RecurrentGCN(node_features=1, filters=4)\n\noptimizer = torch.optim.Adam(model.parameters(), lr=0.01)\n\nmodel.train()\n\nfor epoch in tqdm(range(50)):\n    for time, (xt,yt) in enumerate(zip(linear_X,linear_y)):\n        y_hat = model(xt, edge_index, edge_attr)\n        cost = torch.mean((y_hat-yt)**2)\n        cost.backward()\n        optimizer.step()\n        optimizer.zero_grad()\n\n100%|██████████| 50/50 [04:20&lt;00:00,  5.22s/it]\n\n\n\nlinear_X_fore = linear_f_train[438:,:]\n\n\nlinear_X_fore.shape\n\ntorch.Size([145, 1068, 1])\n\n\n\nlinear_fhat = torch.stack([model(xt, edge_index, edge_attr) for xt in linear_X_fore]).detach().numpy()\n\n\nlinear_X_fore.shape,x_test.shape\n\n(torch.Size([145, 1068, 1]), torch.Size([145, 1068, 1]))"
  },
  {
    "objectID": "posts/2_research/2023-07-17-toy_example_guebin.html",
    "href": "posts/2_research/2023-07-17-toy_example_guebin.html",
    "title": "Toy example using GNAR_2",
    "section": "",
    "text": "Import\n\nimport rpy2\nimport rpy2.robjects as ro \nfrom rpy2.robjects.vectors import FloatVector \nfrom rpy2.robjects.packages import importr\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport pandas as pd\nimport pickle\nimport torch\nimport itstgcn_gb as itstgcn\nimport random\n\ndef save_data(data_dict,fname):\n    with open(fname,'wb') as outfile:\n        pickle.dump(data_dict,outfile)\n\ndef load_data(fname):\n    with open(fname, 'rb') as outfile:\n        data_dict = pickle.load(outfile)\n    return data_dict\n\ndef toy_analyze(FX,edges,lags,train_ratio,mrate,filters,epoch,mtype): \n    data_dict = {'edges':edges, 'node_ids':{i:'node'+str(i) for i in range(FX.shape[-1])}, 'FX':FX}\n    save_data(data_dict,'toy_ex_dataset.pkl')\n    data_dict = load_data('toy_ex_dataset.pkl')\n    loader = itstgcn.DatasetLoader(data_dict)\n    dataset = loader.get_dataset(lags=lags)\n    train_dataset, test_dataset = itstgcn.temporal_signal_split(dataset, train_ratio=train_ratio)\n    mindex_rand = itstgcn.rand_mindex(train_dataset,mrate=mrate)\n    train_dataset_miss_rand = itstgcn.miss(train_dataset,mindex_rand,mtype=mtype)\n    train_dataset_padded_rand = itstgcn.padding(train_dataset_miss_rand) # padding(train_dataset_miss,method='linear'와 같음)\n    lrnr_classic = itstgcn.StgcnLearner(train_dataset_padded_rand)\n    lrnr_proposed = itstgcn.ITStgcnLearner(train_dataset_padded_rand)\n    lrnr_classic.learn(filters=filters,epoch=epoch)\n    lrnr_proposed.learn(filters=filters,epoch=epoch)\n    yhat_classic=lrnr_classic(dataset)['yhat']\n    yhat_proposed=lrnr_proposed(dataset)['yhat']    \n    return yhat_classic,yhat_proposed\n\n\n\nData\n\nT = 50\nt = np.linspace(0,np.pi*2,T)\ne = np.random.randn(T)*0.1\ny1 = np.cos(2*t)\ny2 = np.cos(3*t)\ny3 = y1+y2+e\ny = np.stack([y1,y2,y3],axis=1)\n_, N = y.shape \ntrain_ratio = 0.9\ntest_len = int(T*(1-train_ratio))\ntr_len = T - test_len\ntest_index = [False]*tr_len + [True]*test_len \nedges = [[0,2],[1,2]]\nlags = 8\nmrate = 0.8\nfilters = 2\nepoch = 50\nmtype = 'rand' \n### \nyhat_classic,yhat_proposed = toy_analyze(y,edges,lags,train_ratio,mrate,filters,epoch,mtype)\n\n50/50\n\n\n\nnode = 2\nplt.rcParams['figure.dpi'] = 200\nplt.plot(y[lags:,node],'.')\nplt.plot(yhat_classic[:,node],'--',label='classic')\nplt.plot(yhat_proposed[:,node],'--',label='proposed')\nplt.plot(y[lags:,0]+y[lags:,1],'-',label='true')\nplt.legend()"
  },
  {
    "objectID": "posts/2_research/2023-02-06-ESTGCN_GNAR_DATA_3.html",
    "href": "posts/2_research/2023-02-06-ESTGCN_GNAR_DATA_3.html",
    "title": "Class of Method(GNAR) lag 1 80% Missing repeat",
    "section": "",
    "text": "GNAR fiveNet,fivenodes lag 1"
  },
  {
    "objectID": "posts/2_research/2023-02-06-ESTGCN_GNAR_DATA_3.html#missing-일정",
    "href": "posts/2_research/2023-02-06-ESTGCN_GNAR_DATA_3.html#missing-일정",
    "title": "Class of Method(GNAR) lag 1 80% Missing repeat",
    "section": "missing 일정",
    "text": "missing 일정\n\nstgcn_train1 = []\nstgcn_test1 = []\n\n_zero = Missing(fiveVTS_train)\n_zero.miss(percent = 0.8)\n_zero.second_linear()\n\nmissing_index = _zero.number\ninterpolated_signal = _zero.train_linear\n\nX = torch.tensor(interpolated_signal).reshape(int(T*0.8),N,1).float()[:int(T*0.8-1),:,:]\ny = torch.tensor(interpolated_signal).reshape(int(T*0.8),N,1).float()[1:,:,:]\n\nXX = torch.tensor(fiveVTS_test.reshape(int(T*0.2),N,1)[:-1,:,:]).float()\nyy = torch.tensor(fiveVTS_test.reshape(int(T*0.2),N,1)[1:,:,:]).float()\n\nreal_y = torch.tensor(fiveVTS_train).reshape(int(T*0.8),N,1).float()[1:,:,:]\n\n\nfor i in range(100):\n    net = RecurrentGCN(node_features=1, filters=4)\n    optimizer = torch.optim.Adam(net.parameters(), lr=0.01)\n    net.train()\n    for epoch in range(50):\n        for time, (xt,yt) in enumerate(zip(X,y)):\n            yt_hat = net(xt, edge_index, edge_attr)\n            cost = torch.mean((yt_hat-yt)**2)\n            cost.backward()\n            optimizer.step()\n            optimizer.zero_grad()\n\n    yhat = torch.stack([net(xt, edge_index, edge_attr) for xt in X]).detach().numpy()\n    yyhat = torch.stack([net(xt, edge_index, edge_attr) for xt in XX]).detach().numpy()\n    \n    train_mse_total_stgcn = (((real_y-yhat).squeeze())**2).mean()\n    test_mse_total_stgcn = (((yy-yyhat).squeeze())**2).mean()\n    \n    stgcn_train1.append(train_mse_total_stgcn.tolist())\n    stgcn_test1.append(test_mse_total_stgcn.tolist())\n\n\nplt.figure(figsize=(12, 8))\nplt.boxplot(stgcn_train1);\n\n\n\n\n\n\n\n\n\nplt.figure(figsize=(12, 8))\nplt.boxplot(stgcn_test1);"
  },
  {
    "objectID": "posts/2_research/2023-02-06-ESTGCN_GNAR_DATA_3.html#missing-다르게",
    "href": "posts/2_research/2023-02-06-ESTGCN_GNAR_DATA_3.html#missing-다르게",
    "title": "Class of Method(GNAR) lag 1 80% Missing repeat",
    "section": "missing 다르게",
    "text": "missing 다르게\n\nstgcn_train2 = []\nstgcn_test2 = []\n\nXX = torch.tensor(fiveVTS_test.reshape(int(T*0.2),N,1)[:-1,:,:]).float()\nyy = torch.tensor(fiveVTS_test.reshape(int(T*0.2),N,1)[1:,:,:]).float()\n\nreal_y = torch.tensor(fiveVTS_train).reshape(int(T*0.8),N,1).float()[1:,:,:]\n\n\nfor i in range(100):\n    \n    _zero = Missing(fiveVTS_train)\n    _zero.miss(percent = 0.8)\n    _zero.second_linear()\n\n    missing_index = _zero.number\n    interpolated_signal = _zero.train_linear\n\n\n    X = torch.tensor(interpolated_signal).reshape(int(T*0.8),N,1).float()[:int(T*0.8-1),:,:]\n    y = torch.tensor(interpolated_signal).reshape(int(T*0.8),N,1).float()[1:,:,:]\n\n    net = RecurrentGCN(node_features=1, filters=4)\n    optimizer = torch.optim.Adam(net.parameters(), lr=0.01)\n    net.train()\n    for epoch in range(50):\n        for time, (xt,yt) in enumerate(zip(X,y)):\n            yt_hat = net(xt, edge_index, edge_attr)\n            cost = torch.mean((yt_hat-yt)**2)\n            cost.backward()\n            optimizer.step()\n            optimizer.zero_grad()\n\n    yhat = torch.stack([net(xt, edge_index, edge_attr) for xt in X]).detach().numpy()\n    yyhat = torch.stack([net(xt, edge_index, edge_attr) for xt in XX]).detach().numpy()\n    \n    train_mse_total_stgcn = (((real_y-yhat).squeeze())**2).mean()\n    test_mse_total_stgcn = (((yy-yyhat).squeeze())**2).mean() \n    \n    stgcn_train2.append(train_mse_total_stgcn.tolist())\n    stgcn_test2.append(test_mse_total_stgcn.tolist())\n\n\nplt.figure(figsize=(12, 8))\nplt.boxplot(stgcn_train2);\n\n\n\n\n\n\n\n\n\nplt.figure(figsize=(12, 8))\nplt.boxplot(stgcn_test2);"
  },
  {
    "objectID": "posts/2_research/2023-02-06-ESTGCN_GNAR_DATA_3.html#missing-일정-1",
    "href": "posts/2_research/2023-02-06-ESTGCN_GNAR_DATA_3.html#missing-일정-1",
    "title": "Class of Method(GNAR) lag 1 80% Missing repeat",
    "section": "missing 일정",
    "text": "missing 일정\n\nestgcn_train1 = []\nestgcn_test1 = []\n\n_zero = Missing(fiveVTS_train)\n_zero.miss(percent = 0.8)\n_zero.second_linear()\n\nmissing_index = _zero.number\ninterpolated_signal = _zero.train_linear\n\nX = torch.tensor(interpolated_signal).reshape(int(T*0.8),N,1).float()[:int(T*0.8-1),:,:]\ny = torch.tensor(interpolated_signal).reshape(int(T*0.8),N,1).float()[1:,:,:]\n\nXX = torch.tensor(fiveVTS_test.reshape(int(T*0.2),N,1)[:-1,:,:]).float()\nyy = torch.tensor(fiveVTS_test.reshape(int(T*0.2),N,1)[1:,:,:]).float()\n\nreal_y = torch.tensor(fiveVTS_train).reshape(int(T*0.8),N,1).float()[1:,:,:]\n\n\nfor i in range(100):\n    net = RecurrentGCN(node_features=1, filters=4)\n    optimizer = torch.optim.Adam(net.parameters(), lr=0.01)\n    net.train()\n    signal = interpolated_signal.copy()\n    for epoch in range(50):\n        signal = update_from_freq_domain(signal,missing_index)\n        X = torch.tensor(signal).reshape(int(T*0.8),N,1).float()[:int(T*0.8-1),:,:]\n        y = torch.tensor(signal).reshape(int(T*0.8),N,1).float()[1:,:,:]\n        for time, (xt,yt) in enumerate(zip(X,y)):        \n            yt_hat = net(xt, edge_index, edge_attr)\n            cost = torch.mean((yt_hat-yt)**2)\n            cost.backward()\n            optimizer.step()\n            optimizer.zero_grad()\n        signal = torch.concat([X.squeeze(),yt_hat.detach().squeeze().reshape(1,-1)])        \n\n    yhat = torch.stack([net(xt, edge_index, edge_attr) for xt in X]).detach().numpy()\n    yyhat = torch.stack([net(xt, edge_index, edge_attr) for xt in XX]).detach().numpy()\n\n    train_mse_total_estgcn = (((real_y-yhat).squeeze())**2).mean()\n    test_mse_total_estgcn = (((yy-yyhat).squeeze())**2).mean()\n    \n    estgcn_train1.append(train_mse_total_estgcn.tolist())\n    estgcn_test1.append(test_mse_total_estgcn.tolist())\n\n\nplt.figure(figsize=(12, 8))\nplt.boxplot(estgcn_train1); \n\n\n\n\n\n\n\n\n\nplt.figure(figsize=(12, 8))\nplt.boxplot(estgcn_test1);"
  },
  {
    "objectID": "posts/2_research/2023-02-06-ESTGCN_GNAR_DATA_3.html#missing-매번-다르게",
    "href": "posts/2_research/2023-02-06-ESTGCN_GNAR_DATA_3.html#missing-매번-다르게",
    "title": "Class of Method(GNAR) lag 1 80% Missing repeat",
    "section": "missing 매번 다르게",
    "text": "missing 매번 다르게\n\nestgcn_train2 = []\nestgcn_test2 = []\n\nXX = torch.tensor(fiveVTS_test.reshape(int(T*0.2),N,1)[:-1,:,:]).float()\nyy = torch.tensor(fiveVTS_test.reshape(int(T*0.2),N,1)[1:,:,:]).float()\n\nreal_y = torch.tensor(fiveVTS_train).reshape(int(T*0.8),N,1).float()[1:,:,:]\n\n\nfor i in range(100):\n    \n    _zero = Missing(fiveVTS_train)\n    _zero.miss(percent = 0.8)\n    _zero.second_linear()\n\n    missing_index = _zero.number\n    interpolated_signal = _zero.train_linear\n\n    X = torch.tensor(interpolated_signal).reshape(int(T*0.8),N,1).float()[:int(T*0.8-1),:,:]\n    y = torch.tensor(interpolated_signal).reshape(int(T*0.8),N,1).float()[1:,:,:]\n\n\n    net = RecurrentGCN(node_features=1, filters=4)\n    optimizer = torch.optim.Adam(net.parameters(), lr=0.01)\n    net.train()\n    signal = interpolated_signal.copy()\n    for epoch in range(50):\n        signal = update_from_freq_domain(signal,missing_index)\n        X = torch.tensor(signal).reshape(int(T*0.8),N,1).float()[:int(T*0.8-1),:,:]\n        y = torch.tensor(signal).reshape(int(T*0.8),N,1).float()[1:,:,:]\n        for time, (xt,yt) in enumerate(zip(X,y)):        \n            yt_hat = net(xt, edge_index, edge_attr)\n            cost = torch.mean((yt_hat-yt)**2)\n            cost.backward()\n            optimizer.step()\n            optimizer.zero_grad()\n        signal = torch.concat([X.squeeze(),yt_hat.detach().squeeze().reshape(1,-1)])               \n\n    yhat = torch.stack([net(xt, edge_index, edge_attr) for xt in X]).detach().numpy()\n    yyhat = torch.stack([net(xt, edge_index, edge_attr) for xt in XX]).detach().numpy()\n\n    train_mse_total_estgcn = (((real_y-yhat).squeeze())**2).mean()\n    test_mse_total_estgcn = (((yy-yyhat).squeeze())**2).mean()\n    \n    estgcn_train2.append(train_mse_total_estgcn.tolist())\n    estgcn_test2.append(test_mse_total_estgcn.tolist())\n\n\nplt.figure(figsize=(12, 8))\nplt.boxplot(estgcn_train2);\n\n\n\n\n\n\n\n\n\nplt.figure(figsize=(12, 8))\nplt.boxplot(estgcn_test2);"
  },
  {
    "objectID": "posts/2_research/2023-02-06-ESTGCN_GNAR_DATA_3.html#missing-일정-2",
    "href": "posts/2_research/2023-02-06-ESTGCN_GNAR_DATA_3.html#missing-일정-2",
    "title": "Class of Method(GNAR) lag 1 80% Missing repeat",
    "section": "missing 일정",
    "text": "missing 일정\n\n_zero = Missing(fiveVTS_train)\n_zero.miss(percent = 0.8)\n_zero.second_linear()\n\nmissing_index = _zero.number\ninterpolated_signal = _zero.train_linear\n\nX = np.array(torch.tensor(interpolated_signal).reshape(int(T*0.8),N,1).float()[:int(T*0.8-1),:,:].squeeze())\ny = np.array(torch.tensor(interpolated_signal).reshape(int(T*0.8),N,1).float()[1:,:,:].squeeze())\n\nXX = np.array(torch.tensor(fiveVTS_test.reshape(int(T*0.2),N,1)[:-1,:,:]).float().squeeze())\nyy = np.array(torch.tensor(fiveVTS_test.reshape(int(T*0.2),N,1)[1:,:,:]).float().squeeze())\n\nreal_y = np.array(torch.tensor(fiveVTS_train).reshape(int(T*0.8),N,1).float()[1:,:,:])\n\n\n%R -i X\n%R -i XX\n\n\n%%R\ngnar_train1 &lt;- matrix(ncol=1,nrow=100)\ngnar_test1 &lt;- matrix(ncol=1,nrow=100)\nfor(i in 1:100){\n  answer &lt;- GNARfit(vts = X, net = fiveNet, alphaOrder = 2, betaOrder = c(1, 1))\n  prediction &lt;- predict(answer,n.ahead=40)\n  \n  train_mse_total_gnar &lt;- mean(residuals(answer)**2)\n  test_mse_total_gnar &lt;- mean((XX - prediction[1:40])**2)\n  \n  gnar_train1[i] &lt;- train_mse_total_gnar\n  gnar_test1[i] &lt;- train_mse_total_gnar\n}\n\n\n%R -o gnar_train1\n%R -o gnar_test1\n\n\nplt.figure(figsize=(12, 8))\nplt.boxplot(gnar_train1);\n\n\n\n\n\n\n\n\n\nplt.figure(figsize=(12, 8))\nplt.boxplot(gnar_test1);"
  },
  {
    "objectID": "posts/2_research/2023-02-06-ESTGCN_GNAR_DATA_3.html#missing-다르게-1",
    "href": "posts/2_research/2023-02-06-ESTGCN_GNAR_DATA_3.html#missing-다르게-1",
    "title": "Class of Method(GNAR) lag 1 80% Missing repeat",
    "section": "missing 다르게",
    "text": "missing 다르게\n\nm = robjects.r.matrix(FloatVector([0,0,0,1,1,0,0,1,1,0,0,1,0,1,0,1,1,1,0,0,1,0,0,0,0]), nrow = 5, ncol = 5)\nprint(m)\n\n     [,1] [,2] [,3] [,4] [,5]\n[1,]    0    0    0    1    1\n[2,]    0    0    1    1    0\n[3,]    0    1    0    1    0\n[4,]    1    1    1    0    0\n[5,]    1    0    0    0    0\n\n\n\n\ngnar_train2 = []\ngnar_test2 = []\n\nyy = torch.tensor(fiveVTS_test.reshape(int(T*0.2),N,1)[1:,:,:]).float()\n\n\nfor i in range(100):\n    \n    _zero = Missing(fiveVTS_train)\n    _zero.miss(percent = 0.8)\n    _zero.second_linear()\n\n    missing_index = _zero.number\n    interpolated_signal = _zero.train_linear\n\n    X = torch.tensor(interpolated_signal).reshape(int(T*0.8),N,1).float()[:int(T*0.8-2),:,:]\n\n    answer = GNAR.GNARfit(vts=robjects.r.matrix(rpyn.numpy2rpy(np.array(X).squeeze()), nrow = 160, ncol = 5),net = GNAR.matrixtoGNAR(m), alphaOrder = 2, betaOrder = FloatVector([1, 1]))             \n    predict = GNAR.predict_GNARfit(answer,n_ahead=40)\n\n    \n    train_mse_total_gnar = ((pd.DataFrame(GNAR.residuals_GNARfit(answer)).values.reshape(-1,5))**2).mean()\n    test_mse_total_gnar = ((yy.squeeze() - pd.DataFrame(predict).values.reshape(-1,5)[:-1,:])**2).mean()\n    \n    gnar_train2.append(train_mse_total_gnar.tolist())\n    gnar_test2.append(test_mse_total_gnar.tolist())\n\n\nplt.figure(figsize=(12, 8))\nplt.boxplot(gnar_train2);\n\n\n\n\n\n\n\n\n\nplt.figure(figsize=(12, 8))\nplt.boxplot(gnar_test2);"
  },
  {
    "objectID": "posts/2_research/2023-06-20-TGCN_Simulation_boxplot_reshape.html",
    "href": "posts/2_research/2023-06-20-TGCN_Simulation_boxplot_reshape.html",
    "title": "TGCN_Simulation_reshape",
    "section": "",
    "text": "Simulation Study"
  },
  {
    "objectID": "posts/2_research/2023-06-20-TGCN_Simulation_boxplot_reshape.html#baseline",
    "href": "posts/2_research/2023-06-20-TGCN_Simulation_boxplot_reshape.html#baseline",
    "title": "TGCN_Simulation_reshape",
    "section": "Baseline",
    "text": "Baseline\n\ndata.query(\"method!='GNAR' and mrate==0\").plot.box(backend='plotly',x='mrate',color='method',y='mse',facet_col='lags',facet_row='nof_filters',height=600)"
  },
  {
    "objectID": "posts/2_research/2023-06-20-TGCN_Simulation_boxplot_reshape.html#random",
    "href": "posts/2_research/2023-06-20-TGCN_Simulation_boxplot_reshape.html#random",
    "title": "TGCN_Simulation_reshape",
    "section": "Random",
    "text": "Random\n\ndata.query(\"method!='GNAR' and mtype =='rand'\").plot.box(backend='plotly',x='mrate',color='method',y='mse',facet_col='inter_method',facet_row='nof_filters',height=600)"
  },
  {
    "objectID": "posts/2_research/2023-06-20-TGCN_Simulation_boxplot_reshape.html#block",
    "href": "posts/2_research/2023-06-20-TGCN_Simulation_boxplot_reshape.html#block",
    "title": "TGCN_Simulation_reshape",
    "section": "Block",
    "text": "Block\n\ndata.query(\"method!='GNAR' and mtype =='block' \").plot.box(backend='plotly',x='mrate',color='method',y='mse',facet_col='inter_method',facet_row='nof_filters',height=600)"
  },
  {
    "objectID": "posts/2_research/2023-06-20-TGCN_Simulation_boxplot_reshape.html#baseline-1",
    "href": "posts/2_research/2023-06-20-TGCN_Simulation_boxplot_reshape.html#baseline-1",
    "title": "TGCN_Simulation_reshape",
    "section": "Baseline",
    "text": "Baseline\n\ndata.query(\"method!='GNAR' and mrate ==0 \").plot.box(backend='plotly',x='mrate',color='method',y='mse',facet_col='nof_filters',height=600)"
  },
  {
    "objectID": "posts/2_research/2023-06-20-TGCN_Simulation_boxplot_reshape.html#random-1",
    "href": "posts/2_research/2023-06-20-TGCN_Simulation_boxplot_reshape.html#random-1",
    "title": "TGCN_Simulation_reshape",
    "section": "Random",
    "text": "Random\n\ndata.query(\"method!='GNAR' and mtype =='rand'\").plot.box(backend='plotly',x='mrate',color='method',y='mse',facet_col='nof_filters',facet_row='inter_method',height=800)"
  },
  {
    "objectID": "posts/2_research/2023-06-20-TGCN_Simulation_boxplot_reshape.html#block-1",
    "href": "posts/2_research/2023-06-20-TGCN_Simulation_boxplot_reshape.html#block-1",
    "title": "TGCN_Simulation_reshape",
    "section": "Block",
    "text": "Block\n\ndata.query(\"method!='GNAR' and mtype =='block'\").plot.box(backend='plotly',x='mrate',color='method',y='mse',facet_col='inter_method',facet_row='lags',height=800)"
  },
  {
    "objectID": "posts/2_research/2023-06-20-TGCN_Simulation_boxplot_reshape.html#baseline-2",
    "href": "posts/2_research/2023-06-20-TGCN_Simulation_boxplot_reshape.html#baseline-2",
    "title": "TGCN_Simulation_reshape",
    "section": "Baseline",
    "text": "Baseline\n\ndata.query(\"mrate ==0 and lags!=2\").plot.box(backend='plotly',x='epoch',color='method',y='mse',facet_col='nof_filters',facet_row='lags',height=400)"
  },
  {
    "objectID": "posts/2_research/2023-06-20-TGCN_Simulation_boxplot_reshape.html#random-2",
    "href": "posts/2_research/2023-06-20-TGCN_Simulation_boxplot_reshape.html#random-2",
    "title": "TGCN_Simulation_reshape",
    "section": "random",
    "text": "random\n\ndata.query(\"method!='GNAR' and mtype =='rand' and lags!=2\").plot.box(backend='plotly',x='mrate',color='method',y='mse',facet_col='nof_filters',facet_row='inter_method',height=800)"
  },
  {
    "objectID": "posts/2_research/2023-06-20-TGCN_Simulation_boxplot_reshape.html#block-2",
    "href": "posts/2_research/2023-06-20-TGCN_Simulation_boxplot_reshape.html#block-2",
    "title": "TGCN_Simulation_reshape",
    "section": "block",
    "text": "block\n\ndata.query(\"method!='GNAR' and mtype =='block' and lags!=2 \").plot.box(backend='plotly',x='mrate',color='method',y='mse',facet_col='nof_filters',facet_row='inter_method',height=800)"
  },
  {
    "objectID": "posts/2_research/2023-06-20-TGCN_Simulation_boxplot_reshape.html#weight-matrix-time-node-고려한-결과",
    "href": "posts/2_research/2023-06-20-TGCN_Simulation_boxplot_reshape.html#weight-matrix-time-node-고려한-결과",
    "title": "TGCN_Simulation_reshape",
    "section": "weight matrix time, node 고려한 결과",
    "text": "weight matrix time, node 고려한 결과\n\ndf1 = pd.read_csv('./simulation_results/2023-06-21_22-29-02.csv')\ndf2 = pd.read_csv('./simulation_results/2023-06-21_22-51-34.csv')\n\n\ndata2 = pd.concat([df1,df2],axis=0)\n\n\ndata2.to_csv('./simulation_results/Real_simulation_reshape/TGCN_pedalme_Simulation_itstgcnsnd.csv',index=False)\n\n\ndata2 = pd.read_csv('./simulation_results/Real_simulation_reshape/TGCN_pedalme_Simulation_itstgcnsnd.csv')\n\n\ndata2.query(\"mtype=='rand'\").plot.box(backend='plotly',x='mrate',color='method',y='mse',facet_col='inter_method',facet_row='nof_filters',height=1000)\n\n                                                \n\n\n\ndata2.query(\"mtype=='block'\").plot.box(backend='plotly',x='mrate',color='method',y='mse',facet_col='inter_method',facet_row='nof_filters',height=1000)"
  },
  {
    "objectID": "posts/2_research/2023-06-20-TGCN_Simulation_boxplot_reshape.html#baseline-3",
    "href": "posts/2_research/2023-06-20-TGCN_Simulation_boxplot_reshape.html#baseline-3",
    "title": "TGCN_Simulation_reshape",
    "section": "Baseline",
    "text": "Baseline\n\ndata.query(\"mrate==0 and method!='GNAR'\").plot.box(backend='plotly',x='mrate',color='method',y='mse',height=600)"
  },
  {
    "objectID": "posts/2_research/2023-06-20-TGCN_Simulation_boxplot_reshape.html#random-3",
    "href": "posts/2_research/2023-06-20-TGCN_Simulation_boxplot_reshape.html#random-3",
    "title": "TGCN_Simulation_reshape",
    "section": "random",
    "text": "random\n\ndata.query(\"mtype=='rand' and mrate !=0 and method!='GNAR'\").plot.box(backend='plotly',x='mrate',color='method',y='mse',facet_col='lags',facet_row='nof_filters',height=800)"
  },
  {
    "objectID": "posts/2_research/2023-06-20-TGCN_Simulation_boxplot_reshape.html#block-3",
    "href": "posts/2_research/2023-06-20-TGCN_Simulation_boxplot_reshape.html#block-3",
    "title": "TGCN_Simulation_reshape",
    "section": "block",
    "text": "block\n\ndata.query(\"mtype=='block' and method!='GNAR'\").plot.box(backend='plotly',x='mrate',color='method',y='mse',facet_col='lags',facet_row='nof_filters',height=1200)"
  },
  {
    "objectID": "posts/2_research/2023-06-20-TGCN_Simulation_boxplot_reshape.html#missing-values-on-the-same-nodes",
    "href": "posts/2_research/2023-06-20-TGCN_Simulation_boxplot_reshape.html#missing-values-on-the-same-nodes",
    "title": "TGCN_Simulation_reshape",
    "section": "missing values on the same nodes",
    "text": "missing values on the same nodes\n\ndf1 = pd.read_csv('./simulation_results/2023-06-27_03-08-38.csv') # STGCN IT-STGCN block\ndf2 = pd.read_csv('./simulation_results/2023-06-27_15-49-21.csv') # STGCN IT-STGCN\ndf3 = pd.read_csv('./simulation_results/2023-06-27_09-33-29.csv') \n\n\ndata = pd.concat([df1],axis=0)\n\n\ndata.to_csv('./simulation_results/Real_simulation_reshape/TGCN_wikimath_GSO_st.csv',index=False)\n\n\ndata = pd.read_csv('./simulation_results/Real_simulation_reshape/TGCN_wikimath_GSO_st.csv')\n\n\ndata.query(\"method!='GNAR' and mtype =='block' \").plot.box(backend='plotly',x='mrate',color='method',y='mse',facet_col='inter_method',facet_row='lags',height=800)"
  },
  {
    "objectID": "posts/2_research/2023-06-20-TGCN_Simulation_boxplot_reshape.html#baseline-4",
    "href": "posts/2_research/2023-06-20-TGCN_Simulation_boxplot_reshape.html#baseline-4",
    "title": "TGCN_Simulation_reshape",
    "section": "Baseline",
    "text": "Baseline\n\ndata.query(\"method!='GNAR' and mrate ==0 \").plot.box(backend='plotly',x='mrate',color='method',y='mse',facet_col='lags',height=800)"
  },
  {
    "objectID": "posts/2_research/2023-06-20-TGCN_Simulation_boxplot_reshape.html#random-4",
    "href": "posts/2_research/2023-06-20-TGCN_Simulation_boxplot_reshape.html#random-4",
    "title": "TGCN_Simulation_reshape",
    "section": "random",
    "text": "random\n\ndata.query(\"method!='GNAR' and mtype =='rand' \").plot.box(backend='plotly',x='mrate',color='method',y='mse',facet_col='lags',facet_row='inter_method',height=800)"
  },
  {
    "objectID": "posts/2_research/2023-06-20-TGCN_Simulation_boxplot_reshape.html#block-4",
    "href": "posts/2_research/2023-06-20-TGCN_Simulation_boxplot_reshape.html#block-4",
    "title": "TGCN_Simulation_reshape",
    "section": "block",
    "text": "block\n\ndata.query(\"method!='GNAR' and mtype =='block' \").plot.box(backend='plotly',x='mrate',color='method',y='mse',facet_col='lags',facet_row='inter_method',height=800)"
  },
  {
    "objectID": "posts/2_research/2023-06-20-TGCN_Simulation_boxplot_reshape.html#baseline-5",
    "href": "posts/2_research/2023-06-20-TGCN_Simulation_boxplot_reshape.html#baseline-5",
    "title": "TGCN_Simulation_reshape",
    "section": "Baseline",
    "text": "Baseline\n\ndata.query(\"mrate==0 and method!='GNAR'\").plot.box(backend='plotly',x='mrate',color='method',y='mse',facet_col='lags',facet_row='nof_filters',height=600)"
  },
  {
    "objectID": "posts/2_research/2023-06-20-TGCN_Simulation_boxplot_reshape.html#random-5",
    "href": "posts/2_research/2023-06-20-TGCN_Simulation_boxplot_reshape.html#random-5",
    "title": "TGCN_Simulation_reshape",
    "section": "random",
    "text": "random\n\ndata.query(\"mtype=='rand' and mrate !=0 and method!='GNAR'\").plot.box(backend='plotly',x='mrate',color='method',y='mse',facet_col='lags',facet_row='inter_method',height=800)"
  },
  {
    "objectID": "posts/2_research/2023-06-20-TGCN_Simulation_boxplot_reshape.html#block-5",
    "href": "posts/2_research/2023-06-20-TGCN_Simulation_boxplot_reshape.html#block-5",
    "title": "TGCN_Simulation_reshape",
    "section": "block",
    "text": "block\n\ndata.query(\"mtype=='block' and method!='GNAR'\").plot.box(backend='plotly',x='mrate',color='method',y='mse',facet_col='lags',facet_row='nof_filters',height=1200)"
  },
  {
    "objectID": "posts/2_research/2023-05-04-questions of pytorch geometric temporal.html",
    "href": "posts/2_research/2023-05-04-questions of pytorch geometric temporal.html",
    "title": "Questions of PyTorch Geometric Temporal",
    "section": "",
    "text": "PyTorch Geometric Temporal"
  },
  {
    "objectID": "posts/2_research/2023-05-04-questions of pytorch geometric temporal.html#applications",
    "href": "posts/2_research/2023-05-04-questions of pytorch geometric temporal.html#applications",
    "title": "Questions of PyTorch Geometric Temporal",
    "section": "Applications",
    "text": "Applications\n\nEpidemiological Forecasting\n\nfrom torch_geometric_temporal.dataset import ChickenpoxDatasetLoader\nfrom torch_geometric_temporal.signal import temporal_signal_split\n\nloader = ChickenpoxDatasetLoader()\n\ndataset = loader.get_dataset()\n\ntrain_dataset, test_dataset = temporal_signal_split(dataset, train_ratio=0.2)\n\n\nimport torch\nimport torch.nn.functional as F\nfrom torch_geometric_temporal.nn.recurrent import DCRNN\n\nclass RecurrentGCN(torch.nn.Module):\n    def __init__(self, node_features):\n        super(RecurrentGCN, self).__init__()\n        self.recurrent = DCRNN(node_features, 32, 1)\n        self.linear = torch.nn.Linear(32, 1)\n\n    def forward(self, x, edge_index, edge_weight):\n        h = self.recurrent(x, edge_index, edge_weight)\n        h = F.relu(h)\n        h = self.linear(h)\n        return h\n\n\nfrom tqdm import tqdm\n\nmodel = RecurrentGCN(node_features = 4)\n\noptimizer = torch.optim.Adam(model.parameters(), lr=0.01)\n\nmodel.train()\n\nfor epoch in tqdm(range(200)):\n    cost = 0\n    for time, snapshot in enumerate(train_dataset):\n        y_hat = model(snapshot.x, snapshot.edge_index, snapshot.edge_attr)\n        cost = cost + torch.mean((y_hat-snapshot.y)**2)\n    cost = cost / (time+1)\n    cost.backward()\n    optimizer.step()\n    optimizer.zero_grad()\n    \n    \n###########################################################\n# I added this to check the shape.\nprint(y_hat.shape,snapshot.y.shape,(y_hat-snapshot.y).shape)\n\n100%|██████████| 200/200 [02:40&lt;00:00,  1.24it/s]\n\n\ntorch.Size([20, 1]) torch.Size([20]) torch.Size([20, 20])\n\n\n\nmodel.eval()\ncost = 0\nfor time, snapshot in enumerate(test_dataset):\n    y_hat = model(snapshot.x, snapshot.edge_index, snapshot.edge_attr)\n    cost = cost + torch.mean((y_hat-snapshot.y)**2)\ncost = cost / (time+1)\ncost = cost.item()\nprint(\"MSE: {:.4f}\".format(cost))\n# &gt;&gt;&gt; MSE: 1.0232\n\nMSE: 1.0247\n\n\n\n\nShape Check (1)\n\na = torch.randn(20, 1)\n\n\nb = torch.randn(20)\n\n\nc = a-b\n\n\nprint(a.size(),b.size(),c.size())\n\ntorch.Size([20, 1]) torch.Size([20]) torch.Size([20, 20])\n\n\n\n\n\nDoesn’t it have to ‘y_hat’ be the same shape as snapshot.y?\n\nIf we want to compare the y_hat from the model with the values y, the same shape is appropriate to evaluate.\n\n\nfrom tqdm import tqdm\n\nmodel = RecurrentGCN(node_features = 4)\n\noptimizer = torch.optim.Adam(model.parameters(), lr=0.01)\n\nmodel.train()\n\nfor epoch in tqdm(range(200)):\n    cost = 0\n    for time, snapshot in enumerate(train_dataset):\n        y_hat = model(snapshot.x, snapshot.edge_index, snapshot.edge_attr).reshape(-1)\n        cost = cost + torch.mean((y_hat-snapshot.y)**2)\n    cost = cost / (time+1)\n    cost.backward()\n    optimizer.step()\n    optimizer.zero_grad()\n    \n    \n###########################################################\n# I added this to check the shape.\nprint(y_hat.shape,snapshot.y.shape,(y_hat-snapshot.y).shape)\n\n100%|██████████| 200/200 [01:27&lt;00:00,  2.30it/s]\n\n\ntorch.Size([20]) torch.Size([20]) torch.Size([20])\n\n\n\nmodel.eval()\ncost = 0\nfor time, snapshot in enumerate(test_dataset):\n    y_hat = model(snapshot.x, snapshot.edge_index, snapshot.edge_attr)\n    cost = cost + torch.mean((y_hat-snapshot.y)**2)\ncost = cost / (time+1)\ncost = cost.item()\nprint(\"MSE: {:.4f}\".format(cost))\n# &gt;&gt;&gt; MSE: 1.0232\n\nMSE: 1.2844\n\n\n\n\n\nShape Check (2)\n\na = torch.randn(20, 1).reshape(-1)\n\n\nb = torch.randn(20)\n\n\nc = a-b\n\n\nprint(a.size(),b.size(),c.size())\n\ntorch.Size([20]) torch.Size([20]) torch.Size([20])"
  },
  {
    "objectID": "posts/2_research/2023-05-04-questions of pytorch geometric temporal.html#web-traffic-prediction",
    "href": "posts/2_research/2023-05-04-questions of pytorch geometric temporal.html#web-traffic-prediction",
    "title": "Questions of PyTorch Geometric Temporal",
    "section": "Web Traffic Prediction",
    "text": "Web Traffic Prediction\n\nfrom torch_geometric_temporal.dataset import WikiMathsDatasetLoader\nfrom torch_geometric_temporal.signal import temporal_signal_split\n\nloader = WikiMathsDatasetLoader()\n\ndataset = loader.get_dataset(lags=14)\n\ntrain_dataset, test_dataset = temporal_signal_split(dataset, train_ratio=0.5)\n\n\nimport torch\nimport torch.nn.functional as F\nfrom torch_geometric_temporal.nn.recurrent import GConvGRU\n\nclass RecurrentGCN(torch.nn.Module):\n    def __init__(self, node_features, filters):\n        super(RecurrentGCN, self).__init__()\n        self.recurrent = GConvGRU(node_features, filters, 2)\n        self.linear = torch.nn.Linear(filters, 1)\n\n    def forward(self, x, edge_index, edge_weight):\n        h = self.recurrent(x, edge_index, edge_weight)\n        h = F.relu(h)\n        h = self.linear(h)\n        return h\n\n\nfrom tqdm import tqdm\n\nmodel = RecurrentGCN(node_features=14, filters=32)\n\noptimizer = torch.optim.Adam(model.parameters(), lr=0.01)\n\nmodel.train()\n\nfor epoch in tqdm(range(50)):\n    for time, snapshot in enumerate(train_dataset):\n        y_hat = model(snapshot.x, snapshot.edge_index, snapshot.edge_attr)\n        cost = torch.mean((y_hat-snapshot.y)**2)\n        cost.backward()\n        optimizer.step()\n        optimizer.zero_grad()\n    \n    \n###########################################################\n# I added this to check the shape.\nprint(y_hat.shape,snapshot.y.shape,(y_hat-snapshot.y).shape)\n\n100%|██████████| 50/50 [31:26&lt;00:00, 37.73s/it]\n\n\ntorch.Size([1068, 1]) torch.Size([1068]) torch.Size([1068, 1068])\n\n\n\nmodel.eval()\ncost = 0\nfor time, snapshot in enumerate(test_dataset):\n    y_hat = model(snapshot.x, snapshot.edge_index, snapshot.edge_attr)\n    cost = cost + torch.mean((y_hat-snapshot.y)**2)\ncost = cost / (time+1)\ncost = cost.item()\nprint(\"MSE: {:.4f}\".format(cost))\n# &gt;&gt;&gt; MSE: 0.7760\n\nMSE: 0.7939\n\n\n\n\nShape Check (1)\n\na = torch.randn(1068, 1)\n\n\nb = torch.randn(1068)\n\n\nc = a-b\n\n\nprint(a.size(),b.size(),c.size())\n\ntorch.Size([1068, 1]) torch.Size([1068]) torch.Size([1068, 1068])\n\n\n\n\n\nIf the code changes the shape of y_hat?\n\nfrom tqdm import tqdm\n\nmodel = RecurrentGCN(node_features=14, filters=32)\n\noptimizer = torch.optim.Adam(model.parameters(), lr=0.01)\n\nmodel.train()\n\nfor epoch in tqdm(range(50)):\n    for time, snapshot in enumerate(train_dataset):\n        y_hat = model(snapshot.x, snapshot.edge_index, snapshot.edge_attr)\n        cost = torch.mean((y_hat-snapshot.y)**2)\n        cost.backward()\n        optimizer.step()\n        optimizer.zero_grad()\n    \n    \n###########################################################\n# I added this to check the shape.\nprint(y_hat.shape,snapshot.y.shape,(y_hat-snapshot.y).shape)\n\n100%|██████████| 50/50 [36:39&lt;00:00, 43.99s/it]\n\n\ntorch.Size([1068, 1]) torch.Size([1068]) torch.Size([1068, 1068])\n\n\n\nmodel.eval()\ncost = 0\nfor time, snapshot in enumerate(test_dataset):\n    y_hat = model(snapshot.x, snapshot.edge_index, snapshot.edge_attr)\n    cost = cost + torch.mean((y_hat-snapshot.y)**2)\ncost = cost / (time+1)\ncost = cost.item()\nprint(\"MSE: {:.4f}\".format(cost))\n# &gt;&gt;&gt; MSE: 0.7760\n\nMSE: 0.7807\n\n\n\n\n\nShape Check (2)\n\na = torch.randn(1068, 1).reshape(-1)\n\n\nb = torch.randn(1068)\n\n\nc = a-b\n\n\nprint(a.size(),b.size(),c.size())\n\ntorch.Size([1068]) torch.Size([1068]) torch.Size([1068])\n\n\n\nFix : https://github.com/benedekrozemberczki/pytorch_geometric_temporal/issues/231"
  },
  {
    "objectID": "posts/2_research/2023-02-20-ESTGCN_GNAR_edit_guebin2.html",
    "href": "posts/2_research/2023-02-20-ESTGCN_GNAR_edit_guebin2.html",
    "title": "2nd ITSTGCN",
    "section": "",
    "text": "GNAR fiveNet,fivenodes lag 1"
  },
  {
    "objectID": "posts/2_research/2023-02-20-ESTGCN_GNAR_edit_guebin2.html#stgcn",
    "href": "posts/2_research/2023-02-20-ESTGCN_GNAR_edit_guebin2.html#stgcn",
    "title": "2nd ITSTGCN",
    "section": "STGCN",
    "text": "STGCN\n\nclass STGCN_Missing:\n    def __init__(self,Dataset,df, iterable, Method, Missingrate, Missingtype, lag, Number_of_filters, Interpolation):\n        self.Dataset = Dataset\n        self.df = df\n        self.iterable = iterable\n        self.Method = Method\n        self.Missingrate = Missingrate\n        self.Missingtype = Missingtype\n        self.lag = lag\n        self.Number_of_filters = Number_of_filters\n        self.Interpolation = Interpolation\n    def iter(self):\n        self.XX = torch.tensor(fiveVTS_test.reshape(int(T*0.2),N,1)[:-1,:,:]).float()\n        self.yy = torch.tensor(fiveVTS_test.reshape(int(T*0.2),N,1)[1:,:,:]).float()\n\n        self.real_y = torch.tensor(fiveVTS_train).reshape(int(T*0.8),N,1).float()[1:,:,:]\n        for i in range(self.iterable):\n\n            _zero = Missing(fiveVTS_train)\n            _zero.miss(percent = self.Missingrate)\n            _zero.second_linear()\n\n            missing_index = _zero.number\n            interpolated_signal = _zero.train_linear\n\n            X = torch.tensor(interpolated_signal).reshape(int(T*0.8),N,1).float()[:int(T*0.8-1),:,:]\n            y = torch.tensor(interpolated_signal).reshape(int(T*0.8),N,1).float()[1:,:,:]\n\n            net = RecurrentGCN(node_features=self.lag, filters=self.Number_of_filters)\n            optimizer = torch.optim.Adam(net.parameters(), lr=0.01)\n            net.train()\n            for epoch in range(50):\n                for time, (xt,yt) in enumerate(zip(X,y)):\n                    yt_hat = net(xt, edge_index, edge_attr)\n                    cost = torch.mean((yt_hat-yt)**2)\n                    cost.backward()\n                    optimizer.step()\n                    optimizer.zero_grad()\n\n            yhat = torch.stack([net(xt, edge_index, edge_attr) for xt in X]).detach().numpy()\n            yyhat = torch.stack([net(xt, edge_index, edge_attr) for xt in self.XX]).detach().numpy()\n\n            train_mse_total_stgcn = (((self.real_y-yhat).squeeze())**2).mean()\n            test_mse_total_stgcn = (((self.yy-yyhat).squeeze())**2).mean() \n\n            df_row = pd.DataFrame(columns=col)\n            df_row['Dataset'] = self.Dataset, \n            df_row['iteration'] = i+1, # 1,2,3,...,10 \n            df_row['method'] = self.Method, # 'stgcn','estgcn','gnar' \n            df_row['missingrate'] = self.Missingrate, # 0.0, 0.2, 0.4, 0.6, 0.8 \n            df_row['missingtype'] = self.Missingtype,  # None, 'randomly' and 'block' \n            df_row['lag'] = self.lag, # 1,2,3,4 ... \n            df_row['number_of_filters'] = self.Number_of_filters, # 16,24,32, ... \n            df_row['interpolation'] = self.Interpolation, # None, 'mean', 'linear'\n            df_row['MSE_train'] = train_mse_total_stgcn.tolist()\n            df_row['MSE_test'] = test_mse_total_stgcn.tolist()\n\n            self.df = pd.concat([self.df,df_row])"
  },
  {
    "objectID": "posts/2_research/2023-02-20-ESTGCN_GNAR_edit_guebin2.html#enhencement-of-stgcn",
    "href": "posts/2_research/2023-02-20-ESTGCN_GNAR_edit_guebin2.html#enhencement-of-stgcn",
    "title": "2nd ITSTGCN",
    "section": "Enhencement of STGCN",
    "text": "Enhencement of STGCN\n\nclass ESTGCN_Missing:\n    def __init__(self,Dataset,df, iterable, Method, Missingrate, Missingtype, lag, Number_of_filters, Interpolation):\n        self.Dataset = Dataset\n        self.df = df\n        self.iterable = iterable\n        self.Method = Method\n        self.Missingrate = Missingrate\n        self.Missingtype = Missingtype\n        self.lag = lag\n        self.Number_of_filters = Number_of_filters\n        self.Interpolation = Interpolation\n    def iter(self):\n        self.XX = torch.tensor(fiveVTS_test.reshape(int(T*0.2),N,1)[:-1,:,:]).float()\n        self.yy = torch.tensor(fiveVTS_test.reshape(int(T*0.2),N,1)[1:,:,:]).float()\n\n        self.real_y = torch.tensor(fiveVTS_train).reshape(int(T*0.8),N,1).float()[1:,:,:]\n        for i in range(self.iterable):\n    \n            _zero = Missing(fiveVTS_train)\n            _zero.miss(percent = self.Missingrate)\n            _zero.second_linear()\n\n            missing_index = _zero.number\n            interpolated_signal = _zero.train_linear\n\n            X = torch.tensor(interpolated_signal).reshape(int(T*0.8),N,1).float()[:int(T*0.8-1),:,:]\n            y = torch.tensor(interpolated_signal).reshape(int(T*0.8),N,1).float()[1:,:,:]\n\n\n            net = RecurrentGCN(node_features=self.lag, filters=self.Number_of_filters)\n            optimizer = torch.optim.Adam(net.parameters(), lr=0.01)\n            net.train()\n            signal = interpolated_signal.copy()\n            for epoch in range(50):\n                signal = update_from_freq_domain(signal,missing_index)\n                X = torch.tensor(signal).reshape(int(T*0.8),N,1).float()[:int(T*0.8-1),:,:]\n                y = torch.tensor(signal).reshape(int(T*0.8),N,1).float()[1:,:,:]\n                for time, (xt,yt) in enumerate(zip(X,y)):        \n                    yt_hat = net(xt, edge_index, edge_attr)\n                    cost = torch.mean((yt_hat-yt)**2)\n                    cost.backward()\n                    optimizer.step()\n                    optimizer.zero_grad()\n                signal = torch.concat([X.squeeze(),yt_hat.detach().squeeze().reshape(1,-1)])               \n\n            yhat = torch.stack([net(xt, edge_index, edge_attr) for xt in X]).detach().numpy()\n            yyhat = torch.stack([net(xt, edge_index, edge_attr) for xt in self.XX]).detach().numpy()\n\n            train_mse_total_estgcn = (((self.real_y-yhat).squeeze())**2).mean()\n            test_mse_total_estgcn = (((self.yy-yyhat).squeeze())**2).mean()\n\n            df_row = pd.DataFrame(columns=col)\n            df_row['Dataset'] = self.Dataset,\n            df_row['iteration'] = i+1, # 1,2,3,...,10 \n            df_row['method'] = self.Method, # 'stgcn','estgcn','gnar' \n            df_row['missingrate'] = self.Missingrate, # 0.0, 0.2, 0.4, 0.6, 0.8 \n            df_row['missingtype'] = self.Missingtype,  # None, 'randomly' and 'block' \n            df_row['lag'] = self.lag, # 1,2,3,4 ... \n            df_row['number_of_filters'] = self.Number_of_filters, # 16,24,32, ... \n            df_row['interpolation'] = self.Interpolation, # None, 'mean', 'linear'\n            df_row['MSE_train'] = train_mse_total_estgcn.tolist()\n            df_row['MSE_test'] = test_mse_total_estgcn.tolist()\n\n            self.df = pd.concat([self.df,df_row])"
  },
  {
    "objectID": "posts/2_research/2023-02-20-ESTGCN_GNAR_edit_guebin2.html#gnar",
    "href": "posts/2_research/2023-02-20-ESTGCN_GNAR_edit_guebin2.html#gnar",
    "title": "2nd ITSTGCN",
    "section": "GNAR",
    "text": "GNAR\n\nm = robjects.r.matrix(FloatVector([0,0,0,1,1,0,0,1,1,0,0,1,0,1,0,1,1,1,0,0,1,0,0,0,0]), nrow = 5, ncol = 5)\n\n\nclass GNAR_Missing:\n    def __init__(self,Dataset,df, iterable, Method, Missingrate, Missingtype, lag, Number_of_filters, Interpolation):\n        self.Dataset = Dataset\n        self.df = df\n        self.iterable = iterable\n        self.Method = Method\n        self.Missingrate = Missingrate\n        self.Missingtype = Missingtype\n        self.lag = lag\n        self.Number_of_filters = Number_of_filters\n        self.Interpolation = Interpolation\n    def iter(self):\n        self.yy = torch.tensor(fiveVTS_test.reshape(int(T*0.2),N,1)[1:,:,:]).float()\n        for i in range(self.iterable):\n\n            _zero = Missing(fiveVTS_train)\n            _zero.miss(percent = self.Missingrate)\n            _zero.second_linear()\n\n            missing_index = _zero.number\n            interpolated_signal = _zero.train_linear\n\n            X = torch.tensor(interpolated_signal).reshape(int(T*0.8),N,1).float()[:int(T*0.8-2),:,:]\n\n            answer = GNAR.GNARfit(vts=robjects.r.matrix(rpyn.numpy2rpy(np.array(X).squeeze()), nrow = 160, ncol = 5),net = GNAR.matrixtoGNAR(m), alphaOrder = 2, betaOrder = FloatVector([1, 1]))             \n            predict = GNAR.predict_GNARfit(answer,n_ahead=40)\n\n\n            train_mse_total_gnar = ((pd.DataFrame(GNAR.residuals_GNARfit(answer)).values.reshape(-1,5))**2).mean()\n            test_mse_total_gnar = ((self.yy.squeeze() - pd.DataFrame(predict).values.reshape(-1,5)[:-1,:])**2).mean()\n\n            df_row = pd.DataFrame(columns=col)\n            df_row['Dataset'] = self.Dataset,\n            df_row['iteration'] = i+1, # 1,2,3,...,10 \n            df_row['method'] = self.Method, # 'stgcn','estgcn','gnar' \n            df_row['missingrate'] = self.Missingrate, # 0.0, 0.2, 0.4, 0.6, 0.8 \n            df_row['missingtype'] = self.Missingtype,  # None, 'randomly' and 'block' \n            df_row['lag'] = self.lag, # 1,2,3,4 ... \n            df_row['number_of_filters'] = self.Number_of_filters, # 16,24,32, ... \n            df_row['interpolation'] = self.Interpolation, # None, 'mean', 'linear'\n            df_row['MSE_train'] = train_mse_total_gnar.tolist()\n            df_row['MSE_test'] = test_mse_total_gnar.tolist()\n\n            self.df = pd.concat([self.df,df_row])"
  },
  {
    "objectID": "posts/2_research/2023-02-20-ESTGCN_GNAR_edit_guebin2.html#stgcn-1",
    "href": "posts/2_research/2023-02-20-ESTGCN_GNAR_edit_guebin2.html#stgcn-1",
    "title": "2nd ITSTGCN",
    "section": "STGCN",
    "text": "STGCN\n\nDataset = 'fivenodes'\nMethod = 'stgcn' # 'stgcn','estgcn','gnar' \nMissingtype = 'randomly'  # None, 'randomly' and 'block' \nlag = 1 # 1,2,3,4 ... \nNumber_of_filters = 4 # 16,24,32, ... \nInterpolation = 'Linear' # None, 'mean', 'linear'\niterable = 100\n\n\ndf_stgcn= pd.DataFrame(columns=col)\n\n\nfor Missingrate in rate:\n    df = pd.DataFrame(columns=col)\n    stgcn = STGCN_Missing(Dataset,df, iterable,Method, Missingrate, Missingtype, lag, Number_of_filters, Interpolation)\n    stgcn.iter()\n    df_add = stgcn.df.copy()\n    df_stgcn = pd.concat([df_stgcn,df_add],axis=0)\n\n\nsave_data(df_stgcn, './data/GNAR_stgcn_randomly_by_rate.pkl')"
  },
  {
    "objectID": "posts/2_research/2023-02-20-ESTGCN_GNAR_edit_guebin2.html#enhencement-of-stgcn-1",
    "href": "posts/2_research/2023-02-20-ESTGCN_GNAR_edit_guebin2.html#enhencement-of-stgcn-1",
    "title": "2nd ITSTGCN",
    "section": "Enhencement of STGCN",
    "text": "Enhencement of STGCN\n\nDataset = 'fivenodes'\nMethod = 'estgcn' # 'stgcn','estgcn','gnar' \nMissingtype = 'randomly'  # None, 'randomly' and 'block' \nlag = 1 # 1,2,3,4 ... \nNumber_of_filters = 4 # 16,24,32, ... \nInterpolation = 'Linear' # None, 'mean', 'linear'\niterable = 100\n\n\ndf_estgcn = pd.DataFrame(columns=col)\n\n\nfor Missingrate in rate:\n    df = pd.DataFrame(columns=col)\n    estgcn = ESTGCN_Missing(Dataset,df, iterable,Method, Missingrate, Missingtype, lag, Number_of_filters, Interpolation)\n    estgcn.iter()\n    df_add = estgcn.df.copy()\n    df_estgcn = pd.concat([df_estgcn,df_add],axis=0)\n\n\nsave_data(df_estgcn, './data/GNAR_estgcn_randomly_by_rate.pkl')"
  },
  {
    "objectID": "posts/2_research/2023-02-20-ESTGCN_GNAR_edit_guebin2.html#gnar-1",
    "href": "posts/2_research/2023-02-20-ESTGCN_GNAR_edit_guebin2.html#gnar-1",
    "title": "2nd ITSTGCN",
    "section": "GNAR",
    "text": "GNAR\n\nDataset = 'fivenodes'\nMethod = 'gnar' # 'stgcn','estgcn','gnar' \nMissingtype = 'randomly'  # None, 'randomly' and 'block' \nlag = 1 # 1,2,3,4 ... \nNumber_of_filters = None # 16,24,32, ... \nInterpolation = 'Linear' # None, 'mean', 'linear'\niterable = 100\n\n\ndf_gnar = pd.DataFrame(columns=col)\n\n\nfor Missingrate in rate:\n    df = pd.DataFrame(columns=col)\n    gnar = GNAR_Missing(Dataset,df, iterable,Method, Missingrate, Missingtype, lag, Number_of_filters, Interpolation)\n    gnar.iter()\n    df_add = gnar.df.copy()\n    df_gnar = pd.concat([df_gnar,df_add],axis=0)\n\n\nsave_data(df_gnar, './data/GANR_gnar_randomly_by_rate.pkl')"
  },
  {
    "objectID": "posts/2_research/2023-03-03-ESTGCN_GNAR_edit_guebin2_seoyeon.html",
    "href": "posts/2_research/2023-03-03-ESTGCN_GNAR_edit_guebin2_seoyeon.html",
    "title": "SY 1st ITSTGCN",
    "section": "",
    "text": "edit"
  },
  {
    "objectID": "posts/2_research/2023-03-03-ESTGCN_GNAR_edit_guebin2_seoyeon.html#plnr_stgcn_rand",
    "href": "posts/2_research/2023-03-03-ESTGCN_GNAR_edit_guebin2_seoyeon.html#plnr_stgcn_rand",
    "title": "SY 1st ITSTGCN",
    "section": "PLNR_STGCN_RAND",
    "text": "PLNR_STGCN_RAND\n\n# _data = load_data('./data/fivenodes.pkl')\n\n# _edges = torch.tensor(_data['edges']).nonzero().tolist()\n# _FX = _data['f'].tolist()\n# _node_ids = {'node1':0, 'node2':1, 'node3':2, 'node4':3, 'node5':4} \n\ndata_dict = {'edges':_edges, 'node_ids':_node_ids, 'FX':_FX}\n\n\nloader = DatasetLoader(data_dict)\n\n\nplans_stgcn_rand = {\n    'max_iteration': 3, \n    'method': ['STGCN', 'IT-STGCN'], \n    'mrate': [0.0, 0.2, 0.4],\n    'lags': [2, 4], \n    'nof_filters': [8,16], \n    'inter_method': ['nearest','linear'],\n    'epoch': [1]\n}\n\n\nclass PLNR_STGCN_RAND:\n    def __init__(self,plans,loader,dataset_name=None,simulation_results=None):\n        self.plans = plans\n        col = ['dataset', 'method', 'mrate', 'mtype', 'lags', 'nof_filters', 'inter_method', 'epoch', 'mse']\n        self.loader = loader\n        self.dataset_name = dataset_name\n        self.simulation_results = pd.DataFrame(columns=col) if simulation_results is None else simulation_results \n    def simulate(self):\n        for _ in range(self.plans['max_iteration']):  \n            product_iterator = itertools.product(\n                self.plans['method'], \n                self.plans['mrate'], \n                self.plans['lags'], \n                self.plans['nof_filters'], \n                self.plans['inter_method'],\n                self.plans['epoch']\n            )\n            for prod_iter in product_iterator:\n                method,mrate,lags,nof_filters,inter_method,epoch = prod_iter\n                self.dataset = self.loader.get_dataset(lags=lags)\n                train_dataset, test_dataset = torch_geometric_temporal.signal.temporal_signal_split(self.dataset, train_ratio=0.8)\n                if mrate &gt; 0: \n                    mtype = 'rand'\n                    mindex = rand_mindex(train_dataset,mrate=mrate)\n                    train_dataset = padding(train_dataset_miss = miss(train_dataset,mindex=mindex,mtype=mtype),interpolation_method=inter_method)\n                elif mrate ==0: \n                    mtype = None\n                    inter_method = None \n                if method == 'STGCN':\n                    lrnr = StgcnLearner(train_dataset,dataset_name=self.dataset_name)\n                elif method == 'IT-STGCN':\n                    lrnr = ITStgcnLearner(train_dataset,dataset_name=self.dataset_name)\n                lrnr.learn(filters=nof_filters,epoch=epoch)\n                evtor = Evaluator(lrnr,train_dataset,test_dataset)\n                evtor.calculate_mse()\n                mse = evtor.mse['test']['total']\n                self._record(method,mrate,mtype,lags,nof_filters,inter_method,epoch,mse)\n            print('{}/{} is done'.format(_+1,self.plans['max_iteration']))\n    def _record(self,method,mrate,mtype,lags,nof_filters,inter_method,epoch,mse):\n        dct = {'dataset': self.dataset_name,\n               'method': method,\n               'mrate': mrate,\n               'mtype': mtype, \n               'lags': lags,\n               'nof_filters': nof_filters,\n               'inter_method': inter_method,\n               'epoch': epoch,\n               'mse': mse\n              }\n        simulation_result_new = pd.Series(dct).to_frame().transpose()\n        self.simulation_results = pd.concat([self.simulation_results,simulation_result_new]).reset_index(drop=True)\n\n\nplnr = PLNR_STGCN_RAND(plans,loader,dataset_name='five_nodes')\n\n\nplnr.simulate()\n\n1/3 is done\n2/3 is done\n3/3 is done\n\n\n\ndf = plnr.simulation_results\ndf\n\n\n\n\n\n\n\n\ndataset\nmethod\nmrate\nmtype\nlags\nnof_filters\ninter_method\nepoch\nmse\n\n\n\n\n0\nfive_nodes\nSTGCN\n0.0\nNone\n2\n8\nNone\n1\n1.202111\n\n\n1\nfive_nodes\nSTGCN\n0.0\nNone\n2\n8\nNone\n1\n1.173311\n\n\n2\nfive_nodes\nSTGCN\n0.0\nNone\n2\n16\nNone\n1\n1.170123\n\n\n3\nfive_nodes\nSTGCN\n0.0\nNone\n2\n16\nNone\n1\n1.18629\n\n\n4\nfive_nodes\nSTGCN\n0.0\nNone\n4\n8\nNone\n1\n1.238957\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n139\nfive_nodes\nIT-STGCN\n0.4\nrand\n2\n16\nlinear\n1\n1.195191\n\n\n140\nfive_nodes\nIT-STGCN\n0.4\nrand\n4\n8\nnearest\n1\n1.208371\n\n\n141\nfive_nodes\nIT-STGCN\n0.4\nrand\n4\n8\nlinear\n1\n1.160624\n\n\n142\nfive_nodes\nIT-STGCN\n0.4\nrand\n4\n16\nnearest\n1\n1.15774\n\n\n143\nfive_nodes\nIT-STGCN\n0.4\nrand\n4\n16\nlinear\n1\n1.217873\n\n\n\n\n144 rows × 9 columns"
  },
  {
    "objectID": "posts/2_research/2023-03-03-ESTGCN_GNAR_edit_guebin2_seoyeon.html#plnr_stgcn_block",
    "href": "posts/2_research/2023-03-03-ESTGCN_GNAR_edit_guebin2_seoyeon.html#plnr_stgcn_block",
    "title": "SY 1st ITSTGCN",
    "section": "PLNR_STGCN_BLOCK",
    "text": "PLNR_STGCN_BLOCK\n\n# _data = load_data('./data/fivenodes.pkl')\n\n# _edges = torch.tensor(_data['edges']).nonzero().tolist()\n# _FX = _data['f'].tolist()\n# _node_ids = {'node1':0, 'node2':1, 'node3':2, 'node4':3, 'node5':4} \n\ndata_dict = {'edges':_edges, 'node_ids':_node_ids, 'FX':_FX}\n\n\nloader = DatasetLoader(data_dict)\n\n\nmindex_block = [list(range(10,100)),[],list(range(50,80)),[],[]]\nplans_stgcn_block = {\n    'max_iteration': 3, \n    'method': ['STGCN', 'IT-STGCN'], \n    'mindex': [mindex_block],\n    'lags': [2, 4], \n    'nof_filters': [8,16], \n    'inter_method': ['nearest','linear'],\n    'epoch': [1]\n}\n\n\nclass PLNR_STGCN_BLOCK:\n    def __init__(self,plans,loader,dataset_name=None,simulation_results=None):\n        self.plans = plans\n        col = ['dataset', 'method', 'mrate', 'mtype', 'lags', 'nof_filters', 'inter_method', 'epoch', 'mse']\n        self.loader = loader\n        self.dataset_name = dataset_name\n        self.simulation_results = pd.DataFrame(columns=col) if simulation_results is None else simulation_results \n    def simulate(self):\n        for _ in range(self.plans['max_iteration']):\n            product_iterator = itertools.product(\n                self.plans['method'], \n                self.plans['mindex'],\n                self.plans['lags'],\n                self.plans['nof_filters'],\n                self.plans['inter_method'],\n                self.plans['epoch']\n            )\n            for prod_iter in product_iterator:\n                method,mrate,lags,nof_filters,inter_method,epoch = prod_iter\n                self.dataset = self.loader.get_dataset(lags=lags)\n                train_dataset, test_dataset = torch_geometric_temporal.signal.temporal_signal_split(self.dataset, train_ratio=0.8)\n                mtype = 'block'\n                train_dataset = padding(train_dataset_miss = miss(train_dataset,mindex=mindex,mtype=mtype),interpolation_method=inter_method)\n                if method == 'STGCN':\n                    lrnr = StgcnLearner(train_dataset,dataset_name=self.dataset_name)\n                elif method == 'IT-STGCN':\n                    lrnr = ITStgcnLearner(train_dataset,dataset_name=self.dataset_name)\n                lrnr.learn(filters=nof_filters,epoch=epoch)\n                evtor = Evaluator(lrnr,train_dataset,test_dataset)\n                evtor.calculate_mse()\n                mse = evtor.mse['test']['total']\n                mrate= lrnr.mrate_total\n                self._record(method,mrate,mtype,lags,nof_filters,inter_method,epoch,mse)\n    def _record(self,method,mrate,mtype,lags,nof_filters,inter_method,epoch,mse):\n        dct = {'dataset': self.dataset_name,\n               'method': method,\n               'mrate': mrate,\n               'mtype': mtype, \n               'lags': lags,\n               'nof_filters': nof_filters,\n               'inter_method': inter_method,\n               'epoch': epoch,\n               'mse': mse\n              }\n        simulation_result_new = pd.Series(dct).to_frame().transpose()\n        self.simulation_results = pd.concat([self.simulation_results,simulation_result_new]).reset_index(drop=True)\n\n\nplnr = PLNR_STGCN_BLOCK(plans_stgcn_block,loader,dataset_name='five_nodes')\n\n\nplnr.simulate()\n\n1/1\n\n\n\ndf = plnr.simulation_results\ndf\n\n\n\n\n\n\n\n\ndataset\nmethod\nmrate\nmtype\nlags\nnof_filters\ninter_method\nepoch\nmse\n\n\n\n\n0\nfive_nodes\nSTGCN\n0.5\nblock\n2\n8\nnearest\n1\n1.162601\n\n\n1\nfive_nodes\nSTGCN\n0.5\nblock\n2\n8\nlinear\n1\n1.145895\n\n\n2\nfive_nodes\nSTGCN\n0.5\nblock\n2\n16\nnearest\n1\n1.166197\n\n\n3\nfive_nodes\nSTGCN\n0.5\nblock\n2\n16\nlinear\n1\n1.165355\n\n\n4\nfive_nodes\nSTGCN\n0.5\nblock\n4\n8\nnearest\n1\n1.157954\n\n\n5\nfive_nodes\nSTGCN\n0.5\nblock\n4\n8\nlinear\n1\n1.162674\n\n\n6\nfive_nodes\nSTGCN\n0.5\nblock\n4\n16\nnearest\n1\n1.179143\n\n\n7\nfive_nodes\nSTGCN\n0.5\nblock\n4\n16\nlinear\n1\n1.175561\n\n\n8\nfive_nodes\nIT-STGCN\n0.5\nblock\n2\n8\nnearest\n1\n1.195364\n\n\n9\nfive_nodes\nIT-STGCN\n0.5\nblock\n2\n8\nlinear\n1\n1.2184\n\n\n10\nfive_nodes\nIT-STGCN\n0.5\nblock\n2\n16\nnearest\n1\n1.210481\n\n\n11\nfive_nodes\nIT-STGCN\n0.5\nblock\n2\n16\nlinear\n1\n1.169326\n\n\n12\nfive_nodes\nIT-STGCN\n0.5\nblock\n4\n8\nnearest\n1\n1.193523\n\n\n13\nfive_nodes\nIT-STGCN\n0.5\nblock\n4\n8\nlinear\n1\n1.199567\n\n\n14\nfive_nodes\nIT-STGCN\n0.5\nblock\n4\n16\nnearest\n1\n1.201094\n\n\n15\nfive_nodes\nIT-STGCN\n0.5\nblock\n4\n16\nlinear\n1\n1.210867\n\n\n16\nfive_nodes\nSTGCN\n0.5\nblock\n2\n8\nnearest\n1\n1.169622\n\n\n17\nfive_nodes\nSTGCN\n0.5\nblock\n2\n8\nlinear\n1\n1.173848\n\n\n18\nfive_nodes\nSTGCN\n0.5\nblock\n2\n16\nnearest\n1\n1.176841\n\n\n19\nfive_nodes\nSTGCN\n0.5\nblock\n2\n16\nlinear\n1\n1.15848\n\n\n20\nfive_nodes\nSTGCN\n0.5\nblock\n4\n8\nnearest\n1\n1.191304\n\n\n21\nfive_nodes\nSTGCN\n0.5\nblock\n4\n8\nlinear\n1\n1.155874\n\n\n22\nfive_nodes\nSTGCN\n0.5\nblock\n4\n16\nnearest\n1\n1.188419\n\n\n23\nfive_nodes\nSTGCN\n0.5\nblock\n4\n16\nlinear\n1\n1.197183\n\n\n24\nfive_nodes\nIT-STGCN\n0.5\nblock\n2\n8\nnearest\n1\n1.210021\n\n\n25\nfive_nodes\nIT-STGCN\n0.5\nblock\n2\n8\nlinear\n1\n1.184674\n\n\n26\nfive_nodes\nIT-STGCN\n0.5\nblock\n2\n16\nnearest\n1\n1.274009\n\n\n27\nfive_nodes\nIT-STGCN\n0.5\nblock\n2\n16\nlinear\n1\n1.188723\n\n\n28\nfive_nodes\nIT-STGCN\n0.5\nblock\n4\n8\nnearest\n1\n1.217735\n\n\n29\nfive_nodes\nIT-STGCN\n0.5\nblock\n4\n8\nlinear\n1\n1.202317\n\n\n30\nfive_nodes\nIT-STGCN\n0.5\nblock\n4\n16\nnearest\n1\n1.219543\n\n\n31\nfive_nodes\nIT-STGCN\n0.5\nblock\n4\n16\nlinear\n1\n1.202418\n\n\n32\nfive_nodes\nSTGCN\n0.5\nblock\n2\n8\nnearest\n1\n1.158991\n\n\n33\nfive_nodes\nSTGCN\n0.5\nblock\n2\n8\nlinear\n1\n1.187762\n\n\n34\nfive_nodes\nSTGCN\n0.5\nblock\n2\n16\nnearest\n1\n1.182213\n\n\n35\nfive_nodes\nSTGCN\n0.5\nblock\n2\n16\nlinear\n1\n1.161439\n\n\n36\nfive_nodes\nSTGCN\n0.5\nblock\n4\n8\nnearest\n1\n1.188787\n\n\n37\nfive_nodes\nSTGCN\n0.5\nblock\n4\n8\nlinear\n1\n1.233327\n\n\n38\nfive_nodes\nSTGCN\n0.5\nblock\n4\n16\nnearest\n1\n1.15206\n\n\n39\nfive_nodes\nSTGCN\n0.5\nblock\n4\n16\nlinear\n1\n1.161346\n\n\n40\nfive_nodes\nIT-STGCN\n0.5\nblock\n2\n8\nnearest\n1\n1.215097\n\n\n41\nfive_nodes\nIT-STGCN\n0.5\nblock\n2\n8\nlinear\n1\n1.163064\n\n\n42\nfive_nodes\nIT-STGCN\n0.5\nblock\n2\n16\nnearest\n1\n1.206054\n\n\n43\nfive_nodes\nIT-STGCN\n0.5\nblock\n2\n16\nlinear\n1\n1.177454\n\n\n44\nfive_nodes\nIT-STGCN\n0.5\nblock\n4\n8\nnearest\n1\n1.233471\n\n\n45\nfive_nodes\nIT-STGCN\n0.5\nblock\n4\n8\nlinear\n1\n1.209842\n\n\n46\nfive_nodes\nIT-STGCN\n0.5\nblock\n4\n16\nnearest\n1\n1.221017\n\n\n47\nfive_nodes\nIT-STGCN\n0.5\nblock\n4\n16\nlinear\n1\n1.218403"
  },
  {
    "objectID": "posts/2_research/2023-03-03-ESTGCN_GNAR_edit_guebin2_seoyeon.html#plnr_gnar_rand",
    "href": "posts/2_research/2023-03-03-ESTGCN_GNAR_edit_guebin2_seoyeon.html#plnr_gnar_rand",
    "title": "SY 1st ITSTGCN",
    "section": "PLNR_GNAR_RAND",
    "text": "PLNR_GNAR_RAND\n\n# _data = load_data('./data/fivenodes.pkl')\n\n# _edges = torch.tensor(_data['edges']).nonzero().tolist()\n# _FX = _data['f'].tolist()\n# _node_ids = {'node1':0, 'node2':1, 'node3':2, 'node4':3, 'node5':4} \n\ndata_dict = {'edges':_edges, 'node_ids':_node_ids, 'FX':_FX}\n\n\nloader = DatasetLoader(data_dict)\n\n\nplans_gnar_rand = {\n    'max_iteration': 3, \n#    'method': ['GNAR'], \n    'mrate': [0.0, 0.2, 0.4],\n    'lags': [2, 4], \n#    'nof_filters': [8,16], \n    'inter_method': ['nearest','linear'],\n#    'epoch': [1]\n}\n\n\nclass PLNR_GNAR_RAND:\n    def __init__(self,plans,loader,dataset_name=None,simulation_results=None):\n        self.plans = plans\n        col = ['dataset', 'method', 'mrate', 'mtype', 'lags', 'nof_filters', 'inter_method', 'epoch', 'mse']\n        self.loader = loader\n        self.dataset_name = dataset_name\n        self.simulation_results = pd.DataFrame(columns=col) if simulation_results is None else simulation_results \n    def simulate(self):\n        for _ in range(self.plans['max_iteration']):\n            product_iterator = itertools.product(\n                self.plans['mrate'],\n                self.plans['lags'],\n                self.plans['inter_method']\n            )\n            for prod_iter in product_iterator:\n                mrate,lags,inter_method = prod_iter\n                self.dataset = self.loader.get_dataset(lags=lags)\n                train_dataset, test_dataset = torch_geometric_temporal.signal.temporal_signal_split(self.dataset, train_ratio=0.8)\n                if mrate &gt; 0: \n                    mtype = 'rand'\n                    mindex = rand_mindex(train_dataset,mrate=mrate)\n                    train_dataset = padding(train_dataset_miss = miss(train_dataset,mindex=mindex,mtype=mtype),interpolation_method=inter_method)\n                elif mrate ==0: \n                    mtype = None\n                    inter_method = None \n                method = 'GNAR'\n                lrnr = GNARLearner(train_dataset,dataset_name=self.dataset_name)\n                lrnr.learn()\n                evtor = Evaluator(lrnr,train_dataset,test_dataset)\n                evtor.calculate_mse()\n                mse = evtor.mse['test']['total']\n                nof_filters = None \n                epoch= None\n                self._record(method,mrate,mtype,lags,nof_filters,inter_method,epoch,mse)\n    def _record(self,method,mrate,mtype,lags,nof_filters,inter_method,epoch,mse):\n        dct = {'dataset': self.dataset_name,\n               'method': method,\n               'mrate': mrate,\n               'mtype': mtype, \n               'lags': lags,\n               'nof_filters': nof_filters,\n               'inter_method': inter_method,\n               'epoch': epoch,\n               'mse': mse\n              }\n        simulation_result_new = pd.Series(dct).to_frame().transpose()\n        self.simulation_results = pd.concat([self.simulation_results,simulation_result_new]).reset_index(drop=True)\n\n\nplnr = PLNR_GNAR_RAND(plans_gnar_rand,loader,dataset_name='five_nodes')\nplnr.simulate()\n\nWARNING: diagonal entries present in original matrix, these will be removed\nWARNING: diagonal entries present in original matrix, these will be removed\nWARNING: diagonal entries present in original matrix, these will be removed\nWARNING: diagonal entries present in original matrix, these will be removed\nWARNING: diagonal entries present in original matrix, these will be removed\nWARNING: diagonal entries present in original matrix, these will be removed\nWARNING: diagonal entries present in original matrix, these will be removed\nWARNING: diagonal entries present in original matrix, these will be removed\nWARNING: diagonal entries present in original matrix, these will be removed\nWARNING: diagonal entries present in original matrix, these will be removed\nWARNING: diagonal entries present in original matrix, these will be removed\nWARNING: diagonal entries present in original matrix, these will be removed\nWARNING: diagonal entries present in original matrix, these will be removed\nWARNING: diagonal entries present in original matrix, these will be removed\nWARNING: diagonal entries present in original matrix, these will be removed\nWARNING: diagonal entries present in original matrix, these will be removed\nWARNING: diagonal entries present in original matrix, these will be removed\nWARNING: diagonal entries present in original matrix, these will be removed\nWARNING: diagonal entries present in original matrix, these will be removed\nWARNING: diagonal entries present in original matrix, these will be removed\nWARNING: diagonal entries present in original matrix, these will be removed\nWARNING: diagonal entries present in original matrix, these will be removed\nWARNING: diagonal entries present in original matrix, these will be removed\nWARNING: diagonal entries present in original matrix, these will be removed\nWARNING: diagonal entries present in original matrix, these will be removed\nWARNING: diagonal entries present in original matrix, these will be removed\nWARNING: diagonal entries present in original matrix, these will be removed\nWARNING: diagonal entries present in original matrix, these will be removed\nWARNING: diagonal entries present in original matrix, these will be removed\nWARNING: diagonal entries present in original matrix, these will be removed\nWARNING: diagonal entries present in original matrix, these will be removed\nWARNING: diagonal entries present in original matrix, these will be removed\nWARNING: diagonal entries present in original matrix, these will be removed\nWARNING: diagonal entries present in original matrix, these will be removed\nWARNING: diagonal entries present in original matrix, these will be removed\nWARNING: diagonal entries present in original matrix, these will be removed\nWARNING: diagonal entries present in original matrix, these will be removed\nWARNING: diagonal entries present in original matrix, these will be removed\nWARNING: diagonal entries present in original matrix, these will be removed\nWARNING: diagonal entries present in original matrix, these will be removed\nWARNING: diagonal entries present in original matrix, these will be removed\nWARNING: diagonal entries present in original matrix, these will be removed\nWARNING: diagonal entries present in original matrix, these will be removed\nWARNING: diagonal entries present in original matrix, these will be removed\nWARNING: diagonal entries present in original matrix, these will be removed\nWARNING: diagonal entries present in original matrix, these will be removed\nWARNING: diagonal entries present in original matrix, these will be removed\nWARNING: diagonal entries present in original matrix, these will be removed\nWARNING: diagonal entries present in original matrix, these will be removed\nWARNING: diagonal entries present in original matrix, these will be removed\nWARNING: diagonal entries present in original matrix, these will be removed\nWARNING: diagonal entries present in original matrix, these will be removed\nWARNING: diagonal entries present in original matrix, these will be removed\nWARNING: diagonal entries present in original matrix, these will be removed\nWARNING: diagonal entries present in original matrix, these will be removed\nWARNING: diagonal entries present in original matrix, these will be removed\nWARNING: diagonal entries present in original matrix, these will be removed\nWARNING: diagonal entries present in original matrix, these will be removed\nWARNING: diagonal entries present in original matrix, these will be removed\nWARNING: diagonal entries present in original matrix, these will be removed\nWARNING: diagonal entries present in original matrix, these will be removed\nWARNING: diagonal entries present in original matrix, these will be removed\nWARNING: diagonal entries present in original matrix, these will be removed\nWARNING: diagonal entries present in original matrix, these will be removed\nWARNING: diagonal entries present in original matrix, these will be removed\nWARNING: diagonal entries present in original matrix, these will be removed\nWARNING: diagonal entries present in original matrix, these will be removed\nWARNING: diagonal entries present in original matrix, these will be removed\nWARNING: diagonal entries present in original matrix, these will be removed\nWARNING: diagonal entries present in original matrix, these will be removed\nWARNING: diagonal entries present in original matrix, these will be removed\nWARNING: diagonal entries present in original matrix, these will be removed\nWARNING: diagonal entries present in original matrix, these will be removed\nWARNING: diagonal entries present in original matrix, these will be removed\nWARNING: diagonal entries present in original matrix, these will be removed\nWARNING: diagonal entries present in original matrix, these will be removed\nWARNING: diagonal entries present in original matrix, these will be removed\nWARNING: diagonal entries present in original matrix, these will be removed\nWARNING: diagonal entries present in original matrix, these will be removed\nWARNING: diagonal entries present in original matrix, these will be removed\nWARNING: diagonal entries present in original matrix, these will be removed\nWARNING: diagonal entries present in original matrix, these will be removed\nWARNING: diagonal entries present in original matrix, these will be removed\nWARNING: diagonal entries present in original matrix, these will be removed\nWARNING: diagonal entries present in original matrix, these will be removed\nWARNING: diagonal entries present in original matrix, these will be removed\nWARNING: diagonal entries present in original matrix, these will be removed\nWARNING: diagonal entries present in original matrix, these will be removed\nWARNING: diagonal entries present in original matrix, these will be removed\nWARNING: diagonal entries present in original matrix, these will be removed\nWARNING: diagonal entries present in original matrix, these will be removed\nWARNING: diagonal entries present in original matrix, these will be removed\nWARNING: diagonal entries present in original matrix, these will be removed\nWARNING: diagonal entries present in original matrix, these will be removed\nWARNING: diagonal entries present in original matrix, these will be removed\nWARNING: diagonal entries present in original matrix, these will be removed\nWARNING: diagonal entries present in original matrix, these will be removed\nWARNING: diagonal entries present in original matrix, these will be removed\nWARNING: diagonal entries present in original matrix, these will be removed\nWARNING: diagonal entries present in original matrix, these will be removed\nWARNING: diagonal entries present in original matrix, these will be removed\nWARNING: diagonal entries present in original matrix, these will be removed\nWARNING: diagonal entries present in original matrix, these will be removed\nWARNING: diagonal entries present in original matrix, these will be removed\nWARNING: diagonal entries present in original matrix, these will be removed\nWARNING: diagonal entries present in original matrix, these will be removed\nWARNING: diagonal entries present in original matrix, these will be removed\nWARNING: diagonal entries present in original matrix, these will be removed\n\n\n\nplnr.simulation_results\n\n\n\n\n\n\n\n\ndataset\nmethod\nmrate\nmtype\nlags\nnof_filters\ninter_method\nepoch\nmse\n\n\n\n\n0\nfive_nodes\nGNAR\n0.0\nNone\n2\nNone\nNone\nNone\n1.40683\n\n\n1\nfive_nodes\nGNAR\n0.0\nNone\n2\nNone\nNone\nNone\n1.40683\n\n\n2\nfive_nodes\nGNAR\n0.0\nNone\n4\nNone\nNone\nNone\n1.469004\n\n\n3\nfive_nodes\nGNAR\n0.0\nNone\n4\nNone\nNone\nNone\n1.469004\n\n\n4\nfive_nodes\nGNAR\n0.2\nrand\n2\nNone\nnearest\nNone\n1.40683\n\n\n5\nfive_nodes\nGNAR\n0.2\nrand\n2\nNone\nlinear\nNone\n1.40683\n\n\n6\nfive_nodes\nGNAR\n0.2\nrand\n4\nNone\nnearest\nNone\n1.469004\n\n\n7\nfive_nodes\nGNAR\n0.2\nrand\n4\nNone\nlinear\nNone\n1.469004\n\n\n8\nfive_nodes\nGNAR\n0.4\nrand\n2\nNone\nnearest\nNone\n1.40683\n\n\n9\nfive_nodes\nGNAR\n0.4\nrand\n2\nNone\nlinear\nNone\n1.40683\n\n\n10\nfive_nodes\nGNAR\n0.4\nrand\n4\nNone\nnearest\nNone\n1.469004\n\n\n11\nfive_nodes\nGNAR\n0.4\nrand\n4\nNone\nlinear\nNone\n1.469004\n\n\n12\nfive_nodes\nGNAR\n0.0\nNone\n2\nNone\nNone\nNone\n1.40683\n\n\n13\nfive_nodes\nGNAR\n0.0\nNone\n2\nNone\nNone\nNone\n1.40683\n\n\n14\nfive_nodes\nGNAR\n0.0\nNone\n4\nNone\nNone\nNone\n1.469004\n\n\n15\nfive_nodes\nGNAR\n0.0\nNone\n4\nNone\nNone\nNone\n1.469004\n\n\n16\nfive_nodes\nGNAR\n0.2\nrand\n2\nNone\nnearest\nNone\n1.40683\n\n\n17\nfive_nodes\nGNAR\n0.2\nrand\n2\nNone\nlinear\nNone\n1.40683\n\n\n18\nfive_nodes\nGNAR\n0.2\nrand\n4\nNone\nnearest\nNone\n1.469004\n\n\n19\nfive_nodes\nGNAR\n0.2\nrand\n4\nNone\nlinear\nNone\n1.469004\n\n\n20\nfive_nodes\nGNAR\n0.4\nrand\n2\nNone\nnearest\nNone\n1.40683\n\n\n21\nfive_nodes\nGNAR\n0.4\nrand\n2\nNone\nlinear\nNone\n1.40683\n\n\n22\nfive_nodes\nGNAR\n0.4\nrand\n4\nNone\nnearest\nNone\n1.469004\n\n\n23\nfive_nodes\nGNAR\n0.4\nrand\n4\nNone\nlinear\nNone\n1.469004\n\n\n24\nfive_nodes\nGNAR\n0.0\nNone\n2\nNone\nNone\nNone\n1.40683\n\n\n25\nfive_nodes\nGNAR\n0.0\nNone\n2\nNone\nNone\nNone\n1.40683\n\n\n26\nfive_nodes\nGNAR\n0.0\nNone\n4\nNone\nNone\nNone\n1.469004\n\n\n27\nfive_nodes\nGNAR\n0.0\nNone\n4\nNone\nNone\nNone\n1.469004\n\n\n28\nfive_nodes\nGNAR\n0.2\nrand\n2\nNone\nnearest\nNone\n1.40683\n\n\n29\nfive_nodes\nGNAR\n0.2\nrand\n2\nNone\nlinear\nNone\n1.40683\n\n\n30\nfive_nodes\nGNAR\n0.2\nrand\n4\nNone\nnearest\nNone\n1.469004\n\n\n31\nfive_nodes\nGNAR\n0.2\nrand\n4\nNone\nlinear\nNone\n1.469004\n\n\n32\nfive_nodes\nGNAR\n0.4\nrand\n2\nNone\nnearest\nNone\n1.40683\n\n\n33\nfive_nodes\nGNAR\n0.4\nrand\n2\nNone\nlinear\nNone\n1.40683\n\n\n34\nfive_nodes\nGNAR\n0.4\nrand\n4\nNone\nnearest\nNone\n1.469004\n\n\n35\nfive_nodes\nGNAR\n0.4\nrand\n4\nNone\nlinear\nNone\n1.469004"
  },
  {
    "objectID": "posts/2_research/2023-03-03-ESTGCN_GNAR_edit_guebin2_seoyeon.html#plnr_gnar_block",
    "href": "posts/2_research/2023-03-03-ESTGCN_GNAR_edit_guebin2_seoyeon.html#plnr_gnar_block",
    "title": "SY 1st ITSTGCN",
    "section": "PLNR_GNAR_BLOCK",
    "text": "PLNR_GNAR_BLOCK\n\n# _data = load_data('./data/fivenodes.pkl')\n\n# _edges = torch.tensor(_data['edges']).nonzero().tolist()\n# _FX = _data['f'].tolist()\n# _node_ids = {'node1':0, 'node2':1, 'node3':2, 'node4':3, 'node5':4} \n\ndata_dict = {'edges':_edges, 'node_ids':_node_ids, 'FX':_FX}\n\n\nloader = DatasetLoader(data_dict)\n\n\nmindex_block = [list(range(10,100)),[],list(range(50,80)),[],[]]\nplans_gnar_block = {\n    'max_iteration': 3, \n    'method': ['GNAR'], \n    'mindex': [mindex_block],\n    'lags': [2, 4], \n    'inter_method': ['nearest','linear'],\n}\n\n\nclass PLNR_GNAR_BLOCK:\n    def __init__(self,plans,loader,dataset_name=None,simulation_results=None):\n        self.plans = plans\n        col = ['dataset', 'method', 'mrate', 'mtype', 'lags', 'nof_filters', 'inter_method', 'epoch', 'mse']\n        self.loader = loader\n        self.dataset_name = dataset_name\n        self.simulation_results = pd.DataFrame(columns=col) if simulation_results is None else simulation_results \n    def simulate(self):\n        for _ in range(self.plans['max_iteration']):\n            product_iterator = itertools.product(\n                self.plans['mindex'],\n                self.plans['lags'],\n                self.plans['inter_method']\n            )\n            for prod_iter in product_iterator:\n                mrate,lags,inter_method = prod_iter\n                self.dataset = self.loader.get_dataset(lags=lags)\n                train_dataset, test_dataset = torch_geometric_temporal.signal.temporal_signal_split(self.dataset, train_ratio=0.8)\n                mtype = 'block'\n                train_dataset = padding(train_dataset_miss = miss(train_dataset,mindex=mindex,mtype=mtype),interpolation_method=inter_method)\n                method = 'GNAR'\n                lrnr = GNARLearner(train_dataset,dataset_name=self.dataset_name)\n                lrnr.learn()\n                evtor = Evaluator(lrnr,train_dataset,test_dataset)\n                evtor.calculate_mse()\n                mse = evtor.mse['test']['total']\n                nof_filters = None \n                epoch= None\n                mrate= lrnr.mrate_total\n                self._record(method,mrate,mtype,lags,nof_filters,inter_method,epoch,mse)\n    def _record(self,method,mrate,mtype,lags,nof_filters,inter_method,epoch,mse):\n        dct = {'dataset': self.dataset_name,\n               'method': method,\n               'mrate': mrate,\n               'mtype': mtype, \n               'lags': lags,\n               'nof_filters': nof_filters,\n               'inter_method': inter_method,\n               'epoch': epoch,\n               'mse': mse\n              }\n        simulation_result_new = pd.Series(dct).to_frame().transpose()\n        self.simulation_results = pd.concat([self.simulation_results,simulation_result_new]).reset_index(drop=True)\n\n\nplnr = PLNR_GNAR_BLOCK(plans_gnar_block,loader,dataset_name='five_nodes')\nplnr.simulate()\n\nWARNING: diagonal entries present in original matrix, these will be removed\nWARNING: diagonal entries present in original matrix, these will be removed\nWARNING: diagonal entries present in original matrix, these will be removed\nWARNING: diagonal entries present in original matrix, these will be removed\nWARNING: diagonal entries present in original matrix, these will be removed\nWARNING: diagonal entries present in original matrix, these will be removed\nWARNING: diagonal entries present in original matrix, these will be removed\nWARNING: diagonal entries present in original matrix, these will be removed\nWARNING: diagonal entries present in original matrix, these will be removed\nWARNING: diagonal entries present in original matrix, these will be removed\nWARNING: diagonal entries present in original matrix, these will be removed\nWARNING: diagonal entries present in original matrix, these will be removed\nWARNING: diagonal entries present in original matrix, these will be removed\nWARNING: diagonal entries present in original matrix, these will be removed\nWARNING: diagonal entries present in original matrix, these will be removed\nWARNING: diagonal entries present in original matrix, these will be removed\nWARNING: diagonal entries present in original matrix, these will be removed\nWARNING: diagonal entries present in original matrix, these will be removed\nWARNING: diagonal entries present in original matrix, these will be removed\nWARNING: diagonal entries present in original matrix, these will be removed\nWARNING: diagonal entries present in original matrix, these will be removed\nWARNING: diagonal entries present in original matrix, these will be removed\nWARNING: diagonal entries present in original matrix, these will be removed\nWARNING: diagonal entries present in original matrix, these will be removed\nWARNING: diagonal entries present in original matrix, these will be removed\nWARNING: diagonal entries present in original matrix, these will be removed\nWARNING: diagonal entries present in original matrix, these will be removed\nWARNING: diagonal entries present in original matrix, these will be removed\nWARNING: diagonal entries present in original matrix, these will be removed\nWARNING: diagonal entries present in original matrix, these will be removed\nWARNING: diagonal entries present in original matrix, these will be removed\nWARNING: diagonal entries present in original matrix, these will be removed\nWARNING: diagonal entries present in original matrix, these will be removed\nWARNING: diagonal entries present in original matrix, these will be removed\nWARNING: diagonal entries present in original matrix, these will be removed\nWARNING: diagonal entries present in original matrix, these will be removed\n\n\n\nplnr.simulation_results\n\n\n\n\n\n\n\n\ndataset\nmethod\nmrate\nmtype\nlags\nnof_filters\ninter_method\nepoch\nmse\n\n\n\n\n0\nfive_nodes\nGNAR\n0.5\nblock\n2\nNone\nnearest\nNone\n1.40683\n\n\n1\nfive_nodes\nGNAR\n0.5\nblock\n2\nNone\nlinear\nNone\n1.40683\n\n\n2\nfive_nodes\nGNAR\n0.5\nblock\n4\nNone\nnearest\nNone\n1.469004\n\n\n3\nfive_nodes\nGNAR\n0.5\nblock\n4\nNone\nlinear\nNone\n1.469004\n\n\n4\nfive_nodes\nGNAR\n0.5\nblock\n2\nNone\nnearest\nNone\n1.40683\n\n\n5\nfive_nodes\nGNAR\n0.5\nblock\n2\nNone\nlinear\nNone\n1.40683\n\n\n6\nfive_nodes\nGNAR\n0.5\nblock\n4\nNone\nnearest\nNone\n1.469004\n\n\n7\nfive_nodes\nGNAR\n0.5\nblock\n4\nNone\nlinear\nNone\n1.469004\n\n\n8\nfive_nodes\nGNAR\n0.5\nblock\n2\nNone\nnearest\nNone\n1.40683\n\n\n9\nfive_nodes\nGNAR\n0.5\nblock\n2\nNone\nlinear\nNone\n1.40683\n\n\n10\nfive_nodes\nGNAR\n0.5\nblock\n4\nNone\nnearest\nNone\n1.469004\n\n\n11\nfive_nodes\nGNAR\n0.5\nblock\n4\nNone\nlinear\nNone\n1.469004\n\n\n\n\n\n\n\n\n여기부터 서연이코드\n\nedges_tensor = torch.tensor(data['edges'])\nfiveVTS = np.array(data['f'])\nnonzero_indices = edges_tensor.nonzero()\nfiveNet_edge = np.array(nonzero_indices).T\nT = 200\nN = 5 # number of Nodes\nE = fiveNet_edge\nV = np.array([1,2,3,4,5])\nt = np.arange(0,T)\nnode_features = 1\nedge_index = torch.tensor(E)\nedge_attr = torch.tensor(np.array([1,1,1,1,1,1,1,1,1,1]),dtype=torch.float32)\n\n\nedge_index\n\nNameError: name 'edge_index' is not defined\n\n\n- train / test\n\nfiveVTS_train = fiveVTS[:int(len(fiveVTS)*0.8)]\nfiveVTS_test = fiveVTS[int(len(fiveVTS)*0.8):]"
  },
  {
    "objectID": "posts/2_research/2023-03-03-ESTGCN_GNAR_edit_guebin2_seoyeon.html#random-missing-values",
    "href": "posts/2_research/2023-03-03-ESTGCN_GNAR_edit_guebin2_seoyeon.html#random-missing-values",
    "title": "SY 1st ITSTGCN",
    "section": "Random Missing Values",
    "text": "Random Missing Values\n\nclass Missing:\n    def __init__(self,df):\n        self.df = df\n        self.N = N\n        self.number = []\n    def miss(self,percent=0.5):\n        self.missing = self.df.copy()\n        self.percent = percent\n        for i in range(self.N):\n            #self.seed = np.random.choice(1000,1,replace=False)\n            #np.random.seed(self.seed)\n            self.number.append(np.random.choice(int(len(self.df))-1,int(len(self.df)*self.percent),replace=False))\n            self.missing[self.number[i],i] = float('nan')\n    def first_mean(self):\n        self.train_mean = self.missing.copy()\n        for i in range(self.N):\n            self.train_mean[self.number[i],i] = np.nanmean(self.missing[:,i])\n    def second_linear(self):\n        self.train_linear = pd.DataFrame(self.missing)\n        self.train_linear.interpolate(method='linear', inplace=True)\n        self.train_linear = self.train_linear.fillna(0)\n        self.train_linear = np.array(self.train_linear).reshape(int(len(self.df)),N)\n\n\ncol = ['Dataset','iteration', 'method', 'missingrate', 'missingtype', 'lag', 'number_of_filters', 'interpolation','MSE_train', 'MSE_test']\n\nrate = [i/10 for i in range(10)]"
  },
  {
    "objectID": "posts/2_research/2023-03-03-ESTGCN_GNAR_edit_guebin2_seoyeon.html#class-code-by-method",
    "href": "posts/2_research/2023-03-03-ESTGCN_GNAR_edit_guebin2_seoyeon.html#class-code-by-method",
    "title": "SY 1st ITSTGCN",
    "section": "Class code by Method",
    "text": "Class code by Method"
  },
  {
    "objectID": "posts/2_research/2023-03-03-ESTGCN_GNAR_edit_guebin2_seoyeon.html#stgcn",
    "href": "posts/2_research/2023-03-03-ESTGCN_GNAR_edit_guebin2_seoyeon.html#stgcn",
    "title": "SY 1st ITSTGCN",
    "section": "STGCN",
    "text": "STGCN\n\nclass STGCN_Missing:\n    def __init__(self,Dataset,df, iterable, Method, Missingrate, Missingtype, lag, Number_of_filters, Interpolation):\n        self.Dataset = Dataset\n        self.df = df\n        self.iterable = iterable\n        self.Method = Method\n        self.Missingrate = Missingrate\n        self.Missingtype = Missingtype\n        self.lag = lag\n        self.Number_of_filters = Number_of_filters\n        self.Interpolation = Interpolation\n    def iter(self):\n        self.XX = torch.tensor(fiveVTS_test.reshape(int(T*0.2),N,1)[:-1,:,:]).float()\n        self.yy = torch.tensor(fiveVTS_test.reshape(int(T*0.2),N,1)[1:,:,:]).float()\n\n        self.real_y = torch.tensor(fiveVTS_train).reshape(int(T*0.8),N,1).float()[1:,:,:]\n        for i in range(self.iterable):\n\n            _zero = Missing(fiveVTS_train)\n            _zero.miss(percent = self.Missingrate)\n            _zero.second_linear()\n\n            missing_index = _zero.number\n            interpolated_signal = _zero.train_linear\n\n            X = torch.tensor(interpolated_signal).reshape(int(T*0.8),N,1).float()[:int(T*0.8-1),:,:]\n            y = torch.tensor(interpolated_signal).reshape(int(T*0.8),N,1).float()[1:,:,:]\n\n            net = RecurrentGCN(node_features=self.lag, filters=self.Number_of_filters)\n            optimizer = torch.optim.Adam(net.parameters(), lr=0.01)\n            net.train()\n            for epoch in range(50):\n                for time, (xt,yt) in enumerate(zip(X,y)):\n                    yt_hat = net(xt, edge_index, edge_attr)\n                    cost = torch.mean((yt_hat-yt)**2)\n                    cost.backward()\n                    optimizer.step()\n                    optimizer.zero_grad()\n\n            yhat = torch.stack([net(xt, edge_index, edge_attr) for xt in X]).detach().numpy()\n            yyhat = torch.stack([net(xt, edge_index, edge_attr) for xt in self.XX]).detach().numpy()\n\n            train_mse_total_stgcn = (((self.real_y-yhat).squeeze())**2).mean()\n            test_mse_total_stgcn = (((self.yy-yyhat).squeeze())**2).mean() \n\n            df_row = pd.DataFrame(columns=col)\n            df_row['Dataset'] = self.Dataset, \n            df_row['iteration'] = i+1, # 1,2,3,...,10 \n            df_row['method'] = self.Method, # 'stgcn','estgcn','gnar' \n            df_row['missingrate'] = self.Missingrate, # 0.0, 0.2, 0.4, 0.6, 0.8 \n            df_row['missingtype'] = self.Missingtype,  # None, 'randomly' and 'block' \n            df_row['lag'] = self.lag, # 1,2,3,4 ... \n            df_row['number_of_filters'] = self.Number_of_filters, # 16,24,32, ... \n            df_row['interpolation'] = self.Interpolation, # None, 'mean', 'linear'\n            df_row['MSE_train'] = train_mse_total_stgcn.tolist()\n            df_row['MSE_test'] = test_mse_total_stgcn.tolist()\n\n            self.df = pd.concat([self.df,df_row])"
  },
  {
    "objectID": "posts/2_research/2023-03-03-ESTGCN_GNAR_edit_guebin2_seoyeon.html#enhencement-of-stgcn",
    "href": "posts/2_research/2023-03-03-ESTGCN_GNAR_edit_guebin2_seoyeon.html#enhencement-of-stgcn",
    "title": "SY 1st ITSTGCN",
    "section": "Enhencement of STGCN",
    "text": "Enhencement of STGCN\n\nclass ESTGCN_Missing:\n    def __init__(self,Dataset,df, iterable, Method, Missingrate, Missingtype, lag, Number_of_filters, Interpolation):\n        self.Dataset = Dataset\n        self.df = df\n        self.iterable = iterable\n        self.Method = Method\n        self.Missingrate = Missingrate\n        self.Missingtype = Missingtype\n        self.lag = lag\n        self.Number_of_filters = Number_of_filters\n        self.Interpolation = Interpolation\n    def iter(self):\n        self.XX = torch.tensor(fiveVTS_test.reshape(int(T*0.2),N,1)[:-1,:,:]).float()\n        self.yy = torch.tensor(fiveVTS_test.reshape(int(T*0.2),N,1)[1:,:,:]).float()\n\n        self.real_y = torch.tensor(fiveVTS_train).reshape(int(T*0.8),N,1).float()[1:,:,:]\n        for i in range(self.iterable):\n    \n            _zero = Missing(fiveVTS_train)\n            _zero.miss(percent = self.Missingrate)\n            _zero.second_linear()\n\n            missing_index = _zero.number\n            interpolated_signal = _zero.train_linear\n\n            X = torch.tensor(interpolated_signal).reshape(int(T*0.8),N,1).float()[:int(T*0.8-1),:,:]\n            y = torch.tensor(interpolated_signal).reshape(int(T*0.8),N,1).float()[1:,:,:]\n\n\n            net = RecurrentGCN(node_features=self.lag, filters=self.Number_of_filters)\n            optimizer = torch.optim.Adam(net.parameters(), lr=0.01)\n            net.train()\n            signal = interpolated_signal.copy()\n            for epoch in range(50):\n                signal = update_from_freq_domain(signal,missing_index)\n                X = torch.tensor(signal).reshape(int(T*0.8),N,1).float()[:int(T*0.8-1),:,:]\n                y = torch.tensor(signal).reshape(int(T*0.8),N,1).float()[1:,:,:]\n                for time, (xt,yt) in enumerate(zip(X,y)):        \n                    yt_hat = net(xt, edge_index, edge_attr)\n                    cost = torch.mean((yt_hat-yt)**2)\n                    cost.backward()\n                    optimizer.step()\n                    optimizer.zero_grad()\n                signal = torch.concat([X.squeeze(),yt_hat.detach().squeeze().reshape(1,-1)])               \n\n            yhat = torch.stack([net(xt, edge_index, edge_attr) for xt in X]).detach().numpy()\n            yyhat = torch.stack([net(xt, edge_index, edge_attr) for xt in self.XX]).detach().numpy()\n\n            train_mse_total_estgcn = (((self.real_y-yhat).squeeze())**2).mean()\n            test_mse_total_estgcn = (((self.yy-yyhat).squeeze())**2).mean()\n\n            df_row = pd.DataFrame(columns=col)\n            df_row['Dataset'] = self.Dataset,\n            df_row['iteration'] = i+1, # 1,2,3,...,10 \n            df_row['method'] = self.Method, # 'stgcn','estgcn','gnar' \n            df_row['missingrate'] = self.Missingrate, # 0.0, 0.2, 0.4, 0.6, 0.8 \n            df_row['missingtype'] = self.Missingtype,  # None, 'randomly' and 'block' \n            df_row['lag'] = self.lag, # 1,2,3,4 ... \n            df_row['number_of_filters'] = self.Number_of_filters, # 16,24,32, ... \n            df_row['interpolation'] = self.Interpolation, # None, 'mean', 'linear'\n            df_row['MSE_train'] = train_mse_total_estgcn.tolist()\n            df_row['MSE_test'] = test_mse_total_estgcn.tolist()\n\n            self.df = pd.concat([self.df,df_row])"
  },
  {
    "objectID": "posts/2_research/2023-03-03-ESTGCN_GNAR_edit_guebin2_seoyeon.html#gnar",
    "href": "posts/2_research/2023-03-03-ESTGCN_GNAR_edit_guebin2_seoyeon.html#gnar",
    "title": "SY 1st ITSTGCN",
    "section": "GNAR",
    "text": "GNAR\n\nm = robjects.r.matrix(FloatVector([0,0,0,1,1,0,0,1,1,0,0,1,0,1,0,1,1,1,0,0,1,0,0,0,0]), nrow = 5, ncol = 5)\n\n\nclass GNAR_Missing:\n    def __init__(self,Dataset,df, iterable, Method, Missingrate, Missingtype, lag, Number_of_filters, Interpolation):\n        self.Dataset = Dataset\n        self.df = df\n        self.iterable = iterable\n        self.Method = Method\n        self.Missingrate = Missingrate\n        self.Missingtype = Missingtype\n        self.lag = lag\n        self.Number_of_filters = Number_of_filters\n        self.Interpolation = Interpolation\n    def iter(self):\n        self.yy = torch.tensor(fiveVTS_test.reshape(int(T*0.2),N,1)[1:,:,:]).float()\n        for i in range(self.iterable):\n\n            _zero = Missing(fiveVTS_train)\n            _zero.miss(percent = self.Missingrate)\n            _zero.second_linear()\n\n            missing_index = _zero.number\n            interpolated_signal = _zero.train_linear\n\n            X = torch.tensor(interpolated_signal).reshape(int(T*0.8),N,1).float()[:int(T*0.8-2),:,:]\n\n            answer = GNAR.GNARfit(vts=robjects.r.matrix(rpyn.numpy2rpy(np.array(X).squeeze()), nrow = 160, ncol = 5),net = GNAR.matrixtoGNAR(m), alphaOrder = 2, betaOrder = FloatVector([1, 1]))             \n            predict = GNAR.predict_GNARfit(answer,n_ahead=40)\n\n\n            train_mse_total_gnar = ((pd.DataFrame(GNAR.residuals_GNARfit(answer)).values.reshape(-1,5))**2).mean()\n            test_mse_total_gnar = ((self.yy.squeeze() - pd.DataFrame(predict).values.reshape(-1,5)[:-1,:])**2).mean()\n\n            df_row = pd.DataFrame(columns=col)\n            df_row['Dataset'] = self.Dataset,\n            df_row['iteration'] = i+1, # 1,2,3,...,10 \n            df_row['method'] = self.Method, # 'stgcn','estgcn','gnar' \n            df_row['missingrate'] = self.Missingrate, # 0.0, 0.2, 0.4, 0.6, 0.8 \n            df_row['missingtype'] = self.Missingtype,  # None, 'randomly' and 'block' \n            df_row['lag'] = self.lag, # 1,2,3,4 ... \n            df_row['number_of_filters'] = self.Number_of_filters, # 16,24,32, ... \n            df_row['interpolation'] = self.Interpolation, # None, 'mean', 'linear'\n            df_row['MSE_train'] = train_mse_total_gnar.tolist()\n            df_row['MSE_test'] = test_mse_total_gnar.tolist()\n\n            self.df = pd.concat([self.df,df_row])"
  },
  {
    "objectID": "posts/2_research/2023-03-03-ESTGCN_GNAR_edit_guebin2_seoyeon.html#stgcn-1",
    "href": "posts/2_research/2023-03-03-ESTGCN_GNAR_edit_guebin2_seoyeon.html#stgcn-1",
    "title": "SY 1st ITSTGCN",
    "section": "STGCN",
    "text": "STGCN\n\nDataset = 'fivenodes'\nMethod = 'stgcn' # 'stgcn','estgcn','gnar' \nMissingtype = 'randomly'  # None, 'randomly' and 'block' \nlag = 1 # 1,2,3,4 ... \nNumber_of_filters = 4 # 16,24,32, ... \nInterpolation = 'Linear' # None, 'mean', 'linear'\niterable = 100\n\n\ndf_stgcn= pd.DataFrame(columns=col)\n\n\nfor Missingrate in rate:\n    df = pd.DataFrame(columns=col)\n    stgcn = STGCN_Missing(Dataset,df, iterable,Method, Missingrate, Missingtype, lag, Number_of_filters, Interpolation)\n    stgcn.iter()\n    df_add = stgcn.df.copy()\n    df_stgcn = pd.concat([df_stgcn,df_add],axis=0)\n\n\nsave_data(df_stgcn, './data/GNAR_stgcn_randomly_by_rate.pkl')"
  },
  {
    "objectID": "posts/2_research/2023-03-03-ESTGCN_GNAR_edit_guebin2_seoyeon.html#enhencement-of-stgcn-1",
    "href": "posts/2_research/2023-03-03-ESTGCN_GNAR_edit_guebin2_seoyeon.html#enhencement-of-stgcn-1",
    "title": "SY 1st ITSTGCN",
    "section": "Enhencement of STGCN",
    "text": "Enhencement of STGCN\n\nDataset = 'fivenodes'\nMethod = 'estgcn' # 'stgcn','estgcn','gnar' \nMissingtype = 'randomly'  # None, 'randomly' and 'block' \nlag = 1 # 1,2,3,4 ... \nNumber_of_filters = 4 # 16,24,32, ... \nInterpolation = 'Linear' # None, 'mean', 'linear'\niterable = 100\n\n\ndf_estgcn = pd.DataFrame(columns=col)\n\n\nfor Missingrate in rate:\n    df = pd.DataFrame(columns=col)\n    estgcn = ESTGCN_Missing(Dataset,df, iterable,Method, Missingrate, Missingtype, lag, Number_of_filters, Interpolation)\n    estgcn.iter()\n    df_add = estgcn.df.copy()\n    df_estgcn = pd.concat([df_estgcn,df_add],axis=0)\n\n\nsave_data(df_estgcn, './data/GNAR_estgcn_randomly_by_rate.pkl')"
  },
  {
    "objectID": "posts/2_research/2023-03-03-ESTGCN_GNAR_edit_guebin2_seoyeon.html#gnar-1",
    "href": "posts/2_research/2023-03-03-ESTGCN_GNAR_edit_guebin2_seoyeon.html#gnar-1",
    "title": "SY 1st ITSTGCN",
    "section": "GNAR",
    "text": "GNAR\n\nDataset = 'fivenodes'\nMethod = 'gnar' # 'stgcn','estgcn','gnar' \nMissingtype = 'randomly'  # None, 'randomly' and 'block' \nlag = 1 # 1,2,3,4 ... \nNumber_of_filters = None # 16,24,32, ... \nInterpolation = 'Linear' # None, 'mean', 'linear'\niterable = 100\n\n\ndf_gnar = pd.DataFrame(columns=col)\n\n\nfor Missingrate in rate:\n    df = pd.DataFrame(columns=col)\n    gnar = GNAR_Missing(Dataset,df, iterable,Method, Missingrate, Missingtype, lag, Number_of_filters, Interpolation)\n    gnar.iter()\n    df_add = gnar.df.copy()\n    df_gnar = pd.concat([df_gnar,df_add],axis=0)\n\n\nsave_data(df_gnar, './data/GANR_gnar_randomly_by_rate.pkl')"
  },
  {
    "objectID": "posts/2_research/2023-02-06-ESTGCN_GNAR_DATA_2.html",
    "href": "posts/2_research/2023-02-06-ESTGCN_GNAR_DATA_2.html",
    "title": "Class of Method(GNAR) lag 2",
    "section": "",
    "text": "GNAR fiveNet,fivenodes lag 2"
  },
  {
    "objectID": "posts/2_research/2023-02-06-ESTGCN_GNAR_DATA_2.html#시나리오1-baseline",
    "href": "posts/2_research/2023-02-06-ESTGCN_GNAR_DATA_2.html#시나리오1-baseline",
    "title": "Class of Method(GNAR) lag 2",
    "section": "시나리오1 (Baseline)",
    "text": "시나리오1 (Baseline)\n시나리오1\n\nmissing rate: 0%\n보간방법: None\n\n\nSTGCN 으로 적합 + 예측\n\nX = torch.tensor(np.stack([fiveVTS_train[i:(int(T*0.8)-2+i),:] for i in range(2)],axis = -1)).float()\ny = torch.tensor(fiveVTS_train[2:int(T*0.8),:].reshape(int(T*0.8)-2,N,-1)).float()\n\n\nXX = torch.tensor(np.stack([fiveVTS_test[i:(int(T*0.2)-2+i),:] for i in range(2)],axis = -1)).float()\nyy = fiveVTS_test[2:int(T*0.2),:].reshape(int(T*0.2)-2,N,-1)\n\n\nnet = RecurrentGCN(node_features=2, filters=4)\noptimizer = torch.optim.Adam(net.parameters(), lr=0.01)\nnet.train()\nfor epoch in tqdm(range(50)):\n    for time, (xt,yt) in enumerate(zip(X,y)):\n        yt_hat = net(xt, edge_index, edge_attr)\n        cost = torch.mean((yt_hat-yt)**2)\n        cost.backward()\n        optimizer.step()\n        optimizer.zero_grad()\n\n100%|██████████| 50/50 [00:26&lt;00:00,  1.87it/s]\n\n\n\nyhat = torch.stack([net(xt, edge_index, edge_attr) for xt in X]).detach().numpy()\nyyhat = torch.stack([net(xt, edge_index, edge_attr) for xt in XX]).detach().numpy()\n\n\nstgcn_train = yhat.squeeze() # stgcn은 stgcn에 의한 적합결과를 의미함\nstgcn_test = yyhat.squeeze() \n\n\ntrain_mse_eachnode_stgcn = (((y-yhat).squeeze())**2).mean(axis=0)\ntrain_mse_total_stgcn = (((y-yhat).squeeze())**2).mean()\ntest_mse_eachnode_stgcn = (((yy-yyhat).squeeze())**2).mean(axis=0)\ntest_mse_total_stgcn = (((yy-yyhat).squeeze())**2).mean()\n\n\n\nGNAR 으로 적합 + 예측\n-\n\n%load_ext rpy2.ipython\n\n\n%%R\nlibrary(GNAR)\nlibrary(igraph)\nlibrary(tidyverse)\n\n\n%R -i fiveVTS_train\n\n\n%%R\nanswer &lt;- GNARfit(vts = fiveVTS_train, net = fiveNet, alphaOrder = 2, betaOrder = c(1, 1))\nprediction &lt;- predict(answer,n.ahead=40)\n\n\n%%R\ngnar_train &lt;- residuals(answer)\ngnar_test &lt;- prediction\n\n\n%R -o gnar_train\n%R -o gnar_test\n\n\ntrain_mse_eachnode_gnar = (gnar_train**2).mean(axis=0)\ntrain_mse_total_gnar = (gnar_train**2).mean()\ntest_mse_eachnode_gnar = ((fiveVTS_test - gnar_test.reshape(-1,5))**2).mean(axis=0)\ntest_mse_total_gnar = ((fiveVTS_test - gnar_test.reshape(-1,5))**2).mean()\n\n\n\n결과시각화\n\ntrain_mse_total_gnar,test_mse_total_gnar\n\n(0.995256618187614, 1.2577286248028454)\n\n\n\nfig = plot(fiveVTS,'--.',h=4,color='gray',label='complete data',alpha=0.5)\nax = fig.get_axes()\nfor i,a in enumerate(ax):     \n    a.set_title('node{0} \\n mse(train) = {1:.2f}, mse(test) = {2:.2f} \\n mse(train) = {3:.2f}, mse(test) = {4:.2f}'.format(i,train_mse_eachnode_stgcn[i],test_mse_eachnode_stgcn[i],train_mse_eachnode_gnar[i],test_mse_eachnode_gnar[i]))\n    a.plot(range(2,160),stgcn_train[:,i],label='STCGCN (train)',color='C0')\n    a.plot(range(162,200),stgcn_test[:,i],label='STCGCN (test)',color='C0')\n    a.plot(range(2,160),gnar_train.reshape(-1,5)[:,i],label='GNAR (train)',color='C1')\n    a.plot(range(160,200),gnar_test.reshape(-1,5)[:,i],label='GNAR (test)',color='C1')\n    a.legend()\nfig.set_figwidth(14)\nfig.suptitle(\"Scenario1: STGCN \\n missing=0% \\n interpolation=None \\n\\n STGCN: mse(train) = {0:.2f}, mse(test) = {1:.2f} \\n GNAR: mse(train) = {2:.2f}, mse(test) = {3:.2f} \\n\".format(train_mse_total_stgcn,test_mse_total_stgcn,train_mse_total_gnar,test_mse_total_gnar),size=15)\nfig.tight_layout()\nfig"
  },
  {
    "objectID": "posts/2_research/2023-02-06-ESTGCN_GNAR_DATA_2.html#시나리오2",
    "href": "posts/2_research/2023-02-06-ESTGCN_GNAR_DATA_2.html#시나리오2",
    "title": "Class of Method(GNAR) lag 2",
    "section": "시나리오2",
    "text": "시나리오2\n시나리오2\n\nmissing rate: 50%\n보간방법: linear\n\n- 결측치생성 + 보간\n\n_zero = Missing(fiveVTS_train)\n_zero.miss(percent = 0.5)\n_zero.second_linear()\n\n\nmissing_index = _zero.number\ninterpolated_signal = _zero.train_linear\n\n\nfig = plot(fiveVTS,'--o',h=4,color='gray',label='complete data',alpha=0.2)\nax = fig.get_axes()\nfor i,a in enumerate(ax):     \n    a.plot(missing_index[i],fiveVTS_train[:,i][missing_index[i]],'xk',label='missing')\n    a.plot(interpolated_signal[:,i],'-',color='gray',label='linear interpolation')\n    a.legend()\nfig.set_figwidth(15)\nfig\n\n\n\n\n\n\n\n\n\nSTGCN 으로 적합 + 예측\n\nX = torch.tensor(np.stack([interpolated_signal[i:(int(T*0.8)-2+i),:] for i in range(2)],axis = -1)).float()\ny = torch.tensor(interpolated_signal[2:,:]).float()\n\n\nXX = torch.tensor(np.stack([fiveVTS_test[i:(int(T*0.2)-2+i),:] for i in range(2)],axis = -1)).float()\nyy = fiveVTS_test[2:int(T*0.2),:].reshape(int(T*0.2)-2,N,-1)\n\n\nnet = RecurrentGCN(node_features=2, filters=4)\noptimizer = torch.optim.Adam(net.parameters(), lr=0.01)\nnet.train()\nfor epoch in tqdm(range(50)):\n    for time, (xt,yt) in enumerate(zip(X,y)):\n        yt_hat = net(xt, edge_index, edge_attr)\n        cost = torch.mean((yt_hat-yt)**2)\n        cost.backward()\n        optimizer.step()\n        optimizer.zero_grad()\n\n100%|██████████| 50/50 [00:26&lt;00:00,  1.85it/s]\n\n\n\nyhat = torch.stack([net(xt, edge_index, edge_attr) for xt in X]).detach().numpy()\nyyhat = torch.stack([net(xt, edge_index, edge_attr) for xt in XX]).detach().numpy()\n\n\nreal_y = torch.tensor(fiveVTS_train).reshape(int(T*0.8),N,1).float()[2:,:,:]\n\ntrain_mse_eachnode_stgcn = (((real_y-yhat).squeeze())**2).mean(axis=0)\ntrain_mse_total_stgcn = (((real_y-yhat).squeeze())**2).mean()\ntest_mse_eachnode_stgcn = (((yy-yyhat).squeeze())**2).mean(axis=0)\ntest_mse_total_stgcn = (((yy-yyhat).squeeze())**2).mean()\n\n\nstgcn_train = yhat.squeeze() # stgcn은 stgcn에 의한 적합결과를 의미함\nstgcn_test = yyhat.squeeze() \n\n\n\nESTGCN 으로 적합 + 예측\n\nX = torch.tensor(np.stack([interpolated_signal[i:(int(T*0.8)-2+i),:] for i in range(2)],axis = -1)).float()\ny = torch.tensor(interpolated_signal[2:,:]).float()\n\n\nXX = torch.tensor(np.stack([fiveVTS_test[i:(int(T*0.2)-2+i),:] for i in range(2)],axis = -1)).float()\nyy = fiveVTS_test[2:int(T*0.2),:].reshape(int(T*0.2)-2,N,-1)\n\n- ESTGCN\n\nnet = RecurrentGCN(node_features=2, filters=4)\noptimizer = torch.optim.Adam(net.parameters(), lr=0.01)\nnet.train()\nsignal = interpolated_signal.copy()\nfor epoch in tqdm(range(50)):\n    signal = update_from_freq_domain(signal,missing_index)\n    X = torch.tensor(np.stack([signal[i:(int(T*0.8)-2+i),:] for i in range(2)],axis = -1)).float()\n    y = torch.tensor(signal[2:,:]).float()\n    for time, (xt,yt) in enumerate(zip(X,y)):        \n        yt_hat = net(xt, edge_index, edge_attr)\n        cost = torch.mean((yt_hat-yt)**2)\n        cost.backward()\n        optimizer.step()\n        optimizer.zero_grad()\n    signal = torch.concat([X[:-1,:,0], X[-1,:,:].T, yt_hat.detach().squeeze().reshape(1,-1)])        \n\n100%|██████████| 50/50 [00:28&lt;00:00,  1.78it/s]\n\n\n\nyhat = torch.stack([net(xt, edge_index, edge_attr) for xt in X]).detach().numpy()\nyyhat = torch.stack([net(xt, edge_index, edge_attr) for xt in XX]).detach().numpy()\n\n\nreal_y = torch.tensor(fiveVTS_train).reshape(int(T*0.8),N,1).float()[2:,:,:]\n\ntrain_mse_eachnode_estgcn = (((real_y-yhat).squeeze())**2).mean(axis=0)\ntrain_mse_total_estgcn = (((real_y-yhat).squeeze())**2).mean()\ntest_mse_eachnode_estgcn = (((yy-yyhat).squeeze())**2).mean(axis=0)\ntest_mse_total_estgcn = (((yy-yyhat).squeeze())**2).mean()\n\n\nestgcn_train = yhat.squeeze() # stgcn은 stgcn에 의한 적합결과를 의미함\nestgcn_test = yyhat.squeeze() \n\n\n\nGNAR 으로 적합 + 예측\n-\n\nX_train1 = np.array(interpolated_signal).squeeze()\nX_test1 =  np.array(fiveVTS_test).squeeze()\n\n\n%R -i X_train1\n\n\n%%R\nanswer &lt;- GNARfit(vts = X_train1, net = fiveNet, alphaOrder = 2, betaOrder = c(1, 1))\nprediction &lt;- predict(answer,n.ahead=40)\n\n\n%%R\ngnar_train &lt;- residuals(answer)\ngnar_test &lt;- prediction\n\n\n%R -o gnar_train\n%R -o gnar_test\n\n\ntrain_mse_eachnode_gnar = (gnar_train**2).mean(axis=0)\ntrain_mse_total_gnar = (gnar_train**2).mean()\ntest_mse_eachnode_gnar = ((X_test1 - gnar_test)**2).mean(axis=0)\ntest_mse_total_gnar = ((X_test1 - gnar_test)**2).mean()\n\n\n\n결과시각화\n\ntrain_mse_total_gnar,test_mse_total_gnar\n\n(0.6280648350096797, 1.3222499750457097)\n\n\n\nfig = plot(fiveVTS,'--.',h=4,color='gray',label='complete data',alpha=0.5)\nax = fig.get_axes()\nfor i,a in enumerate(ax):     \n    a.set_title('node{0} \\n STGCN: mse(train) = {1:.2f}, mse(test) = {2:.2f} \\n ESTGCN: mse(train) = {3:.2f}, mse(test) = {4:.2f}\\n GNAR: mse(train) = {5:.2f}, mse(test) = {6:.2f}'.format(i,train_mse_eachnode_stgcn[i],test_mse_eachnode_stgcn[i],train_mse_eachnode_estgcn[i],test_mse_eachnode_estgcn[i],train_mse_eachnode_gnar[i],test_mse_eachnode_gnar[i]))\n    a.plot(missing_index[i],fiveVTS_train[:,i][missing_index[i]],'xk',label='missing')\n    a.plot(interpolated_signal[:,i],'-',color='gray',label='linear interpolation')\n    a.plot(range(2,160),stgcn_train.squeeze()[:,i],'--.',label='STCGCN (train)',color='C0')\n    a.plot(range(162,200),stgcn_test.squeeze()[:,i],'--.',label='STCGCN (test)',color='C0')\n    a.plot(range(2,160),estgcn_train.squeeze()[:,i],label='ESTCGCN (train)',color='C1')\n    a.plot(range(162,200),estgcn_test.squeeze()[:,i],label='ESTCGCN (test)',color='C1')\n    a.plot(range(2,160),gnar_train[:,i],label='GNAR (train)',color='C2')\n    a.plot(range(162,202),gnar_test[:,i],label='GNAR (test)',color='C2')\n    \n    a.legend()\nfig.set_figwidth(14)\nfig.suptitle(\"Scenario2: \\n missing=50% \\n interpolation=linear \\n\\n STGCN: mse(train) = {0:.2f}, mse(test) = {1:.2f} \\n ESTGCN: mse(train) = {2:.2f}, mse(test) = {3:.2f} \\n GNAR: mse(train) = {4:.2f}, mse(test) = {5:.2f} \\n\".format(train_mse_total_stgcn,test_mse_total_stgcn,train_mse_total_estgcn,test_mse_total_estgcn,train_mse_total_gnar,test_mse_total_gnar),size=15)\nfig.tight_layout()\nfig"
  },
  {
    "objectID": "posts/2_research/2023-02-06-ESTGCN_GNAR_DATA_2.html#시나리오3",
    "href": "posts/2_research/2023-02-06-ESTGCN_GNAR_DATA_2.html#시나리오3",
    "title": "Class of Method(GNAR) lag 2",
    "section": "시나리오3",
    "text": "시나리오3\n시나리오3\n\nmissing rate: 80%\n보간방법: linear\n\n- 결측치생성 + 보간\n\n_zero = Missing(fiveVTS_train)\n_zero.miss(percent = 0.8)\n_zero.second_linear()\n\n\nmissing_index = _zero.number\ninterpolated_signal = _zero.train_linear\n\n\nfig = plot(fiveVTS,'--o',h=4,color='gray',label='complete data',alpha=0.2)\nax = fig.get_axes()\nfor i,a in enumerate(ax):     \n    a.plot(missing_index[i],fiveVTS_train[:,i][missing_index[i]],'xk',label='missing')\n    a.plot(interpolated_signal[:,i],'-',color='gray',label='linear interpolation')\n    a.legend()\nfig.set_figwidth(15)\nfig\n\n\n\n\n\n\n\n\n\nSTGCN 으로 적합 + 예측\n\nX = torch.tensor(np.stack([interpolated_signal[i:(int(T*0.8)-2+i),:] for i in range(2)],axis = -1)).float()\ny = torch.tensor(interpolated_signal[2:,:]).float()\n\n\nXX = torch.tensor(np.stack([fiveVTS_test[i:(int(T*0.2)-2+i),:] for i in range(2)],axis = -1)).float()\nyy = fiveVTS_test[2:int(T*0.2),:].reshape(int(T*0.2)-2,N,-1)\n\n\nnet = RecurrentGCN(node_features=2, filters=4)\noptimizer = torch.optim.Adam(net.parameters(), lr=0.01)\nnet.train()\nfor epoch in tqdm(range(50)):\n    for time, (xt,yt) in enumerate(zip(X,y)):\n        yt_hat = net(xt, edge_index, edge_attr)\n        cost = torch.mean((yt_hat-yt)**2)\n        cost.backward()\n        optimizer.step()\n        optimizer.zero_grad()\n\n100%|██████████| 50/50 [00:27&lt;00:00,  1.85it/s]\n\n\n\nyhat = torch.stack([net(xt, edge_index, edge_attr) for xt in X]).detach().numpy()\nyyhat = torch.stack([net(xt, edge_index, edge_attr) for xt in XX]).detach().numpy()\n\n\nreal_y = torch.tensor(fiveVTS_train).reshape(int(T*0.8),N,1).float()[2:,:,:]\n\ntrain_mse_eachnode_stgcn = (((real_y-yhat).squeeze())**2).mean(axis=0)\ntrain_mse_total_stgcn = (((real_y-yhat).squeeze())**2).mean()\ntest_mse_eachnode_stgcn = (((yy-yyhat).squeeze())**2).mean(axis=0)\ntest_mse_total_stgcn = (((yy-yyhat).squeeze())**2).mean()\n\n\nstgcn_train = yhat.squeeze() # stgcn은 stgcn에 의한 적합결과를 의미함\nstgcn_test = yyhat.squeeze() \n\n\n\nESTGCN 으로 적합 + 예측\n\nX = torch.tensor(np.stack([interpolated_signal[i:(int(T*0.8)-2+i),:] for i in range(2)],axis = -1)).float()\ny = torch.tensor(interpolated_signal[2:,:]).float()\n\n\nXX = torch.tensor(np.stack([fiveVTS_test[i:(int(T*0.2)-2+i),:] for i in range(2)],axis = -1)).float()\nyy = fiveVTS_test[2:int(T*0.2),:].reshape(int(T*0.2)-2,N,-1)\n\n- ESTGCN\n\nnet = RecurrentGCN(node_features=2, filters=4)\noptimizer = torch.optim.Adam(net.parameters(), lr=0.01)\nnet.train()\nsignal = interpolated_signal.copy()\nfor epoch in tqdm(range(50)):\n    signal = update_from_freq_domain(signal,missing_index)\n    X = torch.tensor(np.stack([signal[i:(int(T*0.8)-2+i),:] for i in range(2)],axis = -1)).float()\n    y = torch.tensor(signal[2:,:]).float()\n    for time, (xt,yt) in enumerate(zip(X,y)):        \n        yt_hat = net(xt, edge_index, edge_attr)\n        cost = torch.mean((yt_hat-yt)**2)\n        cost.backward()\n        optimizer.step()\n        optimizer.zero_grad()\n    signal = torch.concat([X[:-1,:,0], X[-1,:,:].T, yt_hat.detach().squeeze().reshape(1,-1)])        \n\n100%|██████████| 50/50 [00:28&lt;00:00,  1.77it/s]\n\n\n\nyhat = torch.stack([net(xt, edge_index, edge_attr) for xt in X]).detach().numpy()\nyyhat = torch.stack([net(xt, edge_index, edge_attr) for xt in XX]).detach().numpy()\n\n\nreal_y = torch.tensor(fiveVTS_train).reshape(int(T*0.8),N,1).float()[2:,:,:]\n\ntrain_mse_eachnode_estgcn = (((real_y-yhat).squeeze())**2).mean(axis=0)\ntrain_mse_total_estgcn = (((real_y-yhat).squeeze())**2).mean()\ntest_mse_eachnode_estgcn = (((yy-yyhat).squeeze())**2).mean(axis=0)\ntest_mse_total_estgcn = (((yy-yyhat).squeeze())**2).mean()\n\n\nestgcn_train = yhat.squeeze() # stgcn은 stgcn에 의한 적합결과를 의미함\nestgcn_test = yyhat.squeeze() \n\n\n\nGNAR 으로 적합 + 예측\n-\n\nX_train1 = np.array(interpolated_signal).squeeze()\nX_test1 =  np.array(fiveVTS_test).squeeze()\n\n\n%R -i X_train1\n\n\n%%R\nanswer &lt;- GNARfit(vts = X_train1, net = fiveNet, alphaOrder = 2, betaOrder = c(1, 1))\nprediction &lt;- predict(answer,n.ahead=40)\n\n\n%%R\ngnar_train &lt;- residuals(answer)\ngnar_test &lt;- prediction\n\n\n%R -o gnar_train\n%R -o gnar_test\n\n\ntrain_mse_eachnode_gnar = (gnar_train**2).mean(axis=0)\ntrain_mse_total_gnar = (gnar_train**2).mean()\ntest_mse_eachnode_gnar = ((X_test1 - gnar_test)**2).mean(axis=0)\ntest_mse_total_gnar = ((X_test1 - gnar_test)**2).mean()\n\n\n\n결과시각화\n\ntrain_mse_total_gnar,test_mse_total_gnar\n\n(0.2092691948436627, 1.5191113001100904)\n\n\n\nfig = plot(fiveVTS,'--.',h=4,color='gray',label='complete data',alpha=0.5)\nax = fig.get_axes()\nfor i,a in enumerate(ax):     \n    a.set_title('node{0} \\n STGCN: mse(train) = {1:.2f}, mse(test) = {2:.2f} \\n ESTGCN: mse(train) = {3:.2f}, mse(test) = {4:.2f}\\n GNAR: mse(train) = {5:.2f}, mse(test) = {6:.2f}'.format(i,train_mse_eachnode_stgcn[i],test_mse_eachnode_stgcn[i],train_mse_eachnode_estgcn[i],test_mse_eachnode_estgcn[i],train_mse_eachnode_gnar[i],test_mse_eachnode_gnar[i]))\n    a.plot(missing_index[i],fiveVTS_train[:,i][missing_index[i]],'xk',label='missing')\n    a.plot(interpolated_signal[:,i],'-',color='gray',label='linear interpolation')\n    a.plot(range(2,160),stgcn_train.squeeze()[:,i],'--.',label='STCGCN (train)',color='C0')\n    a.plot(range(162,200),stgcn_test.squeeze()[:,i],'--.',label='STCGCN (test)',color='C0')\n    a.plot(range(2,160),estgcn_train.squeeze()[:,i],label='ESTCGCN (train)',color='C1')\n    a.plot(range(162,200),estgcn_test.squeeze()[:,i],label='ESTCGCN (test)',color='C1')\n    a.plot(range(2,160),gnar_train[:,i],label='GNAR (train)',color='C2')\n    a.plot(range(162,202),gnar_test[:,i],label='GNAR (test)',color='C2')\n    \n    a.legend()\nfig.set_figwidth(14)\nfig.suptitle(\"Scenario3: \\n missing=80% \\n interpolation=linear \\n\\n STGCN: mse(train) = {0:.2f}, mse(test) = {1:.2f} \\n ESTGCN: mse(train) = {2:.2f}, mse(test) = {3:.2f} \\n GNAR: mse(train) = {4:.2f}, mse(test) = {5:.2f} \\n\".format(train_mse_total_stgcn,test_mse_total_stgcn,train_mse_total_estgcn,test_mse_total_estgcn,train_mse_total_gnar,test_mse_total_gnar),size=15)\nfig.tight_layout()\nfig"
  },
  {
    "objectID": "posts/2_research/2023-02-06-ESTGCN_GNAR_DATA_2.html#시나리오4",
    "href": "posts/2_research/2023-02-06-ESTGCN_GNAR_DATA_2.html#시나리오4",
    "title": "Class of Method(GNAR) lag 2",
    "section": "시나리오4",
    "text": "시나리오4\n시나리오4\n\nmissing rate: 30%\n보간방법: linear\n\n- 결측치생성 + 보간\n\n_zero = Missing(fiveVTS_train)\n_zero.miss(percent = 0.3)\n_zero.second_linear()\n\n\nmissing_index = _zero.number\ninterpolated_signal = _zero.train_linear\n\n\nfig = plot(fiveVTS,'--o',h=4,color='gray',label='complete data',alpha=0.2)\nax = fig.get_axes()\nfor i,a in enumerate(ax):     \n    a.plot(missing_index[i],fiveVTS_train[:,i][missing_index[i]],'xk',label='missing')\n    a.plot(interpolated_signal[:,i],'-',color='gray',label='linear interpolation')\n    a.legend()\nfig.set_figwidth(15)\nfig\n\n\n\n\n\n\n\n\n\nSTGCN 으로 적합 + 예측\n\nX = torch.tensor(np.stack([interpolated_signal[i:(int(T*0.8)-2+i),:] for i in range(2)],axis = -1)).float()\ny = torch.tensor(interpolated_signal[2:,:]).float()\n\n\nXX = torch.tensor(np.stack([fiveVTS_test[i:(int(T*0.2)-2+i),:] for i in range(2)],axis = -1)).float()\nyy = fiveVTS_test[2:int(T*0.2),:].reshape(int(T*0.2)-2,N,-1)\n\n\nnet = RecurrentGCN(node_features=2, filters=4)\noptimizer = torch.optim.Adam(net.parameters(), lr=0.01)\nnet.train()\nfor epoch in tqdm(range(50)):\n    for time, (xt,yt) in enumerate(zip(X,y)):\n        yt_hat = net(xt, edge_index, edge_attr)\n        cost = torch.mean((yt_hat-yt)**2)\n        cost.backward()\n        optimizer.step()\n        optimizer.zero_grad()\n\n100%|██████████| 50/50 [00:26&lt;00:00,  1.86it/s]\n\n\n\nyhat = torch.stack([net(xt, edge_index, edge_attr) for xt in X]).detach().numpy()\nyyhat = torch.stack([net(xt, edge_index, edge_attr) for xt in XX]).detach().numpy()\n\n\nreal_y = torch.tensor(fiveVTS_train).reshape(int(T*0.8),N,1).float()[2:,:,:]\n\ntrain_mse_eachnode_stgcn = (((real_y-yhat).squeeze())**2).mean(axis=0)\ntrain_mse_total_stgcn = (((real_y-yhat).squeeze())**2).mean()\ntest_mse_eachnode_stgcn = (((yy-yyhat).squeeze())**2).mean(axis=0)\ntest_mse_total_stgcn = (((yy-yyhat).squeeze())**2).mean()\n\n\nstgcn_train = yhat.squeeze() # stgcn은 stgcn에 의한 적합결과를 의미함\nstgcn_test = yyhat.squeeze() \n\n\n\nESTGCN 으로 적합 + 예측\n\nX = torch.tensor(np.stack([interpolated_signal[i:(int(T*0.8)-2+i),:] for i in range(2)],axis = -1)).float()\ny = torch.tensor(interpolated_signal[2:,:]).float()\n\n\nXX = torch.tensor(np.stack([fiveVTS_test[i:(int(T*0.2)-2+i),:] for i in range(2)],axis = -1)).float()\nyy = fiveVTS_test[2:int(T*0.2),:].reshape(int(T*0.2)-2,N,-1)\n\n- ESTGCN\n\nnet = RecurrentGCN(node_features=2, filters=4)\noptimizer = torch.optim.Adam(net.parameters(), lr=0.01)\nnet.train()\nsignal = interpolated_signal.copy()\nfor epoch in tqdm(range(50)):\n    signal = update_from_freq_domain(signal,missing_index)\n    X = torch.tensor(np.stack([signal[i:(int(T*0.8)-2+i),:] for i in range(2)],axis = -1)).float()\n    y = torch.tensor(signal[2:,:]).float()\n    for time, (xt,yt) in enumerate(zip(X,y)):        \n        yt_hat = net(xt, edge_index, edge_attr)\n        cost = torch.mean((yt_hat-yt)**2)\n        cost.backward()\n        optimizer.step()\n        optimizer.zero_grad()\n    signal = torch.concat([X[:-1,:,0], X[-1,:,:].T, yt_hat.detach().squeeze().reshape(1,-1)])        \n\n100%|██████████| 50/50 [00:27&lt;00:00,  1.79it/s]\n\n\n\nyhat = torch.stack([net(xt, edge_index, edge_attr) for xt in X]).detach().numpy()\nyyhat = torch.stack([net(xt, edge_index, edge_attr) for xt in XX]).detach().numpy()\n\n\nreal_y = torch.tensor(fiveVTS_train).reshape(int(T*0.8),N,1).float()[2:,:,:]\n\ntrain_mse_eachnode_estgcn = (((real_y-yhat).squeeze())**2).mean(axis=0)\ntrain_mse_total_estgcn = (((real_y-yhat).squeeze())**2).mean()\ntest_mse_eachnode_estgcn = (((yy-yyhat).squeeze())**2).mean(axis=0)\ntest_mse_total_estgcn = (((yy-yyhat).squeeze())**2).mean()\n\n\nestgcn_train = yhat.squeeze() # stgcn은 stgcn에 의한 적합결과를 의미함\nestgcn_test = yyhat.squeeze() \n\n\n\nGNAR 으로 적합 + 예측\n-\n\nX_train1 = np.array(interpolated_signal).squeeze()\nX_test1 =  np.array(fiveVTS_test).squeeze()\n\n\n%R -i X_train1\n\n\n%%R\nanswer &lt;- GNARfit(vts = X_train1, net = fiveNet, alphaOrder = 2, betaOrder = c(1, 1))\nprediction &lt;- predict(answer,n.ahead=40)\n\n\n%%R\ngnar_train &lt;- residuals(answer)\ngnar_test &lt;- prediction\n\n\n%R -o gnar_train\n%R -o gnar_test\n\n\ntrain_mse_eachnode_gnar = (gnar_train**2).mean(axis=0)\ntrain_mse_total_gnar = (gnar_train**2).mean()\ntest_mse_eachnode_gnar = ((X_test1 - gnar_test)**2).mean(axis=0)\ntest_mse_total_gnar = ((X_test1 - gnar_test)**2).mean()\n\n\n\n결과시각화\n\ntrain_mse_total_gnar,test_mse_total_gnar\n\n(0.7594163080873634, 1.2656937545324825)\n\n\n\nfig = plot(fiveVTS,'--.',h=4,color='gray',label='complete data',alpha=0.5)\nax = fig.get_axes()\nfor i,a in enumerate(ax):     \n    a.set_title('node{0} \\n STGCN: mse(train) = {1:.2f}, mse(test) = {2:.2f} \\n ESTGCN: mse(train) = {3:.2f}, mse(test) = {4:.2f}\\n GNAR: mse(train) = {5:.2f}, mse(test) = {6:.2f}'.format(i,train_mse_eachnode_stgcn[i],test_mse_eachnode_stgcn[i],train_mse_eachnode_estgcn[i],test_mse_eachnode_estgcn[i],train_mse_eachnode_gnar[i],test_mse_eachnode_gnar[i]))\n    a.plot(missing_index[i],fiveVTS_train[:,i][missing_index[i]],'xk',label='missing')\n    a.plot(interpolated_signal[:,i],'-',color='gray',label='linear interpolation')\n    a.plot(range(2,160),stgcn_train.squeeze()[:,i],'--.',label='STCGCN (train)',color='C0')\n    a.plot(range(162,200),stgcn_test.squeeze()[:,i],'--.',label='STCGCN (test)',color='C0')\n    a.plot(range(2,160),estgcn_train.squeeze()[:,i],label='ESTCGCN (train)',color='C1')\n    a.plot(range(162,200),estgcn_test.squeeze()[:,i],label='ESTCGCN (test)',color='C1')\n    a.plot(range(2,160),gnar_train[:,i],label='GNAR (train)',color='C2')\n    a.plot(range(162,202),gnar_test[:,i],label='GNAR (test)',color='C2')\n    \n    a.legend()\nfig.set_figwidth(14)\nfig.suptitle(\"Scenario3: \\n missing=80% \\n interpolation=linear \\n\\n STGCN: mse(train) = {0:.2f}, mse(test) = {1:.2f} \\n ESTGCN: mse(train) = {2:.2f}, mse(test) = {3:.2f} \\n GNAR: mse(train) = {4:.2f}, mse(test) = {5:.2f} \\n\".format(train_mse_total_stgcn,test_mse_total_stgcn,train_mse_total_estgcn,test_mse_total_estgcn,train_mse_total_gnar,test_mse_total_gnar),size=15)\nfig.tight_layout()\nfig"
  },
  {
    "objectID": "posts/2_research/2023-01-21-Class.html",
    "href": "posts/2_research/2023-01-21-Class.html",
    "title": "Class of Method",
    "section": "",
    "text": "Class"
  },
  {
    "objectID": "posts/2_research/2023-01-21-Class.html#mean",
    "href": "posts/2_research/2023-01-21-Class.html#mean",
    "title": "Class of Method",
    "section": "Mean",
    "text": "Mean\n\nt1= time.time()\nttt = 160\n_mse = []\n_mae = []\n_train_result = []\n_test_result = []\nb = ___zero.train_mean\nc = ___zero.number\nd = train_X_mean\nf = train_y_mean\nfor i in range(60):\n    a = Method(b,ttt,c,d,f)\n    a.FT()\n    _mse.append(a.mse)\n    _mae.append(a.mae)\n    _train_result.append(a.train_result)\n    _test_result.append(a.test_result)\n    b = a.FT_result\nt2 = time.time()\nt2-t1\n\n100%|██████████| 100/100 [00:54&lt;00:00,  1.85it/s]\n100%|██████████| 100/100 [00:54&lt;00:00,  1.85it/s]\n100%|██████████| 100/100 [00:54&lt;00:00,  1.84it/s]\n100%|██████████| 100/100 [00:54&lt;00:00,  1.85it/s]\n100%|██████████| 100/100 [00:53&lt;00:00,  1.87it/s]\n100%|██████████| 100/100 [00:54&lt;00:00,  1.85it/s]\n100%|██████████| 100/100 [00:53&lt;00:00,  1.85it/s]\n100%|██████████| 100/100 [00:53&lt;00:00,  1.87it/s]\n100%|██████████| 100/100 [00:54&lt;00:00,  1.85it/s]\n100%|██████████| 100/100 [00:53&lt;00:00,  1.86it/s]\n100%|██████████| 100/100 [00:53&lt;00:00,  1.86it/s]\n100%|██████████| 100/100 [00:54&lt;00:00,  1.85it/s]\n100%|██████████| 100/100 [00:53&lt;00:00,  1.86it/s]\n100%|██████████| 100/100 [00:53&lt;00:00,  1.87it/s]\n100%|██████████| 100/100 [00:53&lt;00:00,  1.87it/s]\n100%|██████████| 100/100 [00:53&lt;00:00,  1.85it/s]\n100%|██████████| 100/100 [00:54&lt;00:00,  1.85it/s]\n100%|██████████| 100/100 [00:53&lt;00:00,  1.86it/s]\n100%|██████████| 100/100 [00:53&lt;00:00,  1.86it/s]\n100%|██████████| 100/100 [00:53&lt;00:00,  1.87it/s]\n100%|██████████| 100/100 [00:53&lt;00:00,  1.88it/s]\n100%|██████████| 100/100 [00:53&lt;00:00,  1.87it/s]\n100%|██████████| 100/100 [00:53&lt;00:00,  1.86it/s]\n100%|██████████| 100/100 [00:54&lt;00:00,  1.85it/s]\n100%|██████████| 100/100 [00:53&lt;00:00,  1.86it/s]\n100%|██████████| 100/100 [00:53&lt;00:00,  1.86it/s]\n100%|██████████| 100/100 [00:53&lt;00:00,  1.86it/s]\n100%|██████████| 100/100 [00:53&lt;00:00,  1.88it/s]\n100%|██████████| 100/100 [00:53&lt;00:00,  1.85it/s]\n100%|██████████| 100/100 [00:53&lt;00:00,  1.85it/s]\n100%|██████████| 100/100 [00:54&lt;00:00,  1.85it/s]\n100%|██████████| 100/100 [00:54&lt;00:00,  1.85it/s]\n100%|██████████| 100/100 [00:53&lt;00:00,  1.85it/s]\n100%|██████████| 100/100 [00:53&lt;00:00,  1.85it/s]\n100%|██████████| 100/100 [00:53&lt;00:00,  1.85it/s]\n100%|██████████| 100/100 [00:54&lt;00:00,  1.85it/s]\n100%|██████████| 100/100 [00:53&lt;00:00,  1.86it/s]\n100%|██████████| 100/100 [00:53&lt;00:00,  1.88it/s]\n100%|██████████| 100/100 [00:53&lt;00:00,  1.88it/s]\n100%|██████████| 100/100 [00:53&lt;00:00,  1.86it/s]\n100%|██████████| 100/100 [00:53&lt;00:00,  1.87it/s]\n100%|██████████| 100/100 [00:53&lt;00:00,  1.86it/s]\n100%|██████████| 100/100 [00:53&lt;00:00,  1.86it/s]\n100%|██████████| 100/100 [00:53&lt;00:00,  1.85it/s]\n100%|██████████| 100/100 [00:53&lt;00:00,  1.88it/s]\n100%|██████████| 100/100 [00:53&lt;00:00,  1.86it/s]\n100%|██████████| 100/100 [00:53&lt;00:00,  1.86it/s]\n100%|██████████| 100/100 [00:53&lt;00:00,  1.86it/s]\n100%|██████████| 100/100 [00:54&lt;00:00,  1.85it/s]\n100%|██████████| 100/100 [00:53&lt;00:00,  1.86it/s]\n100%|██████████| 100/100 [00:53&lt;00:00,  1.87it/s]\n100%|██████████| 100/100 [00:54&lt;00:00,  1.85it/s]\n100%|██████████| 100/100 [00:53&lt;00:00,  1.86it/s]\n100%|██████████| 100/100 [00:54&lt;00:00,  1.85it/s]\n100%|██████████| 100/100 [00:54&lt;00:00,  1.85it/s]\n100%|██████████| 100/100 [00:53&lt;00:00,  1.87it/s]\n100%|██████████| 100/100 [00:53&lt;00:00,  1.87it/s]\n100%|██████████| 100/100 [00:53&lt;00:00,  1.88it/s]\n100%|██████████| 100/100 [00:53&lt;00:00,  1.87it/s]\n100%|██████████| 100/100 [00:53&lt;00:00,  1.87it/s]\n\n\n3282.4832243919373\n\n\n\nmean_mse100_10 = pd.DataFrame(_mse)\n\n\nmean_mae100_10 = pd.DataFrame(_mae)\n\n\n_train_result_mean10 = _train_result.copy()\n\n\n_test_result_mean10 = _test_result.copy()\n\n\nplt.plot(mean_mse100_10.T);\n\n\n\n\n\n\n\n\n\nplt.plot(mean_mae100_10.T);\n\n\n\n\n\n\n\n\n\nvis2(_zero.train_mean,_train_result[59]);\n\n\n\n\n\n\n\n\n\nvis2(fiveVTS_test[1:],_test_result[0]);"
  },
  {
    "objectID": "posts/2_research/2023-01-21-Class.html#linear",
    "href": "posts/2_research/2023-01-21-Class.html#linear",
    "title": "Class of Method",
    "section": "Linear",
    "text": "Linear\n\nt1= time.time()\nttt = 160\n_mse = []\n_mae = []\n_train_result = []\n_test_result = []\nb = ___zero.second_linear\nc = ___zero.number\nd = train_X_linear\nf = train_y_linear\nfor i in range(60):\n    a = Method(b,ttt,c,d,f)\n    a.FT()\n    _mse.append(a.mse)\n    _mae.append(a.mae)\n    _train_result.append(a.train_result)\n    _test_result.append(a.test_result)\n    b = a.FT_result\nt2 = time.time()\nt2-t1\n\n100%|██████████| 100/100 [00:53&lt;00:00,  1.85it/s]\n100%|██████████| 100/100 [00:54&lt;00:00,  1.85it/s]\n100%|██████████| 100/100 [00:53&lt;00:00,  1.88it/s]\n100%|██████████| 100/100 [00:53&lt;00:00,  1.88it/s]\n100%|██████████| 100/100 [00:53&lt;00:00,  1.86it/s]\n100%|██████████| 100/100 [00:53&lt;00:00,  1.87it/s]\n100%|██████████| 100/100 [00:53&lt;00:00,  1.85it/s]\n100%|██████████| 100/100 [00:54&lt;00:00,  1.85it/s]\n100%|██████████| 100/100 [00:53&lt;00:00,  1.85it/s]\n100%|██████████| 100/100 [00:54&lt;00:00,  1.85it/s]\n100%|██████████| 100/100 [00:54&lt;00:00,  1.85it/s]\n100%|██████████| 100/100 [00:53&lt;00:00,  1.85it/s]\n100%|██████████| 100/100 [00:53&lt;00:00,  1.85it/s]\n100%|██████████| 100/100 [00:53&lt;00:00,  1.87it/s]\n100%|██████████| 100/100 [00:53&lt;00:00,  1.86it/s]\n100%|██████████| 100/100 [00:53&lt;00:00,  1.86it/s]\n100%|██████████| 100/100 [00:53&lt;00:00,  1.87it/s]\n100%|██████████| 100/100 [00:54&lt;00:00,  1.85it/s]\n100%|██████████| 100/100 [00:53&lt;00:00,  1.85it/s]\n100%|██████████| 100/100 [00:54&lt;00:00,  1.85it/s]\n100%|██████████| 100/100 [00:54&lt;00:00,  1.85it/s]\n100%|██████████| 100/100 [00:54&lt;00:00,  1.85it/s]\n100%|██████████| 100/100 [00:53&lt;00:00,  1.86it/s]\n100%|██████████| 100/100 [00:54&lt;00:00,  1.84it/s]\n100%|██████████| 100/100 [00:54&lt;00:00,  1.84it/s]\n100%|██████████| 100/100 [00:54&lt;00:00,  1.84it/s]\n100%|██████████| 100/100 [00:54&lt;00:00,  1.85it/s]\n100%|██████████| 100/100 [00:53&lt;00:00,  1.85it/s]\n100%|██████████| 100/100 [00:53&lt;00:00,  1.85it/s]\n100%|██████████| 100/100 [00:53&lt;00:00,  1.86it/s]\n100%|██████████| 100/100 [00:53&lt;00:00,  1.85it/s]\n100%|██████████| 100/100 [00:54&lt;00:00,  1.84it/s]\n100%|██████████| 100/100 [00:54&lt;00:00,  1.85it/s]\n100%|██████████| 100/100 [00:53&lt;00:00,  1.85it/s]\n100%|██████████| 100/100 [00:54&lt;00:00,  1.85it/s]\n100%|██████████| 100/100 [00:54&lt;00:00,  1.84it/s]\n100%|██████████| 100/100 [00:54&lt;00:00,  1.85it/s]\n100%|██████████| 100/100 [00:53&lt;00:00,  1.86it/s]\n100%|██████████| 100/100 [00:54&lt;00:00,  1.84it/s]\n100%|██████████| 100/100 [00:54&lt;00:00,  1.84it/s]\n100%|██████████| 100/100 [00:53&lt;00:00,  1.86it/s]\n100%|██████████| 100/100 [00:54&lt;00:00,  1.85it/s]\n100%|██████████| 100/100 [00:53&lt;00:00,  1.86it/s]\n100%|██████████| 100/100 [00:53&lt;00:00,  1.87it/s]\n100%|██████████| 100/100 [00:54&lt;00:00,  1.83it/s]\n100%|██████████| 100/100 [00:53&lt;00:00,  1.86it/s]\n100%|██████████| 100/100 [00:54&lt;00:00,  1.85it/s]\n100%|██████████| 100/100 [00:54&lt;00:00,  1.85it/s]\n100%|██████████| 100/100 [00:54&lt;00:00,  1.85it/s]\n100%|██████████| 100/100 [00:54&lt;00:00,  1.85it/s]\n100%|██████████| 100/100 [00:53&lt;00:00,  1.86it/s]\n100%|██████████| 100/100 [00:54&lt;00:00,  1.85it/s]\n100%|██████████| 100/100 [00:54&lt;00:00,  1.84it/s]\n100%|██████████| 100/100 [00:54&lt;00:00,  1.84it/s]\n100%|██████████| 100/100 [00:54&lt;00:00,  1.85it/s]\n100%|██████████| 100/100 [00:54&lt;00:00,  1.85it/s]\n100%|██████████| 100/100 [00:54&lt;00:00,  1.84it/s]\n100%|██████████| 100/100 [00:53&lt;00:00,  1.85it/s]\n100%|██████████| 100/100 [00:54&lt;00:00,  1.85it/s]\n100%|██████████| 100/100 [00:54&lt;00:00,  1.84it/s]\n\n\n3295.478456020355\n\n\n\nlinear_mse100_10 = pd.DataFrame(_mse)\n\n\nlinear_mae100_10 = pd.DataFrame(_mae)\n\n\n_train_result_linear10 = _train_result.copy()\n\n\n_test_result_linear10 = _test_result.copy()\n\n\nplt.plot(linear_mse100_10.T);\n\n\n\n\n\n\n\n\n\nplt.plot(linear_mae100_10.T);\n\n\n\n\n\n\n\n\n\nvis2(_zero.train_mean,_train_result_linear10[59]);\n\n\n\n\n\n\n\n\n\nvis2(fiveVTS_test[1:],_test_result_linear10[0]);"
  },
  {
    "objectID": "posts/2_research/2023-01-21-Class.html#mean-1",
    "href": "posts/2_research/2023-01-21-Class.html#mean-1",
    "title": "Class of Method",
    "section": "Mean",
    "text": "Mean\n\nt1= time.time()\nttt = 160\n_mse = []\n_mae = []\n_train_result = []\n_test_result = []\nb = ___zero20.train_mean\nc = ___zero20.number\nd = train_X_mean\nf = train_y_mean\nfor i in range(60):\n    a = Method(b,ttt,c,d,f)\n    a.FT()\n    _mse.append(a.mse)\n    _mae.append(a.mae)\n    _train_result.append(a.train_result)\n    _test_result.append(a.test_result)\n    b = a.FT_result\nt2 = time.time()\nt2-t1\n\n100%|██████████| 100/100 [00:53&lt;00:00,  1.86it/s]\n100%|██████████| 100/100 [00:53&lt;00:00,  1.86it/s]\n100%|██████████| 100/100 [00:54&lt;00:00,  1.85it/s]\n100%|██████████| 100/100 [00:53&lt;00:00,  1.87it/s]\n100%|██████████| 100/100 [00:53&lt;00:00,  1.86it/s]\n100%|██████████| 100/100 [00:53&lt;00:00,  1.86it/s]\n100%|██████████| 100/100 [00:53&lt;00:00,  1.86it/s]\n100%|██████████| 100/100 [00:53&lt;00:00,  1.86it/s]\n100%|██████████| 100/100 [00:54&lt;00:00,  1.85it/s]\n100%|██████████| 100/100 [00:53&lt;00:00,  1.86it/s]\n100%|██████████| 100/100 [00:53&lt;00:00,  1.87it/s]\n100%|██████████| 100/100 [00:53&lt;00:00,  1.86it/s]\n100%|██████████| 100/100 [00:53&lt;00:00,  1.87it/s]\n100%|██████████| 100/100 [00:52&lt;00:00,  1.89it/s]\n100%|██████████| 100/100 [00:52&lt;00:00,  1.89it/s]\n100%|██████████| 100/100 [00:53&lt;00:00,  1.87it/s]\n100%|██████████| 100/100 [00:53&lt;00:00,  1.86it/s]\n100%|██████████| 100/100 [00:53&lt;00:00,  1.86it/s]\n100%|██████████| 100/100 [00:53&lt;00:00,  1.87it/s]\n100%|██████████| 100/100 [00:53&lt;00:00,  1.86it/s]\n100%|██████████| 100/100 [00:53&lt;00:00,  1.86it/s]\n100%|██████████| 100/100 [00:53&lt;00:00,  1.86it/s]\n100%|██████████| 100/100 [00:53&lt;00:00,  1.88it/s]\n100%|██████████| 100/100 [00:53&lt;00:00,  1.87it/s]\n100%|██████████| 100/100 [00:53&lt;00:00,  1.86it/s]\n100%|██████████| 100/100 [00:53&lt;00:00,  1.87it/s]\n100%|██████████| 100/100 [00:53&lt;00:00,  1.87it/s]\n100%|██████████| 100/100 [00:54&lt;00:00,  1.85it/s]\n100%|██████████| 100/100 [00:53&lt;00:00,  1.88it/s]\n100%|██████████| 100/100 [00:53&lt;00:00,  1.86it/s]\n100%|██████████| 100/100 [00:53&lt;00:00,  1.86it/s]\n100%|██████████| 100/100 [00:53&lt;00:00,  1.88it/s]\n100%|██████████| 100/100 [00:53&lt;00:00,  1.88it/s]\n100%|██████████| 100/100 [00:53&lt;00:00,  1.86it/s]\n100%|██████████| 100/100 [00:53&lt;00:00,  1.86it/s]\n100%|██████████| 100/100 [00:53&lt;00:00,  1.88it/s]\n100%|██████████| 100/100 [00:53&lt;00:00,  1.88it/s]\n100%|██████████| 100/100 [00:53&lt;00:00,  1.86it/s]\n100%|██████████| 100/100 [00:53&lt;00:00,  1.86it/s]\n100%|██████████| 100/100 [00:53&lt;00:00,  1.87it/s]\n100%|██████████| 100/100 [00:53&lt;00:00,  1.87it/s]\n100%|██████████| 100/100 [00:53&lt;00:00,  1.87it/s]\n100%|██████████| 100/100 [00:53&lt;00:00,  1.87it/s]\n100%|██████████| 100/100 [00:53&lt;00:00,  1.86it/s]\n100%|██████████| 100/100 [00:54&lt;00:00,  1.85it/s]\n100%|██████████| 100/100 [00:53&lt;00:00,  1.86it/s]\n100%|██████████| 100/100 [00:53&lt;00:00,  1.85it/s]\n100%|██████████| 100/100 [00:54&lt;00:00,  1.85it/s]\n100%|██████████| 100/100 [00:54&lt;00:00,  1.85it/s]\n100%|██████████| 100/100 [00:53&lt;00:00,  1.85it/s]\n100%|██████████| 100/100 [00:54&lt;00:00,  1.85it/s]\n100%|██████████| 100/100 [00:54&lt;00:00,  1.85it/s]\n100%|██████████| 100/100 [00:53&lt;00:00,  1.86it/s]\n100%|██████████| 100/100 [00:53&lt;00:00,  1.86it/s]\n100%|██████████| 100/100 [00:54&lt;00:00,  1.85it/s]\n100%|██████████| 100/100 [00:53&lt;00:00,  1.86it/s]\n100%|██████████| 100/100 [00:53&lt;00:00,  1.86it/s]\n100%|██████████| 100/100 [00:53&lt;00:00,  1.85it/s]\n100%|██████████| 100/100 [00:53&lt;00:00,  1.86it/s]\n100%|██████████| 100/100 [00:53&lt;00:00,  1.86it/s]\n\n\n3277.0478909015656\n\n\n\nmean_mse100_20 = pd.DataFrame(_mse)\n\n\nmean_mae100_20 = pd.DataFrame(_mae)\n\n\n_train_result_mean20 = _train_result.copy()\n\n\n_test_result_mean20 = _test_result.copy()\n\n\nplt.plot(mean_mse100_20.T);\n\n\n\n\n\n\n\n\n\nplt.plot(mean_mae100_20.T);\n\n\n\n\n\n\n\n\n\nvis2(___zero20.train_mean,_train_result_mean20[59]);\n\n\n\n\n\n\n\n\n\nvis2(fiveVTS_test[1:],_test_result_mean20[0]);\n\n\n\n\n\n\n\n\n\nshow_lrpr(fiveVTS_test[1:,0],fiveVTS_test[1:,1],fiveVTS_test[1:,2],fiveVTS_test[1:,3],fiveVTS_test[1:,4],_test_result_mean20)\n\n\n\n\n\n\n\n\n  \n  \n    \n    \n      \n          \n      \n        \n      \n          \n      \n          \n      \n          \n      \n          \n      \n          \n      \n          \n      \n          \n    \n    \n      \n      Once\n      \n      Loop\n      \n      Reflect"
  },
  {
    "objectID": "posts/2_research/2023-01-21-Class.html#linear-1",
    "href": "posts/2_research/2023-01-21-Class.html#linear-1",
    "title": "Class of Method",
    "section": "Linear",
    "text": "Linear\n\nt1= time.time()\nttt = 160\n_mse = []\n_mae = []\n_train_result = []\n_test_result = []\nb = ___zero20.second_linear\nc = ___zero20.number\nd = train_X_linear\nf = train_y_linear\nfor i in range(60):\n    a = Method(b,ttt,c,d,f)\n    a.FT()\n    _mse.append(a.mse)\n    _mae.append(a.mae)\n    _train_result.append(a.train_result)\n    _test_result.append(a.test_result)\n    b = a.FT_result\nt2 = time.time()\nt2-t1\n\n100%|██████████| 100/100 [00:54&lt;00:00,  1.84it/s]\n100%|██████████| 100/100 [00:54&lt;00:00,  1.85it/s]\n100%|██████████| 100/100 [00:54&lt;00:00,  1.85it/s]\n100%|██████████| 100/100 [00:53&lt;00:00,  1.86it/s]\n100%|██████████| 100/100 [00:53&lt;00:00,  1.86it/s]\n100%|██████████| 100/100 [00:53&lt;00:00,  1.85it/s]\n100%|██████████| 100/100 [00:54&lt;00:00,  1.85it/s]\n100%|██████████| 100/100 [00:53&lt;00:00,  1.86it/s]\n100%|██████████| 100/100 [00:53&lt;00:00,  1.86it/s]\n100%|██████████| 100/100 [00:53&lt;00:00,  1.86it/s]\n100%|██████████| 100/100 [00:53&lt;00:00,  1.87it/s]\n100%|██████████| 100/100 [00:53&lt;00:00,  1.86it/s]\n100%|██████████| 100/100 [00:54&lt;00:00,  1.85it/s]\n100%|██████████| 100/100 [00:54&lt;00:00,  1.84it/s]\n100%|██████████| 100/100 [00:54&lt;00:00,  1.84it/s]\n100%|██████████| 100/100 [00:54&lt;00:00,  1.84it/s]\n100%|██████████| 100/100 [00:54&lt;00:00,  1.85it/s]\n100%|██████████| 100/100 [00:54&lt;00:00,  1.84it/s]\n100%|██████████| 100/100 [00:54&lt;00:00,  1.84it/s]\n100%|██████████| 100/100 [00:54&lt;00:00,  1.85it/s]\n100%|██████████| 100/100 [00:53&lt;00:00,  1.85it/s]\n100%|██████████| 100/100 [00:53&lt;00:00,  1.85it/s]\n100%|██████████| 100/100 [00:54&lt;00:00,  1.85it/s]\n100%|██████████| 100/100 [00:54&lt;00:00,  1.85it/s]\n100%|██████████| 100/100 [00:54&lt;00:00,  1.85it/s]\n100%|██████████| 100/100 [00:54&lt;00:00,  1.85it/s]\n100%|██████████| 100/100 [00:53&lt;00:00,  1.86it/s]\n100%|██████████| 100/100 [00:53&lt;00:00,  1.86it/s]\n100%|██████████| 100/100 [00:53&lt;00:00,  1.85it/s]\n100%|██████████| 100/100 [00:54&lt;00:00,  1.84it/s]\n100%|██████████| 100/100 [00:53&lt;00:00,  1.85it/s]\n100%|██████████| 100/100 [00:54&lt;00:00,  1.83it/s]\n100%|██████████| 100/100 [00:54&lt;00:00,  1.84it/s]\n100%|██████████| 100/100 [00:54&lt;00:00,  1.84it/s]\n100%|██████████| 100/100 [00:54&lt;00:00,  1.85it/s]\n100%|██████████| 100/100 [00:54&lt;00:00,  1.85it/s]\n100%|██████████| 100/100 [00:54&lt;00:00,  1.85it/s]\n100%|██████████| 100/100 [00:53&lt;00:00,  1.87it/s]\n100%|██████████| 100/100 [00:54&lt;00:00,  1.84it/s]\n100%|██████████| 100/100 [00:54&lt;00:00,  1.84it/s]\n100%|██████████| 100/100 [00:53&lt;00:00,  1.86it/s]\n100%|██████████| 100/100 [00:54&lt;00:00,  1.84it/s]\n100%|██████████| 100/100 [00:54&lt;00:00,  1.84it/s]\n100%|██████████| 100/100 [00:54&lt;00:00,  1.85it/s]\n100%|██████████| 100/100 [00:53&lt;00:00,  1.87it/s]\n100%|██████████| 100/100 [00:54&lt;00:00,  1.84it/s]\n100%|██████████| 100/100 [00:54&lt;00:00,  1.85it/s]\n100%|██████████| 100/100 [00:54&lt;00:00,  1.84it/s]\n100%|██████████| 100/100 [00:54&lt;00:00,  1.84it/s]\n100%|██████████| 100/100 [00:54&lt;00:00,  1.85it/s]\n100%|██████████| 100/100 [00:53&lt;00:00,  1.86it/s]\n100%|██████████| 100/100 [00:54&lt;00:00,  1.85it/s]\n100%|██████████| 100/100 [00:53&lt;00:00,  1.87it/s]\n100%|██████████| 100/100 [00:53&lt;00:00,  1.86it/s]\n100%|██████████| 100/100 [00:54&lt;00:00,  1.85it/s]\n100%|██████████| 100/100 [00:54&lt;00:00,  1.84it/s]\n100%|██████████| 100/100 [00:54&lt;00:00,  1.84it/s]\n100%|██████████| 100/100 [00:54&lt;00:00,  1.85it/s]\n100%|██████████| 100/100 [00:53&lt;00:00,  1.85it/s]\n100%|██████████| 100/100 [00:54&lt;00:00,  1.84it/s]\n\n\n3298.6988050937653\n\n\n\nlinear_mse100_20 = pd.DataFrame(_mse)\n\n\nlinear_mae100_20 = pd.DataFrame(_mae)\n\n\n_train_result_linear20 = _train_result.copy()\n\n\n_test_result_linear20 = _test_result.copy()\n\n\nplt.plot(linear_mse100_20.T);\n\n\n\n\n\n\n\n\n\nplt.plot(linear_mae100_20.T);\n\n\n\n\n\n\n\n\n\nvis2(___zero20.train_mean,_train_result_linear20[59]);\n\n\n\n\n\n\n\n\n\nvis2(fiveVTS_test[1:],_test_result_linear20[0]);\n\n\n\n\n\n\n\n\n\nshow_lrpr(fiveVTS_test[1:,0],fiveVTS_test[1:,1],fiveVTS_test[1:,2],fiveVTS_test[1:,3],fiveVTS_test[1:,4],_test_result_linear20)\n\n\n\n\n\n\n\n\n  \n  \n    \n    \n      \n          \n      \n        \n      \n          \n      \n          \n      \n          \n      \n          \n      \n          \n      \n          \n      \n          \n    \n    \n      \n      Once\n      \n      Loop\n      \n      Reflect"
  },
  {
    "objectID": "posts/2_research/2023-01-21-Class.html#mean-2",
    "href": "posts/2_research/2023-01-21-Class.html#mean-2",
    "title": "Class of Method",
    "section": "Mean",
    "text": "Mean\n\nt1= time.time()\nttt = 160\n_mse = []\n_mae = []\n_train_result = []\n_test_result = []\nb = ___zero30.train_mean\nc = ___zero30.number\nd = train_X_mean\nf = train_y_mean\nfor i in range(60):\n    a = Method(b,ttt,c,d,f)\n    a.FT()\n    _mse.append(a.mse)\n    _mae.append(a.mae)\n    _train_result.append(a.train_result)\n    _test_result.append(a.test_result)\n    b = a.FT_result\nt2 = time.time()\nt2-t1\n\n100%|██████████| 100/100 [00:53&lt;00:00,  1.87it/s]\n100%|██████████| 100/100 [00:53&lt;00:00,  1.87it/s]\n100%|██████████| 100/100 [00:53&lt;00:00,  1.87it/s]\n100%|██████████| 100/100 [00:53&lt;00:00,  1.87it/s]\n100%|██████████| 100/100 [00:53&lt;00:00,  1.88it/s]\n100%|██████████| 100/100 [00:53&lt;00:00,  1.85it/s]\n100%|██████████| 100/100 [00:53&lt;00:00,  1.87it/s]\n100%|██████████| 100/100 [00:53&lt;00:00,  1.85it/s]\n100%|██████████| 100/100 [00:53&lt;00:00,  1.85it/s]\n100%|██████████| 100/100 [00:54&lt;00:00,  1.85it/s]\n100%|██████████| 100/100 [00:53&lt;00:00,  1.86it/s]\n100%|██████████| 100/100 [00:54&lt;00:00,  1.85it/s]\n100%|██████████| 100/100 [00:54&lt;00:00,  1.85it/s]\n100%|██████████| 100/100 [00:53&lt;00:00,  1.85it/s]\n100%|██████████| 100/100 [00:53&lt;00:00,  1.85it/s]\n100%|██████████| 100/100 [00:54&lt;00:00,  1.85it/s]\n100%|██████████| 100/100 [00:53&lt;00:00,  1.85it/s]\n100%|██████████| 100/100 [00:53&lt;00:00,  1.85it/s]\n100%|██████████| 100/100 [00:53&lt;00:00,  1.85it/s]\n100%|██████████| 100/100 [00:53&lt;00:00,  1.87it/s]\n100%|██████████| 100/100 [00:53&lt;00:00,  1.87it/s]\n100%|██████████| 100/100 [00:53&lt;00:00,  1.87it/s]\n100%|██████████| 100/100 [00:54&lt;00:00,  1.84it/s]\n100%|██████████| 100/100 [00:53&lt;00:00,  1.86it/s]\n100%|██████████| 100/100 [00:53&lt;00:00,  1.87it/s]\n100%|██████████| 100/100 [00:53&lt;00:00,  1.86it/s]\n100%|██████████| 100/100 [00:53&lt;00:00,  1.87it/s]\n100%|██████████| 100/100 [00:53&lt;00:00,  1.86it/s]\n100%|██████████| 100/100 [00:53&lt;00:00,  1.86it/s]\n100%|██████████| 100/100 [00:53&lt;00:00,  1.87it/s]\n100%|██████████| 100/100 [00:53&lt;00:00,  1.85it/s]\n100%|██████████| 100/100 [00:54&lt;00:00,  1.85it/s]\n100%|██████████| 100/100 [00:53&lt;00:00,  1.87it/s]\n100%|██████████| 100/100 [00:53&lt;00:00,  1.86it/s]\n100%|██████████| 100/100 [00:54&lt;00:00,  1.84it/s]\n100%|██████████| 100/100 [00:53&lt;00:00,  1.87it/s]\n100%|██████████| 100/100 [00:53&lt;00:00,  1.86it/s]\n100%|██████████| 100/100 [00:54&lt;00:00,  1.85it/s]\n100%|██████████| 100/100 [00:53&lt;00:00,  1.85it/s]\n100%|██████████| 100/100 [00:53&lt;00:00,  1.87it/s]\n100%|██████████| 100/100 [00:53&lt;00:00,  1.86it/s]\n100%|██████████| 100/100 [00:53&lt;00:00,  1.87it/s]\n100%|██████████| 100/100 [00:54&lt;00:00,  1.84it/s]\n100%|██████████| 100/100 [00:54&lt;00:00,  1.84it/s]\n100%|██████████| 100/100 [00:54&lt;00:00,  1.84it/s]\n100%|██████████| 100/100 [00:53&lt;00:00,  1.87it/s]\n100%|██████████| 100/100 [00:54&lt;00:00,  1.85it/s]\n100%|██████████| 100/100 [00:53&lt;00:00,  1.87it/s]\n100%|██████████| 100/100 [00:53&lt;00:00,  1.86it/s]\n100%|██████████| 100/100 [00:53&lt;00:00,  1.87it/s]\n100%|██████████| 100/100 [00:53&lt;00:00,  1.88it/s]\n100%|██████████| 100/100 [00:53&lt;00:00,  1.88it/s]\n100%|██████████| 100/100 [00:53&lt;00:00,  1.87it/s]\n100%|██████████| 100/100 [00:53&lt;00:00,  1.86it/s]\n100%|██████████| 100/100 [00:53&lt;00:00,  1.86it/s]\n100%|██████████| 100/100 [00:53&lt;00:00,  1.87it/s]\n100%|██████████| 100/100 [00:53&lt;00:00,  1.87it/s]\n100%|██████████| 100/100 [00:53&lt;00:00,  1.86it/s]\n100%|██████████| 100/100 [00:54&lt;00:00,  1.85it/s]\n100%|██████████| 100/100 [00:53&lt;00:00,  1.87it/s]\n\n\n3280.9767186641693\n\n\n\nmean_mse_30 = pd.DataFrame(_mse)\n\n\nmean_mae_30 = pd.DataFrame(_mae)\n\n\n_train_result_mean30 = _train_result.copy()\n\n\n_test_result_mean30 = _test_result.copy()\n\n\nplt.plot(mean_mse_30.T);\n\n\n\n\n\n\n\n\n\nplt.plot(mean_mae_30.T);\n\n\n\n\n\n\n\n\n\nvis2(___zero30.train_mean,_train_result_mean30[59]);\n\n\n\n\n\n\n\n\n\nvis2(fiveVTS_test[1:],_test_result_mean30[0]);\n\n\n\n\n\n\n\n\n\nshow_lrpr(fiveVTS_test[1:,0],fiveVTS_test[1:,1],fiveVTS_test[1:,2],fiveVTS_test[1:,3],fiveVTS_test[1:,4],_test_result_mean30)\n\n\n\n\n\n\n\n\n  \n  \n    \n    \n      \n          \n      \n        \n      \n          \n      \n          \n      \n          \n      \n          \n      \n          \n      \n          \n      \n          \n    \n    \n      \n      Once\n      \n      Loop\n      \n      Reflect"
  },
  {
    "objectID": "posts/2_research/2023-01-21-Class.html#linear-2",
    "href": "posts/2_research/2023-01-21-Class.html#linear-2",
    "title": "Class of Method",
    "section": "Linear",
    "text": "Linear\n\nt1= time.time()\nttt = 160\n_mse = []\n_mae = []\n_train_result = []\n_test_result = []\nb = ___zero30.second_linear\nc = ___zero30.number\nd = train_X_linear\nf = train_y_linear\nfor i in range(60):\n    a = Method(b,ttt,c,d,f)\n    a.FT()\n    _mse.append(a.mse)\n    _mae.append(a.mae)\n    _train_result.append(a.train_result)\n    _test_result.append(a.test_result)\n    b = a.FT_result\nt2 = time.time()\nt2-t1\n\n100%|██████████| 100/100 [00:54&lt;00:00,  1.84it/s]\n100%|██████████| 100/100 [00:54&lt;00:00,  1.85it/s]\n100%|██████████| 100/100 [00:54&lt;00:00,  1.85it/s]\n100%|██████████| 100/100 [00:53&lt;00:00,  1.85it/s]\n100%|██████████| 100/100 [00:53&lt;00:00,  1.86it/s]\n100%|██████████| 100/100 [00:54&lt;00:00,  1.85it/s]\n100%|██████████| 100/100 [00:54&lt;00:00,  1.85it/s]\n100%|██████████| 100/100 [00:54&lt;00:00,  1.84it/s]\n100%|██████████| 100/100 [00:54&lt;00:00,  1.84it/s]\n100%|██████████| 100/100 [00:54&lt;00:00,  1.84it/s]\n100%|██████████| 100/100 [00:54&lt;00:00,  1.83it/s]\n100%|██████████| 100/100 [00:53&lt;00:00,  1.86it/s]\n100%|██████████| 100/100 [00:54&lt;00:00,  1.84it/s]\n100%|██████████| 100/100 [00:54&lt;00:00,  1.83it/s]\n100%|██████████| 100/100 [00:54&lt;00:00,  1.84it/s]\n100%|██████████| 100/100 [00:54&lt;00:00,  1.84it/s]\n100%|██████████| 100/100 [00:54&lt;00:00,  1.84it/s]\n100%|██████████| 100/100 [00:54&lt;00:00,  1.85it/s]\n100%|██████████| 100/100 [00:54&lt;00:00,  1.84it/s]\n100%|██████████| 100/100 [00:54&lt;00:00,  1.84it/s]\n100%|██████████| 100/100 [00:53&lt;00:00,  1.86it/s]\n100%|██████████| 100/100 [00:54&lt;00:00,  1.84it/s]\n100%|██████████| 100/100 [00:54&lt;00:00,  1.85it/s]\n100%|██████████| 100/100 [00:53&lt;00:00,  1.86it/s]\n100%|██████████| 100/100 [00:53&lt;00:00,  1.86it/s]\n100%|██████████| 100/100 [00:54&lt;00:00,  1.85it/s]\n100%|██████████| 100/100 [00:54&lt;00:00,  1.84it/s]\n100%|██████████| 100/100 [00:54&lt;00:00,  1.84it/s]\n100%|██████████| 100/100 [00:53&lt;00:00,  1.87it/s]\n100%|██████████| 100/100 [00:53&lt;00:00,  1.87it/s]\n100%|██████████| 100/100 [00:54&lt;00:00,  1.84it/s]\n100%|██████████| 100/100 [00:54&lt;00:00,  1.83it/s]\n100%|██████████| 100/100 [00:54&lt;00:00,  1.85it/s]\n100%|██████████| 100/100 [00:53&lt;00:00,  1.86it/s]\n100%|██████████| 100/100 [00:54&lt;00:00,  1.83it/s]\n100%|██████████| 100/100 [00:54&lt;00:00,  1.84it/s]\n100%|██████████| 100/100 [00:54&lt;00:00,  1.83it/s]\n100%|██████████| 100/100 [00:54&lt;00:00,  1.83it/s]\n100%|██████████| 100/100 [00:54&lt;00:00,  1.84it/s]\n100%|██████████| 100/100 [00:54&lt;00:00,  1.85it/s]\n100%|██████████| 100/100 [00:53&lt;00:00,  1.86it/s]\n100%|██████████| 100/100 [00:54&lt;00:00,  1.84it/s]\n100%|██████████| 100/100 [00:54&lt;00:00,  1.85it/s]\n100%|██████████| 100/100 [00:54&lt;00:00,  1.84it/s]\n100%|██████████| 100/100 [00:53&lt;00:00,  1.86it/s]\n100%|██████████| 100/100 [00:54&lt;00:00,  1.85it/s]\n100%|██████████| 100/100 [00:54&lt;00:00,  1.85it/s]\n100%|██████████| 100/100 [00:54&lt;00:00,  1.85it/s]\n100%|██████████| 100/100 [00:54&lt;00:00,  1.85it/s]\n100%|██████████| 100/100 [00:54&lt;00:00,  1.84it/s]\n100%|██████████| 100/100 [00:53&lt;00:00,  1.85it/s]\n100%|██████████| 100/100 [00:54&lt;00:00,  1.85it/s]\n100%|██████████| 100/100 [00:54&lt;00:00,  1.83it/s]\n100%|██████████| 100/100 [00:53&lt;00:00,  1.86it/s]\n100%|██████████| 100/100 [00:54&lt;00:00,  1.84it/s]\n100%|██████████| 100/100 [00:54&lt;00:00,  1.84it/s]\n100%|██████████| 100/100 [00:53&lt;00:00,  1.86it/s]\n100%|██████████| 100/100 [00:53&lt;00:00,  1.87it/s]\n100%|██████████| 100/100 [00:54&lt;00:00,  1.84it/s]\n100%|██████████| 100/100 [00:54&lt;00:00,  1.84it/s]\n\n\n3305.375289440155\n\n\n\nlinear_mse_30 = pd.DataFrame(_mse)\n\n\nlinear_mae_30 = pd.DataFrame(_mae)\n\n\n_train_result_linear30 = _train_result.copy()\n\n\n_test_result_linear30 = _test_result.copy()\n\n\nplt.plot(linear_mse_30.T);\n\n\n\n\n\n\n\n\n\nplt.plot(linear_mae_30.T);\n\n\n\n\n\n\n\n\n\nvis2(___zero30.train_mean,_train_result_linear30[59]);\n\n\n\n\n\n\n\n\n\nvis2(fiveVTS_test[1:],_test_result_linear30[0]);\n\n\n\n\n\n\n\n\n\nshow_lrpr(fiveVTS_test[1:,0],fiveVTS_test[1:,1],fiveVTS_test[1:,2],fiveVTS_test[1:,3],fiveVTS_test[1:,4],_test_result_linear30)\n\n\n\n\n\n\n\n\n  \n  \n    \n    \n      \n          \n      \n        \n      \n          \n      \n          \n      \n          \n      \n          \n      \n          \n      \n          \n      \n          \n    \n    \n      \n      Once\n      \n      Loop\n      \n      Reflect"
  },
  {
    "objectID": "posts/2_research/2023-01-21-Class.html#mean-3",
    "href": "posts/2_research/2023-01-21-Class.html#mean-3",
    "title": "Class of Method",
    "section": "Mean",
    "text": "Mean\n\nt1= time.time()\nttt = 160\n_mse = []\n_mae = []\n_train_result = []\n_test_result = []\nb = ___zero40.train_mean\nc = ___zero40.number\nd = train_X_mean\nf = train_y_mean\nfor i in range(60):\n    a = Method(b,ttt,c,d,f)\n    a.FT()\n    _mse.append(a.mse)\n    _mae.append(a.mae)\n    _train_result.append(a.train_result)\n    _test_result.append(a.test_result)\n    b = a.FT_result\nt2 = time.time()\nt2-t1\n\n100%|██████████| 100/100 [00:53&lt;00:00,  1.87it/s]\n100%|██████████| 100/100 [00:54&lt;00:00,  1.85it/s]\n100%|██████████| 100/100 [00:54&lt;00:00,  1.85it/s]\n100%|██████████| 100/100 [00:54&lt;00:00,  1.85it/s]\n100%|██████████| 100/100 [00:53&lt;00:00,  1.86it/s]\n100%|██████████| 100/100 [00:53&lt;00:00,  1.85it/s]\n100%|██████████| 100/100 [00:53&lt;00:00,  1.86it/s]\n100%|██████████| 100/100 [00:53&lt;00:00,  1.86it/s]\n100%|██████████| 100/100 [00:54&lt;00:00,  1.85it/s]\n100%|██████████| 100/100 [00:53&lt;00:00,  1.85it/s]\n100%|██████████| 100/100 [00:54&lt;00:00,  1.84it/s]\n100%|██████████| 100/100 [00:54&lt;00:00,  1.84it/s]\n100%|██████████| 100/100 [00:53&lt;00:00,  1.87it/s]\n100%|██████████| 100/100 [00:53&lt;00:00,  1.87it/s]\n100%|██████████| 100/100 [00:53&lt;00:00,  1.88it/s]\n100%|██████████| 100/100 [00:53&lt;00:00,  1.87it/s]\n100%|██████████| 100/100 [00:54&lt;00:00,  1.85it/s]\n100%|██████████| 100/100 [00:53&lt;00:00,  1.86it/s]\n100%|██████████| 100/100 [00:53&lt;00:00,  1.86it/s]\n100%|██████████| 100/100 [00:54&lt;00:00,  1.85it/s]\n100%|██████████| 100/100 [00:54&lt;00:00,  1.84it/s]\n100%|██████████| 100/100 [00:53&lt;00:00,  1.86it/s]\n100%|██████████| 100/100 [00:53&lt;00:00,  1.87it/s]\n100%|██████████| 100/100 [00:53&lt;00:00,  1.87it/s]\n100%|██████████| 100/100 [00:53&lt;00:00,  1.88it/s]\n100%|██████████| 100/100 [00:53&lt;00:00,  1.88it/s]\n100%|██████████| 100/100 [00:54&lt;00:00,  1.85it/s]\n100%|██████████| 100/100 [00:53&lt;00:00,  1.87it/s]\n100%|██████████| 100/100 [00:54&lt;00:00,  1.85it/s]\n100%|██████████| 100/100 [00:53&lt;00:00,  1.86it/s]\n100%|██████████| 100/100 [00:53&lt;00:00,  1.85it/s]\n100%|██████████| 100/100 [00:53&lt;00:00,  1.87it/s]\n100%|██████████| 100/100 [00:53&lt;00:00,  1.88it/s]\n100%|██████████| 100/100 [00:53&lt;00:00,  1.87it/s]\n100%|██████████| 100/100 [00:54&lt;00:00,  1.85it/s]\n100%|██████████| 100/100 [00:54&lt;00:00,  1.85it/s]\n100%|██████████| 100/100 [00:53&lt;00:00,  1.85it/s]\n100%|██████████| 100/100 [00:54&lt;00:00,  1.85it/s]\n100%|██████████| 100/100 [00:54&lt;00:00,  1.84it/s]\n100%|██████████| 100/100 [00:53&lt;00:00,  1.86it/s]\n100%|██████████| 100/100 [00:54&lt;00:00,  1.85it/s]\n100%|██████████| 100/100 [00:54&lt;00:00,  1.84it/s]\n100%|██████████| 100/100 [00:53&lt;00:00,  1.85it/s]\n100%|██████████| 100/100 [00:54&lt;00:00,  1.85it/s]\n100%|██████████| 100/100 [00:54&lt;00:00,  1.84it/s]\n100%|██████████| 100/100 [00:54&lt;00:00,  1.84it/s]\n100%|██████████| 100/100 [00:54&lt;00:00,  1.85it/s]\n100%|██████████| 100/100 [00:54&lt;00:00,  1.84it/s]\n100%|██████████| 100/100 [00:54&lt;00:00,  1.85it/s]\n100%|██████████| 100/100 [00:53&lt;00:00,  1.86it/s]\n100%|██████████| 100/100 [00:54&lt;00:00,  1.85it/s]\n100%|██████████| 100/100 [00:53&lt;00:00,  1.86it/s]\n100%|██████████| 100/100 [00:53&lt;00:00,  1.87it/s]\n100%|██████████| 100/100 [00:53&lt;00:00,  1.86it/s]\n100%|██████████| 100/100 [00:53&lt;00:00,  1.87it/s]\n100%|██████████| 100/100 [00:53&lt;00:00,  1.87it/s]\n100%|██████████| 100/100 [00:53&lt;00:00,  1.86it/s]\n100%|██████████| 100/100 [00:53&lt;00:00,  1.87it/s]\n100%|██████████| 100/100 [00:53&lt;00:00,  1.86it/s]\n100%|██████████| 100/100 [00:54&lt;00:00,  1.85it/s]\n\n\n3287.529237985611\n\n\n\nmean_mse_40 = pd.DataFrame(_mse)\n\n\nmean_mae_40 = pd.DataFrame(_mae)\n\n\n_train_result_mean40 = _train_result.copy()\n\n\n_test_result_mean40 = _test_result.copy()\n\n\nplt.plot(mean_mse_40.T);\n\n\n\n\n\n\n\n\n\nplt.plot(mean_mae_40.T);\n\n\n\n\n\n\n\n\n\nvis2(___zero40.train_mean,_train_result_mean40[59]);\n\n\n\n\n\n\n\n\n\nvis2(fiveVTS_test[1:],_test_result_mean40[0]);\n\n\n\n\n\n\n\n\n\nshow_lrpr(fiveVTS_test[1:,0],fiveVTS_test[1:,1],fiveVTS_test[1:,2],fiveVTS_test[1:,3],fiveVTS_test[1:,4],_test_result_mean40)\n\n\n\n\n\n\n\n\n  \n  \n    \n    \n      \n          \n      \n        \n      \n          \n      \n          \n      \n          \n      \n          \n      \n          \n      \n          \n      \n          \n    \n    \n      \n      Once\n      \n      Loop\n      \n      Reflect"
  },
  {
    "objectID": "posts/2_research/2023-01-21-Class.html#linear-3",
    "href": "posts/2_research/2023-01-21-Class.html#linear-3",
    "title": "Class of Method",
    "section": "Linear",
    "text": "Linear\n\nt1= time.time()\nttt = 160\n_mse = []\n_mae = []\n_train_result = []\n_test_result = []\nb = ___zero40.second_linear\nc = ___zero40.number\nd = train_X_linear\nf = train_y_linear\nfor i in range(60):\n    a = Method(b,ttt,c,d,f)\n    a.FT()\n    _mse.append(a.mse)\n    _mae.append(a.mae)\n    _train_result.append(a.train_result)\n    _test_result.append(a.test_result)\n    b = a.FT_result\nt2 = time.time()\nt2-t1\n\n100%|██████████| 100/100 [00:53&lt;00:00,  1.85it/s]\n100%|██████████| 100/100 [00:53&lt;00:00,  1.86it/s]\n100%|██████████| 100/100 [00:54&lt;00:00,  1.85it/s]\n100%|██████████| 100/100 [00:54&lt;00:00,  1.85it/s]\n100%|██████████| 100/100 [00:53&lt;00:00,  1.85it/s]\n100%|██████████| 100/100 [00:54&lt;00:00,  1.84it/s]\n100%|██████████| 100/100 [00:54&lt;00:00,  1.85it/s]\n100%|██████████| 100/100 [00:53&lt;00:00,  1.86it/s]\n100%|██████████| 100/100 [00:53&lt;00:00,  1.85it/s]\n100%|██████████| 100/100 [00:53&lt;00:00,  1.85it/s]\n100%|██████████| 100/100 [00:54&lt;00:00,  1.85it/s]\n100%|██████████| 100/100 [00:54&lt;00:00,  1.84it/s]\n100%|██████████| 100/100 [00:54&lt;00:00,  1.84it/s]\n100%|██████████| 100/100 [00:54&lt;00:00,  1.84it/s]\n100%|██████████| 100/100 [00:53&lt;00:00,  1.85it/s]\n100%|██████████| 100/100 [00:54&lt;00:00,  1.84it/s]\n100%|██████████| 100/100 [00:54&lt;00:00,  1.84it/s]\n100%|██████████| 100/100 [00:54&lt;00:00,  1.83it/s]\n100%|██████████| 100/100 [00:53&lt;00:00,  1.86it/s]\n100%|██████████| 100/100 [00:53&lt;00:00,  1.86it/s]\n100%|██████████| 100/100 [00:54&lt;00:00,  1.83it/s]\n100%|██████████| 100/100 [00:54&lt;00:00,  1.84it/s]\n100%|██████████| 100/100 [00:54&lt;00:00,  1.84it/s]\n100%|██████████| 100/100 [00:54&lt;00:00,  1.84it/s]\n100%|██████████| 100/100 [00:54&lt;00:00,  1.85it/s]\n100%|██████████| 100/100 [00:54&lt;00:00,  1.84it/s]\n100%|██████████| 100/100 [00:54&lt;00:00,  1.83it/s]\n100%|██████████| 100/100 [00:54&lt;00:00,  1.84it/s]\n100%|██████████| 100/100 [00:54&lt;00:00,  1.82it/s]\n100%|██████████| 100/100 [00:54&lt;00:00,  1.83it/s]\n100%|██████████| 100/100 [00:54&lt;00:00,  1.84it/s]\n100%|██████████| 100/100 [00:54&lt;00:00,  1.84it/s]\n100%|██████████| 100/100 [00:54&lt;00:00,  1.84it/s]\n100%|██████████| 100/100 [00:54&lt;00:00,  1.84it/s]\n100%|██████████| 100/100 [00:53&lt;00:00,  1.86it/s]\n100%|██████████| 100/100 [00:53&lt;00:00,  1.86it/s]\n100%|██████████| 100/100 [00:53&lt;00:00,  1.86it/s]\n100%|██████████| 100/100 [00:53&lt;00:00,  1.86it/s]\n100%|██████████| 100/100 [00:53&lt;00:00,  1.86it/s]\n100%|██████████| 100/100 [00:54&lt;00:00,  1.85it/s]\n100%|██████████| 100/100 [00:53&lt;00:00,  1.86it/s]\n100%|██████████| 100/100 [00:53&lt;00:00,  1.85it/s]\n100%|██████████| 100/100 [00:54&lt;00:00,  1.85it/s]\n100%|██████████| 100/100 [00:53&lt;00:00,  1.86it/s]\n100%|██████████| 100/100 [00:53&lt;00:00,  1.85it/s]\n100%|██████████| 100/100 [00:53&lt;00:00,  1.86it/s]\n100%|██████████| 100/100 [00:53&lt;00:00,  1.85it/s]\n100%|██████████| 100/100 [00:54&lt;00:00,  1.84it/s]\n100%|██████████| 100/100 [00:54&lt;00:00,  1.84it/s]\n100%|██████████| 100/100 [00:54&lt;00:00,  1.83it/s]\n100%|██████████| 100/100 [00:54&lt;00:00,  1.83it/s]\n100%|██████████| 100/100 [00:53&lt;00:00,  1.86it/s]\n100%|██████████| 100/100 [00:53&lt;00:00,  1.87it/s]\n100%|██████████| 100/100 [00:53&lt;00:00,  1.86it/s]\n100%|██████████| 100/100 [00:53&lt;00:00,  1.86it/s]\n100%|██████████| 100/100 [00:53&lt;00:00,  1.86it/s]\n100%|██████████| 100/100 [00:53&lt;00:00,  1.86it/s]\n100%|██████████| 100/100 [00:54&lt;00:00,  1.83it/s]\n100%|██████████| 100/100 [00:54&lt;00:00,  1.84it/s]\n100%|██████████| 100/100 [00:54&lt;00:00,  1.84it/s]\n\n\n3303.96302652359\n\n\n\nlinear_mse_40 = pd.DataFrame(_mse)\n\n\nlinear_mae_40 = pd.DataFrame(_mae)\n\n\n_train_result_linear40 = _train_result.copy()\n\n\n_test_result_linear40 = _test_result.copy()\n\n\nplt.plot(linear_mse_40.T);\n\n\n\n\n\n\n\n\n\nplt.plot(linear_mae_40.T);\n\n\n\n\n\n\n\n\n\nvis2(___zero40.train_mean,_train_result_linear40[59]);\n\n\n\n\n\n\n\n\n\nvis2(fiveVTS_test[1:],_test_result_linear40[0]);\n\n\n\n\n\n\n\n\n\nshow_lrpr(fiveVTS_test[1:,0],fiveVTS_test[1:,1],fiveVTS_test[1:,2],fiveVTS_test[1:,3],fiveVTS_test[1:,4],_test_result_linear40)\n\n\n\n\n\n\n\n\n  \n  \n    \n    \n      \n          \n      \n        \n      \n          \n      \n          \n      \n          \n      \n          \n      \n          \n      \n          \n      \n          \n    \n    \n      \n      Once\n      \n      Loop\n      \n      Reflect"
  },
  {
    "objectID": "posts/2_research/2023-01-21-Class.html#mean-4",
    "href": "posts/2_research/2023-01-21-Class.html#mean-4",
    "title": "Class of Method",
    "section": "Mean",
    "text": "Mean\n\nt1= time.time()\nttt = 160\n_mse = []\n_mae = []\n_train_result = []\n_test_result = []\nb = ___zero50.train_mean\nc = ___zero50.number\nd = train_X_mean\nf = train_y_mean\nfor i in range(60):\n    a = Method(b,ttt,c,d,f)\n    a.FT()\n    _mse.append(a.mse)\n    _mae.append(a.mae)\n    _train_result.append(a.train_result)\n    _test_result.append(a.test_result)\n    b = a.FT_result\nt2 = time.time()\nt2-t1\n\n\nmean_mse_50 = pd.DataFrame(_mse)\n\n\nmean_mae_50 = pd.DataFrame(_mae)\n\n\n_train_result_mean50 = _train_result.copy()\n\n\n_test_result_mean50 = _test_result.copy()\n\n\nplt.plot(mean_mse_50.T);\n\n\nplt.plot(mean_mae_50.T);\n\n\nvis2(_zero.train_mean,_train_result_mean50[59]);\n\n\nvis2(fiveVTS_test[1:],_test_result_mean50[0]);"
  },
  {
    "objectID": "posts/2_research/2023-01-21-Class.html#linear-4",
    "href": "posts/2_research/2023-01-21-Class.html#linear-4",
    "title": "Class of Method",
    "section": "Linear",
    "text": "Linear\n\nt1= time.time()\nttt = 160\n_mse = []\n_mae = []\n_train_result = []\n_test_result = []\nb = ___zero50.second_linear\nc = ___zero50.number\nd = train_X_linear\nf = train_y_linear\nfor i in range(60):\n    a = Method(b,ttt,c,d,f)\n    a.FT()\n    _mse.append(a.mse)\n    _mae.append(a.mae)\n    _train_result.append(a.train_result)\n    _test_result.append(a.test_result)\n    b = a.FT_result\nt2 = time.time()\nt2-t1\n\n\nlinear_mse_50 = pd.DataFrame(_mse)\n\n\nlinear_mae_50 = pd.DataFrame(_mae)\n\n\n_train_result_linear50 = _train_result.copy()\n\n\n_test_result_linear50 = _test_result.copy()\n\n\nplt.plot(linear_mse_50.T);\n\n\nplt.plot(linear_mae_50.T);\n\n\nvis2(_zero.train_mean,_train_result_linear50[59]);\n\n\nvis2(fiveVTS_test[1:],_test_result_linear50[0]);"
  },
  {
    "objectID": "posts/2_research/2023-02-20-ESTGCN_GNAR_edit_guebin.html",
    "href": "posts/2_research/2023-02-20-ESTGCN_GNAR_edit_guebin.html",
    "title": "1st ITSTGCN",
    "section": "",
    "text": "GNAR fiveNet,fivenodes lag 1"
  },
  {
    "objectID": "posts/2_research/2023-02-20-ESTGCN_GNAR_edit_guebin.html#stgcn",
    "href": "posts/2_research/2023-02-20-ESTGCN_GNAR_edit_guebin.html#stgcn",
    "title": "1st ITSTGCN",
    "section": "STGCN",
    "text": "STGCN\n\nclass STGCN_Missing:\n    def __init__(self,Dataset,df, iterable, Method, Missingrate, Missingtype, lag, Number_of_filters, Interpolation):\n        self.Dataset = Dataset\n        self.df = df\n        self.iterable = iterable\n        self.Method = Method\n        self.Missingrate = Missingrate\n        self.Missingtype = Missingtype\n        self.lag = lag\n        self.Number_of_filters = Number_of_filters\n        self.Interpolation = Interpolation\n    def iter(self):\n        self.XX = torch.tensor(fiveVTS_test.reshape(int(T*0.2),N,1)[:-1,:,:]).float()\n        self.yy = torch.tensor(fiveVTS_test.reshape(int(T*0.2),N,1)[1:,:,:]).float()\n\n        self.real_y = torch.tensor(fiveVTS_train).reshape(int(T*0.8),N,1).float()[1:,:,:]\n        for i in range(self.iterable):\n\n            _zero = Missing(fiveVTS_train)\n            _zero.miss(percent = self.Missingrate)\n            _zero.second_linear()\n\n            missing_index = _zero.number\n            interpolated_signal = _zero.train_linear\n\n            X = torch.tensor(interpolated_signal).reshape(int(T*0.8),N,1).float()[:int(T*0.8-1),:,:]\n            y = torch.tensor(interpolated_signal).reshape(int(T*0.8),N,1).float()[1:,:,:]\n\n            net = RecurrentGCN(node_features=self.lag, filters=self.Number_of_filters)\n            optimizer = torch.optim.Adam(net.parameters(), lr=0.01)\n            net.train()\n            for epoch in range(50):\n                for time, (xt,yt) in enumerate(zip(X,y)):\n                    yt_hat = net(xt, edge_index, edge_attr)\n                    cost = torch.mean((yt_hat-yt)**2)\n                    cost.backward()\n                    optimizer.step()\n                    optimizer.zero_grad()\n\n            yhat = torch.stack([net(xt, edge_index, edge_attr) for xt in X]).detach().numpy()\n            yyhat = torch.stack([net(xt, edge_index, edge_attr) for xt in self.XX]).detach().numpy()\n\n            train_mse_total_stgcn = (((self.real_y-yhat).squeeze())**2).mean()\n            test_mse_total_stgcn = (((self.yy-yyhat).squeeze())**2).mean() \n\n            df_row = pd.DataFrame(columns=col)\n            df_row['Dataset'] = self.Dataset, \n            df_row['iteration'] = i+1, # 1,2,3,...,10 \n            df_row['method'] = self.Method, # 'stgcn','estgcn','gnar' \n            df_row['missingrate'] = self.Missingrate, # 0.0, 0.2, 0.4, 0.6, 0.8 \n            df_row['missingtype'] = self.Missingtype,  # None, 'randomly' and 'block' \n            df_row['lag'] = self.lag, # 1,2,3,4 ... \n            df_row['number_of_filters'] = self.Number_of_filters, # 16,24,32, ... \n            df_row['interpolation'] = self.Interpolation, # None, 'mean', 'linear'\n            df_row['MSE_train'] = train_mse_total_stgcn.tolist()\n            df_row['MSE_test'] = test_mse_total_stgcn.tolist()\n\n            self.df = pd.concat([self.df,df_row])"
  },
  {
    "objectID": "posts/2_research/2023-02-20-ESTGCN_GNAR_edit_guebin.html#enhencement-of-stgcn",
    "href": "posts/2_research/2023-02-20-ESTGCN_GNAR_edit_guebin.html#enhencement-of-stgcn",
    "title": "1st ITSTGCN",
    "section": "Enhencement of STGCN",
    "text": "Enhencement of STGCN\n\nclass ESTGCN_Missing:\n    def __init__(self,Dataset,df, iterable, Method, Missingrate, Missingtype, lag, Number_of_filters, Interpolation):\n        self.Dataset = Dataset\n        self.df = df\n        self.iterable = iterable\n        self.Method = Method\n        self.Missingrate = Missingrate\n        self.Missingtype = Missingtype\n        self.lag = lag\n        self.Number_of_filters = Number_of_filters\n        self.Interpolation = Interpolation\n    def iter(self):\n        self.XX = torch.tensor(fiveVTS_test.reshape(int(T*0.2),N,1)[:-1,:,:]).float()\n        self.yy = torch.tensor(fiveVTS_test.reshape(int(T*0.2),N,1)[1:,:,:]).float()\n\n        self.real_y = torch.tensor(fiveVTS_train).reshape(int(T*0.8),N,1).float()[1:,:,:]\n        for i in range(self.iterable):\n    \n            _zero = Missing(fiveVTS_train)\n            _zero.miss(percent = self.Missingrate)\n            _zero.second_linear()\n\n            missing_index = _zero.number\n            interpolated_signal = _zero.train_linear\n\n            X = torch.tensor(interpolated_signal).reshape(int(T*0.8),N,1).float()[:int(T*0.8-1),:,:]\n            y = torch.tensor(interpolated_signal).reshape(int(T*0.8),N,1).float()[1:,:,:]\n\n\n            net = RecurrentGCN(node_features=self.lag, filters=self.Number_of_filters)\n            optimizer = torch.optim.Adam(net.parameters(), lr=0.01)\n            net.train()\n            signal = interpolated_signal.copy()\n            for epoch in range(50):\n                signal = update_from_freq_domain(signal,missing_index)\n                X = torch.tensor(signal).reshape(int(T*0.8),N,1).float()[:int(T*0.8-1),:,:]\n                y = torch.tensor(signal).reshape(int(T*0.8),N,1).float()[1:,:,:]\n                for time, (xt,yt) in enumerate(zip(X,y)):        \n                    yt_hat = net(xt, edge_index, edge_attr)\n                    cost = torch.mean((yt_hat-yt)**2)\n                    cost.backward()\n                    optimizer.step()\n                    optimizer.zero_grad()\n                signal = torch.concat([X.squeeze(),yt_hat.detach().squeeze().reshape(1,-1)])               \n\n            yhat = torch.stack([net(xt, edge_index, edge_attr) for xt in X]).detach().numpy()\n            yyhat = torch.stack([net(xt, edge_index, edge_attr) for xt in self.XX]).detach().numpy()\n\n            train_mse_total_estgcn = (((self.real_y-yhat).squeeze())**2).mean()\n            test_mse_total_estgcn = (((self.yy-yyhat).squeeze())**2).mean()\n\n            df_row = pd.DataFrame(columns=col)\n            df_row['Dataset'] = self.Dataset,\n            df_row['iteration'] = i+1, # 1,2,3,...,10 \n            df_row['method'] = self.Method, # 'stgcn','estgcn','gnar' \n            df_row['missingrate'] = self.Missingrate, # 0.0, 0.2, 0.4, 0.6, 0.8 \n            df_row['missingtype'] = self.Missingtype,  # None, 'randomly' and 'block' \n            df_row['lag'] = self.lag, # 1,2,3,4 ... \n            df_row['number_of_filters'] = self.Number_of_filters, # 16,24,32, ... \n            df_row['interpolation'] = self.Interpolation, # None, 'mean', 'linear'\n            df_row['MSE_train'] = train_mse_total_estgcn.tolist()\n            df_row['MSE_test'] = test_mse_total_estgcn.tolist()\n\n            self.df = pd.concat([self.df,df_row])"
  },
  {
    "objectID": "posts/2_research/2023-02-20-ESTGCN_GNAR_edit_guebin.html#gnar",
    "href": "posts/2_research/2023-02-20-ESTGCN_GNAR_edit_guebin.html#gnar",
    "title": "1st ITSTGCN",
    "section": "GNAR",
    "text": "GNAR\n\nm = robjects.r.matrix(FloatVector([0,0,0,1,1,0,0,1,1,0,0,1,0,1,0,1,1,1,0,0,1,0,0,0,0]), nrow = 5, ncol = 5)\n\n\nclass GNAR_Missing:\n    def __init__(self,Dataset,df, iterable, Method, Missingrate, Missingtype, lag, Number_of_filters, Interpolation):\n        self.Dataset = Dataset\n        self.df = df\n        self.iterable = iterable\n        self.Method = Method\n        self.Missingrate = Missingrate\n        self.Missingtype = Missingtype\n        self.lag = lag\n        self.Number_of_filters = Number_of_filters\n        self.Interpolation = Interpolation\n    def iter(self):\n        self.yy = torch.tensor(fiveVTS_test.reshape(int(T*0.2),N,1)[1:,:,:]).float()\n        for i in range(self.iterable):\n\n            _zero = Missing(fiveVTS_train)\n            _zero.miss(percent = self.Missingrate)\n            _zero.second_linear()\n\n            missing_index = _zero.number\n            interpolated_signal = _zero.train_linear\n\n            X = torch.tensor(interpolated_signal).reshape(int(T*0.8),N,1).float()[:int(T*0.8-2),:,:]\n\n            answer = GNAR.GNARfit(vts=robjects.r.matrix(rpyn.numpy2rpy(np.array(X).squeeze()), nrow = 160, ncol = 5),net = GNAR.matrixtoGNAR(m), alphaOrder = 2, betaOrder = FloatVector([1, 1]))             \n            predict = GNAR.predict_GNARfit(answer,n_ahead=40)\n\n\n            train_mse_total_gnar = ((pd.DataFrame(GNAR.residuals_GNARfit(answer)).values.reshape(-1,5))**2).mean()\n            test_mse_total_gnar = ((self.yy.squeeze() - pd.DataFrame(predict).values.reshape(-1,5)[:-1,:])**2).mean()\n\n            df_row = pd.DataFrame(columns=col)\n            df_row['Dataset'] = self.Dataset,\n            df_row['iteration'] = i+1, # 1,2,3,...,10 \n            df_row['method'] = self.Method, # 'stgcn','estgcn','gnar' \n            df_row['missingrate'] = self.Missingrate, # 0.0, 0.2, 0.4, 0.6, 0.8 \n            df_row['missingtype'] = self.Missingtype,  # None, 'randomly' and 'block' \n            df_row['lag'] = self.lag, # 1,2,3,4 ... \n            df_row['number_of_filters'] = self.Number_of_filters, # 16,24,32, ... \n            df_row['interpolation'] = self.Interpolation, # None, 'mean', 'linear'\n            df_row['MSE_train'] = train_mse_total_gnar.tolist()\n            df_row['MSE_test'] = test_mse_total_gnar.tolist()\n\n            self.df = pd.concat([self.df,df_row])"
  },
  {
    "objectID": "posts/2_research/2023-02-20-ESTGCN_GNAR_edit_guebin.html#stgcn-1",
    "href": "posts/2_research/2023-02-20-ESTGCN_GNAR_edit_guebin.html#stgcn-1",
    "title": "1st ITSTGCN",
    "section": "STGCN",
    "text": "STGCN\n\nDataset = 'fivenodes'\nMethod = 'stgcn' # 'stgcn','estgcn','gnar' \nMissingtype = 'randomly'  # None, 'randomly' and 'block' \nlag = 1 # 1,2,3,4 ... \nNumber_of_filters = 4 # 16,24,32, ... \nInterpolation = 'Linear' # None, 'mean', 'linear'\niterable = 100\n\n\ndf_stgcn= pd.DataFrame(columns=col)\n\n\nfor Missingrate in rate:\n    df = pd.DataFrame(columns=col)\n    stgcn = STGCN_Missing(Dataset,df, iterable,Method, Missingrate, Missingtype, lag, Number_of_filters, Interpolation)\n    stgcn.iter()\n    df_add = stgcn.df.copy()\n    df_stgcn = pd.concat([df_stgcn,df_add],axis=0)\n\n\nsave_data(df_stgcn, './data/GNAR_stgcn_randomly_by_rate.pkl')"
  },
  {
    "objectID": "posts/2_research/2023-02-20-ESTGCN_GNAR_edit_guebin.html#enhencement-of-stgcn-1",
    "href": "posts/2_research/2023-02-20-ESTGCN_GNAR_edit_guebin.html#enhencement-of-stgcn-1",
    "title": "1st ITSTGCN",
    "section": "Enhencement of STGCN",
    "text": "Enhencement of STGCN\n\nDataset = 'fivenodes'\nMethod = 'estgcn' # 'stgcn','estgcn','gnar' \nMissingtype = 'randomly'  # None, 'randomly' and 'block' \nlag = 1 # 1,2,3,4 ... \nNumber_of_filters = 4 # 16,24,32, ... \nInterpolation = 'Linear' # None, 'mean', 'linear'\niterable = 100\n\n\ndf_estgcn = pd.DataFrame(columns=col)\n\n\nfor Missingrate in rate:\n    df = pd.DataFrame(columns=col)\n    estgcn = ESTGCN_Missing(Dataset,df, iterable,Method, Missingrate, Missingtype, lag, Number_of_filters, Interpolation)\n    estgcn.iter()\n    df_add = estgcn.df.copy()\n    df_estgcn = pd.concat([df_estgcn,df_add],axis=0)\n\n\nsave_data(df_estgcn, './data/GNAR_estgcn_randomly_by_rate.pkl')"
  },
  {
    "objectID": "posts/2_research/2023-02-20-ESTGCN_GNAR_edit_guebin.html#gnar-1",
    "href": "posts/2_research/2023-02-20-ESTGCN_GNAR_edit_guebin.html#gnar-1",
    "title": "1st ITSTGCN",
    "section": "GNAR",
    "text": "GNAR\n\nDataset = 'fivenodes'\nMethod = 'gnar' # 'stgcn','estgcn','gnar' \nMissingtype = 'randomly'  # None, 'randomly' and 'block' \nlag = 1 # 1,2,3,4 ... \nNumber_of_filters = None # 16,24,32, ... \nInterpolation = 'Linear' # None, 'mean', 'linear'\niterable = 100\n\n\ndf_gnar = pd.DataFrame(columns=col)\n\n\nfor Missingrate in rate:\n    df = pd.DataFrame(columns=col)\n    gnar = GNAR_Missing(Dataset,df, iterable,Method, Missingrate, Missingtype, lag, Number_of_filters, Interpolation)\n    gnar.iter()\n    df_add = gnar.df.copy()\n    df_gnar = pd.concat([df_gnar,df_add],axis=0)\n\n\nsave_data(df_gnar, './data/GANR_gnar_randomly_by_rate.pkl')"
  },
  {
    "objectID": "posts/2_research/2023-06-19-LRGCN_simulation_table_reshape.html",
    "href": "posts/2_research/2023-06-19-LRGCN_simulation_table_reshape.html",
    "title": "LRGCN_Simulation Tables_reshape",
    "section": "",
    "text": "Simulation Tables"
  },
  {
    "objectID": "posts/2_research/2023-06-19-LRGCN_simulation_table_reshape.html#baseline",
    "href": "posts/2_research/2023-06-19-LRGCN_simulation_table_reshape.html#baseline",
    "title": "LRGCN_Simulation Tables_reshape",
    "section": "Baseline",
    "text": "Baseline\n\npd.merge(data.query(\"dataset=='fivenodes' and mtype!='rand' and mtype!='block'\").groupby(['nof_filters','method','lags'])['mse'].mean().reset_index(),\n         data.query(\"dataset=='fivenodes' and mtype!='rand' and mtype!='block'\").groupby(['nof_filters','method','lags'])['mse'].std().reset_index(),\n         on=['method','nof_filters','lags']).rename(columns={'mse_x':'mean','mse_y':'std'}).round(3)\n\n\n\n\n\n\n\n\nnof_filters\nmethod\nlags\nmean\nstd\n\n\n\n\n0\n4\nIT-STGCN\n2\n1.211\n0.021\n\n\n1\n4\nSTGCN\n2\n1.213\n0.026"
  },
  {
    "objectID": "posts/2_research/2023-06-19-LRGCN_simulation_table_reshape.html#random",
    "href": "posts/2_research/2023-06-19-LRGCN_simulation_table_reshape.html#random",
    "title": "LRGCN_Simulation Tables_reshape",
    "section": "Random",
    "text": "Random\n\npd.merge(data.query(\"dataset=='fivenodes' and mtype=='rand'\").groupby(['mrate','nof_filters','method','lags'])['mse'].mean().reset_index(),\n         data.query(\"dataset=='fivenodes' and mtype=='rand'\").groupby(['mrate','nof_filters','method','lags'])['mse'].std().reset_index(),\n         on=['method','nof_filters','mrate','lags']).rename(columns={'mse_x':'mean','mse_y':'std'}).round(3)\n\n\n\n\n\n\n\n\nmrate\nnof_filters\nmethod\nlags\nmean\nstd\n\n\n\n\n0\n0.3\n4\nIT-STGCN\n2\n1.220\n0.026\n\n\n1\n0.3\n4\nSTGCN\n2\n1.216\n0.020\n\n\n2\n0.5\n4\nIT-STGCN\n2\n1.215\n0.025\n\n\n3\n0.5\n4\nSTGCN\n2\n1.237\n0.038\n\n\n4\n0.6\n4\nIT-STGCN\n2\n1.226\n0.031\n\n\n5\n0.6\n4\nSTGCN\n2\n1.234\n0.036\n\n\n6\n0.7\n4\nIT-STGCN\n2\n1.232\n0.036\n\n\n7\n0.7\n4\nSTGCN\n2\n1.250\n0.048\n\n\n8\n0.8\n4\nIT-STGCN\n2\n1.232\n0.038\n\n\n9\n0.8\n4\nSTGCN\n2\n1.250\n0.042"
  },
  {
    "objectID": "posts/2_research/2023-06-19-LRGCN_simulation_table_reshape.html#block",
    "href": "posts/2_research/2023-06-19-LRGCN_simulation_table_reshape.html#block",
    "title": "LRGCN_Simulation Tables_reshape",
    "section": "Block",
    "text": "Block\n\npd.merge(data.query(\"dataset=='fivenodes' and mtype=='block'\").groupby(['mrate','nof_filters','method'])['mse'].mean().reset_index(),\n         data.query(\"dataset=='fivenodes' and mtype=='block'\").groupby(['mrate','nof_filters','method'])['mse'].std().reset_index(),\n         on=['method','nof_filters','mrate']).rename(columns={'mse_x':'mean','mse_y':'std'}).round(3)\n\n\n\n\n\n\n\n\nmrate\nnof_filters\nmethod\nmean\nstd\n\n\n\n\n0\n0.125\n4\nIT-STGCN\n1.218\n0.025\n\n\n1\n0.125\n4\nSTGCN\n1.245\n0.036"
  },
  {
    "objectID": "posts/2_research/2023-06-19-LRGCN_simulation_table_reshape.html#baseline-1",
    "href": "posts/2_research/2023-06-19-LRGCN_simulation_table_reshape.html#baseline-1",
    "title": "LRGCN_Simulation Tables_reshape",
    "section": "Baseline",
    "text": "Baseline\n\npd.merge(data.query(\"dataset=='chickenpox' and mtype!='rand' and mtype!='block'\").groupby(['nof_filters','method'])['mse'].mean().reset_index(),\n         data.query(\"dataset=='chickenpox' and mtype!='rand' and mtype!='block'\").groupby(['nof_filters','method'])['mse'].std().reset_index(),\n         on=['method','nof_filters']).rename(columns={'mse_x':'mean','mse_y':'std'}).round(3)\n\n\n\n\n\n\n\n\nnof_filters\nmethod\nmean\nstd\n\n\n\n\n0\n8\nIT-STGCN\n0.865\n0.040\n\n\n1\n8\nSTGCN\n0.872\n0.053"
  },
  {
    "objectID": "posts/2_research/2023-06-19-LRGCN_simulation_table_reshape.html#random-1",
    "href": "posts/2_research/2023-06-19-LRGCN_simulation_table_reshape.html#random-1",
    "title": "LRGCN_Simulation Tables_reshape",
    "section": "Random",
    "text": "Random\n\npd.merge(data.query(\"dataset=='chickenpox' and mtype=='rand'\").groupby(['mrate','inter_method','nof_filters','method'])['mse'].mean().reset_index(),\n         data.query(\"dataset=='chickenpox' and mtype=='rand'\").groupby(['mrate','inter_method','nof_filters','method'])['mse'].std().reset_index(),\n         on=['method','inter_method','mrate','nof_filters']).rename(columns={'mse_x':'mean','mse_y':'std'}).round(3)\n\n\n\n\n\n\n\n\nmrate\ninter_method\nnof_filters\nmethod\nmean\nstd\n\n\n\n\n0\n0.3\nlinear\n8\nIT-STGCN\n0.870\n0.035\n\n\n1\n0.3\nlinear\n8\nSTGCN\n1.086\n0.029\n\n\n2\n0.5\nlinear\n8\nIT-STGCN\n0.931\n0.034\n\n\n3\n0.5\nlinear\n8\nSTGCN\n1.458\n0.068\n\n\n4\n0.6\nlinear\n8\nIT-STGCN\n1.017\n0.029\n\n\n5\n0.6\nlinear\n8\nSTGCN\n1.615\n0.134\n\n\n6\n0.8\nlinear\n8\nIT-STGCN\n1.334\n0.071\n\n\n7\n0.8\nlinear\n8\nSTGCN\n1.632\n0.156"
  },
  {
    "objectID": "posts/2_research/2023-06-19-LRGCN_simulation_table_reshape.html#block-1",
    "href": "posts/2_research/2023-06-19-LRGCN_simulation_table_reshape.html#block-1",
    "title": "LRGCN_Simulation Tables_reshape",
    "section": "Block",
    "text": "Block\n\npd.merge(data.query(\"dataset=='chickenpox' and mtype=='block'\").groupby(['inter_method','mrate','nof_filters','method'])['mse'].mean().reset_index(),\n         data.query(\"dataset=='chickenpox' and mtype=='block'\").groupby(['inter_method','mrate','nof_filters','method'])['mse'].std().reset_index(),\n         on=['method','inter_method','mrate','nof_filters']).rename(columns={'mse_x':'mean','mse_y':'std'})\n\n\n\n\n\n\n\n\ninter_method\nmrate\nnof_filters\nmethod\nmean\nstd\n\n\n\n\n0\nlinear\n0.28777\n8\nIT-STGCN\n0.888210\n0.034704\n\n\n1\nlinear\n0.28777\n8\nSTGCN\n0.910671\n0.047118\n\n\n2\nnearest\n0.28777\n8\nIT-STGCN\n0.888035\n0.040932\n\n\n3\nnearest\n0.28777\n8\nSTGCN\n0.902404\n0.039003"
  },
  {
    "objectID": "posts/2_research/2023-06-19-LRGCN_simulation_table_reshape.html#baseline-2",
    "href": "posts/2_research/2023-06-19-LRGCN_simulation_table_reshape.html#baseline-2",
    "title": "LRGCN_Simulation Tables_reshape",
    "section": "Baseline",
    "text": "Baseline\n\npd.merge(data.query(\"dataset=='pedalme' and mtype!='rand' and mtype!='block'\").groupby(['lags','nof_filters','method'])['mse'].mean().reset_index(),\n         data.query(\"dataset=='pedalme' and mtype!='rand' and mtype!='block'\").groupby(['lags','nof_filters','method'])['mse'].std().reset_index(),\n         on=['method','lags','nof_filters']).rename(columns={'mse_x':'mean','mse_y':'std'}).round(3).query(\"lags==4\")\n\n\n\n\n\n\n\n\nlags\nnof_filters\nmethod\nmean\nstd\n\n\n\n\n0\n4\n8\nIT-STGCN\n1.193\n0.059\n\n\n1\n4\n8\nSTGCN\n1.188\n0.050"
  },
  {
    "objectID": "posts/2_research/2023-06-19-LRGCN_simulation_table_reshape.html#random-2",
    "href": "posts/2_research/2023-06-19-LRGCN_simulation_table_reshape.html#random-2",
    "title": "LRGCN_Simulation Tables_reshape",
    "section": "Random",
    "text": "Random\n\npd.merge(data.query(\"dataset=='pedalme' and mtype=='rand'\").groupby(['mrate','lags','inter_method','method'])['mse'].mean().reset_index(),\n         data.query(\"dataset=='pedalme' and mtype=='rand'\").groupby(['mrate','lags','inter_method','method'])['mse'].std().reset_index(),\n         on=['method','mrate','lags','inter_method']).rename(columns={'mse_x':'mean','mse_y':'std'}).round(3)\n\n\n\n\n\n\n\n\nmrate\nlags\ninter_method\nmethod\nmean\nstd\n\n\n\n\n0\n0.3\n4\nlinear\nIT-STGCN\n1.210\n0.039\n\n\n1\n0.3\n4\nlinear\nSTGCN\n1.274\n0.045\n\n\n2\n0.3\n4\nnearest\nIT-STGCN\n1.220\n0.046\n\n\n3\n0.3\n4\nnearest\nSTGCN\n1.287\n0.065\n\n\n4\n0.5\n4\nlinear\nIT-STGCN\n1.242\n0.057\n\n\n5\n0.5\n4\nlinear\nSTGCN\n1.432\n0.077\n\n\n6\n0.5\n4\nnearest\nIT-STGCN\n1.240\n0.036\n\n\n7\n0.5\n4\nnearest\nSTGCN\n1.379\n0.079\n\n\n8\n0.6\n4\nlinear\nIT-STGCN\n1.286\n0.053\n\n\n9\n0.6\n4\nlinear\nSTGCN\n1.459\n0.073\n\n\n10\n0.6\n4\nnearest\nIT-STGCN\n1.286\n0.033\n\n\n11\n0.6\n4\nnearest\nSTGCN\n1.462\n0.084\n\n\n12\n0.8\n4\nlinear\nIT-STGCN\n1.440\n0.094\n\n\n13\n0.8\n4\nlinear\nSTGCN\n1.496\n0.086\n\n\n14\n0.8\n4\nnearest\nIT-STGCN\n1.436\n0.091\n\n\n15\n0.8\n4\nnearest\nSTGCN\n1.529\n0.071"
  },
  {
    "objectID": "posts/2_research/2023-06-19-LRGCN_simulation_table_reshape.html#block-2",
    "href": "posts/2_research/2023-06-19-LRGCN_simulation_table_reshape.html#block-2",
    "title": "LRGCN_Simulation Tables_reshape",
    "section": "Block",
    "text": "Block\n\npd.merge(data.query(\"dataset=='pedalme' and mtype=='block'\").groupby(['mrate','lags','inter_method','method'])['mse'].mean().reset_index(),\n         data.query(\"dataset=='pedalme' and mtype=='block'\").groupby(['mrate','lags','inter_method','method'])['mse'].std().reset_index(),\n         on=['method','mrate','lags','inter_method']).rename(columns={'mse_x':'mean','mse_y':'std'}).round(3).query(\"lags==4\")\n\n\n\n\n\n\n\n\nmrate\nlags\ninter_method\nmethod\nmean\nstd\n\n\n\n\n0\n0.286\n4\nlinear\nIT-STGCN\n1.169\n0.040\n\n\n1\n0.286\n4\nlinear\nSTGCN\n1.204\n0.032\n\n\n2\n0.286\n4\nnearest\nIT-STGCN\n1.165\n0.035\n\n\n3\n0.286\n4\nnearest\nSTGCN\n1.263\n0.033"
  },
  {
    "objectID": "posts/2_research/2023-06-19-LRGCN_simulation_table_reshape.html#w_st",
    "href": "posts/2_research/2023-06-19-LRGCN_simulation_table_reshape.html#w_st",
    "title": "LRGCN_Simulation Tables_reshape",
    "section": "W_st",
    "text": "W_st\n\npd.merge(data_pedal2.query(\"mtype=='rand'\").groupby(['mrate','lags','inter_method','method'])['mse'].mean().reset_index(),\n         data_pedal2.query(\"mtype=='rand'\").groupby(['mrate','lags','inter_method','method'])['mse'].std().reset_index(),\n         on=['method','mrate','lags','inter_method']).rename(columns={'mse_x':'mean','mse_y':'std'}).round(3).query(\"lags==4\")\n\n\n\n\n\n\n\n\nmrate\nlags\ninter_method\nmethod\nmean\nstd\n\n\n\n\n0\n0.3\n4\nlinear\nIT-STGCN\n1.299\n0.147\n\n\n1\n0.3\n4\nlinear\nSTGCN\n1.325\n0.086\n\n\n2\n0.3\n4\nnearest\nIT-STGCN\n1.260\n0.117\n\n\n3\n0.3\n4\nnearest\nSTGCN\n1.269\n0.087\n\n\n4\n0.6\n4\nlinear\nIT-STGCN\n1.265\n0.100\n\n\n5\n0.6\n4\nlinear\nSTGCN\n1.466\n0.085\n\n\n6\n0.6\n4\nnearest\nIT-STGCN\n1.331\n0.120\n\n\n7\n0.6\n4\nnearest\nSTGCN\n1.453\n0.115\n\n\n\n\n\n\n\n\npd.merge(data_pedal2.query(\"mtype=='block'\").groupby(['mrate','lags','inter_method','method'])['mse'].mean().reset_index(),\n         data_pedal2.query(\"mtype=='block'\").groupby(['mrate','lags','inter_method','method'])['mse'].std().reset_index(),\n         on=['method','mrate','lags','inter_method']).rename(columns={'mse_x':'mean','mse_y':'std'}).round(3).query(\"lags==4\")\n\n\n\n\n\n\n\n\nmrate\nlags\ninter_method\nmethod\nmean\nstd\n\n\n\n\n0\n0.286\n4\nlinear\nIT-STGCN\n1.201\n0.081\n\n\n1\n0.286\n4\nlinear\nSTGCN\n1.227\n0.070\n\n\n2\n0.286\n4\nnearest\nIT-STGCN\n1.197\n0.106\n\n\n3\n0.286\n4\nnearest\nSTGCN\n1.347\n0.117"
  },
  {
    "objectID": "posts/2_research/2023-06-19-LRGCN_simulation_table_reshape.html#baseline-3",
    "href": "posts/2_research/2023-06-19-LRGCN_simulation_table_reshape.html#baseline-3",
    "title": "LRGCN_Simulation Tables_reshape",
    "section": "Baseline",
    "text": "Baseline\n\npd.merge(data.query(\"dataset=='wikimath' and mrate==0\").groupby(['lags','nof_filters','method'])['mse'].mean().reset_index(),\n         data.query(\"dataset=='wikimath' and mrate==0\").groupby(['lags','nof_filters','method'])['mse'].std().reset_index(),\n         on=['lags','nof_filters','method']).rename(columns={'mse_x':'mean','mse_y':'std'}).round(3)\n\n\n\n\n\n\n\n\nlags\nnof_filters\nmethod\nmean\nstd\n\n\n\n\n0\n8\n32\nIT-STGCN\n0.610\n0.017\n\n\n1\n8\n32\nSTGCN\n0.608\n0.014"
  },
  {
    "objectID": "posts/2_research/2023-06-19-LRGCN_simulation_table_reshape.html#random-3",
    "href": "posts/2_research/2023-06-19-LRGCN_simulation_table_reshape.html#random-3",
    "title": "LRGCN_Simulation Tables_reshape",
    "section": "Random",
    "text": "Random\n\npd.merge(data.query(\"dataset=='wikimath' and mtype=='rand'\").groupby(['mrate','lags','method'])['mse'].mean().reset_index(),\n         data.query(\"dataset=='wikimath' and mtype=='rand'\").groupby(['mrate','lags','method'])['mse'].std().reset_index(),\n         on=['method','mrate','lags']).rename(columns={'mse_x':'mean','mse_y':'std'}).round(3)\n\n\n\n\n\n\n\n\nmrate\nlags\nmethod\nmean\nstd\n\n\n\n\n0\n0.3\n8\nIT-STGCN\n0.619\n0.019\n\n\n1\n0.3\n8\nSTGCN\n0.689\n0.032\n\n\n2\n0.5\n8\nIT-STGCN\n0.621\n0.015\n\n\n3\n0.5\n8\nSTGCN\n0.838\n0.054\n\n\n4\n0.6\n8\nIT-STGCN\n0.648\n0.024\n\n\n5\n0.6\n8\nSTGCN\n0.917\n0.063\n\n\n6\n0.8\n8\nIT-STGCN\n0.769\n0.045\n\n\n7\n0.8\n8\nSTGCN\n1.105\n0.099"
  },
  {
    "objectID": "posts/2_research/2023-06-19-LRGCN_simulation_table_reshape.html#block-3",
    "href": "posts/2_research/2023-06-19-LRGCN_simulation_table_reshape.html#block-3",
    "title": "LRGCN_Simulation Tables_reshape",
    "section": "Block",
    "text": "Block\n\npd.merge(data.query(\"dataset=='wikimath' and mtype=='block'\").groupby(['mrate','lags','method'])['mse'].mean().reset_index(),\n         data.query(\"dataset=='wikimath' and mtype=='block'\").groupby(['mrate','lags','method'])['mse'].std().reset_index(),\n         on=['method','mrate','lags']).rename(columns={'mse_x':'mean','mse_y':'std'})\n\n\n\n\n\n\n\n\nmrate\nlags\nmethod\nmean\nstd\n\n\n\n\n0\n0.119837\n8\nIT-STGCN\n0.608375\n0.012177\n\n\n1\n0.119837\n8\nSTGCN\n0.624250\n0.023857"
  },
  {
    "objectID": "posts/2_research/2023-06-19-LRGCN_simulation_table_reshape.html#missing-values-on-the-same-nodes",
    "href": "posts/2_research/2023-06-19-LRGCN_simulation_table_reshape.html#missing-values-on-the-same-nodes",
    "title": "LRGCN_Simulation Tables_reshape",
    "section": "missing values on the same nodes",
    "text": "missing values on the same nodes\n\npd.merge(data_wiki_GSO.groupby(['mrate','lags','method'])['mse'].mean().reset_index(),\n        data_wiki_GSO.groupby(['mrate','lags','method'])['mse'].std().reset_index(),\n         on=['method','mrate','lags']).rename(columns={'mse_x':'mean','mse_y':'std'}).round(3)\n\n\n\n\n\n\n\n\nmrate\nlags\nmethod\nmean\nstd\n\n\n\n\n0\n0.512\n8\nIT-STGCN\n0.624\n0.019\n\n\n1\n0.512\n8\nSTGCN\n0.810\n0.064"
  },
  {
    "objectID": "posts/2_research/2023-06-19-LRGCN_simulation_table_reshape.html#baseline-4",
    "href": "posts/2_research/2023-06-19-LRGCN_simulation_table_reshape.html#baseline-4",
    "title": "LRGCN_Simulation Tables_reshape",
    "section": "Baseline",
    "text": "Baseline\n\npd.merge(data.query(\"dataset=='windmillsmall' and mrate==0\").groupby(['lags','method'])['mse'].mean().reset_index(),\n         data.query(\"dataset=='windmillsmall' and mrate==0\").groupby(['lags','method'])['mse'].std().reset_index(),\n         on=['method','lags']).rename(columns={'mse_x':'mean','mse_y':'std'}).round(3)\n\n\n\n\n\n\n\n\nlags\nmethod\nmean\nstd\n\n\n\n\n0\n8\nIT-STGCN\n0.987\n0.006\n\n\n1\n8\nSTGCN\n0.988\n0.006"
  },
  {
    "objectID": "posts/2_research/2023-06-19-LRGCN_simulation_table_reshape.html#random-4",
    "href": "posts/2_research/2023-06-19-LRGCN_simulation_table_reshape.html#random-4",
    "title": "LRGCN_Simulation Tables_reshape",
    "section": "Random",
    "text": "Random\n\npd.merge(data.query(\"dataset=='windmillsmall' and mtype=='rand'\").groupby(['mrate','lags','method'])['mse'].mean().reset_index(),\n         data.query(\"dataset=='windmillsmall' and mtype=='rand'\").groupby(['mrate','lags','method'])['mse'].std().reset_index(),\n         on=['method','mrate','lags']).rename(columns={'mse_x':'mean','mse_y':'std'}).round(3)\n\n\n\n\n\n\n\n\nmrate\nlags\nmethod\nmean\nstd\n\n\n\n\n0\n0.7\n8\nIT-STGCN\n1.110\n0.012\n\n\n1\n0.7\n8\nSTGCN\n1.492\n0.087"
  },
  {
    "objectID": "posts/2_research/2023-06-19-LRGCN_simulation_table_reshape.html#block-4",
    "href": "posts/2_research/2023-06-19-LRGCN_simulation_table_reshape.html#block-4",
    "title": "LRGCN_Simulation Tables_reshape",
    "section": "Block",
    "text": "Block\n\npd.merge(data.query(\"dataset=='windmillsmall' and mtype=='block'\").groupby(['mrate','lags','method'])['mse'].mean().reset_index(),\n         data.query(\"dataset=='windmillsmall' and mtype=='block'\").groupby(['mrate','lags','method'])['mse'].std().reset_index(),\n         on=['method','mrate','lags']).rename(columns={'mse_x':'mean','mse_y':'std'}).round(3)\n\n\n\n\n\n\n\n\nmrate\nlags\nmethod\nmean\nstd\n\n\n\n\n0\n0.081\n8\nIT-STGCN\n0.985\n0.002\n\n\n1\n0.081\n8\nSTGCN\n0.985\n0.003"
  },
  {
    "objectID": "posts/2_research/2023-06-19-LRGCN_simulation_table_reshape.html#baseline-5",
    "href": "posts/2_research/2023-06-19-LRGCN_simulation_table_reshape.html#baseline-5",
    "title": "LRGCN_Simulation Tables_reshape",
    "section": "Baseline",
    "text": "Baseline\n\npd.merge(data.query(\"dataset=='monte' and mrate==0\").groupby(['lags','method'])['mse'].mean().reset_index(),\n         data.query(\"dataset=='monte' and mrate==0\").groupby(['lags','method'])['mse'].std().reset_index(),\n         on=['method','lags']).rename(columns={'mse_x':'mean','mse_y':'std'}).round(3)\n\n\n\n\n\n\n\n\nlags\nmethod\nmean\nstd\n\n\n\n\n0\n4\nIT-STGCN\n0.977\n0.024\n\n\n1\n4\nSTGCN\n0.983\n0.024"
  },
  {
    "objectID": "posts/2_research/2023-06-19-LRGCN_simulation_table_reshape.html#random-5",
    "href": "posts/2_research/2023-06-19-LRGCN_simulation_table_reshape.html#random-5",
    "title": "LRGCN_Simulation Tables_reshape",
    "section": "Random",
    "text": "Random\n\npd.merge(data.query(\"dataset=='monte' and mtype=='rand'\").groupby(['mrate','lags','inter_method','method'])['mse'].mean().reset_index(),\n         data.query(\"dataset=='monte' and mtype=='rand'\").groupby(['mrate','lags','inter_method','method'])['mse'].std().reset_index(),\n         on=['mrate','inter_method','method','mrate','lags']).rename(columns={'mse_x':'mean','mse_y':'std'})\n\n\n\n\n\n\n\n\nmrate\nlags\ninter_method\nmethod\nmean\nstd\n\n\n\n\n0\n0.3\n4\nnearest\nIT-STGCN\n0.973762\n0.013469\n\n\n1\n0.3\n4\nnearest\nSTGCN\n0.982182\n0.019160\n\n\n2\n0.5\n4\nnearest\nIT-STGCN\n0.984241\n0.022417\n\n\n3\n0.5\n4\nnearest\nSTGCN\n0.977705\n0.021292\n\n\n4\n0.7\n4\nnearest\nIT-STGCN\n0.980846\n0.022604\n\n\n5\n0.7\n4\nnearest\nSTGCN\n0.988757\n0.024863\n\n\n6\n0.8\n4\nnearest\nIT-STGCN\n0.982006\n0.013261\n\n\n7\n0.8\n4\nnearest\nSTGCN\n0.989219\n0.028653"
  },
  {
    "objectID": "posts/2_research/2023-06-19-LRGCN_simulation_table_reshape.html#block-5",
    "href": "posts/2_research/2023-06-19-LRGCN_simulation_table_reshape.html#block-5",
    "title": "LRGCN_Simulation Tables_reshape",
    "section": "Block",
    "text": "Block\n\npd.merge(data.query(\"dataset=='monte' and mtype=='block'\").groupby(['mrate','lags','inter_method','method'])['mse'].mean().reset_index(),\n         data.query(\"dataset=='monte' and mtype=='block'\").groupby(['mrate','lags','inter_method','method'])['mse'].std().reset_index(),\n         on=['method','mrate','inter_method','lags']).rename(columns={'mse_x':'mean','mse_y':'std'})\n\n\n\n\n\n\n\n\nmrate\nlags\ninter_method\nmethod\nmean\nstd\n\n\n\n\n0\n0.149142\n4\nnearest\nIT-STGCN\n0.978381\n0.024324\n\n\n1\n0.149142\n4\nnearest\nSTGCN\n0.976996\n0.020351"
  },
  {
    "objectID": "posts/2_research/2023-04-27-toy_example_notes.html",
    "href": "posts/2_research/2023-04-27-toy_example_notes.html",
    "title": "Toy Example Note",
    "section": "",
    "text": "import numpy as np\nimport matplotlib.pyplot as plt\n\n\nimport pandas as pd\nimport pickle\n\n\nT = 50\nt = np.arange(T)/T * 10 \n\n\ndef load_data(fname):\n    with open(fname, 'rb') as outfile:\n        data_dict = pickle.load(outfile)\n    return data_dict\n\n\ndef save_data(data_dict,fname):\n    with open(fname,'wb') as outfile:\n        pickle.dump(data_dict,outfile)\n\n\nimport torch\n\n\nx = 50*np.sin(2*t)#+30*np.sin(5*t)\neps_x  = np.random.normal(size=T)\ny = x.copy()\nfor i in range(2,T):\n    y[i] = 0.35*x[i-1] - 0.15*x[i-2] + 50*np.cos(0.5*t[i]) \neps_y  = np.random.normal(size=T)\n\n\nplt.plot(t,x,color='C0',lw=5)\nplt.plot(t,x+eps_x,alpha=0.5,color='C0')\nplt.plot(t,y,color='C1',lw=5)\nplt.plot(t,y+eps_y,alpha=0.5,color='C1')\n_node_ids = {'node1':0, 'node2':1}\n\n\n\n_node_ids = {'node1':0, 'node2':1}\n\n_FX1 = np.stack([x,y],axis=1).tolist()\n\n_edges1 = torch.tensor([[1,0]]).tolist()\n\ndata_dict1 = {'edges':_edges1, 'node_ids':_node_ids, 'FX':_FX1}\n#data_dict = itstgcn.load_data('./data/fivenodes.pkl')\n\nsave_data(data_dict1, './data/toy_example1.pkl')\n\ndata1 = pd.DataFrame({'x':x,'y':y,'xer':x,'yer':y})\n\nsave_data(data1, './data/toy_example_true1.csv')\n\n\n\n\n\n\n\n\n\nnp.stack([x+eps_x,y+eps_y],axis=1).shape\n\n(800, 2)\n\n\n\n_node_ids = {'node1':0, 'node2':1}\n\n\n_FX = np.stack([x+eps_x,y+eps_y],axis=1).tolist()\n\n\n_edges = torch.tensor([[0,0],[0,1],[1,0],[1,1]]).tolist()\n\n\ndata_dict = {'edges':_edges, 'node_ids':_node_ids, 'FX':_FX}\n\n\ndata_dict['edges']\n\n[[0, 0], [0, 1], [1, 0], [1, 1]]\n\n\n\nnp.array(data_dict['edges']).shape\n\n(4, 2)\n\n\n\nsave_data(data_dict, './data/toy_example.pkl')\n\n\ndata_dict = load_data('./data/toy_example.pkl')\n\n\ndata_dict.keys()\n\ndict_keys(['edges', 'node_ids', 'FX'])\n\n\n\ndata_dict['edges']\n\n[[0, 0], [0, 1], [1, 0], [1, 1]]\n\n\n\ndata_dict['node_ids']\n\n{'node1': 0, 'node2': 1}\n\n\n\nnp.array(data_dict['FX']).shape\n\n(800, 2)\n\n\n\ndata = pd.DataFrame({'x':x,'y':y,'xer':x+eps_x,'yer':y+eps_y})\n\n\nsave_data(data, './data/toy_example_true.csv')\n\n\ndata = load_data('./data/toy_example_true.csv')\n\n\n_node_ids = {'node1':0, 'node2':1}\n\n\n_FX1 = np.stack([x,y],axis=1).tolist()\n\n\n_edges1 = torch.tensor([[1,0]]).tolist()\n\n\ndata_dict1 = {'edges':_edges1, 'node_ids':_node_ids, 'FX':_FX1}\n#data_dict = itstgcn.load_data('./data/fivenodes.pkl')\n\n\nsave_data(data_dict1, './data/toy_example1.pkl')\n\n\ndata1 = pd.DataFrame({'x':x,'y':y,'xer':x,'yer':y})\n\n\nsave_data(data1, './data/toy_example_true1.csv')"
  },
  {
    "objectID": "posts/2_research/2023-06-25-EvolveGCNO_simulation_table_reshape.html",
    "href": "posts/2_research/2023-06-25-EvolveGCNO_simulation_table_reshape.html",
    "title": "EvolveGCNO_Simulation Tables_reshape",
    "section": "",
    "text": "Simulation Tables"
  },
  {
    "objectID": "posts/2_research/2023-06-25-EvolveGCNO_simulation_table_reshape.html#baseline",
    "href": "posts/2_research/2023-06-25-EvolveGCNO_simulation_table_reshape.html#baseline",
    "title": "EvolveGCNO_Simulation Tables_reshape",
    "section": "Baseline",
    "text": "Baseline\n\npd.merge(data.query(\"dataset=='fivenodes' and mtype!='rand' and mtype!='block'\").groupby(['nof_filters','method','lags'])['mse'].mean().reset_index(),\n         data.query(\"dataset=='fivenodes' and mtype!='rand' and mtype!='block'\").groupby(['nof_filters','method','lags'])['mse'].std().reset_index(),\n         on=['method','nof_filters','lags']).rename(columns={'mse_x':'mean','mse_y':'std'}).round(3)\n\n\n\n\n\n\n\n\nnof_filters\nmethod\nlags\nmean\nstd\n\n\n\n\n0\n12\nIT-STGCN\n2\n1.172\n0.064\n\n\n1\n12\nSTGCN\n2\n1.164\n0.065"
  },
  {
    "objectID": "posts/2_research/2023-06-25-EvolveGCNO_simulation_table_reshape.html#random",
    "href": "posts/2_research/2023-06-25-EvolveGCNO_simulation_table_reshape.html#random",
    "title": "EvolveGCNO_Simulation Tables_reshape",
    "section": "Random",
    "text": "Random\n\npd.merge(data.query(\"dataset=='fivenodes' and mtype=='rand'\").groupby(['mrate','nof_filters','method','lags'])['mse'].mean().reset_index(),\n         data.query(\"dataset=='fivenodes' and mtype=='rand'\").groupby(['mrate','nof_filters','method','lags'])['mse'].std().reset_index(),\n         on=['method','nof_filters','mrate','lags']).rename(columns={'mse_x':'mean','mse_y':'std'}).round(3).query(\"nof_filters==12\")\n\n\n\n\n\n\n\n\nmrate\nnof_filters\nmethod\nlags\nmean\nstd\n\n\n\n\n0\n0.7\n12\nIT-STGCN\n2\n1.173\n0.048\n\n\n1\n0.7\n12\nSTGCN\n2\n1.201\n0.064\n\n\n2\n0.8\n12\nIT-STGCN\n2\n1.209\n0.073\n\n\n3\n0.8\n12\nSTGCN\n2\n1.216\n0.058"
  },
  {
    "objectID": "posts/2_research/2023-06-25-EvolveGCNO_simulation_table_reshape.html#block",
    "href": "posts/2_research/2023-06-25-EvolveGCNO_simulation_table_reshape.html#block",
    "title": "EvolveGCNO_Simulation Tables_reshape",
    "section": "Block",
    "text": "Block\n\npd.merge(data.query(\"dataset=='fivenodes' and mtype=='block'\").groupby(['mrate','nof_filters','method'])['mse'].mean().reset_index(),\n         data.query(\"dataset=='fivenodes' and mtype=='block'\").groupby(['mrate','nof_filters','method'])['mse'].std().reset_index(),\n         on=['method','nof_filters','mrate']).rename(columns={'mse_x':'mean','mse_y':'std'}).round(3)\n\n\n\n\n\n\n\n\nmrate\nnof_filters\nmethod\nmean\nstd\n\n\n\n\n0\n0.125\n12\nIT-STGCN\n1.165\n0.051\n\n\n1\n0.125\n12\nSTGCN\n1.185\n0.061"
  },
  {
    "objectID": "posts/2_research/2023-06-25-EvolveGCNO_simulation_table_reshape.html#baseline-1",
    "href": "posts/2_research/2023-06-25-EvolveGCNO_simulation_table_reshape.html#baseline-1",
    "title": "EvolveGCNO_Simulation Tables_reshape",
    "section": "Baseline",
    "text": "Baseline\n\npd.merge(data.query(\"dataset=='chickenpox' and mtype!='rand' and mtype!='block'\").groupby(['nof_filters','method'])['mse'].mean().reset_index(),\n         data.query(\"dataset=='chickenpox' and mtype!='rand' and mtype!='block'\").groupby(['nof_filters','method'])['mse'].std().reset_index(),\n         on=['method','nof_filters']).rename(columns={'mse_x':'mean','mse_y':'std'}).round(3)\n\n\n\n\n\n\n\n\nnof_filters\nmethod\nmean\nstd\n\n\n\n\n0\n32\nIT-STGCN\n0.984\n0.016\n\n\n1\n32\nSTGCN\n0.988\n0.019"
  },
  {
    "objectID": "posts/2_research/2023-06-25-EvolveGCNO_simulation_table_reshape.html#random-1",
    "href": "posts/2_research/2023-06-25-EvolveGCNO_simulation_table_reshape.html#random-1",
    "title": "EvolveGCNO_Simulation Tables_reshape",
    "section": "Random",
    "text": "Random\n\npd.merge(data.query(\"dataset=='chickenpox' and mtype=='rand'\").groupby(['mrate','inter_method','nof_filters','method'])['mse'].mean().reset_index(),\n         data.query(\"dataset=='chickenpox' and mtype=='rand'\").groupby(['mrate','inter_method','nof_filters','method'])['mse'].std().reset_index(),\n         on=['method','inter_method','mrate','nof_filters']).rename(columns={'mse_x':'mean','mse_y':'std'}).round(3)\n\n\n\n\n\n\n\n\nmrate\ninter_method\nnof_filters\nmethod\nmean\nstd\n\n\n\n\n0\n0.3\nlinear\n32\nIT-STGCN\n0.998\n0.019\n\n\n1\n0.3\nlinear\n32\nSTGCN\n1.054\n0.011\n\n\n2\n0.8\nlinear\n32\nIT-STGCN\n1.161\n0.054\n\n\n3\n0.8\nlinear\n32\nSTGCN\n1.234\n0.096"
  },
  {
    "objectID": "posts/2_research/2023-06-25-EvolveGCNO_simulation_table_reshape.html#block-1",
    "href": "posts/2_research/2023-06-25-EvolveGCNO_simulation_table_reshape.html#block-1",
    "title": "EvolveGCNO_Simulation Tables_reshape",
    "section": "Block",
    "text": "Block\n\npd.merge(data.query(\"dataset=='chickenpox' and mtype=='block'\").groupby(['inter_method','mrate','nof_filters','method'])['mse'].mean().reset_index(),\n         data.query(\"dataset=='chickenpox' and mtype=='block'\").groupby(['inter_method','mrate','nof_filters','method'])['mse'].std().reset_index(),\n         on=['method','inter_method','mrate','nof_filters']).rename(columns={'mse_x':'mean','mse_y':'std'})\n\n\n\n\n\n\n\n\ninter_method\nmrate\nnof_filters\nmethod\nmean\nstd\n\n\n\n\n0\nlinear\n0.28777\n32\nIT-STGCN\n1.002350\n0.015102\n\n\n1\nlinear\n0.28777\n32\nSTGCN\n1.027605\n0.015945\n\n\n2\nnearest\n0.28777\n32\nIT-STGCN\n0.998713\n0.021721\n\n\n3\nnearest\n0.28777\n32\nSTGCN\n1.025797\n0.014844"
  },
  {
    "objectID": "posts/2_research/2023-06-25-EvolveGCNO_simulation_table_reshape.html#baseline-2",
    "href": "posts/2_research/2023-06-25-EvolveGCNO_simulation_table_reshape.html#baseline-2",
    "title": "EvolveGCNO_Simulation Tables_reshape",
    "section": "Baseline",
    "text": "Baseline\n\npd.merge(data.query(\"dataset=='pedalme' and mtype!='rand' and mtype!='block'\").groupby(['lags','nof_filters','method'])['mse'].mean().reset_index(),\n         data.query(\"dataset=='pedalme' and mtype!='rand' and mtype!='block'\").groupby(['lags','nof_filters','method'])['mse'].std().reset_index(),\n         on=['method','lags','nof_filters']).rename(columns={'mse_x':'mean','mse_y':'std'}).round(3).query(\"lags==4\")\n\n\n\n\n\n\n\n\nlags\nnof_filters\nmethod\nmean\nstd\n\n\n\n\n0\n4\n2\nIT-STGCN\n1.213\n0.045\n\n\n1\n4\n2\nSTGCN\n1.234\n0.055"
  },
  {
    "objectID": "posts/2_research/2023-06-25-EvolveGCNO_simulation_table_reshape.html#random-2",
    "href": "posts/2_research/2023-06-25-EvolveGCNO_simulation_table_reshape.html#random-2",
    "title": "EvolveGCNO_Simulation Tables_reshape",
    "section": "Random",
    "text": "Random\n\npd.merge(data.query(\"dataset=='pedalme' and mtype=='rand'\").groupby(['mrate','lags','inter_method','method'])['mse'].mean().reset_index(),\n         data.query(\"dataset=='pedalme' and mtype=='rand'\").groupby(['mrate','lags','inter_method','method'])['mse'].std().reset_index(),\n         on=['method','mrate','lags','inter_method']).rename(columns={'mse_x':'mean','mse_y':'std'}).round(3)\n\n\n\n\n\n\n\n\nmrate\nlags\ninter_method\nmethod\nmean\nstd\n\n\n\n\n0\n0.3\n4\nlinear\nIT-STGCN\n1.251\n0.072\n\n\n1\n0.3\n4\nlinear\nSTGCN\n1.267\n0.072\n\n\n2\n0.3\n4\nnearest\nIT-STGCN\n1.251\n0.057\n\n\n3\n0.3\n4\nnearest\nSTGCN\n1.265\n0.056\n\n\n4\n0.6\n4\nlinear\nIT-STGCN\n1.280\n0.065\n\n\n5\n0.6\n4\nlinear\nSTGCN\n1.305\n0.092\n\n\n6\n0.6\n4\nnearest\nIT-STGCN\n1.267\n0.067\n\n\n7\n0.6\n4\nnearest\nSTGCN\n1.292\n0.075"
  },
  {
    "objectID": "posts/2_research/2023-06-25-EvolveGCNO_simulation_table_reshape.html#block-2",
    "href": "posts/2_research/2023-06-25-EvolveGCNO_simulation_table_reshape.html#block-2",
    "title": "EvolveGCNO_Simulation Tables_reshape",
    "section": "Block",
    "text": "Block\n\npd.merge(data.query(\"dataset=='pedalme' and mtype=='block'\").groupby(['mrate','lags','inter_method','method'])['mse'].mean().reset_index(),\n         data.query(\"dataset=='pedalme' and mtype=='block'\").groupby(['mrate','lags','inter_method','method'])['mse'].std().reset_index(),\n         on=['method','mrate','lags','inter_method']).rename(columns={'mse_x':'mean','mse_y':'std'}).round(3).query(\"lags==4\")\n\n\n\n\n\n\n\n\nmrate\nlags\ninter_method\nmethod\nmean\nstd\n\n\n\n\n0\n0.286\n4\nlinear\nIT-STGCN\n1.246\n0.034\n\n\n1\n0.286\n4\nlinear\nSTGCN\n1.230\n0.056\n\n\n2\n0.286\n4\nnearest\nIT-STGCN\n1.245\n0.045\n\n\n3\n0.286\n4\nnearest\nSTGCN\n1.246\n0.035"
  },
  {
    "objectID": "posts/2_research/2023-06-25-EvolveGCNO_simulation_table_reshape.html#w_st",
    "href": "posts/2_research/2023-06-25-EvolveGCNO_simulation_table_reshape.html#w_st",
    "title": "EvolveGCNO_Simulation Tables_reshape",
    "section": "W_st",
    "text": "W_st\n\npd.merge(data_pedal2.query(\"mtype=='rand'\").groupby(['mrate','lags','inter_method','method'])['mse'].mean().reset_index(),\n         data_pedal2.query(\"mtype=='rand'\").groupby(['mrate','lags','inter_method','method'])['mse'].std().reset_index(),\n         on=['method','mrate','lags','inter_method']).rename(columns={'mse_x':'mean','mse_y':'std'}).round(3).query(\"lags==4\")\n\n\n\n\n\n\n\n\nmrate\nlags\ninter_method\nmethod\nmean\nstd\n\n\n\n\n0\n0.3\n4\nlinear\nIT-STGCN\n1.223\n0.041\n\n\n1\n0.3\n4\nlinear\nSTGCN\n1.263\n0.048\n\n\n2\n0.3\n4\nnearest\nIT-STGCN\n1.234\n0.046\n\n\n3\n0.3\n4\nnearest\nSTGCN\n1.252\n0.071\n\n\n4\n0.6\n4\nlinear\nIT-STGCN\n1.269\n0.092\n\n\n5\n0.6\n4\nlinear\nSTGCN\n1.304\n0.061\n\n\n6\n0.6\n4\nnearest\nIT-STGCN\n1.248\n0.072\n\n\n7\n0.6\n4\nnearest\nSTGCN\n1.321\n0.094\n\n\n\n\n\n\n\n\npd.merge(data_pedal2.query(\"mtype=='block'\").groupby(['mrate','lags','inter_method','method'])['mse'].mean().reset_index(),\n         data_pedal2.query(\"mtype=='block'\").groupby(['mrate','lags','inter_method','method'])['mse'].std().reset_index(),\n         on=['method','mrate','lags','inter_method']).rename(columns={'mse_x':'mean','mse_y':'std'}).round(3).query(\"lags==4\")\n\n\n\n\n\n\n\n\nmrate\nlags\ninter_method\nmethod\nmean\nstd\n\n\n\n\n0\n0.286\n4\nlinear\nIT-STGCN\n1.204\n0.033\n\n\n1\n0.286\n4\nlinear\nSTGCN\n1.210\n0.058\n\n\n2\n0.286\n4\nnearest\nIT-STGCN\n1.211\n0.033\n\n\n3\n0.286\n4\nnearest\nSTGCN\n1.241\n0.095"
  },
  {
    "objectID": "posts/2_research/2023-06-25-EvolveGCNO_simulation_table_reshape.html#baseline-3",
    "href": "posts/2_research/2023-06-25-EvolveGCNO_simulation_table_reshape.html#baseline-3",
    "title": "EvolveGCNO_Simulation Tables_reshape",
    "section": "Baseline",
    "text": "Baseline\n\npd.merge(data.query(\"dataset=='wikimath' and mrate==0\").groupby(['lags','nof_filters','method'])['mse'].mean().reset_index(),\n         data.query(\"dataset=='wikimath' and mrate==0\").groupby(['lags','nof_filters','method'])['mse'].std().reset_index(),\n         on=['lags','nof_filters','method']).rename(columns={'mse_x':'mean','mse_y':'std'}).round(3)\n\n\n\n\n\n\n\n\nlags\nnof_filters\nmethod\nmean\nstd\n\n\n\n\n0\n8\n12\nIT-STGCN\n0.735\n0.023\n\n\n1\n8\n12\nSTGCN\n0.734\n0.025"
  },
  {
    "objectID": "posts/2_research/2023-06-25-EvolveGCNO_simulation_table_reshape.html#random-3",
    "href": "posts/2_research/2023-06-25-EvolveGCNO_simulation_table_reshape.html#random-3",
    "title": "EvolveGCNO_Simulation Tables_reshape",
    "section": "Random",
    "text": "Random\n\npd.merge(data.query(\"dataset=='wikimath' and mtype=='rand'\").groupby(['mrate','lags','method'])['mse'].mean().reset_index(),\n         data.query(\"dataset=='wikimath' and mtype=='rand'\").groupby(['mrate','lags','method'])['mse'].std().reset_index(),\n         on=['method','mrate','lags']).rename(columns={'mse_x':'mean','mse_y':'std'}).round(3)\n\n\n\n\n\n\n\n\nmrate\nlags\nmethod\nmean\nstd\n\n\n\n\n0\n0.3\n8\nIT-STGCN\n0.738\n0.018\n\n\n1\n0.3\n8\nSTGCN\n0.743\n0.024\n\n\n2\n0.5\n8\nIT-STGCN\n0.744\n0.021\n\n\n3\n0.5\n8\nSTGCN\n0.759\n0.021\n\n\n4\n0.6\n8\nIT-STGCN\n0.745\n0.019\n\n\n5\n0.6\n8\nSTGCN\n0.775\n0.026\n\n\n6\n0.8\n8\nIT-STGCN\n0.780\n0.027\n\n\n7\n0.8\n8\nSTGCN\n0.863\n0.038"
  },
  {
    "objectID": "posts/2_research/2023-06-25-EvolveGCNO_simulation_table_reshape.html#block-3",
    "href": "posts/2_research/2023-06-25-EvolveGCNO_simulation_table_reshape.html#block-3",
    "title": "EvolveGCNO_Simulation Tables_reshape",
    "section": "Block",
    "text": "Block\n\npd.merge(data.query(\"dataset=='wikimath' and mtype=='block'\").groupby(['mrate','lags','method'])['mse'].mean().reset_index(),\n         data.query(\"dataset=='wikimath' and mtype=='block'\").groupby(['mrate','lags','method'])['mse'].std().reset_index(),\n         on=['method','mrate','lags']).rename(columns={'mse_x':'mean','mse_y':'std'})\n\n\n\n\n\n\n\n\nmrate\nlags\nmethod\nmean\nstd\n\n\n\n\n0\n0.119837\n8\nIT-STGCN\n0.732454\n0.025087\n\n\n1\n0.119837\n8\nSTGCN\n0.734875\n0.021822"
  },
  {
    "objectID": "posts/2_research/2023-06-25-EvolveGCNO_simulation_table_reshape.html#missing-values-on-the-same-nodes",
    "href": "posts/2_research/2023-06-25-EvolveGCNO_simulation_table_reshape.html#missing-values-on-the-same-nodes",
    "title": "EvolveGCNO_Simulation Tables_reshape",
    "section": "missing values on the same nodes",
    "text": "missing values on the same nodes\n\npd.merge(data_wiki_GSO.groupby(['mrate','lags','method'])['mse'].mean().reset_index(),\n        data_wiki_GSO.groupby(['mrate','lags','method'])['mse'].std().reset_index(),\n         on=['method','mrate','lags']).rename(columns={'mse_x':'mean','mse_y':'std'}).round(3)\n\n\n\n\n\n\n\n\nmrate\nlags\nmethod\nmean\nstd\n\n\n\n\n0\n0.512\n8\nIT-STGCN\n0.745\n0.017\n\n\n1\n0.512\n8\nSTGCN\n0.753\n0.026"
  },
  {
    "objectID": "posts/2_research/2023-06-25-EvolveGCNO_simulation_table_reshape.html#baseline-4",
    "href": "posts/2_research/2023-06-25-EvolveGCNO_simulation_table_reshape.html#baseline-4",
    "title": "EvolveGCNO_Simulation Tables_reshape",
    "section": "Baseline",
    "text": "Baseline\n\npd.merge(data.query(\"dataset=='windmillsmall' and mrate==0\").groupby(['lags','method'])['mse'].mean().reset_index(),\n         data.query(\"dataset=='windmillsmall' and mrate==0\").groupby(['lags','method'])['mse'].std().reset_index(),\n         on=['method','lags']).rename(columns={'mse_x':'mean','mse_y':'std'}).round(3)\n\n\n\n\n\n\n\n\nlags\nmethod\nmean\nstd\n\n\n\n\n0\n8\nIT-STGCN\n0.984\n0.001\n\n\n1\n8\nSTGCN\n0.983\n0.001"
  },
  {
    "objectID": "posts/2_research/2023-06-25-EvolveGCNO_simulation_table_reshape.html#random-4",
    "href": "posts/2_research/2023-06-25-EvolveGCNO_simulation_table_reshape.html#random-4",
    "title": "EvolveGCNO_Simulation Tables_reshape",
    "section": "Random",
    "text": "Random\n\npd.merge(data.query(\"dataset=='windmillsmall' and mtype=='rand'\").groupby(['mrate','lags','method'])['mse'].mean().reset_index(),\n         data.query(\"dataset=='windmillsmall' and mtype=='rand'\").groupby(['mrate','lags','method'])['mse'].std().reset_index(),\n         on=['method','mrate','lags']).rename(columns={'mse_x':'mean','mse_y':'std'}).round(3)\n\n\n\n\n\n\n\n\nmrate\nlags\nmethod\nmean\nstd\n\n\n\n\n0\n0.7\n8\nIT-STGCN\n1.149\n0.026\n\n\n1\n0.7\n8\nSTGCN\n1.495\n0.137"
  },
  {
    "objectID": "posts/2_research/2023-06-25-EvolveGCNO_simulation_table_reshape.html#block-4",
    "href": "posts/2_research/2023-06-25-EvolveGCNO_simulation_table_reshape.html#block-4",
    "title": "EvolveGCNO_Simulation Tables_reshape",
    "section": "Block",
    "text": "Block\n\npd.merge(data.query(\"dataset=='windmillsmall' and mtype=='block'\").groupby(['mrate','lags','method'])['mse'].mean().reset_index(),\n         data.query(\"dataset=='windmillsmall' and mtype=='block'\").groupby(['mrate','lags','method'])['mse'].std().reset_index(),\n         on=['method','mrate','lags']).rename(columns={'mse_x':'mean','mse_y':'std'}).round(3)\n\n\n\n\n\n\n\n\nmrate\nlags\nmethod\nmean\nstd\n\n\n\n\n0\n0.081\n8\nIT-STGCN\n0.983\n0.002\n\n\n1\n0.081\n8\nSTGCN\n0.990\n0.002"
  },
  {
    "objectID": "posts/2_research/2023-06-25-EvolveGCNO_simulation_table_reshape.html#baseline-5",
    "href": "posts/2_research/2023-06-25-EvolveGCNO_simulation_table_reshape.html#baseline-5",
    "title": "EvolveGCNO_Simulation Tables_reshape",
    "section": "Baseline",
    "text": "Baseline\n\npd.merge(data.query(\"dataset=='monte' and mrate==0\").groupby(['lags','method'])['mse'].mean().reset_index(),\n         data.query(\"dataset=='monte' and mrate==0\").groupby(['lags','method'])['mse'].std().reset_index(),\n         on=['method','lags']).rename(columns={'mse_x':'mean','mse_y':'std'}).round(3)\n\n\n\n\n\n\n\n\nlags\nmethod\nmean\nstd\n\n\n\n\n0\n4\nIT-STGCN\n1.317\n0.118\n\n\n1\n4\nSTGCN\n0.997\n0.004"
  },
  {
    "objectID": "posts/2_research/2023-06-25-EvolveGCNO_simulation_table_reshape.html#random-5",
    "href": "posts/2_research/2023-06-25-EvolveGCNO_simulation_table_reshape.html#random-5",
    "title": "EvolveGCNO_Simulation Tables_reshape",
    "section": "Random",
    "text": "Random\n\npd.merge(data.query(\"dataset=='monte' and mtype=='rand'\").groupby(['mrate','lags','inter_method','method'])['mse'].mean().reset_index(),\n         data.query(\"dataset=='monte' and mtype=='rand'\").groupby(['mrate','lags','inter_method','method'])['mse'].std().reset_index(),\n         on=['mrate','inter_method','method','mrate','lags']).rename(columns={'mse_x':'mean','mse_y':'std'})\n\n\n\n\n\n\n\n\nmrate\nlags\ninter_method\nmethod\nmean\nstd\n\n\n\n\n0\n0.3\n4\nnearest\nIT-STGCN\n1.401606\n0.147293\n\n\n1\n0.3\n4\nnearest\nSTGCN\n1.634467\n0.161082\n\n\n2\n0.5\n4\nnearest\nIT-STGCN\n1.457940\n0.093312\n\n\n3\n0.5\n4\nnearest\nSTGCN\n1.928135\n0.303906\n\n\n4\n0.7\n4\nnearest\nIT-STGCN\n1.968742\n0.235623\n\n\n5\n0.7\n4\nnearest\nSTGCN\n2.447478\n0.499375\n\n\n6\n0.8\n4\nnearest\nIT-STGCN\n2.263371\n0.476410\n\n\n7\n0.8\n4\nnearest\nSTGCN\n2.622998\n0.693321"
  },
  {
    "objectID": "posts/2_research/2023-06-25-EvolveGCNO_simulation_table_reshape.html#block-5",
    "href": "posts/2_research/2023-06-25-EvolveGCNO_simulation_table_reshape.html#block-5",
    "title": "EvolveGCNO_Simulation Tables_reshape",
    "section": "Block",
    "text": "Block\n\npd.merge(data.query(\"dataset=='monte' and mtype=='block'\").groupby(['mrate','lags','inter_method','method'])['mse'].mean().reset_index(),\n         data.query(\"dataset=='monte' and mtype=='block'\").groupby(['mrate','lags','inter_method','method'])['mse'].std().reset_index(),\n         on=['method','mrate','inter_method','lags']).rename(columns={'mse_x':'mean','mse_y':'std'})\n\n\n\n\n\n\n\n\nmrate\nlags\ninter_method\nmethod\nmean\nstd\n\n\n\n\n0\n0.149142\n4\nnearest\nIT-STGCN\n1.345316\n0.110313\n\n\n1\n0.149142\n4\nnearest\nSTGCN\n1.766133\n0.123163"
  },
  {
    "objectID": "posts/2_research/2022-12-28-gcn_simulation.html",
    "href": "posts/2_research/2022-12-28-gcn_simulation.html",
    "title": "Simulation of geometric-temporal",
    "section": "",
    "text": "Simulation\n\nhttps://pytorch-geometric-temporal.readthedocs.io/en/latest/modules/dataset.html#module-torch_geometric_temporal.dataset.chickenpox\n\nimport\n\nimport networkx as nx\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport pandas as pd\nimport torch\n\n\n\n공식 홈페이지 예제\n\ndata\n\nfrom torch_geometric_temporal.dataset import WikiMathsDatasetLoader\nfrom torch_geometric_temporal.signal import temporal_signal_split\n\nloader = WikiMathsDatasetLoader()\n\ndataset = loader.get_dataset(lags=14)\n\ntrain_dataset, test_dataset = temporal_signal_split(dataset, train_ratio=0.5)\n\n\n\nRecurrentGCN\n\nimport torch\nimport torch.nn.functional as F\nfrom torch_geometric_temporal.nn.recurrent import GConvGRU\n\nclass RecurrentGCN(torch.nn.Module):\n    def __init__(self, node_features, filters):\n        super(RecurrentGCN, self).__init__()\n        self.recurrent = GConvGRU(node_features, filters, 2)\n        self.linear = torch.nn.Linear(filters, 1)\n\n    def forward(self, x, edge_index, edge_weight):\n        h = self.recurrent(x, edge_index, edge_weight)\n        h = F.relu(h)\n        h = self.linear(h)\n        return h\n\n\n\nLearn\n\nfrom tqdm import tqdm\n\nmodel = RecurrentGCN(node_features=14, filters=32)\n\noptimizer = torch.optim.Adam(model.parameters(), lr=0.01)\n\nmodel.train()\n\nfor epoch in tqdm(range(1)):\n    for time, snapshot in enumerate(train_dataset):\n        y_hat = model(snapshot.x, snapshot.edge_index, snapshot.edge_attr)\n        cost = torch.mean((y_hat-snapshot.y)**2)\n        cost.backward()\n        optimizer.step()\n        optimizer.zero_grad()\n\n100%|██████████| 1/1 [00:11&lt;00:00, 11.93s/it]\n\n\n\nfor time, snapshot in enumerate(train_dataset):\n    _x = snapshot.x \n    _edge_index = snapshot.edge_index \n    _edge_attr = snapshot.edge_attr\n    _y = snapshot.y\n    break\n\n\n_x.shape\n\ntorch.Size([1068, 14])\n\n\n\nsnapshot.y.shape\n\ntorch.Size([1068])\n\n\n\n1068개의 nodes\n한 개의 node에 mapping된 차원의 수\n\n\n_edge_index.shape\n\ntorch.Size([2, 27079])\n\n\n\n_edge_attr.shape\n\ntorch.Size([27079])\n\n\n\n_y.shape\n\ntorch.Size([1068])\n\n\n\n\n\n우리 예제\n\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport pandas as pd\nimport torch\n\n\nT = 100\nN = 4 # number of Nodes\nE = np.array([[0,1],[1,2],[2,3],[3,0]]).T\nV = np.array([1,2,3,4])\nAMP = np.array([3,2,1,2.2])\nt = np.arange(0,T)\nnode_features = 1\n\n\nf = np.stack([a*np.sin(2*t**2/1000)+np.random.normal(loc=0,scale=0.2,size=T) for a in AMP],axis=1).reshape(T,N,node_features)\nf = torch.tensor(f).float()\n\n\nf.shape\n\ntorch.Size([100, 4, 1])\n\n\n\nX = f[:99,:,:]\ny = f[1:,:,:]\n\n\nplt.plot(y[:,0,0],label=\"v1\")\nplt.plot(y[:,1,0],label=\"v2\")\nplt.plot(y[:,2,0],label=\"v3\")\nplt.plot(y[:,3,0],label=\"v4\")\nplt.legend()\n\n\n\n\n\n\n\n\n\nedge_index = torch.tensor(E)\nedge_attr = torch.tensor(np.array([1,1,1,1]),dtype=torch.float32)\n\n\n_ee = enumerate(zip(X,y))\n\n\nfrom tqdm import tqdm\n\nmodel = RecurrentGCN(node_features=1, filters=4)\n\noptimizer = torch.optim.Adam(model.parameters(), lr=0.01)\n\nmodel.train()\n\nfor epoch in tqdm(range(50)):\n    for time, (xt,yt) in enumerate(zip(X,y)):\n        y_hat = model(xt, edge_index, edge_attr)\n        cost = torch.mean((y_hat-yt)**2)\n        cost.backward()\n        optimizer.step()\n        optimizer.zero_grad()\n\n100%|██████████| 50/50 [00:16&lt;00:00,  3.01it/s]\n\n\n\nyhat = torch.stack([model(xt, edge_index, edge_attr) for xt in X]).detach().numpy()\n\n\nplt.plot(y[:,0,0],label=\"y in V1\")\nplt.plot(yhat[:,0,0],label=\"yhat in V1\")\nplt.legend()\n\n\n\n\n\n\n\n\n\nplt.plot(y[:,1,0],label=\"y in V2\")\nplt.plot(yhat[:,1,0],label=\"yhat in V2\")\nplt.legend()\n\n\n\n\n\n\n\n\n\nplt.plot(y[:,2,0],label=\"y in V3\")\nplt.plot(yhat[:,2,0],label=\"yhat in V3\")\nplt.legend()\n\n\n\n\n\n\n\n\n\nplt.plot(y[:,3,0],label=\"y in V4\")\nplt.plot(yhat[:,3,0],label=\"yhat in V4\")\nplt.legend()\n\n\n\n\n\n\n\n\n\n\nGNAR\n\nimport rpy2\nimport rpy2.robjects as ro \nfrom rpy2.robjects.vectors import FloatVector \nfrom rpy2.robjects.packages import importr\n\n\n%load_ext rpy2.ipython\n\n\n%%R\nlibrary(GNAR)\nlibrary(igraph)\n\nR[write to console]: Loading required package: igraph\n\nR[write to console]: \nAttaching package: ‘igraph’\n\n\nR[write to console]: The following objects are masked from ‘package:stats’:\n\n    decompose, spectrum\n\n\nR[write to console]: The following object is masked from ‘package:base’:\n\n    union\n\n\nR[write to console]: Loading required package: wordcloud\n\nR[write to console]: Loading required package: RColorBrewer\n\n\n\n\n%%R\nsummary(fiveNet)\n\nGNARnet with 5 nodes and 10 edges\n of equal length  1\n\n\n\n%%R\nedges &lt;- as.matrix(fiveNet)\nedges\n\n     [,1] [,2] [,3] [,4] [,5]\n[1,]    0    0    0    1    1\n[2,]    0    0    1    1    0\n[3,]    0    1    0    1    0\n[4,]    1    1    1    0    0\n[5,]    1    0    0    0    0\n\n\n\n%%R\nprint(fiveNet)\n\nGNARnet with 5 nodes \nedges:1--4 1--5 2--3 2--4 3--2 3--4 4--1 4--2 4--3 5--1 \n     \n edges of each of length  1 \n\n\n\n%%R\ndata(\"fiveNode\")\nanswer &lt;- GNARfit(vts = fiveVTS, net = fiveNet, alphaOrder = 2, betaOrder = c(1, 1))\nanswer\n\nModel: \nGNAR(2,[1,1]) \n\nCall:\nlm(formula = yvec ~ dmat + 0)\n\nCoefficients:\n dmatalpha1  dmatbeta1.1   dmatalpha2  dmatbeta2.1  \n    0.20624      0.50277      0.02124     -0.09523  \n\n\n\n\n%%R\nlayout(matrix(c(1,2,3,4), 2, 2, byrow = TRUE))\nplot(fiveVTS[, 1], ylab = \"Node A Time Series\")\nlines(fitted(answer)[, 1], col = 2)\nplot(fiveVTS[, 2], ylab = \"Node B Time Series\")\nlines(fitted(answer)[, 2], col = 2)\nplot(fiveVTS[, 3], ylab = \"Node C Time Series\")\nlines(fitted(answer)[, 3], col = 2)\nplot(fiveVTS[, 4], ylab = \"Node D Time Series\")\nlines(fitted(answer)[, 4], col = 2)\n\n\n\n\n\n\n\n\n\n%R -o fiveVTS\n%R -o edges\n\n\nnode: 5\ntime 200\n\n\nedges_tensor = torch.tensor(edges)\n\n\nnonzero_indices = edges_tensor.nonzero()\n\n\nfiveNet_edge = np.array(nonzero_indices).T\nfiveNet_edge\n\narray([[0, 0, 1, 1, 2, 2, 3, 3, 3, 4],\n       [3, 4, 2, 3, 1, 3, 0, 1, 2, 0]])\n\n\n\nfiveVTS.shape\n\n(200, 5)\n\n\n\nT = 200\nN = 5 # number of Nodes\nE = fiveNet_edge\nV = np.array([1,2,3,4,5])\nt = np.arange(0,T)\nnode_features = 1\n\n\nf = torch.tensor(fiveVTS).reshape(200,5,1).float()\n\n\nX = f[:199,:,:]\ny = f[1:,:,:]\n\n\nedge_index = torch.tensor(E)\nedge_attr = torch.tensor(np.array([1,1,1,1,1,1,1,1,1,1]),dtype=torch.float32)\n\n\n_ee = enumerate(zip(X,y))\n\n\nfrom tqdm import tqdm\n\nmodel = RecurrentGCN(node_features=1, filters=8)\n\noptimizer = torch.optim.Adam(model.parameters(), lr=0.05)\n\nmodel.train()\n\nfor epoch in tqdm(range(50)):\n    for time, (xt,yt) in enumerate(zip(X,y)):\n        y_hat = model(xt, edge_index, edge_attr)\n        cost = torch.mean((y_hat-yt)**2)\n        cost.backward()\n        optimizer.step()\n        optimizer.zero_grad()\n\n100%|█| 50/50 [00:34&lt;00:00,  1.45it/\n\n\n\nyhat = torch.stack([model(xt, edge_index, edge_attr) for xt in X]).detach().numpy()\n\n\nyhat.shape\n\n(199, 5, 1)\n\n\n\nplt.plot(y[:,1])\nplt.plot(yhat[:,1].data)\n\n\n\n\n\n\n\n\n\n\nWind network time series\nthe data suite vswind that contains a number of R objects pertaining to 721 wind speeds taken at each of 102 weather stations in England and Wales. The suite contains the vector time series vswindts, the associated network vswindnet, a character vector of the weather station location names in vswindnames and coordinates of the stations in the two column matrix vswindcoords. The data originate from the UK Met Office site http://wow.metoffice.gov.uk and full details can be found in the vswind help file in the GNAR package.\n\n%%R\noldpar &lt;- par(cex = 0.75)\nwindnetplot()\npar(oldpar)\n\n\n\n\n\n\n\n\n\n%%R\nedges_wind &lt;- as.matrix(vswindnet)\n\n\n%R -o vswindts\n%R -o edges_wind\n\n\nnodes : 102\ntime step : 721\n\n\nvswindts.shape\n\n(721, 102)\n\n\n\nedges_wind.shape\n\n(102, 102)\n\n\n\nedges_winds = torch.tensor(edges_wind)\n\n\nnonzero_indices_wind = edges_winds.nonzero()\n\n\nvswindnet_edge = np.array(nonzero_indices_wind).T\nvswindnet_edge.shape\n\n(2, 202)\n\n\n\nT = 721\nN = 102 # number of Nodes\nE = vswindnet_edge\nV = np.array(range(101))\nt = np.arange(0,T)\nnode_features = 1\n\n\nf = torch.tensor(vswindts).reshape(721,102,1).float()\n\n\nX = f[:720,:,:]\ny = f[1:,:,:]\n\n\nedge_index = torch.tensor(E)\nedge_attr = torch.tensor(np.array([1]*202),dtype=torch.float32)\n\n\n_ee = enumerate(zip(X,y))\n\n\nfrom tqdm import tqdm\n\nmodel = RecurrentGCN(node_features=1, filters=4)\n\noptimizer = torch.optim.Adam(model.parameters(), lr=0.01)\n\nmodel.train()\n\nfor epoch in tqdm(range(50)):\n    for time, (xt,yt) in enumerate(zip(X,y)):\n        y_hat = model(xt, edge_index, edge_attr)\n        cost = torch.mean((y_hat-yt)**2)\n        cost.backward()\n        optimizer.step()\n        optimizer.zero_grad()\n\n100%|██████████| 50/50 [02:16&lt;00:00,  2.73s/it]\n\n\n\nyhat = torch.stack([model(xt, edge_index, edge_attr) for xt in X]).detach().numpy()\n\n\nyhat.shape\n\n(720, 102, 1)\n\n\n\nplt.plot(y[:,1])\nplt.plot(yhat[:,1].data)\n\n\n\n\n\n\n\n\n\n\nOECD GDP\n해당예제는 GNAR 패키지에서 네트워크(엣지)를 맞추는 예제로서 나옴, 그렇기에 네트워크 존재하지 않아 연구 예제로서 사용하지 않을 예정\n이 데이터는 네트워크를 추정하여 fit 및 predict함\nGOP growth rate time series\n\n35 countries from the OECD website\ntime series : 1961 - 2013\nT = 52\nNodes = 35\nIn this data set 20.8% (379 out of 1820) of the observations were missing due to some nodes not being included from the start.\n\n\n%%R\nlibrary(\"fields\")\n\n\n%R -o gdpVTS\n\n\ngdpVTS.shape\n\n(52, 35)\n\n\n\nplt.plot(gdpVTS[:,1])"
  },
  {
    "objectID": "posts/2_research/2023-04-05-Simulation.html",
    "href": "posts/2_research/2023-04-05-Simulation.html",
    "title": "Simulation",
    "section": "",
    "text": "Simulation Study"
  },
  {
    "objectID": "posts/2_research/2023-04-05-Simulation.html#random",
    "href": "posts/2_research/2023-04-05-Simulation.html#random",
    "title": "Simulation",
    "section": "random",
    "text": "random\n\ndf1 = pd.read_csv('./simulation_results/fivenodes/fivenodes_STGCN_ITSTGCN_random_epoch50.csv')\ndf2 = pd.read_csv('./simulation_results/fivenodes/fivenodes_STGCN_ITSTGCN_random_epoch100.csv')\ndf3 = pd.read_csv('./simulation_results/fivenodes/fivenodes_STGCN_ITSTGCN_random_epoch150.csv')\ndf4 = pd.read_csv('./simulation_results/fivenodes/fivenodes_STGCN_ITSTGCN_random_epoch200.csv')\n\n\ndf_gnar = pd.read_csv('./simulation_results/fivenodes/fivenodes_GNAR_random.csv')\n\n\ndata = pd.concat([df1,df2,df3,df4,df_gnar],axis=0)\n\n\ndata.query(\"method!='GNAR' and inter_method=='linear' and lags==2\").plot.box(backend='plotly',x='mrate',color='method',y='mse',facet_col='epoch',facet_row='nof_filters',height=1200)\n\n                                                \n\n\n\n시뮬 예정(평균 시간, 평균mse)\n0.7,0.75,0.8,0.85\n12,16\n150\n\n# 1. mrate = 0.8, filter = 12, epoch = 150\ndata.query(\"mrate==0.8 and inter_method=='linear' and nof_filters==12 and epoch==150 and lags==2\")['calculation_time'].mean(),data.query(\"mrate==0.8 and inter_method=='linear' and nof_filters==12 and epoch==150 and lags==2\")['mse'].mean()\n\n(109.59549897114435, 1.2304790377616883)\n\n\n\ndata.query(\"mrate==0.8 and inter_method=='linear' and nof_filters==12 and epoch==150 and lags==2\").plot.box(backend='plotly',x='mrate',color='method',y='mse',facet_col='epoch',facet_row='nof_filters',height=400)"
  },
  {
    "objectID": "posts/2_research/2023-04-05-Simulation.html#block",
    "href": "posts/2_research/2023-04-05-Simulation.html#block",
    "title": "Simulation",
    "section": "block",
    "text": "block\n\ndf1 = pd.read_csv('./simulation_results/fivenodes/fivenodes_STGCN_ITSTGCN_block_node1_epoch50.csv')\ndf2 = pd.read_csv('./simulation_results/fivenodes/fivenodes_STGCN_ITSTGCN_block_node1_epoch100.csv')\ndf3 = pd.read_csv('./simulation_results/fivenodes/fivenodes_STGCN_ITSTGCN_block_node1_epoch150.csv')\ndf4 = pd.read_csv('./simulation_results/fivenodes/fivenodes_STGCN_ITSTGCN_block_node1_epoch200.csv')\ndf5 = pd.read_csv('./simulation_results/fivenodes/fivenodes_STGCN_ITSTGCN_block_node2_epoch50.csv')\ndf6 = pd.read_csv('./simulation_results/fivenodes/fivenodes_STGCN_ITSTGCN_block_node2_epoch100.csv')\ndf7 = pd.read_csv('./simulation_results/fivenodes/fivenodes_STGCN_ITSTGCN_block_node2_epoch150.csv')\ndf8 = pd.read_csv('./simulation_results/fivenodes/fivenodes_GNAR_block_node1.csv')\ndf9 = pd.read_csv('./simulation_results/fivenodes/fivenodes_GNAR_block_node2.csv')\n\n\ndf1['block']=1\ndf2['block']=1\ndf3['block']=1\ndf4['block']=1\ndf5['block']=2\ndf6['block']=2\ndf7['block']=2\ndf8['block']=1\ndf9['block']=2\n\n\ndata2 = pd.concat([df1,df2,df3,df4,df5,df6,df7,df8,df9],axis=0)\n\n\ndata2.query(\"method=='GNAR' and block == 1\")['mse'].mean(),data2.query(\"method=='GNAR' and block == 2\")['mse'].mean()\n\n(1.455923080444336, 1.5004450678825378)\n\n\n\ndata2.query(\"method=='GNAR' and inter_method == 'linear'\")['mse'].mean(),data2.query(\"method=='GNAR' and inter_method == 'nearest'\")['mse'].mean() # 차이 없음\n\n(1.4813642161233085, 1.4813642161233085)\n\n\n\ndata2.query(\"epoch==50\")['calculation_time'].mean(),data2.query(\"epoch==50\")['calculation_time'].max()\n\n(39.11611335332747, 56.8712797164917)\n\n\n\ndata2.query(\"epoch==150\")['calculation_time'].mean(),data2.query(\"epoch==150\")['calculation_time'].max()\n\n(102.26520284502594, 152.8869686126709)\n\n\n\ndata2.query(\"method!='GNAR' and lags == 2 and inter_method=='nearest'\").plot.box(backend='plotly',x='block',color='method',y='mse',facet_col='epoch',facet_row='nof_filters',height=800)\n\n                                                \n\n\n\ndata2.query(\"inter_method=='linear' and epoch==150\").plot.box(backend='plotly',x='block',color='method',y='mse',facet_col='lags',facet_row='nof_filters',height=1200)\n\n                                                \n\n\n\n시뮬 예정(평균 시간, 평균mse)\nblock 1,2 위 세팅 그대로\n랜덤ㅁ 말고 block만\n\n# 1. block = 2 interpolation = linear, filter = 12, epoch = 150\ndata2.query(\"block==1 and inter_method=='linear' and nof_filters==12 and epoch==50 and lags==2\")['calculation_time'].mean(),data2.query(\"block==2 and inter_method=='linear' and nof_filters==12 and epoch==50 and lags==2\")['mse'].mean()\n\n(40.18422634204229, 1.2096982955932618)\n\n\n\ndata2.query(\"block==1 and inter_method=='linear' and nof_filters==12 and epoch==50 and lags==2\").plot.box(backend='plotly',x='block',color='method',y='mse',facet_col='epoch',facet_row='nof_filters',height=400)"
  },
  {
    "objectID": "posts/2_research/2023-04-05-Simulation.html#random-1",
    "href": "posts/2_research/2023-04-05-Simulation.html#random-1",
    "title": "Simulation",
    "section": "random",
    "text": "random\n\n공식 패키지: lags 4 지정\nmrate = 0.3\n\n결측값 비율 크니까 오차 많이 커지는 경향 있어서\n\nnof_filters = 4\n\n차이 없어서\n\nlags = 4, 8\n\n클 수록 작아지는 경향 있어서\n\nGNAR보다 MSE는 낮음\ncal_time\n\nmean = 10\nmax = 21\n\nblock 은 임의로 한 노드만 해 본 결과임\n\n\ndata = pd.read_csv('./simulation_results/chickenpox_random.csv').sort_values(by='lags')\n\n\ndata.query(\"method!='GNAR'\")['calculation_time'].mean(),data.query(\"method!='GNAR'\")['calculation_time'].max(),data.query(\"method!='GNAR'\")['calculation_time'].min()\n\n(10.42619569649299, 21.886654376983643, 7.567165851593018)\n\n\n\ndata.query(\"method!='GNAR' and inter_method=='cubic' and mrate==0.3\").plot.box(backend='plotly',x='mrate',color='method',y='mse',facet_col='lags',facet_row='nof_filters',height=1200)\n\n                                                \n\n\n\ndata.query(\"method=='GNAR' and inter_method=='linear'\").plot.box(backend='plotly',x='mrate',color='method',y='mse',facet_col='lags',height=600)\n\n                                                \n\n\n\n시뮬 예정(평균 시간, 평균mse)\nepoch = 50\nmrate = 0.3~0.5\nfilter 32 공식예제로 가기 하고 샆으면 3개 정도 추가로\n\n# 1. mrate = 0.3, filter = 4, epoch = 50, lags = 4\ndata.query(\"method !='GNAR' and mrate==0.3 and inter_method=='cubic' and nof_filters==4 and lags==2\")['calculation_time'].mean(),data.query(\"method != 'GNAR' and mrate==0.3 and inter_method=='cubic' and nof_filters==4 and lags==2\")['mse'].mean()\n\n(10.115000387032827, 1.0320488701264063)\n\n\n\ndata.query(\"method !='GNAR' and mrate==0.3 and inter_method=='cubic' and nof_filters==4 and lags==2\").plot.box(backend='plotly',x='mrate',color='method',y='mse',facet_col='lags',facet_row='nof_filters',height=400)"
  },
  {
    "objectID": "posts/2_research/2023-04-05-Simulation.html#block-1",
    "href": "posts/2_research/2023-04-05-Simulation.html#block-1",
    "title": "Simulation",
    "section": "block",
    "text": "block\n\ndata = pd.read_csv('./simulation_results/chickenpox_block.csv')\n\n\ndata.query(\"method != 'GNAR' and lags!=4 and lags!=6 and inter_method !='linear'\").plot.box(backend='plotly',x='nof_filters',color='method',y='mse',facet_col='lags',facet_row='inter_method',height=600)\n\n                                                \n\n\n\ndata.query(\"method=='GNAR'\").plot.box(backend='plotly',x='mrate',color='inter_method',y='mse',facet_col='lags',height=600)\n\n                                                \n\n\n\n시뮬 예정(평균 시간, 평균mse)\nblock, rand 다\n공식예제 수 따라\nepoch 50\n나중에 시간 남으면 100\n\ndata.query(\"inter_method=='cubic' and nof_filters==4 and lags==8\").plot.box(backend='plotly',x='mrate',color='method',y='mse',facet_col='lags',facet_row='nof_filters',height=400)"
  },
  {
    "objectID": "posts/2_research/2023-04-05-Simulation.html#block-2",
    "href": "posts/2_research/2023-04-05-Simulation.html#block-2",
    "title": "Simulation",
    "section": "block",
    "text": "block\n\ndata = pd.read_csv('./simulation_results/pedalme_block.csv');data\n\n\n\n\n\n\n\n\ndataset\nmethod\nmrate\nmtype\nlags\nnof_filters\ninter_method\nepoch\nmse\ncalculation_time\n\n\n\n\n0\npedalme\nIT-STGCN\n0.047619\nblock\n2\n4.0\ncubic\n5.0\n1.229210\n0.758090\n\n\n1\npedalme\nSTGCN\n0.047619\nblock\n2\n12.0\nlinear\n5.0\n1.223644\n0.681700\n\n\n2\npedalme\nSTGCN\n0.047619\nblock\n2\n12.0\ncubic\n5.0\n1.237086\n0.684113\n\n\n3\npedalme\nSTGCN\n0.047619\nblock\n2\n4.0\nlinear\n5.0\n1.225114\n0.659210\n\n\n4\npedalme\nSTGCN\n0.047619\nblock\n2\n4.0\ncubic\n5.0\n1.216191\n0.664208\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n715\npedalme\nIT-STGCN\n0.045977\nblock\n8\n4.0\ncubic\n5.0\n1.425474\n0.640063\n\n\n716\npedalme\nSTGCN\n0.045977\nblock\n8\n12.0\ncubic\n5.0\n1.302402\n0.718187\n\n\n717\npedalme\nSTGCN\n0.045977\nblock\n8\n12.0\nlinear\n5.0\n1.336038\n0.719500\n\n\n718\npedalme\nIT-STGCN\n0.045977\nblock\n8\n12.0\nlinear\n5.0\n1.311962\n0.831888\n\n\n719\npedalme\nIT-STGCN\n0.045977\nblock\n8\n12.0\ncubic\n5.0\n1.315647\n0.667004\n\n\n\n\n720 rows × 10 columns\n\n\n\nmissing rate 조정하기 30~50% 여러개 block 해서\n\ndata.query(\"method!='GNAR'\").plot.box(backend='plotly',x='inter_method',color='method',y='mse',facet_col='lags',facet_row='nof_filters',height=1200)\n\n                                                \n\n\n\n시뮬 예정(평균 시간, 평균mse)\n\ndata.query(\"inter_method=='linear' and nof_filters==12 and lags==4\").plot.box(backend='plotly',x='mrate',color='method',y='mse',facet_col='lags',facet_row='nof_filters',height=400)"
  },
  {
    "objectID": "posts/2_research/2023-04-05-Simulation.html#random-3",
    "href": "posts/2_research/2023-04-05-Simulation.html#random-3",
    "title": "Simulation",
    "section": "random",
    "text": "random\n\ndf1 = pd.read_csv('./simulation_results/2023-04-15_16-58-03.csv')\ndf2 = pd.read_csv('./simulation_results/2023-04-15_17-01-39.csv')\ndf3 = pd.read_csv('./simulation_results/2023-04-15_17-07-23.csv')\ndf4 = pd.read_csv('./simulation_results/2023-04-15_17-13-13.csv')\ndf5 = pd.read_csv('./simulation_results/2023-04-15_17-29-49.csv')\n\n\ndata = pd.concat([df1,df2,df3,df4,df5],axis=0)\n\n\ndata.query(\"method=='STGCN'\").sort_values(['mrate','lags','nof_filters'])\n\n\n\n\n\n\n\n\ndataset\nmethod\nmrate\nmtype\nlags\nnof_filters\ninter_method\nepoch\nmse\ncalculation_time\n\n\n\n\n0\nwikimath\nSTGCN\n0.3\nrand\n4\n12\nlinear\n1\n0.863623\n25.504817\n\n\n0\nwikimath\nSTGCN\n0.3\nrand\n4\n12\ncubic\n1\n0.847675\n27.086116\n\n\n0\nwikimath\nSTGCN\n0.4\nrand\n2\n12\nlinear\n1\n0.912734\n30.048937\n\n\n1\nwikimath\nSTGCN\n0.4\nrand\n2\n12\ncubic\n1\n0.916843\n27.104823\n\n\n0\nwikimath\nSTGCN\n0.4\nrand\n4\n12\nlinear\n1\n0.907305\n24.776503\n\n\n1\nwikimath\nSTGCN\n0.4\nrand\n4\n12\ncubic\n1\n0.854127\n24.608104\n\n\n2\nwikimath\nSTGCN\n0.4\nrand\n8\n12\nlinear\n1\n0.788011\n24.233431\n\n\n3\nwikimath\nSTGCN\n0.4\nrand\n8\n12\ncubic\n1\n0.795219\n24.228026\n\n\n0\nwikimath\nSTGCN\n0.5\nrand\n4\n12\nlinear\n1\n0.914080\n26.301605\n\n\n1\nwikimath\nSTGCN\n0.5\nrand\n4\n12\ncubic\n1\n0.975948\n27.855870\n\n\n\n\n\n\n\n\ndata.query(\"method!='STGCN'\").sort_values(['mrate','lags','nof_filters'])\n\n\n\n\n\n\n\n\ndataset\nmethod\nmrate\nmtype\nlags\nnof_filters\ninter_method\nepoch\nmse\ncalculation_time\n\n\n\n\n1\nwikimath\nIT-STGCN\n0.3\nrand\n4\n12\nlinear\n1\n0.908916\n28.928112\n\n\n1\nwikimath\nIT-STGCN\n0.3\nrand\n4\n12\ncubic\n1\n0.856639\n29.759748\n\n\n4\nwikimath\nIT-STGCN\n0.4\nrand\n2\n12\nlinear\n1\n0.864580\n29.660712\n\n\n5\nwikimath\nIT-STGCN\n0.4\nrand\n2\n12\ncubic\n1\n0.926426\n30.838968\n\n\n2\nwikimath\nIT-STGCN\n0.4\nrand\n4\n12\nlinear\n1\n0.871146\n29.008776\n\n\n3\nwikimath\nIT-STGCN\n0.4\nrand\n4\n12\ncubic\n1\n0.905354\n30.405766\n\n\n6\nwikimath\nIT-STGCN\n0.4\nrand\n8\n12\nlinear\n1\n0.822462\n32.329447\n\n\n7\nwikimath\nIT-STGCN\n0.4\nrand\n8\n12\ncubic\n1\n0.817621\n29.447260\n\n\n2\nwikimath\nIT-STGCN\n0.5\nrand\n4\n12\nlinear\n1\n0.878943\n31.140878\n\n\n3\nwikimath\nIT-STGCN\n0.5\nrand\n4\n12\ncubic\n1\n1.002361\n28.461372\n\n\n\n\n\n\n\n\ndata.query(\"method!='GNAR'\")['calculation_time'].mean(),data.query(\"method!='GNAR'\")['calculation_time'].max(),data.query(\"method!='GNAR'\")['calculation_time'].min()\n\n\ndata.query(\"mtype=='rand' and mrate != 0.9 and method!='GNAR' and inter_method=='cubic'\").plot.box(backend='plotly',x='mrate',color='method',y='mse',facet_col='lags',facet_row='nof_filters',height=1200)\n\n\ndata.query(\"mtype=='rand' and method!='GNAR' and inter_method=='linear'\").plot.box(backend='plotly',x='mrate',color='method',y='mse',facet_col='lags',facet_row='nof_filters',height=1200)\n\n\n시뮬 예정(평균 시간, 평균mse)\n\ndata.query(\"method !='GNAR' and mrate==0.3 and inter_method=='cubic' and nof_filters==12 and lags==8\")['calculation_time'].mean(),data.query(\"method !='GNAR' and mrate==0.3 and inter_method=='cubic' and nof_filters==12 and lags==8\")['mse'].mean()\n\n\ndata.query(\"method !='GNAR' and mrate==0.3 and inter_method=='linear' and nof_filters==12 and lags==8\")['calculation_time'].mean(),data.query(\"method !='GNAR' and mrate==0.3 and inter_method=='linear' and nof_filters==12 and lags==8\")['mse'].mean()\n\n\ndata.query(\"method !='GNAR' and mrate==0.3 and nof_filters==12 and lags==8\").plot.box(backend='plotly',x='mrate',color='method',y='mse',facet_col='lags',facet_row='nof_filters',height=400)"
  },
  {
    "objectID": "posts/2_research/2023-04-05-Simulation.html#block-3",
    "href": "posts/2_research/2023-04-05-Simulation.html#block-3",
    "title": "Simulation",
    "section": "block",
    "text": "block\n\ndata = pd.read_csv('./simulation_results/wiki_block.csv');data\n\n\ndata.query(\"method!='GNAR'\")['calculation_time'].mean(),data.query(\"method!='GNAR'\")['calculation_time'].max(),data.query(\"method!='GNAR'\")['calculation_time'].min()\n\n\ndata.query(\"method!='GNAR' and inter_method=='cubic'\").plot.box(backend='plotly',x='mrate',color='method',y='mse',facet_col='lags',facet_row='nof_filters',height=1200)\n\n\ndata.query(\"inter_method=='linear'\").plot.box(backend='plotly',x='mrate',color='method',y='mse',facet_col='lags',facet_row='nof_filters',height=1200)\n\n\n시뮬 예정(평균 시간, 평균mse)\n\ndata.query(\"inter_method=='linear' and nof_filters==12 and lags==8\")['calculation_time'].mean(),data.query(\"inter_method=='linear' and nof_filters==12 and lags==8\")['mse'].mean()\n\n\ndata.query(\"inter_method=='linear' and nof_filters==12 and lags==8\").plot.box(backend='plotly',x='mrate',color='method',y='mse',facet_col='lags',facet_row='nof_filters',height=400)"
  },
  {
    "objectID": "posts/2_research/2023-04-05-Simulation.html#random-4",
    "href": "posts/2_research/2023-04-05-Simulation.html#random-4",
    "title": "Simulation",
    "section": "random",
    "text": "random"
  },
  {
    "objectID": "posts/2_research/2023-04-05-Simulation.html#block-4",
    "href": "posts/2_research/2023-04-05-Simulation.html#block-4",
    "title": "Simulation",
    "section": "block",
    "text": "block"
  },
  {
    "objectID": "posts/2_research/2023-04-05-Simulation.html#random-5",
    "href": "posts/2_research/2023-04-05-Simulation.html#random-5",
    "title": "Simulation",
    "section": "random",
    "text": "random"
  },
  {
    "objectID": "posts/2_research/2023-04-05-Simulation.html#block-5",
    "href": "posts/2_research/2023-04-05-Simulation.html#block-5",
    "title": "Simulation",
    "section": "block",
    "text": "block"
  },
  {
    "objectID": "posts/2_research/2023-05-11-PyGGeometricTemporalEx.html",
    "href": "posts/2_research/2023-05-11-PyGGeometricTemporalEx.html",
    "title": "PyG Geometric Temporal Examples",
    "section": "",
    "text": "Examples\n\nRefer: https://github.com/benedekrozemberczki/pytorch_geometric_temporal/tree/master/examples/recurrent\n\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport pandas as pd\nimport torch\n\n\nT = 250\nt = np.arange(T)/T * 5\n\nx = 1*np.sin(2*t)+0.3*np.random.rand(T)+0.5+np.sin(4*t)+1.5*np.sin(8*t)\neps_x  = np.random.normal(size=T)*0\ny = x.copy()\nfor i in range(2,T):\n    y[i] = 0.35*x[i-1] - 0.15*x[i-2] + 0.5*np.cos(0.4*t[i]) \neps_y  = np.random.normal(size=T)*0\nx = x\ny = y\nplt.plot(t,x,color='C0',lw=5)\nplt.plot(t,x+eps_x,alpha=0.5,color='C0')\nplt.plot(t,y,color='C1',lw=5)\nplt.plot(t,y+eps_y,alpha=0.5,color='C1')\n_node_ids = {'node1':0, 'node2':1}\n\n_FX1 = np.stack([x+eps_x,y+eps_y],axis=1).tolist()\n\n_edges1 = torch.tensor([[0,1]]).tolist()\n\ndata_dict = {'edges':_edges1, 'node_ids':_node_ids, 'FX':_FX1}\n\n# save_data(data_dict1, './data/toy_example1.pkl')\n\ndata = pd.DataFrame({'x':x,'y':y,'xer':x,'yer':y})\n\n# save_data(data1, './data/toy_example_true1.csv')\n\n\n\n\n\n\n\n\n\nimport itstgcn\n\n\nloader = itstgcn.DatasetLoader(data_dict)\n\n\nGConvGRU(Done)\n\nGConvGRU?\n\n\nInit signature:\nGConvGRU(\n    in_channels: int,\n    out_channels: int,\n    K: int,\n    normalization: str = 'sym',\n    bias: bool = True,\n)\nDocstring:     \nAn implementation of the Chebyshev Graph Convolutional Gated Recurrent Unit\nCell. For details see this paper: `\"Structured Sequence Modeling with Graph\nConvolutional Recurrent Networks.\" &lt;https://arxiv.org/abs/1612.07659&gt;`_\nArgs:\n    in_channels (int): Number of input features.\n    out_channels (int): Number of output features.\n    K (int): Chebyshev filter size :math:`K`.\n    normalization (str, optional): The normalization scheme for the graph\n        Laplacian (default: :obj:`\"sym\"`):\n        1. :obj:`None`: No normalization\n        :math:`\\mathbf{L} = \\mathbf{D} - \\mathbf{A}`\n        2. :obj:`\"sym\"`: Symmetric normalization\n        :math:`\\mathbf{L} = \\mathbf{I} - \\mathbf{D}^{-1/2} \\mathbf{A}\n        \\mathbf{D}^{-1/2}`\n        3. :obj:`\"rw\"`: Random-walk normalization\n        :math:`\\mathbf{L} = \\mathbf{I} - \\mathbf{D}^{-1} \\mathbf{A}`\n        You need to pass :obj:`lambda_max` to the :meth:`forward` method of\n        this operator in case the normalization is non-symmetric.\n        :obj:`\\lambda_max` should be a :class:`torch.Tensor` of size\n        :obj:`[num_graphs]` in a mini-batch scenario and a\n        scalar/zero-dimensional tensor when operating on single graphs.\n        You can pre-compute :obj:`lambda_max` via the\n        :class:`torch_geometric.transforms.LaplacianLambdaMax` transform.\n    bias (bool, optional): If set to :obj:`False`, the layer will not learn\n        an additive bias. (default: :obj:`True`)\nInit docstring: Initializes internal Module state, shared by both nn.Module and ScriptModule.\nFile:           ~/anaconda3/envs/temp_csy/lib/python3.8/site-packages/torch_geometric_temporal/nn/recurrent/gconv_gru.py\nType:           type\nSubclasses:     \n\n\n\n\n\n# from torch_geometric_temporal.dataset import WikiMathsDatasetLoader\nfrom torch_geometric_temporal.signal import temporal_signal_split\n\n# loader = WikiMathsDatasetLoader()\n\ndataset = loader.get_dataset(lags=4)\n\n# train_dataset, test_dataset = temporal_signal_split(dataset, train_ratio=0.5)\n\ntrain_dataset, test_dataset = temporal_signal_split(dataset, train_ratio=0.2)\n\n\nimport torch\nimport torch.nn.functional as F\nfrom torch_geometric_temporal.nn.recurrent import GConvGRU\n\nclass RecurrentGCN(torch.nn.Module):\n    def __init__(self, node_features, filters):\n        super(RecurrentGCN, self).__init__()\n        self.recurrent = GConvGRU(node_features, filters, 2)\n        self.linear = torch.nn.Linear(filters, 1)\n\n    def forward(self, x, edge_index, edge_weight):\n        h = self.recurrent(x, edge_index, edge_weight)\n        h = F.relu(h)\n        h = self.linear(h)\n        return h\n\n\nfrom tqdm import tqdm\n\nmodel = RecurrentGCN(node_features=4, filters=32)\n\noptimizer = torch.optim.Adam(model.parameters(), lr=0.01)\n\nmodel.train()\n\nfor epoch in tqdm(range(50)):# 50\n    _b=[]\n    _d=[]\n    for time, snapshot in enumerate(train_dataset):\n        y_hat = model(snapshot.x, snapshot.edge_index, snapshot.edge_attr).reshape(-1)\n        _b.append(y_hat)\n        mean_diff = torch.mean((y_hat-snapshot.y), dim=0)\n        cost = torch.square(mean_diff)\n        _d.append(cost)\n        cost.backward()\n        optimizer.step()\n        optimizer.zero_grad()\n\n100%|██████████| 50/50 [00:16&lt;00:00,  3.00it/s]\n\n\n\nmodel.eval()\ncost = 0\n_a = []\n_a1 = []\nfor time, snapshot in enumerate(test_dataset):\n    y_hat = model(snapshot.x, snapshot.edge_index, snapshot.edge_attr).reshape(-1)\n    cost = cost + torch.mean((y_hat-snapshot.y)**2)\n    _a.append(y_hat)\n    _a1.append(cost)\ncost = cost / (time+1)\ncost = cost.item()\nprint(\"MSE: {:.4f}\".format(cost))\n\nMSE: 0.4200\n\n\n\n_e = [_d[i].detach() for i in range(len(_d))]\n\n\n_c = [_a1[i].detach() for i in range(len(_a1))]\n\n\nfig, (( ax1,ax2),(ax3,ax4),(ax5,ax6)) = plt.subplots(3,2,figsize=(30,20))\n\nax1.set_title('train node1')\nax1.plot([train_dataset.targets[i][0] for i in range(train_dataset.snapshot_count)])\nax1.plot(torch.tensor([_b[i].detach()[0] for i in range(train_dataset.snapshot_count)]))\n\nax2.set_title('test node1')\nax2.plot([test_dataset.targets[i][0] for i in range(test_dataset.snapshot_count)])\nax2.plot(torch.tensor([_a[i].detach()[0] for i in range(test_dataset.snapshot_count)]))\n\nax3.set_title('train node2')\nax3.plot([train_dataset.targets[i][1] for i in range(train_dataset.snapshot_count)])\nax3.plot(torch.tensor([_b[i].detach()[1] for i in range(train_dataset.snapshot_count)]))\n\n\nax4.set_title('test node2')\nax4.plot([test_dataset.targets[i][1] for i in range(test_dataset.snapshot_count)])\nax4.plot(torch.tensor([_a[i].detach()[1] for i in range(test_dataset.snapshot_count)]))\n\nax5.set_title('train cost')\nax5.plot(_e)\n\nax6.set_title('test cost')\nax6.plot(_c)\n\n\n\n\n\n\n\n\n\n\nA3GCN2\n\n# import numpy as np\n# import matplotlib.pyplot as plt\n# import seaborn as sns\n\n\n# import torch\n# import torch.nn.functional as F\n# from torch_geometric.nn import GCNConv\n# from torch_geometric_temporal.nn.recurrent import A3TGCN2\n\n\n# # GPU support\n# DEVICE = torch.device('cuda') # cuda\n# shuffle=True\n# batch_size = 32\n\n\n#Dataset\n#Traffic forecasting dataset based on Los Angeles Metropolitan traffic\n#207 loop detectors on highways\n#March 2012 - June 2012\n#From the paper: Diffusion Convolutional Recurrent Neural Network\n\n\n# from torch_geometric_temporal.dataset import METRLADatasetLoader\n# loader = METRLADatasetLoader()\n# dataset = loader.get_dataset(num_timesteps_in=12, num_timesteps_out=12)\n\n\n# # Visualize traffic over time\n# sensor_number = 1\n# hours = 24\n# sensor_labels = [bucket.y[sensor_number][0].item() for bucket in list(dataset)[:hours]]\n# plt.plot(sensor_labels)\n\n\n# # Train test split \n\n# from torch_geometric_temporal.signal import temporal_signal_split\n# train_dataset, test_dataset = temporal_signal_split(dataset, train_ratio=0.8)\n\n\n# # Creating Dataloaders\n\n# train_input = np.array(train_dataset.features) # (27399, 207, 2, 12)\n# train_target = np.array(train_dataset.targets) # (27399, 207, 12)\n# train_x_tensor = torch.from_numpy(train_input).type(torch.FloatTensor).to(DEVICE)  # (B, N, F, T)\n# train_target_tensor = torch.from_numpy(train_target).type(torch.FloatTensor).to(DEVICE)  # (B, N, T)\n# train_dataset_new = torch.utils.data.TensorDataset(train_x_tensor, train_target_tensor)\n# train_loader = torch.utils.data.DataLoader(train_dataset_new, batch_size=batch_size, shuffle=shuffle,drop_last=True)\n\n\n# test_input = np.array(test_dataset.features) # (, 207, 2, 12)\n# test_target = np.array(test_dataset.targets) # (, 207, 12)\n# test_x_tensor = torch.from_numpy(test_input).type(torch.FloatTensor).to(DEVICE)  # (B, N, F, T)\n# test_target_tensor = torch.from_numpy(test_target).type(torch.FloatTensor).to(DEVICE)  # (B, N, T)\n# test_dataset_new = torch.utils.data.TensorDataset(test_x_tensor, test_target_tensor)\n# test_loader = torch.utils.data.DataLoader(test_dataset_new, batch_size=batch_size, shuffle=shuffle,drop_last=True)\n\n\n# # Making the model \n# class TemporalGNN(torch.nn.Module):\n#     def __init__(self, node_features, periods, batch_size):\n#         super(TemporalGNN, self).__init__()\n#         # Attention Temporal Graph Convolutional Cell\n#         self.tgnn = A3TGCN2(in_channels=node_features,  out_channels=32, periods=periods,batch_size=batch_size) # node_features=2, periods=12\n#         # Equals single-shot prediction\n#         self.linear = torch.nn.Linear(32, periods)\n\n#     def forward(self, x, edge_index):\n#         \"\"\"\n#         x = Node features for T time steps\n#         edge_index = Graph edge indices\n#         \"\"\"\n#         h = self.tgnn(x, edge_index) # x [b, 207, 2, 12]  returns h [b, 207, 12]\n#         h = F.relu(h) \n#         h = self.linear(h)\n#         return h\n\n\n# TemporalGNN(node_features=2, periods=12, batch_size=2)\n\n\n# # Create model and optimizers\n# model = TemporalGNN(node_features=2, periods=12, batch_size=batch_size).to(DEVICE)\n# optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n# loss_fn = torch.nn.MSELoss()\n\n\n# print('Net\\'s state_dict:')\n# total_param = 0\n# for param_tensor in model.state_dict():\n#     print(param_tensor, '\\t', model.state_dict()[param_tensor].size())\n#     total_param += np.prod(model.state_dict()[param_tensor].size())\n# print('Net\\'s total params:', total_param)\n# #--------------------------------------------------\n# print('Optimizer\\'s state_dict:')  # If you notice here the Attention is a trainable parameter\n# for var_name in optimizer.state_dict():\n#     print(var_name, '\\t', optimizer.state_dict()[var_name])\n\n\n# # Loading the graph once because it's a static graph\n\n# for snapshot in train_dataset:\n#     static_edge_index = snapshot.edge_index.to(DEVICE)\n#     break;\n\n\n# # Training the model \n# model.train()\n\n# for epoch in range(3): # 30\n#     step = 0\n#     loss_list = []\n#     for encoder_inputs, labels in train_loader:\n#         y_hat = model(encoder_inputs, static_edge_index)         # Get model predictions\n#         loss = loss_fn(y_hat, labels) # Mean squared error #loss = torch.mean((y_hat-labels)**2)  sqrt to change it to rmse\n#         loss.backward()\n#         optimizer.step()\n#         optimizer.zero_grad()\n#         step= step+ 1\n#         loss_list.append(loss.item())\n#         if step % 100 == 0 :\n#             print(sum(loss_list)/len(loss_list))\n#     print(\"Epoch {} train RMSE: {:.4f}\".format(epoch, sum(loss_list)/len(loss_list)))\n\n\n## Evaluation\n\n#- Lets get some sample predictions for a specific horizon (e.g. 288/12 = 24 hours)\n#- The model always gets one hour and needs to predict the next hour\n\n\n# model.eval()\n# step = 0\n# # Store for analysis\n# total_loss = []\n# for encoder_inputs, labels in test_loader:\n#     # Get model predictions\n#     y_hat = model(encoder_inputs, static_edge_index)\n#     # Mean squared error\n#     loss = loss_fn(y_hat, labels)\n#     total_loss.append(loss.item())\n#     # Store for analysis below\n#     #test_labels.append(labels)\n#     #predictions.append(y_hat)\n\n\n# print(\"Test MSE: {:.4f}\".format(sum(total_loss)/len(total_loss)))\n\n\n## Visualization\n\n# - The further away the point in time is, the worse the predictions get\n# - Predictions shape: [num_data_points, num_sensors, num_timesteps]\n\n\n# sensor = 123\n# timestep = 11 \n# preds = np.asarray([pred[sensor][timestep].detach().cpu().numpy() for pred in y_hat])\n# labs  = np.asarray([label[sensor][timestep].cpu().numpy() for label in labels])\n# print(\"Data points:,\", preds.shape)\n\n\n# plt.figure(figsize=(20,5))\n# sns.lineplot(data=preds, label=\"pred\")\n# sns.lineplot(data=labs, label=\"true\")\n\n\n\nA3GCN(cuda 문제)\n\n# try:\n#     from tqdm import tqdm\n# except ImportError:\n#     def tqdm(iterable):\n#         return iterable\n\n\n# # import torch\n# # import torch.nn.functional as F\n# from torch_geometric_temporal.nn.recurrent import A3TGCN\n\n\n# # from torch_geometric_temporal.dataset import ChickenpoxDatasetLoader\n# from torch_geometric_temporal.signal import temporal_signal_split\n\n\n# # loader = ChickenpoxDatasetLoader()\n\n# dataset = loader.get_dataset(lags=4)\n\n# train_dataset, test_dataset = temporal_signal_split(dataset, train_ratio=0.2)\n\n\n# class RecurrentGCN(torch.nn.Module):\n#     def __init__(self, node_features, periods):\n#         super(RecurrentGCN, self).__init__()\n#         self.recurrent = A3TGCN(node_features, 32, periods)\n#         self.linear = torch.nn.Linear(32, 1)\n\n#     def forward(self, x, edge_index, edge_weight):\n#         h = self.recurrent(x.to(\"cuda:0\").view(x.shape[0], 1, x.shape[1]), edge_index, edge_weight)\n#         h = F.relu(h)\n#         h = self.linear(h)\n#         return h\n\n\n# model = RecurrentGCN(node_features = 4, periods = 4)\n\n# optimizer = torch.optim.Adam(model.parameters(), lr=0.01)\n\n# model.train()\n\n# for epoch in tqdm(range(50)):\n#     cost = 0\n#     for time, snapshot in enumerate(train_dataset):\n#         y_hat = model(snapshot.x, snapshot.edge_index, snapshot.edge_attr)\n#         cost = cost + torch.mean((y_hat-snapshot.y)**2)\n#     cost = cost / (time+1)\n#     cost.backward()\n#     optimizer.step()\n#     optimizer.zero_grad()\n\n\n# model.eval()\n# cost = 0\n# for time, snapshot in enumerate(test_dataset):\n#     y_hat = model(snapshot.x, snapshot.edge_index, snapshot.edge_attr)\n#     cost = cost + torch.mean((y_hat-snapshot.y)**2)\n# cost = cost / (time+1)\n# cost = cost.item()\n# print(\"MSE: {:.4f}\".format(cost))\n\n\n\nAGCRN\n\nAGCRN?\n\n\nInit signature:\nAGCRN(\n    number_of_nodes: int,\n    in_channels: int,\n    out_channels: int,\n    K: int,\n    embedding_dimensions: int,\n)\nDocstring:     \nAn implementation of the Adaptive Graph Convolutional Recurrent Unit.\nFor details see: `\"Adaptive Graph Convolutional Recurrent Network\nfor Traffic Forecasting\" &lt;https://arxiv.org/abs/2007.02842&gt;`_\nArgs:\n    number_of_nodes (int): Number of vertices.\n    in_channels (int): Number of input features.\n    out_channels (int): Number of output features.\n    K (int): Filter size :math:`K`.\n    embedding_dimensions (int): Number of node embedding dimensions.\nInit docstring: Initializes internal Module state, shared by both nn.Module and ScriptModule.\nFile:           ~/anaconda3/envs/temp_csy/lib/python3.8/site-packages/torch_geometric_temporal/nn/recurrent/agcrn.py\nType:           type\nSubclasses:     \n\n\n\n\n\ntry:\n    from tqdm import tqdm\nexcept ImportError:\n    def tqdm(iterable):\n        return iterable\n\n\nimport torch\nimport torch.nn.functional as F\nfrom torch_geometric_temporal.nn.recurrent import AGCRN\n\nfrom torch_geometric_temporal.dataset import ChickenpoxDatasetLoader\nfrom torch_geometric_temporal.signal import temporal_signal_split\n\nloader1 = ChickenpoxDatasetLoader()\n\ndataset = loader1.get_dataset(lags=8)\n\ntrain_dataset, test_dataset = temporal_signal_split(dataset, train_ratio=0.2)\n\n\nclass RecurrentGCN(torch.nn.Module):\n    def __init__(self, node_features,number_of_nodes):\n        super(RecurrentGCN, self).__init__()\n        self.recurrent = AGCRN(number_of_nodes = 20,\n                              in_channels = node_features,\n                              out_channels = 2,\n                              K = 2,\n                              embedding_dimensions = 4)\n        self.linear = torch.nn.Linear(2, 1)\n\n    def forward(self, x, e, h):\n        h_0 = self.recurrent(x, e, h)\n        y = F.relu(h_0)\n        y = self.linear(y)\n        return y, h_0\n\ntorch.nn.init.xavier_uniform_(e) 가중치 초기화\n\ne = torch.empty(20, 4)\n\n\ne\n\ntensor([[ 2.3516e+23,  3.0646e-41,  1.2073e+23,  3.0646e-41],\n        [ 1.2839e+00,  4.5579e-41,  0.0000e+00,  0.0000e+00],\n        [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n        [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n        [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n        [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n        [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n        [ 3.1494e-02,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n        [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n        [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n        [ 1.2162e-01,  3.0726e-01,  5.5876e+24,  3.0646e-41],\n        [        nan,  4.5912e-41,         nan,  4.5912e-41],\n        [ 0.0000e+00,  0.0000e+00, -5.1905e-35,  4.5579e-41],\n        [ 4.5606e-01,  1.7410e-01,  4.3082e-01,  4.4134e-01],\n        [ 4.2527e-01,  3.9021e-01,  6.6931e-02,  1.1973e-03],\n        [ 8.9360e-02,  3.5911e-02,  3.8786e-02,  3.9897e-01],\n        [ 4.7854e-01,  0.0000e+00,  1.4659e-01,  3.2289e-01],\n        [ 4.2682e-01,  9.1125e-02,  1.1351e-43,  0.0000e+00],\n        [ 8.2566e+26,  3.0646e-41, -7.3231e+36,  4.5579e-41],\n        [ 3.2420e-02,  3.5085e-02,  2.4460e-02,  2.4794e-01]])\n\n\n\ntorch.nn.init.xavier_uniform_(e)\n\ntensor([[-0.1886, -0.1182, -0.2437,  0.4621],\n        [-0.2045, -0.0095, -0.2639, -0.3215],\n        [-0.3641,  0.1362, -0.2829,  0.3273],\n        [ 0.1198, -0.0813,  0.2029,  0.1687],\n        [ 0.2984, -0.3694,  0.2065, -0.4666],\n        [ 0.2634, -0.4748,  0.2762, -0.1667],\n        [-0.1677,  0.3808,  0.1978, -0.4734],\n        [-0.3368, -0.1218, -0.4826, -0.0898],\n        [ 0.1866,  0.0516, -0.4581,  0.0136],\n        [-0.2521, -0.3840, -0.2820,  0.0543],\n        [ 0.4000, -0.1176, -0.3463, -0.3728],\n        [-0.0128, -0.1869, -0.2293,  0.3790],\n        [-0.4311, -0.1795, -0.3970,  0.2133],\n        [-0.0487,  0.3308, -0.1300, -0.2409],\n        [ 0.4507, -0.3846,  0.1356, -0.3181],\n        [ 0.3372, -0.2599, -0.4767,  0.0201],\n        [-0.4959,  0.0642, -0.0844, -0.2929],\n        [-0.1447, -0.3859,  0.4434, -0.2623],\n        [ 0.0794,  0.2285, -0.1525,  0.4936],\n        [ 0.2819, -0.1921,  0.3888, -0.2040]])\n\n\n\ne\n\ntensor([[-0.1886, -0.1182, -0.2437,  0.4621],\n        [-0.2045, -0.0095, -0.2639, -0.3215],\n        [-0.3641,  0.1362, -0.2829,  0.3273],\n        [ 0.1198, -0.0813,  0.2029,  0.1687],\n        [ 0.2984, -0.3694,  0.2065, -0.4666],\n        [ 0.2634, -0.4748,  0.2762, -0.1667],\n        [-0.1677,  0.3808,  0.1978, -0.4734],\n        [-0.3368, -0.1218, -0.4826, -0.0898],\n        [ 0.1866,  0.0516, -0.4581,  0.0136],\n        [-0.2521, -0.3840, -0.2820,  0.0543],\n        [ 0.4000, -0.1176, -0.3463, -0.3728],\n        [-0.0128, -0.1869, -0.2293,  0.3790],\n        [-0.4311, -0.1795, -0.3970,  0.2133],\n        [-0.0487,  0.3308, -0.1300, -0.2409],\n        [ 0.4507, -0.3846,  0.1356, -0.3181],\n        [ 0.3372, -0.2599, -0.4767,  0.0201],\n        [-0.4959,  0.0642, -0.0844, -0.2929],\n        [-0.1447, -0.3859,  0.4434, -0.2623],\n        [ 0.0794,  0.2285, -0.1525,  0.4936],\n        [ 0.2819, -0.1921,  0.3888, -0.2040]])\n\n\n\nmodel = RecurrentGCN(node_features = 8,number_of_nodes=20)\n\noptimizer = torch.optim.Adam(model.parameters(), lr=0.01)\n\nmodel.train()\n\ne = torch.empty(20, 4)\n\ntorch.nn.init.xavier_uniform_(e)\n\nfor epoch in tqdm(range(50)):\n    cost = 0\n    h = None\n    _b=[]\n    _d=[]\n    for time, snapshot in enumerate(train_dataset):\n        x = snapshot.x.view(1, 20, 8)\n        y_hat, h = model(x, e, h)\n        y_hat = y_hat.reshape(-1)\n        cost = cost + torch.mean((y_hat-snapshot.y)**2)\n        _b.append(y_hat)\n        _d.append(cost)\n    cost = cost / (time+1)\n    cost.backward()\n    optimizer.step()\n    optimizer.zero_grad()\n\n100%|██████████| 50/50 [00:11&lt;00:00,  4.26it/s]\n\n\n\nmodel.eval()\ncost = 0\n_a=[]\n_a1=[]\nfor time, snapshot in enumerate(test_dataset):\n    x = snapshot.x.view(1, 20, 8)\n    y_hat, h = model(x, e, h)\n    y_hat = y_hat.reshape(-1)\n    cost = cost + torch.mean((y_hat-snapshot.y)**2)\n    _a.append(y_hat)\n    _a1.append(cost)\ncost = cost / (time+1)\ncost = cost.item()\nprint(\"MSE: {:.4f}\".format(cost))\n\nMSE: 1.1103\n\n\n\n_e = [_d[i].detach() for i in range(len(_d))]\n\n\n_c = [_a1[i].detach() for i in range(len(_a1))]\n\n\nfig, (( ax1,ax2),(ax3,ax4),(ax5,ax6)) = plt.subplots(3,2,figsize=(30,20))\n\nax1.set_title('train node1')\nax1.plot([train_dataset.targets[i][0] for i in range(train_dataset.snapshot_count)])\nax1.plot(torch.tensor([_b[i].detach()[0] for i in range(train_dataset.snapshot_count)]))\n\nax2.set_title('test node1')\nax2.plot([test_dataset.targets[i][0] for i in range(test_dataset.snapshot_count)])\nax2.plot(torch.tensor([_a[i].detach()[0] for i in range(test_dataset.snapshot_count)]))\n\nax3.set_title('train node2')\nax3.plot([train_dataset.targets[i][1] for i in range(train_dataset.snapshot_count)])\nax3.plot(torch.tensor([_b[i].detach()[1] for i in range(train_dataset.snapshot_count)]))\n\n\nax4.set_title('test node2')\nax4.plot([test_dataset.targets[i][1] for i in range(test_dataset.snapshot_count)])\nax4.plot(torch.tensor([_a[i].detach()[1] for i in range(test_dataset.snapshot_count)]))\n\nax5.set_title('train cost')\nax5.plot(_e)\n\nax6.set_title('test cost')\nax6.plot(_c)\n\n\n\n\n\n\n\n\n\n\nDCRNN(Done)\n\nDCRNN?\n\n\nInit signature: DCRNN(in_channels: int, out_channels: int, K: int, bias: bool = True)\nDocstring:     \nAn implementation of the Diffusion Convolutional Gated Recurrent Unit.\nFor details see: `\"Diffusion Convolutional Recurrent Neural Network:\nData-Driven Traffic Forecasting\" &lt;https://arxiv.org/abs/1707.01926&gt;`_\nArgs:\n    in_channels (int): Number of input features.\n    out_channels (int): Number of output features.\n    K (int): Filter size :math:`K`.\n    bias (bool, optional): If set to :obj:`False`, the layer\n        will not learn an additive bias (default :obj:`True`)\nInit docstring: Initializes internal Module state, shared by both nn.Module and ScriptModule.\nFile:           ~/anaconda3/envs/temp_csy/lib/python3.8/site-packages/torch_geometric_temporal/nn/recurrent/dcrnn.py\nType:           type\nSubclasses:     \n\n\n\n\n\ntry:\n    from tqdm import tqdm\nexcept ImportError:\n    def tqdm(iterable):\n        return iterable\n\n\n# import torch\n# import torch.nn.functional as F\nfrom torch_geometric_temporal.nn.recurrent import DCRNN\n\n# from torch_geometric_temporal.dataset import ChickenpoxDatasetLoader\nfrom torch_geometric_temporal.signal import temporal_signal_split\n\n# loader = ChickenpoxDatasetLoader()\n\ndataset = loader.get_dataset()\n\ntrain_dataset, test_dataset = temporal_signal_split(dataset, train_ratio=0.2)\n\n\nclass RecurrentGCN(torch.nn.Module):\n    def __init__(self, node_features):\n        super(RecurrentGCN, self).__init__()\n        self.recurrent = DCRNN(node_features, 32, 1)\n        self.linear = torch.nn.Linear(32, 1)\n\n    def forward(self, x, edge_index, edge_weight):\n        h = self.recurrent(x, edge_index, edge_weight)\n        h = F.relu(h)\n        h = self.linear(h)\n        return h\n\n\nmodel = RecurrentGCN(node_features = 4)\n\noptimizer = torch.optim.Adam(model.parameters(), lr=0.01)\n\nmodel.train()\n\nfor epoch in tqdm(range(200)):\n    cost = 0\n    _b=[]\n    _d=[]\n    for time, snapshot in enumerate(train_dataset):\n        y_hat = model(snapshot.x, snapshot.edge_index, snapshot.edge_attr).reshape(-1)\n        cost = cost + torch.mean((y_hat-snapshot.y)**2)\n        _b.append(y_hat)\n        _d.append(cost)\n    cost = cost / (time+1)\n    cost.backward()\n    optimizer.step()\n    optimizer.zero_grad()\n\n100%|██████████| 200/200 [00:21&lt;00:00,  9.44it/s]\n\n\n\nmodel.eval()\ncost = 0\n_a = []\n_a1=[]\nfor time, snapshot in enumerate(test_dataset):\n    y_hat = model(snapshot.x, snapshot.edge_index, snapshot.edge_attr).reshape(-1)\n    cost = cost + torch.mean((y_hat-snapshot.y)**2)\n    _a.append(y_hat)\n    _a1.append(cost)\ncost = cost / (time+1)\ncost = cost.item()\nprint(\"MSE: {:.4f}\".format(cost))\n\nMSE: 0.1927\n\n\n\n_e = [_d[i].detach() for i in range(len(_d))]\n\n\n_c = [_a1[i].detach() for i in range(len(_a1))]\n\n\nfig, (( ax1,ax2),(ax3,ax4),(ax5,ax6)) = plt.subplots(3,2,figsize=(30,20))\n\nax1.set_title('train node1')\nax1.plot([train_dataset.targets[i][0] for i in range(train_dataset.snapshot_count)])\nax1.plot(torch.tensor([_b[i].detach()[0] for i in range(train_dataset.snapshot_count)]))\n\nax2.set_title('test node1')\nax2.plot([test_dataset.targets[i][0] for i in range(test_dataset.snapshot_count)])\nax2.plot(torch.tensor([_a[i].detach()[0] for i in range(test_dataset.snapshot_count)]))\n\nax3.set_title('train node2')\nax3.plot([train_dataset.targets[i][1] for i in range(train_dataset.snapshot_count)])\nax3.plot(torch.tensor([_b[i].detach()[1] for i in range(train_dataset.snapshot_count)]))\n\n\nax4.set_title('test node2')\nax4.plot([test_dataset.targets[i][1] for i in range(test_dataset.snapshot_count)])\nax4.plot(torch.tensor([_a[i].detach()[1] for i in range(test_dataset.snapshot_count)]))\n\nax5.set_title('train cost')\nax5.plot(_e)\n\nax6.set_title('test cost')\nax6.plot(_c)\n\n\n\n\n\n\n\n\n\n\nDYGRENCODER(Done)\n\nDyGrEncoder?\n\n\nInit signature:\nDyGrEncoder(\n    conv_out_channels: int,\n    conv_num_layers: int,\n    conv_aggr: str,\n    lstm_out_channels: int,\n    lstm_num_layers: int,\n)\nDocstring:     \nAn implementation of the integrated Gated Graph Convolution Long Short\nTerm Memory Layer. For details see this paper: `\"Predictive Temporal Embedding\nof Dynamic Graphs.\" &lt;https://ieeexplore.ieee.org/document/9073186&gt;`_\nArgs:\n    conv_out_channels (int): Number of output channels for the GGCN.\n    conv_num_layers (int): Number of Gated Graph Convolutions.\n    conv_aggr (str): Aggregation scheme to use\n        (:obj:`\"add\"`, :obj:`\"mean\"`, :obj:`\"max\"`).\n    lstm_out_channels (int): Number of LSTM channels.\n    lstm_num_layers (int): Number of neurons in LSTM.\nInit docstring: Initializes internal Module state, shared by both nn.Module and ScriptModule.\nFile:           ~/anaconda3/envs/temp_csy/lib/python3.8/site-packages/torch_geometric_temporal/nn/recurrent/dygrae.py\nType:           type\nSubclasses:     \n\n\n\n\n\ntry:\n    from tqdm import tqdm\nexcept ImportError:\n    def tqdm(iterable):\n        return iterable\n\n\n# import torch\n# import torch.nn.functional as F\nfrom torch_geometric_temporal.nn.recurrent import DyGrEncoder\n\n# from torch_geometric_temporal.dataset import ChickenpoxDatasetLoader\nfrom torch_geometric_temporal.signal import temporal_signal_split\n\n# loader = ChickenpoxDatasetLoader()\n\ndataset = loader.get_dataset(lags=4)\n\ntrain_dataset, test_dataset = temporal_signal_split(dataset, train_ratio=0.2)\n\n\nclass RecurrentGCN(torch.nn.Module):\n    def __init__(self, node_features):\n        super(RecurrentGCN, self).__init__()\n        self.recurrent = DyGrEncoder(conv_out_channels=4, conv_num_layers=1, conv_aggr=\"mean\", lstm_out_channels=32, lstm_num_layers=1)\n        self.linear = torch.nn.Linear(32, 1)\n\n    def forward(self, x, edge_index, edge_weight, h_0, c_0):\n        h, h_0, c_0 = self.recurrent(x, edge_index, edge_weight, h_0, c_0)\n        h = F.relu(h)\n        h = self.linear(h)\n        return h, h_0, c_0\n\n\nmodel = RecurrentGCN(node_features = 4)\n\noptimizer = torch.optim.Adam(model.parameters(), lr=0.01)\n\nmodel.train()\n\nfor epoch in tqdm(range(200)):\n    cost = 0\n    h, c = None, None\n    _b=[]\n    _d=[]\n    for time, snapshot in enumerate(train_dataset):\n        y_hat, h, c = model(snapshot.x, snapshot.edge_index, snapshot.edge_attr, h, c)\n        y_hat = y_hat.reshape(-1)\n        cost = cost + torch.mean((y_hat-snapshot.y)**2)\n        _b.append(y_hat)\n        _d.append(cost)\n    cost = cost / (time+1)\n    cost.backward()\n    optimizer.step()\n    optimizer.zero_grad()\n\n100%|██████████| 200/200 [00:20&lt;00:00,  9.58it/s]\n\n\n\nmodel.eval()\ncost = 0\nh, c = None, None\n_a=[]\n_a1=[]\nfor time, snapshot in enumerate(test_dataset):\n    y_hat, h, c = model(snapshot.x, snapshot.edge_index, snapshot.edge_attr, h, c)\n    y_hat = y_hat.reshape(-1)\n    cost = cost + torch.mean((y_hat-snapshot.y)**2)\n    _a.append(y_hat)\n    _a1.append(cost)\ncost = cost / (time+1)\ncost = cost.item()\nprint(\"MSE: {:.4f}\".format(cost))\n\nMSE: 0.4587\n\n\n\n_e = [_d[i].detach() for i in range(len(_d))]\n\n\n_c = [_a1[i].detach() for i in range(len(_a1))]\n\n\nfig, (( ax1,ax2),(ax3,ax4),(ax5,ax6)) = plt.subplots(3,2,figsize=(30,20))\n\nax1.set_title('train node1')\nax1.plot([train_dataset.targets[i][0] for i in range(train_dataset.snapshot_count)])\nax1.plot(torch.tensor([_b[i].detach()[0] for i in range(train_dataset.snapshot_count)]))\n\nax2.set_title('test node1')\nax2.plot([test_dataset.targets[i][0] for i in range(test_dataset.snapshot_count)])\nax2.plot(torch.tensor([_a[i].detach()[0] for i in range(test_dataset.snapshot_count)]))\n\nax3.set_title('train node2')\nax3.plot([train_dataset.targets[i][1] for i in range(train_dataset.snapshot_count)])\nax3.plot(torch.tensor([_b[i].detach()[1] for i in range(train_dataset.snapshot_count)]))\n\n\nax4.set_title('test node2')\nax4.plot([test_dataset.targets[i][1] for i in range(test_dataset.snapshot_count)])\nax4.plot(torch.tensor([_a[i].detach()[1] for i in range(test_dataset.snapshot_count)]))\n\nax5.set_title('train cost')\nax5.plot(_e)\n\nax6.set_title('test cost')\nax6.plot(_c)\n\n\n\n\n\n\n\n\n\n\nEvolveGCNH(Done)\n\nEvolveGCNH?\n\n\nInit signature:\nEvolveGCNH(\n    num_of_nodes: int,\n    in_channels: int,\n    improved: bool = False,\n    cached: bool = False,\n    normalize: bool = True,\n    add_self_loops: bool = True,\n)\nDocstring:     \nAn implementation of the Evolving Graph Convolutional Hidden Layer.\nFor details see this paper: `\"EvolveGCN: Evolving Graph Convolutional\nNetworks for Dynamic Graph.\" &lt;https://arxiv.org/abs/1902.10191&gt;`_\nArgs:\n    num_of_nodes (int): Number of vertices.\n    in_channels (int): Number of filters.\n    improved (bool, optional): If set to :obj:`True`, the layer computes\n        :math:`\\mathbf{\\hat{A}}` as :math:`\\mathbf{A} + 2\\mathbf{I}`.\n        (default: :obj:`False`)\n    cached (bool, optional): If set to :obj:`True`, the layer will cache\n        the computation of :math:`\\mathbf{\\hat{D}}^{-1/2} \\mathbf{\\hat{A}}\n        \\mathbf{\\hat{D}}^{-1/2}` on first execution, and will use the\n        cached version for further executions.\n        This parameter should only be set to :obj:`True` in transductive\n        learning scenarios. (default: :obj:`False`)\n    normalize (bool, optional): Whether to add self-loops and apply\n        symmetric normalization. (default: :obj:`True`)\n    add_self_loops (bool, optional): If set to :obj:`False`, will not add\n        self-loops to the input graph. (default: :obj:`True`)\nInit docstring: Initializes internal Module state, shared by both nn.Module and ScriptModule.\nFile:           ~/anaconda3/envs/temp_csy/lib/python3.8/site-packages/torch_geometric_temporal/nn/recurrent/evolvegcnh.py\nType:           type\nSubclasses:     \n\n\n\n\n\ntry:\n    from tqdm import tqdm\nexcept ImportError:\n    def tqdm(iterable):\n        return iterable\n\n\n# import torch\n# import torch.nn.functional as F\nfrom torch_geometric_temporal.nn.recurrent import EvolveGCNH\n\nfrom torch_geometric_temporal.dataset import ChickenpoxDatasetLoader\nfrom torch_geometric_temporal.signal import temporal_signal_split\n\nloader1 = ChickenpoxDatasetLoader()\n\ndataset = loader1.get_dataset(lags=4)\n\ntrain_dataset, test_dataset = temporal_signal_split(dataset, train_ratio=0.2)\n\n\nclass RecurrentGCN(torch.nn.Module):\n    def __init__(self, num_of_nodes, in_channels):\n        super(RecurrentGCN, self).__init__()\n        self.recurrent = EvolveGCNH(num_of_nodes, in_channels)\n        self.linear = torch.nn.Linear(in_channels, 1)\n\n    def forward(self, x, edge_index, edge_weight):\n        h = self.recurrent(x, edge_index, edge_weight)\n        h = F.relu(h)\n        h = self.linear(h)\n        return h\n\n\nmodel = RecurrentGCN(num_of_nodes = 20,in_channels = 4)\n\noptimizer = torch.optim.Adam(model.parameters(), lr=0.01)\n\nmodel.train()\n\nfor epoch in tqdm(range(200)):\n    cost = 0\n    _b=[]\n    _d=[]\n    for time, snapshot in enumerate(train_dataset):\n        y_hat = model(snapshot.x, snapshot.edge_index, snapshot.edge_attr).reshape(-1)\n        cost = cost + torch.mean((y_hat-snapshot.y)**2)\n        _b.append(y_hat)\n        _d.append(cost)\n    cost = cost / (time+1)\n    cost.backward()\n    optimizer.step()\n    optimizer.zero_grad()\n\n100%|██████████| 200/200 [00:33&lt;00:00,  5.96it/s]\n\n\n\nmodel.eval()\ncost = 0\n_a=[]\n_a1=[]\nfor time, snapshot in enumerate(test_dataset):\n    y_hat = model(snapshot.x, snapshot.edge_index, snapshot.edge_attr).reshape(-1)\n    cost = cost + torch.mean((y_hat-snapshot.y)**2)\n    _a.append(y_hat)\n    _a1.append(cost)\ncost = cost / (time+1)\ncost = cost.item()\nprint(\"MSE: {:.4f}\".format(cost))\n\nMSE: 0.9995\n\n\n\n_e = [_d[i].detach() for i in range(len(_d))]\n\n\n_c = [_a1[i].detach() for i in range(len(_a1))]\n\n\nfig, (( ax1,ax2),(ax3,ax4),(ax5,ax6)) = plt.subplots(3,2,figsize=(30,20))\n\nax1.set_title('train node1')\nax1.plot([train_dataset.targets[i][0] for i in range(train_dataset.snapshot_count)])\nax1.plot(torch.tensor([_b[i].detach()[0] for i in range(train_dataset.snapshot_count)]))\n\nax2.set_title('test node1')\nax2.plot([test_dataset.targets[i][0] for i in range(test_dataset.snapshot_count)])\nax2.plot(torch.tensor([_a[i].detach()[0] for i in range(test_dataset.snapshot_count)]))\n\nax3.set_title('train node2')\nax3.plot([train_dataset.targets[i][1] for i in range(train_dataset.snapshot_count)])\nax3.plot(torch.tensor([_b[i].detach()[1] for i in range(train_dataset.snapshot_count)]))\n\n\nax4.set_title('test node2')\nax4.plot([test_dataset.targets[i][1] for i in range(test_dataset.snapshot_count)])\nax4.plot(torch.tensor([_a[i].detach()[1] for i in range(test_dataset.snapshot_count)]))\n\nax5.set_title('train cost')\nax5.plot(_e)\n\nax6.set_title('test cost')\nax6.plot(_c)\n\n\n\n\n\n\n\n\n\n\nEVOLVEGCNO(Done)\n\nEvolveGCNO?\n\n\nInit signature:\nEvolveGCNO(\n    in_channels: int,\n    improved: bool = False,\n    cached: bool = False,\n    normalize: bool = True,\n    add_self_loops: bool = True,\n)\nDocstring:     \nAn implementation of the Evolving Graph Convolutional without Hidden Layer.\nFor details see this paper: `\"EvolveGCN: Evolving Graph Convolutional\nNetworks for Dynamic Graph.\" &lt;https://arxiv.org/abs/1902.10191&gt;`_\nArgs:\n    in_channels (int): Number of filters.\n    improved (bool, optional): If set to :obj:`True`, the layer computes\n        :math:`\\mathbf{\\hat{A}}` as :math:`\\mathbf{A} + 2\\mathbf{I}`.\n        (default: :obj:`False`)\n    cached (bool, optional): If set to :obj:`True`, the layer will cache\n        the computation of :math:`\\mathbf{\\hat{D}}^{-1/2} \\mathbf{\\hat{A}}\n        \\mathbf{\\hat{D}}^{-1/2}` on first execution, and will use the\n        cached version for further executions.\n        This parameter should only be set to :obj:`True` in transductive\n        learning scenarios. (default: :obj:`False`)\n    normalize (bool, optional): Whether to add self-loops and apply\n        symmetric normalization. (default: :obj:`True`)\n    add_self_loops (bool, optional): If set to :obj:`False`, will not add\n        self-loops to the input graph. (default: :obj:`True`)\nInit docstring: Initializes internal Module state, shared by both nn.Module and ScriptModule.\nFile:           ~/anaconda3/envs/temp_csy/lib/python3.8/site-packages/torch_geometric_temporal/nn/recurrent/evolvegcno.py\nType:           type\nSubclasses:     \n\n\n\n\n\ntry:\n    from tqdm import tqdm\nexcept ImportError:\n    def tqdm(iterable):\n        return iterable\n\n\n# import torch\n# import torch.nn.functional as F\nfrom torch_geometric_temporal.nn.recurrent import EvolveGCNO\n\n# from torch_geometric_temporal.dataset import ChickenpoxDatasetLoader\nfrom torch_geometric_temporal.signal import temporal_signal_split\n\n# loader = ChickenpoxDatasetLoader()\n\ndataset = loader.get_dataset(lags=4)\n\ntrain_dataset, test_dataset = temporal_signal_split(dataset, train_ratio=0.2)\n\n\nclass RecurrentGCN(torch.nn.Module):\n    def __init__(self, node_features):\n        super(RecurrentGCN, self).__init__()\n        self.recurrent = EvolveGCNO(node_features)\n        self.linear = torch.nn.Linear(node_features, 1)\n\n    def forward(self, x, edge_index, edge_weight):\n        h = self.recurrent(x, edge_index, edge_weight)\n        h = F.relu(h)\n        h = self.linear(h)\n        return h\n\n\nmodel = RecurrentGCN(node_features = 4)\nfor param in model.parameters():\n    param.retain_grad()\n\noptimizer = torch.optim.Adam(model.parameters(), lr=0.01)\n\nmodel.train()\n\nfor epoch in tqdm(range(200)):\n    cost = 0\n    _b=[]\n    _d=[]\n    for time, snapshot in enumerate(train_dataset):\n        y_hat = model(snapshot.x, snapshot.edge_index, snapshot.edge_attr).reshape(-1)\n        cost = cost + torch.mean((y_hat-snapshot.y)**2)\n        _b.append(y_hat)\n        _d.append(cost)\n    cost = cost / (time+1)\n    cost.backward(retain_graph=True)\n    optimizer.step()\n    optimizer.zero_grad()\n\n100%|██████████| 200/200 [00:08&lt;00:00, 22.31it/s]\n\n\n\nmodel.eval()\ncost = 0\n_a=[]\n_a1=[]\nfor time, snapshot in enumerate(test_dataset):\n    if time == 0:\n        model.recurrent.weight = None\n    y_hat = model(snapshot.x, snapshot.edge_index, snapshot.edge_attr).reshape(-1)\n    cost = cost + torch.mean((y_hat-snapshot.y)**2)\n    _a.append(y_hat)\n    _a1.append(cost)\ncost = cost / (time+1)\ncost = cost.item()\nprint(\"MSE: {:.4f}\".format(cost))\n\nMSE: 0.5661\n\n\n\n_e = [_d[i].detach() for i in range(len(_d))]\n\n\n_c = [_a1[i].detach() for i in range(len(_a1))]\n\n\nfig, (( ax1,ax2),(ax3,ax4),(ax5,ax6)) = plt.subplots(3,2,figsize=(30,20))\n\nax1.set_title('train node1')\nax1.plot([train_dataset.targets[i][0] for i in range(train_dataset.snapshot_count)])\nax1.plot(torch.tensor([_b[i].detach()[0] for i in range(train_dataset.snapshot_count)]))\n\nax2.set_title('test node1')\nax2.plot([test_dataset.targets[i][0] for i in range(test_dataset.snapshot_count)])\nax2.plot(torch.tensor([_a[i].detach()[0] for i in range(test_dataset.snapshot_count)]))\n\nax3.set_title('train node2')\nax3.plot([train_dataset.targets[i][1] for i in range(train_dataset.snapshot_count)])\nax3.plot(torch.tensor([_b[i].detach()[1] for i in range(train_dataset.snapshot_count)]))\n\n\nax4.set_title('test node2')\nax4.plot([test_dataset.targets[i][1] for i in range(test_dataset.snapshot_count)])\nax4.plot(torch.tensor([_a[i].detach()[1] for i in range(test_dataset.snapshot_count)]))\n\nax5.set_title('train cost')\nax5.plot(_e)\n\nax6.set_title('test cost')\nax6.plot(_c)\n\n\n\n\n\n\n\n\n\n\nGCLSTM(Done)\n\nGCLSTM?\n\n\nInit signature:\nGCLSTM(\n    in_channels: int,\n    out_channels: int,\n    K: int,\n    normalization: str = 'sym',\n    bias: bool = True,\n)\nDocstring:     \nAn implementation of the the Integrated Graph Convolutional Long Short Term\nMemory Cell. For details see this paper: `\"GC-LSTM: Graph Convolution Embedded LSTM\nfor Dynamic Link Prediction.\" &lt;https://arxiv.org/abs/1812.04206&gt;`_\nArgs:\n    in_channels (int): Number of input features.\n    out_channels (int): Number of output features.\n    K (int): Chebyshev filter size :math:`K`.\n    normalization (str, optional): The normalization scheme for the graph\n        Laplacian (default: :obj:`\"sym\"`):\n        1. :obj:`None`: No normalization\n        :math:`\\mathbf{L} = \\mathbf{D} - \\mathbf{A}`\n        2. :obj:`\"sym\"`: Symmetric normalization\n        :math:`\\mathbf{L} = \\mathbf{I} - \\mathbf{D}^{-1/2} \\mathbf{A}\n        \\mathbf{D}^{-1/2}`\n        3. :obj:`\"rw\"`: Random-walk normalization\n        :math:`\\mathbf{L} = \\mathbf{I} - \\mathbf{D}^{-1} \\mathbf{A}`\n        You need to pass :obj:`lambda_max` to the :meth:`forward` method of\n        this operator in case the normalization is non-symmetric.\n        :obj:`\\lambda_max` should be a :class:`torch.Tensor` of size\n        :obj:`[num_graphs]` in a mini-batch scenario and a\n        scalar/zero-dimensional tensor when operating on single graphs.\n        You can pre-compute :obj:`lambda_max` via the\n        :class:`torch_geometric.transforms.LaplacianLambdaMax` transform.\n    bias (bool, optional): If set to :obj:`False`, the layer will not learn\n        an additive bias. (default: :obj:`True`)\nInit docstring: Initializes internal Module state, shared by both nn.Module and ScriptModule.\nFile:           ~/anaconda3/envs/temp_csy/lib/python3.8/site-packages/torch_geometric_temporal/nn/recurrent/gc_lstm.py\nType:           type\nSubclasses:     \n\n\n\n\n\ntry:\n    from tqdm import tqdm\nexcept ImportError:\n    def tqdm(iterable):\n        return iterable\n\n\nimport torch\nimport torch.nn.functional as F\nfrom torch_geometric_temporal.nn.recurrent import GCLSTM\n\n# from torch_geometric_temporal.dataset import ChickenpoxDatasetLoader\nfrom torch_geometric_temporal.signal import temporal_signal_split\n\n# loader = ChickenpoxDatasetLoader()\n\ndataset = loader.get_dataset()\n\ntrain_dataset, test_dataset = temporal_signal_split(dataset, train_ratio=0.2)\n\n\nclass RecurrentGCN(torch.nn.Module):\n    def __init__(self, node_features):\n        super(RecurrentGCN, self).__init__()\n        self.recurrent = GCLSTM(node_features, 32, 1)\n        self.linear = torch.nn.Linear(32, 1)\n\n    def forward(self, x, edge_index, edge_weight, h, c):\n        h_0, c_0 = self.recurrent(x, edge_index, edge_weight, h, c)\n        h = F.relu(h_0)\n        h = self.linear(h)\n        return h, h_0, c_0\n\n\nmodel = RecurrentGCN(node_features=4)\n\noptimizer = torch.optim.Adam(model.parameters(), lr=0.01)\n\nmodel.train()\n\nfor epoch in tqdm(range(100)): #200\n    cost = 0\n    h, c = None, None\n    _b=[]\n    _d=[]\n    for time, snapshot in enumerate(train_dataset):\n        y_hat, h, c = model(snapshot.x, snapshot.edge_index, snapshot.edge_attr, h, c)\n        y_hat = y_hat.reshape(-1)\n        cost = cost + torch.mean((y_hat-snapshot.y)**2)\n        _b.append(y_hat)\n        _d.append(cost)\n    cost = cost / (time+1)\n    cost.backward()\n    optimizer.step()\n    optimizer.zero_grad()\n\n100%|██████████| 100/100 [00:10&lt;00:00,  9.17it/s]\n\n\n\nmodel.eval()\ncost = 0\n_a=[]\n_a1=[]\nfor time, snapshot in enumerate(test_dataset):\n    y_hat, h, c = model(snapshot.x, snapshot.edge_index, snapshot.edge_attr, h, c)\n    y_hat = y_hat.reshape(-1)\n    cost = cost + torch.mean((y_hat-snapshot.y)**2)\n    _a.append(y_hat)\n    _a1.append(cost)\ncost = cost / (time+1)\ncost = cost.item()\nprint(\"MSE: {:.4f}\".format(cost))\n\nMSE: 0.2557\n\n\n\n_e = [_d[i].detach() for i in range(len(_d))]\n\n\n_c = [_a1[i].detach() for i in range(len(_a1))]\n\n\nfig, (( ax1,ax2),(ax3,ax4),(ax5,ax6)) = plt.subplots(3,2,figsize=(30,20))\n\nax1.set_title('train node1')\nax1.plot([train_dataset.targets[i][0] for i in range(train_dataset.snapshot_count)])\nax1.plot(torch.tensor([_b[i].detach()[0] for i in range(train_dataset.snapshot_count)]))\n\nax2.set_title('test node1')\nax2.plot([test_dataset.targets[i][0] for i in range(test_dataset.snapshot_count)])\nax2.plot(torch.tensor([_a[i].detach()[0] for i in range(test_dataset.snapshot_count)]))\n\nax3.set_title('train node2')\nax3.plot([train_dataset.targets[i][1] for i in range(train_dataset.snapshot_count)])\nax3.plot(torch.tensor([_b[i].detach()[1] for i in range(train_dataset.snapshot_count)]))\n\n\nax4.set_title('test node2')\nax4.plot([test_dataset.targets[i][1] for i in range(test_dataset.snapshot_count)])\nax4.plot(torch.tensor([_a[i].detach()[1] for i in range(test_dataset.snapshot_count)]))\n\nax5.set_title('train cost')\nax5.plot(_e)\n\nax6.set_title('test cost')\nax6.plot(_c)\n\n\n\n\n\n\n\n\n\n\nGConvLSTM(Done)\n\nGConvLSTM?\n\n\nInit signature:\nGConvLSTM(\n    in_channels: int,\n    out_channels: int,\n    K: int,\n    normalization: str = 'sym',\n    bias: bool = True,\n)\nDocstring:     \nAn implementation of the Chebyshev Graph Convolutional Long Short Term Memory\nCell. For details see this paper: `\"Structured Sequence Modeling with Graph\nConvolutional Recurrent Networks.\" &lt;https://arxiv.org/abs/1612.07659&gt;`_\nArgs:\n    in_channels (int): Number of input features.\n    out_channels (int): Number of output features.\n    K (int): Chebyshev filter size :math:`K`.\n    normalization (str, optional): The normalization scheme for the graph\n        Laplacian (default: :obj:`\"sym\"`):\n        1. :obj:`None`: No normalization\n        :math:`\\mathbf{L} = \\mathbf{D} - \\mathbf{A}`\n        2. :obj:`\"sym\"`: Symmetric normalization\n        :math:`\\mathbf{L} = \\mathbf{I} - \\mathbf{D}^{-1/2} \\mathbf{A}\n        \\mathbf{D}^{-1/2}`\n        3. :obj:`\"rw\"`: Random-walk normalization\n        :math:`\\mathbf{L} = \\mathbf{I} - \\mathbf{D}^{-1} \\mathbf{A}`\n        You need to pass :obj:`lambda_max` to the :meth:`forward` method of\n        this operator in case the normalization is non-symmetric.\n        :obj:`\\lambda_max` should be a :class:`torch.Tensor` of size\n        :obj:`[num_graphs]` in a mini-batch scenario and a\n        scalar/zero-dimensional tensor when operating on single graphs.\n        You can pre-compute :obj:`lambda_max` via the\n        :class:`torch_geometric.transforms.LaplacianLambdaMax` transform.\n    bias (bool, optional): If set to :obj:`False`, the layer will not learn\n        an additive bias. (default: :obj:`True`)\nInit docstring: Initializes internal Module state, shared by both nn.Module and ScriptModule.\nFile:           ~/anaconda3/envs/temp_csy/lib/python3.8/site-packages/torch_geometric_temporal/nn/recurrent/gconv_lstm.py\nType:           type\nSubclasses:     \n\n\n\n\n\ntry:\n    from tqdm import tqdm\nexcept ImportError:\n    def tqdm(iterable):\n        return iterable\n\n\n# import torch\n# import torch.nn.functional as F\nfrom torch_geometric_temporal.nn.recurrent import GConvLSTM\n\nfrom torch_geometric_temporal.dataset import ChickenpoxDatasetLoader\nfrom torch_geometric_temporal.signal import temporal_signal_split\n\nloader1 = ChickenpoxDatasetLoader()\n\ndataset = loader1.get_dataset(lags=4)\n\ntrain_dataset, test_dataset = temporal_signal_split(dataset, train_ratio=0.2)\n\n\nclass RecurrentGCN(torch.nn.Module):\n    def __init__(self, node_features):\n        super(RecurrentGCN, self).__init__()\n        self.recurrent = GConvLSTM(node_features, 8, 1)\n        self.linear = torch.nn.Linear(8, 1)\n\n    def forward(self, x, edge_index, edge_weight, h, c):\n        h_0, c_0 = self.recurrent(x, edge_index, edge_weight, h, c)\n        h = F.relu(h_0)\n        h = self.linear(h)\n        return h, h_0, c_0\n\n\nmodel = RecurrentGCN(node_features=4)\n\noptimizer = torch.optim.Adam(model.parameters(), lr=0.01)\n\nmodel.train()\n\nfor epoch in tqdm(range(50)): #200\n    cost = 0\n    h, c = None, None\n    _b = []\n    _d=[]\n    for time, snapshot in enumerate(train_dataset):\n        y_hat, h, c = model(snapshot.x, snapshot.edge_index, snapshot.edge_attr, h, c)\n        y_hat = y_hat.reshape(-1)\n        cost = cost + torch.mean((y_hat-snapshot.y)**2)\n        _b.append(y_hat)\n        _d.append(cost)\n    cost = cost / (time+1)\n    cost.backward()\n    optimizer.step()\n    optimizer.zero_grad()\n\n100%|██████████| 50/50 [00:30&lt;00:00,  1.66it/s]\n\n\n\nmodel.eval()\ncost = 0\n_a=[]\n_a1=[]\nfor time, snapshot in enumerate(test_dataset):\n    y_hat, h, c = model(snapshot.x, snapshot.edge_index, snapshot.edge_attr, h, c)\n    y_hat = y_hat.reshape(-1)\n    cost = cost + torch.mean((y_hat-snapshot.y)**2)\n    _a.append(y_hat)\n    _a1.append(cost)\ncost = cost / (time+1)\ncost = cost.item()\nprint(\"MSE: {:.4f}\".format(cost))\n\nMSE: 0.7228\n\n\n\n_e = [_d[i].detach() for i in range(len(_d))]\n\n\n_c = [_a1[i].detach() for i in range(len(_a1))]\n\n\nfig, (( ax1,ax2),(ax3,ax4),(ax5,ax6)) = plt.subplots(3,2,figsize=(30,20))\n\nax1.set_title('train node1')\nax1.plot([train_dataset.targets[i][0] for i in range(train_dataset.snapshot_count)])\nax1.plot(torch.tensor([_b[i].detach()[0] for i in range(train_dataset.snapshot_count)]))\n\nax2.set_title('test node1')\nax2.plot([test_dataset.targets[i][0] for i in range(test_dataset.snapshot_count)])\nax2.plot(torch.tensor([_a[i].detach()[0] for i in range(test_dataset.snapshot_count)]))\n\nax3.set_title('train node2')\nax3.plot([train_dataset.targets[i][1] for i in range(train_dataset.snapshot_count)])\nax3.plot(torch.tensor([_b[i].detach()[1] for i in range(train_dataset.snapshot_count)]))\n\n\nax4.set_title('test node2')\nax4.plot([test_dataset.targets[i][1] for i in range(test_dataset.snapshot_count)])\nax4.plot(torch.tensor([_a[i].detach()[1] for i in range(test_dataset.snapshot_count)]))\n\nax5.set_title('train cost')\nax5.plot(_e)\n\nax6.set_title('test cost')\nax6.plot(_c)\n\n\n\n\n\n\n\n\n\n\nLightning(설치 안 됨)\n\n# import torch\n# from torch.nn import functional as F\n\n# import pytorch_lightning as pl\n# from pytorch_lightning.callbacks.early_stopping import EarlyStopping\n\n# from torch_geometric_temporal.nn.recurrent import DCRNN\n# from torch_geometric_temporal.dataset import ChickenpoxDatasetLoader\n# from torch_geometric_temporal.signal import temporal_signal_split\n\n\n# class LitDiffConvModel(pl.LightningModule):\n\n#     def __init__(self, node_features, filters):\n#         super().__init__()\n#         self.recurrent = DCRNN(node_features, filters, 1)\n#         self.linear = torch.nn.Linear(filters, 1)\n\n\n#     def configure_optimizers(self):\n#         optimizer = torch.optim.Adam(self.parameters(), lr=1e-2)\n#         return optimizer\n\n#     def training_step(self, train_batch, batch_idx):\n#         x = train_batch.x\n#         y = train_batch.y.view(-1, 1)\n#         edge_index = train_batch.edge_index\n#         h = self.recurrent(x, edge_index)\n#         h = F.relu(h)\n#         h = self.linear(h)\n#         loss = F.mse_loss(h, y)\n#         return loss\n\n#     def validation_step(self, val_batch, batch_idx):\n#         x = val_batch.x\n#         y = val_batch.y.view(-1, 1)\n#         edge_index = val_batch.edge_index\n#         h = self.recurrent(x, edge_index)\n#         h = F.relu(h)\n#         h = self.linear(h)\n#         loss = F.mse_loss(h, y)\n#         metrics = {'val_loss': loss}\n#         self.log_dict(metrics)\n#         return metrics\n\n\n# loader = ChickenpoxDatasetLoader()\n\n# dataset_loader = loader.get_dataset(lags=32)\n\n# train_loader, val_loader = temporal_signal_split(dataset_loader,\n#                                                  train_ratio=0.2)\n\n\n# model = LitDiffConvModel(node_features=32,\n#                          filters=16)\n\n\n# early_stop_callback = EarlyStopping(monitor='val_loss',\n#                                     min_delta=0.00,\n#                                     patience=10,\n#                                     verbose=False,\n#                                     mode='max')\n\n\n# trainer = pl.Trainer(callbacks=[early_stop_callback])\n\n\n# trainer.fit(model, train_loader, val_loader)\n\n\n\nLRGCN(Done)\n\nLRGCN?\n\n\nInit signature:\nLRGCN(\n    in_channels: int,\n    out_channels: int,\n    num_relations: int,\n    num_bases: int,\n)\nDocstring:     \nAn implementation of the Long Short Term Memory Relational\nGraph Convolution Layer. For details see this paper: `\"Predicting Path\nFailure In Time-Evolving Graphs.\" &lt;https://arxiv.org/abs/1905.03994&gt;`_\nArgs:\n    in_channels (int): Number of input features.\n    out_channels (int): Number of output features.\n    num_relations (int): Number of relations.\n    num_bases (int): Number of bases.\nInit docstring: Initializes internal Module state, shared by both nn.Module and ScriptModule.\nFile:           ~/anaconda3/envs/temp_csy/lib/python3.8/site-packages/torch_geometric_temporal/nn/recurrent/lrgcn.py\nType:           type\nSubclasses:     \n\n\n\n\n\ntry:\n    from tqdm import tqdm\nexcept ImportError:\n    def tqdm(iterable):\n        return iterable\n\n\n# import torch\n# import torch.nn.functional as F\nfrom torch_geometric_temporal.nn.recurrent import LRGCN\n\n# from torch_geometric_temporal.dataset import ChickenpoxDatasetLoader\nfrom torch_geometric_temporal.signal import temporal_signal_split\n\n# loader = ChickenpoxDatasetLoader()\n\ndataset = loader.get_dataset()\n\ntrain_dataset, test_dataset = temporal_signal_split(dataset, train_ratio=0.2)\n\n\nclass RecurrentGCN(torch.nn.Module):\n    def __init__(self, node_features):\n        super(RecurrentGCN, self).__init__()\n        self.recurrent = LRGCN(node_features, 32, 1, 1)\n        self.linear = torch.nn.Linear(32, 1)\n\n    def forward(self, x, edge_index, edge_weight, h_0, c_0):\n        h_0, c_0 = self.recurrent(x, edge_index, edge_weight, h_0, c_0)\n        h = F.relu(h_0)\n        h = self.linear(h)\n        return h, h_0, c_0\n\n\nmodel = RecurrentGCN(node_features = 4)\n\noptimizer = torch.optim.Adam(model.parameters(), lr=0.01)\n\nmodel.train()\n\nfor epoch in tqdm(range(200)):\n    cost = 0\n    h, c = None, None\n    _b=[]\n    _d=[]\n    for time, snapshot in enumerate(train_dataset):\n        y_hat, h, c = model(snapshot.x, snapshot.edge_index, snapshot.edge_attr, h, c)\n        y_hat = y_hat.reshape(-1)\n        cost = cost + torch.mean((y_hat-snapshot.y)**2)\n        _b.append(y_hat)\n        _d.append(cost)\n    cost = cost / (time+1)\n    cost.backward()\n    optimizer.step()\n    optimizer.zero_grad()\n\n100%|██████████| 200/200 [00:47&lt;00:00,  4.23it/s]\n\n\n\nmodel.eval()\ncost = 0\nh, c = None, None\n_a=[]\n_a1=[]\nfor time, snapshot in enumerate(test_dataset):\n    y_hat, h, c = model(snapshot.x, snapshot.edge_index, snapshot.edge_attr, h, c)\n    y_hat = y_hat.reshape(-1)\n    cost = cost + torch.mean((y_hat-snapshot.y)**2)\n    _a.append(y_hat)\n    _a1.append(cost)\ncost = cost / (time+1)\ncost = cost.item()\nprint(\"MSE: {:.4f}\".format(cost))\n\nMSE: 0.2608\n\n\n\n_e = [_d[i].detach() for i in range(len(_d))]\n\n\n_c = [_a1[i].detach() for i in range(len(_a1))]\n\n\nfig, (( ax1,ax2),(ax3,ax4),(ax5,ax6)) = plt.subplots(3,2,figsize=(30,20))\n\nax1.set_title('train node1')\nax1.plot([train_dataset.targets[i][0] for i in range(train_dataset.snapshot_count)])\nax1.plot(torch.tensor([_b[i].detach()[0] for i in range(train_dataset.snapshot_count)]))\n\nax2.set_title('test node1')\nax2.plot([test_dataset.targets[i][0] for i in range(test_dataset.snapshot_count)])\nax2.plot(torch.tensor([_a[i].detach()[0] for i in range(test_dataset.snapshot_count)]))\n\nax3.set_title('train node2')\nax3.plot([train_dataset.targets[i][1] for i in range(train_dataset.snapshot_count)])\nax3.plot(torch.tensor([_b[i].detach()[1] for i in range(train_dataset.snapshot_count)]))\n\n\nax4.set_title('test node2')\nax4.plot([test_dataset.targets[i][1] for i in range(test_dataset.snapshot_count)])\nax4.plot(torch.tensor([_a[i].detach()[1] for i in range(test_dataset.snapshot_count)]))\n\nax5.set_title('train cost')\nax5.plot(_e)\n\nax6.set_title('test cost')\nax6.plot(_c)\n\n\n\n\n\n\n\n\n\n\nMPNNLSTM\n\nMPNNLSTM?\n\n\nInit signature:\nMPNNLSTM(\n    in_channels: int,\n    hidden_size: int,\n    num_nodes: int,\n    window: int,\n    dropout: float,\n)\nDocstring:     \nAn implementation of the Message Passing Neural Network with Long Short Term Memory.\nFor details see this paper: `\"Transfer Graph Neural Networks for Pandemic Forecasting.\" &lt;https://arxiv.org/abs/2009.08388&gt;`_\nArgs:\n    in_channels (int): Number of input features.\n    hidden_size (int): Dimension of hidden representations.\n    num_nodes (int): Number of nodes in the network.\n    window (int): Number of past samples included in the input.\n    dropout (float): Dropout rate.\nInit docstring: Initializes internal Module state, shared by both nn.Module and ScriptModule.\nFile:           ~/anaconda3/envs/temp_csy/lib/python3.8/site-packages/torch_geometric_temporal/nn/recurrent/mpnn_lstm.py\nType:           type\nSubclasses:     \n\n\n\n\n\ntry:\n    from tqdm import tqdm\nexcept ImportError:\n    def tqdm(iterable):\n        return iterable\n\n\n# import torch\n# import torch.nn.functional as F\nfrom torch_geometric_temporal.nn.recurrent import MPNNLSTM\n\n# from torch_geometric_temporal.dataset import ChickenpoxDatasetLoader\nfrom torch_geometric_temporal.signal import temporal_signal_split\n\n# loader = ChickenpoxDatasetLoader()\n\ndataset = loader.get_dataset()\n\ntrain_dataset, test_dataset = temporal_signal_split(dataset, train_ratio=0.2)\n\n\nnum_nodes=2\n\n\nclass RecurrentGCN(torch.nn.Module):\n    def __init__(self, node_features):\n        super(RecurrentGCN, self).__init__()\n        self.recurrent = MPNNLSTM(node_features, 8,  num_nodes, 1, 0.3) # 32, 32, 20, 1, 0.5 이었는데 position 잘못되었다해서 32하나 뺌\n        self.linear = torch.nn.Linear(num_nodes*8 + node_features, 1)\n\n    def forward(self, x, edge_index, edge_weight):\n        h = self.recurrent(x, edge_index, edge_weight)\n        h = F.relu(h)\n        h = self.linear(h)\n        return h\n\n\nmodel = RecurrentGCN(node_features = 4)\n\n\noptimizer = torch.optim.Adam(model.parameters(), lr=0.01)\n\nmodel.train()\n\nfor epoch in tqdm(range(50)):\n    cost = 0\n    _b=[]\n    _d=[]\n    for time, snapshot in enumerate(train_dataset):\n        y_hat = model(snapshot.x, snapshot.edge_index, snapshot.edge_attr).reshape(-1)\n        cost = cost + torch.mean((y_hat-snapshot.y)**2)\n        _b.append(y_hat)\n        _d.append(cost)\n    cost = cost / (time+1)\n    cost.backward()\n    optimizer.step()\n    optimizer.zero_grad()\n\n100%|██████████| 50/50 [00:57&lt;00:00,  1.14s/it]\n\n\n\nmodel.eval()\ncost = 0\n_a=[]\n_a1=[]\nfor time, snapshot in enumerate(test_dataset):\n    y_hat = model(snapshot.x, snapshot.edge_index, snapshot.edge_attr).reshape(-1)\n    cost = cost + torch.mean((y_hat-snapshot.y)**2)\n    _a.append(y_hat)\n    _a1.append(cost)\ncost = cost / (time+1)\ncost = cost.item()\nprint(\"MSE: {:.4f}\".format(cost))\n\nMSE: 0.3623\n\n\n\n_e = [_d[i].detach() for i in range(len(_d))]\n\n\n_c = [_a1[i].detach() for i in range(len(_a1))]\n\n\nfig, (( ax1,ax2),(ax3,ax4),(ax5,ax6)) = plt.subplots(3,2,figsize=(30,20))\n\nax1.set_title('train node1')\nax1.plot([train_dataset.targets[i][0] for i in range(train_dataset.snapshot_count)])\nax1.plot(torch.tensor([_b[i].detach()[0] for i in range(train_dataset.snapshot_count)]))\n\nax2.set_title('test node1')\nax2.plot([test_dataset.targets[i][0] for i in range(test_dataset.snapshot_count)])\nax2.plot(torch.tensor([_a[i].detach()[0] for i in range(test_dataset.snapshot_count)]))\n\nax3.set_title('train node2')\nax3.plot([train_dataset.targets[i][1] for i in range(train_dataset.snapshot_count)])\nax3.plot(torch.tensor([_b[i].detach()[1] for i in range(train_dataset.snapshot_count)]))\n\n\nax4.set_title('test node2')\nax4.plot([test_dataset.targets[i][1] for i in range(test_dataset.snapshot_count)])\nax4.plot(torch.tensor([_a[i].detach()[1] for i in range(test_dataset.snapshot_count)]))\n\nax5.set_title('train cost')\nax5.plot(_e)\n\nax6.set_title('test cost')\nax6.plot(_c)\n\n\n\n\n\n\n\n\n\n\nTGCN(Done)\n\nTGCN?\n\n\nInit signature:\nTGCN(\n    in_channels: int,\n    out_channels: int,\n    improved: bool = False,\n    cached: bool = False,\n    add_self_loops: bool = True,\n)\nDocstring:     \nAn implementation of the Temporal Graph Convolutional Gated Recurrent Cell.\nFor details see this paper: `\"T-GCN: A Temporal Graph ConvolutionalNetwork for\nTraffic Prediction.\" &lt;https://arxiv.org/abs/1811.05320&gt;`_\nArgs:\n    in_channels (int): Number of input features.\n    out_channels (int): Number of output features.\n    improved (bool): Stronger self loops. Default is False.\n    cached (bool): Caching the message weights. Default is False.\n    add_self_loops (bool): Adding self-loops for smoothing. Default is True.\nInit docstring: Initializes internal Module state, shared by both nn.Module and ScriptModule.\nFile:           ~/anaconda3/envs/temp_csy/lib/python3.8/site-packages/torch_geometric_temporal/nn/recurrent/temporalgcn.py\nType:           type\nSubclasses:     \n\n\n\n\n\ntry:\n    from tqdm import tqdm\nexcept ImportError:\n    def tqdm(iterable):\n        return iterable\n\n\n# import torch\n# import torch.nn.functional as F\nfrom torch_geometric_temporal.nn.recurrent import TGCN\n\n# from torch_geometric_temporal.dataset import ChickenpoxDatasetLoader\nfrom torch_geometric_temporal.signal import temporal_signal_split\n\n# loader = ChickenpoxDatasetLoader()\n\ndataset = loader.get_dataset(lags=4)\n\ntrain_dataset, test_dataset = temporal_signal_split(dataset, train_ratio=0.2)\n\n\nclass RecurrentGCN(torch.nn.Module):\n    def __init__(self, node_features):\n        super(RecurrentGCN, self).__init__()\n        self.recurrent = TGCN(node_features, 8)\n        self.linear = torch.nn.Linear(8, 1)\n\n    def forward(self, x, edge_index, edge_weight, prev_hidden_state):\n        h = self.recurrent(x, edge_index, edge_weight, prev_hidden_state)\n        y = F.relu(h)\n        y = self.linear(y)\n        return y, h\n\n\nmodel = RecurrentGCN(node_features = 4)\n\noptimizer = torch.optim.Adam(model.parameters(), lr=0.01)\n\nmodel.train()\n\nfor epoch in tqdm(range(50)):\n    cost = 0\n    hidden_state = None\n    _b=[]\n    _d=[]\n    for time, snapshot in enumerate(train_dataset):\n        y_hat, hidden_state = model(snapshot.x, snapshot.edge_index, snapshot.edge_attr,hidden_state)\n        y_hat = y_hat.reshape(-1)\n        cost = cost + torch.mean((y_hat-snapshot.y)**2)\n        _b.append(y_hat)\n        _d.append(cost)\n    cost = cost / (time+1)\n    cost.backward()\n    optimizer.step()\n    optimizer.zero_grad()\n\n100%|██████████| 50/50 [00:06&lt;00:00,  8.10it/s]\n\n\n\nmodel.eval()\ncost = 0\nhidden_state = None\n_a=[]\n_a1=[]\nfor time, snapshot in enumerate(test_dataset):\n    y_hat, hidden_state = model(snapshot.x, snapshot.edge_index, snapshot.edge_attr, hidden_state)\n    y_hat = y_hat.reshape(-1)\n    cost = cost + torch.mean((y_hat-snapshot.y)**2)\n    _a.append(y_hat)\n    _a1.append(cost)\ncost = cost / (time+1)\ncost = cost.item()\nprint(\"MSE: {:.4f}\".format(cost))\n\nMSE: 0.8115\n\n\n\n_e = [_d[i].detach() for i in range(len(_d))]\n\n\n_c = [_a1[i].detach() for i in range(len(_a1))]\n\n\nfig, (( ax1,ax2),(ax3,ax4),(ax5,ax6)) = plt.subplots(3,2,figsize=(30,20))\n\nax1.set_title('train node1')\nax1.plot([train_dataset.targets[i][0] for i in range(train_dataset.snapshot_count)])\nax1.plot(torch.tensor([_b[i].detach()[0] for i in range(train_dataset.snapshot_count)]))\n\nax2.set_title('test node1')\nax2.plot([test_dataset.targets[i][0] for i in range(test_dataset.snapshot_count)])\nax2.plot(torch.tensor([_a[i].detach()[0] for i in range(test_dataset.snapshot_count)]))\n\nax3.set_title('train node2')\nax3.plot([train_dataset.targets[i][1] for i in range(train_dataset.snapshot_count)])\nax3.plot(torch.tensor([_b[i].detach()[1] for i in range(train_dataset.snapshot_count)]))\n\n\nax4.set_title('test node2')\nax4.plot([test_dataset.targets[i][1] for i in range(test_dataset.snapshot_count)])\nax4.plot(torch.tensor([_a[i].detach()[1] for i in range(test_dataset.snapshot_count)]))\n\nax5.set_title('train cost')\nax5.plot(_e)\n\nax6.set_title('test cost')\nax6.plot(_c)"
  },
  {
    "objectID": "posts/2_research/2023-06-19-LRGCN_Simulation_boxplot_reshape.html",
    "href": "posts/2_research/2023-06-19-LRGCN_Simulation_boxplot_reshape.html",
    "title": "LRGCN_Simulation_reshape",
    "section": "",
    "text": "Simulation Study"
  },
  {
    "objectID": "posts/2_research/2023-06-19-LRGCN_Simulation_boxplot_reshape.html#baseline",
    "href": "posts/2_research/2023-06-19-LRGCN_Simulation_boxplot_reshape.html#baseline",
    "title": "LRGCN_Simulation_reshape",
    "section": "Baseline",
    "text": "Baseline\n\ndata.query(\"method!='GNAR' and mrate==0\").plot.box(backend='plotly',x='mrate',color='method',y='mse',facet_col='lags',facet_row='nof_filters',height=600)"
  },
  {
    "objectID": "posts/2_research/2023-06-19-LRGCN_Simulation_boxplot_reshape.html#random",
    "href": "posts/2_research/2023-06-19-LRGCN_Simulation_boxplot_reshape.html#random",
    "title": "LRGCN_Simulation_reshape",
    "section": "Random",
    "text": "Random\n\ndata.query(\"method!='GNAR' and mtype =='rand'\").plot.box(backend='plotly',x='mrate',color='method',y='mse',facet_col='inter_method',facet_row='nof_filters',height=600)"
  },
  {
    "objectID": "posts/2_research/2023-06-19-LRGCN_Simulation_boxplot_reshape.html#block",
    "href": "posts/2_research/2023-06-19-LRGCN_Simulation_boxplot_reshape.html#block",
    "title": "LRGCN_Simulation_reshape",
    "section": "Block",
    "text": "Block\n\ndata.query(\"method!='GNAR' and mtype =='block' \").plot.box(backend='plotly',x='mrate',color='method',y='mse',facet_col='inter_method',facet_row='nof_filters',height=600)"
  },
  {
    "objectID": "posts/2_research/2023-06-19-LRGCN_Simulation_boxplot_reshape.html#baseline-1",
    "href": "posts/2_research/2023-06-19-LRGCN_Simulation_boxplot_reshape.html#baseline-1",
    "title": "LRGCN_Simulation_reshape",
    "section": "Baseline",
    "text": "Baseline\n\ndata.query(\"method!='GNAR' and mrate ==0 \").plot.box(backend='plotly',x='mrate',color='method',y='mse',facet_col='nof_filters',height=600)"
  },
  {
    "objectID": "posts/2_research/2023-06-19-LRGCN_Simulation_boxplot_reshape.html#random-1",
    "href": "posts/2_research/2023-06-19-LRGCN_Simulation_boxplot_reshape.html#random-1",
    "title": "LRGCN_Simulation_reshape",
    "section": "Random",
    "text": "Random\n\ndata.query(\"method!='GNAR' and mtype =='rand'\").plot.box(backend='plotly',x='mrate',color='method',y='mse',facet_col='nof_filters',facet_row='inter_method',height=800)"
  },
  {
    "objectID": "posts/2_research/2023-06-19-LRGCN_Simulation_boxplot_reshape.html#block-1",
    "href": "posts/2_research/2023-06-19-LRGCN_Simulation_boxplot_reshape.html#block-1",
    "title": "LRGCN_Simulation_reshape",
    "section": "Block",
    "text": "Block\n\ndata.query(\"method!='GNAR' and mtype =='block'\").plot.box(backend='plotly',x='mrate',color='method',y='mse',facet_col='inter_method',facet_row='lags',height=800)"
  },
  {
    "objectID": "posts/2_research/2023-06-19-LRGCN_Simulation_boxplot_reshape.html#baseline-2",
    "href": "posts/2_research/2023-06-19-LRGCN_Simulation_boxplot_reshape.html#baseline-2",
    "title": "LRGCN_Simulation_reshape",
    "section": "Baseline",
    "text": "Baseline\n\ndata.query(\"mrate ==0 and lags!=2\").plot.box(backend='plotly',x='epoch',color='method',y='mse',facet_col='nof_filters',facet_row='lags',height=400)"
  },
  {
    "objectID": "posts/2_research/2023-06-19-LRGCN_Simulation_boxplot_reshape.html#random-2",
    "href": "posts/2_research/2023-06-19-LRGCN_Simulation_boxplot_reshape.html#random-2",
    "title": "LRGCN_Simulation_reshape",
    "section": "random",
    "text": "random\n\ndata.query(\"method!='GNAR' and mtype =='rand' and lags!=2\").plot.box(backend='plotly',x='mrate',color='method',y='mse',facet_col='nof_filters',facet_row='inter_method',height=800)"
  },
  {
    "objectID": "posts/2_research/2023-06-19-LRGCN_Simulation_boxplot_reshape.html#block-2",
    "href": "posts/2_research/2023-06-19-LRGCN_Simulation_boxplot_reshape.html#block-2",
    "title": "LRGCN_Simulation_reshape",
    "section": "block",
    "text": "block\n\ndata.query(\"method!='GNAR' and mtype =='block' and lags!=2 \").plot.box(backend='plotly',x='mrate',color='method',y='mse',facet_col='nof_filters',facet_row='inter_method',height=800)"
  },
  {
    "objectID": "posts/2_research/2023-06-19-LRGCN_Simulation_boxplot_reshape.html#weight-matrix-time-node-고려한-결과",
    "href": "posts/2_research/2023-06-19-LRGCN_Simulation_boxplot_reshape.html#weight-matrix-time-node-고려한-결과",
    "title": "LRGCN_Simulation_reshape",
    "section": "weight matrix time, node 고려한 결과",
    "text": "weight matrix time, node 고려한 결과\n\ndf1 = pd.read_csv('./simulation_results/2023-06-20_19-13-42.csv')\ndf2 = pd.read_csv('./simulation_results/2023-06-20_19-46-03.csv')\n\n\ndata2 = pd.concat([df1,df2],axis=0)\n\n\ndata2.to_csv('./simulation_results/Real_simulation_reshape/LRGCN_pedalme_Simulation_itstgcnsnd.csv',index=False)\n\n\ndata2 = pd.read_csv('./simulation_results/Real_simulation_reshape/LRGCN_pedalme_Simulation_itstgcnsnd.csv')\n\n\ndata2.query(\"mtype=='rand'\").plot.box(backend='plotly',x='mrate',color='method',y='mse',facet_col='inter_method',facet_row='nof_filters',height=1000)\n\n                                                \n\n\n\ndata2.query(\"mtype=='block'\").plot.box(backend='plotly',x='mrate',color='method',y='mse',facet_col='inter_method',facet_row='nof_filters',height=1000)"
  },
  {
    "objectID": "posts/2_research/2023-06-19-LRGCN_Simulation_boxplot_reshape.html#baseline-3",
    "href": "posts/2_research/2023-06-19-LRGCN_Simulation_boxplot_reshape.html#baseline-3",
    "title": "LRGCN_Simulation_reshape",
    "section": "Baseline",
    "text": "Baseline\n\ndata.query(\"mrate==0 and method!='GNAR'\").plot.box(backend='plotly',x='mrate',color='method',y='mse',height=600)"
  },
  {
    "objectID": "posts/2_research/2023-06-19-LRGCN_Simulation_boxplot_reshape.html#random-3",
    "href": "posts/2_research/2023-06-19-LRGCN_Simulation_boxplot_reshape.html#random-3",
    "title": "LRGCN_Simulation_reshape",
    "section": "random",
    "text": "random\n\ndata.query(\"mtype=='rand' and mrate !=0 and method!='GNAR'\").plot.box(backend='plotly',x='mrate',color='method',y='mse',facet_col='lags',facet_row='nof_filters',height=800)"
  },
  {
    "objectID": "posts/2_research/2023-06-19-LRGCN_Simulation_boxplot_reshape.html#block-3",
    "href": "posts/2_research/2023-06-19-LRGCN_Simulation_boxplot_reshape.html#block-3",
    "title": "LRGCN_Simulation_reshape",
    "section": "block",
    "text": "block\n\ndata.query(\"mtype=='block' and method!='GNAR'\").plot.box(backend='plotly',x='mrate',color='method',y='mse',facet_col='lags',facet_row='nof_filters',height=1200)"
  },
  {
    "objectID": "posts/2_research/2023-06-19-LRGCN_Simulation_boxplot_reshape.html#missing-values-on-the-same-nodes",
    "href": "posts/2_research/2023-06-19-LRGCN_Simulation_boxplot_reshape.html#missing-values-on-the-same-nodes",
    "title": "LRGCN_Simulation_reshape",
    "section": "missing values on the same nodes",
    "text": "missing values on the same nodes\n\ndf1 = pd.read_csv('./simulation_results/2023-06-21_19-31-38.csv') # STGCN IT-STGCN block\ndf2 = pd.read_csv('./simulation_results/2023-06-21_22-43-39.csv') # STGCN IT-STGCN\ndf3 = pd.read_csv('./simulation_results/2023-06-22_02-04-05.csv') \n\n\ndata = pd.concat([df1,df2,df3],axis=0)\n\n\ndata.to_csv('./simulation_results/Real_simulation_reshape/LRGCN_wikimath_GSO_st.csv',index=False)\n\n\ndata = pd.read_csv('./simulation_results/Real_simulation_reshape/LRGCN_wikimath_GSO_st.csv')\n\n\ndata.query(\"method!='GNAR' and mtype =='block' \").plot.box(backend='plotly',x='mrate',color='method',y='mse',facet_col='inter_method',facet_row='lags',height=800)"
  },
  {
    "objectID": "posts/2_research/2023-06-19-LRGCN_Simulation_boxplot_reshape.html#baseline-4",
    "href": "posts/2_research/2023-06-19-LRGCN_Simulation_boxplot_reshape.html#baseline-4",
    "title": "LRGCN_Simulation_reshape",
    "section": "Baseline",
    "text": "Baseline\n\ndata.query(\"method!='GNAR' and mrate ==0 \").plot.box(backend='plotly',x='mrate',color='method',y='mse',facet_col='lags',height=800)"
  },
  {
    "objectID": "posts/2_research/2023-06-19-LRGCN_Simulation_boxplot_reshape.html#random-4",
    "href": "posts/2_research/2023-06-19-LRGCN_Simulation_boxplot_reshape.html#random-4",
    "title": "LRGCN_Simulation_reshape",
    "section": "random",
    "text": "random\n\ndata.query(\"method!='GNAR' and mtype =='rand' \").plot.box(backend='plotly',x='mrate',color='method',y='mse',facet_col='lags',facet_row='inter_method',height=800)"
  },
  {
    "objectID": "posts/2_research/2023-06-19-LRGCN_Simulation_boxplot_reshape.html#block-4",
    "href": "posts/2_research/2023-06-19-LRGCN_Simulation_boxplot_reshape.html#block-4",
    "title": "LRGCN_Simulation_reshape",
    "section": "block",
    "text": "block\n\ndata.query(\"method!='GNAR' and mtype =='block' \").plot.box(backend='plotly',x='mrate',color='method',y='mse',facet_col='lags',facet_row='inter_method',height=800)"
  },
  {
    "objectID": "posts/2_research/2023-06-19-LRGCN_Simulation_boxplot_reshape.html#baseline-5",
    "href": "posts/2_research/2023-06-19-LRGCN_Simulation_boxplot_reshape.html#baseline-5",
    "title": "LRGCN_Simulation_reshape",
    "section": "Baseline",
    "text": "Baseline\n\ndata.query(\"mrate==0 and method!='GNAR'\").plot.box(backend='plotly',x='mrate',color='method',y='mse',facet_col='lags',facet_row='nof_filters',height=600)"
  },
  {
    "objectID": "posts/2_research/2023-06-19-LRGCN_Simulation_boxplot_reshape.html#random-5",
    "href": "posts/2_research/2023-06-19-LRGCN_Simulation_boxplot_reshape.html#random-5",
    "title": "LRGCN_Simulation_reshape",
    "section": "random",
    "text": "random\n\ndata.query(\"mtype=='rand' and mrate !=0 and method!='GNAR'\").plot.box(backend='plotly',x='mrate',color='method',y='mse',facet_col='lags',facet_row='inter_method',height=800)"
  },
  {
    "objectID": "posts/2_research/2023-06-19-LRGCN_Simulation_boxplot_reshape.html#block-5",
    "href": "posts/2_research/2023-06-19-LRGCN_Simulation_boxplot_reshape.html#block-5",
    "title": "LRGCN_Simulation_reshape",
    "section": "block",
    "text": "block\n\ndata.query(\"mtype=='block' and method!='GNAR'\").plot.box(backend='plotly',x='mrate',color='method',y='mse',facet_col='lags',facet_row='nof_filters',height=1200)"
  },
  {
    "objectID": "posts/2_research/2099-03-22-Baseline_SimulationPlanner-Tutorial_test_test.html",
    "href": "posts/2_research/2099-03-22-Baseline_SimulationPlanner-Tutorial_test_test.html",
    "title": "SimualtionPlanner-Tutorial",
    "section": "",
    "text": "table"
  },
  {
    "objectID": "posts/2_research/2099-03-22-Baseline_SimulationPlanner-Tutorial_test_test.html#plnr_stgcn_rand",
    "href": "posts/2_research/2099-03-22-Baseline_SimulationPlanner-Tutorial_test_test.html#plnr_stgcn_rand",
    "title": "SimualtionPlanner-Tutorial",
    "section": "PLNR_STGCN_RAND",
    "text": "PLNR_STGCN_RAND\n\nplans_stgcn_rand = {\n    'max_iteration': 30, \n    'method': ['STGCN', 'IT-STGCN'],\n    'mrate': [0,0.3,0.6],\n    'lags': [4], \n    'nof_filters': [12], \n    'inter_method': ['linear','nearest'],\n    'epoch': [50]\n}\n\n\nplnr = itstgcn.planner.PLNR_STGCN_RAND(plans_stgcn_rand,loader2,dataset_name='pedalme')\n\nplnr.simulate()\n\n\nplans_stgcn_rand = {\n    'max_iteration': 15, \n    'method': ['STGCN', 'IT-STGCN'],\n    'mrate': [0.3],\n    'lags': [8], \n    'nof_filters': [12], \n    'inter_method': ['linear'],\n    'epoch': [50]\n}\n\n\nplnr = itstgcn.planner.PLNR_STGCN_RAND(plans_stgcn_rand,loader3,dataset_name='wikimath')\n\nplnr.simulate()\n\n1/15 is done\n2/15 is done\n3/15 is done\n4/15 is done\n5/500\n\n\n\nplnr = itstgcn.planner.PLNR_STGCN_RAND(plans_stgcn_rand,loader3,dataset_name='wikimath')\n\nplnr.simulate()\n\n\nplans_stgcn_rand = {\n    'max_iteration': 15, \n    'method': ['STGCN', 'IT-STGCN'],\n    'mrate': [0.8],\n    'lags': [8], \n    'nof_filters': [12], \n    'inter_method': ['linear'],\n    'epoch': [50]\n}\n\n\nplnr = itstgcn.planner.PLNR_STGCN_RAND(plans_stgcn_rand,loader3,dataset_name='wikimath')\n\nplnr.simulate()\n\n\nplnr = itstgcn.planner.PLNR_STGCN_RAND(plans_stgcn_rand,loader3,dataset_name='wikimath')\n\nplnr.simulate()\n\n\nplans_stgcn_rand = {\n    'max_iteration': 15, \n    'method': ['STGCN', 'IT-STGCN'],\n    'mrate': [0],\n    'lags': [8], \n    'nof_filters': [12], \n    'inter_method': ['linear'],\n    'epoch': [50]\n}\n\n\nplnr = itstgcn.planner.PLNR_STGCN_RAND(plans_stgcn_rand,loader3,dataset_name='wikimath')\n\nplnr.simulate()\n\n\nplans_stgcn_rand = {\n    'max_iteration': 1, \n    'method': ['STGCN', 'IT-STGCN'],\n    'mrate': [0.7],\n    'lags': [8], \n    'nof_filters': [16], \n    'inter_method': ['linear'],\n    'epoch': [50]\n}\n\n\nplnr = itstgcnGConvLSTM.planner.PLNR_STGCN_RAND(plans_stgcn_rand,loader6,dataset_name='windmillsmall')\nplnr.simulate()\n\n\nplans_stgcn_rand = {\n    'max_iteration': 1, \n    'method': ['STGCN', 'IT-STGCN'],\n    'mrate': [0],\n    'lags': [8], \n    'nof_filters': [12], \n    'inter_method': ['linear'],\n    'epoch': [50]\n}\n\n\nplnr = itstgcnEvolveGCNH.planner.PLNR_STGCN_RAND(plans_stgcn_rand,loader6,dataset_name='windmillsmall')\nplnr.simulate()\n\n\nplans_stgcn_rand = {\n    'max_iteration': 1, \n    'method': ['STGCN', 'IT-STGCN'], \n    'mrate': [0],\n    'lags': [8], \n    'nof_filters': [12], \n    'inter_method': ['linear'],\n    'epoch': [50]\n}\n\n\nplnr = itstgcn.planner.PLNR_STGCN_RAND(plans_stgcn_rand,loader5,dataset_name='windmillmedium')\nplnr.simulate()"
  },
  {
    "objectID": "posts/2_research/2099-03-22-Baseline_SimulationPlanner-Tutorial_test_test.html#plnr_stgcn_manual",
    "href": "posts/2_research/2099-03-22-Baseline_SimulationPlanner-Tutorial_test_test.html#plnr_stgcn_manual",
    "title": "SimualtionPlanner-Tutorial",
    "section": "PLNR_STGCN_MANUAL",
    "text": "PLNR_STGCN_MANUAL\n\nmy_list = [[] for _ in range(20)] #chickenpox\nanother_list = list(range(100,200))\nmy_list[10] = another_list\nmindex = my_list\n\n\nmy_list = [[] for _ in range(15)] #pedalme\nanother_list = list(range(5,25))\nmy_list[1] = another_list\nmy_list[3] = another_list\nmy_list[5] = another_list\nmy_list[7] = another_list\nmy_list[9] = another_list\nmy_list[11] = another_list\nmindex = my_list\n\n\nmy_list = [[] for _ in range(1068)] #wikimath\nanother_list = list(range(200,500))\nmy_list[10] = another_list\nmindex = my_list\n\n\nmy_list = [[] for _ in range(26)] #windmilmedi\nanother_list = list(range(200,500)) # 676*0.8 = 540.8\nmy_list[10] = another_list\nmindex = my_list\n\n\nimport numpy as np\n\n\nimport random\n\n\nmy_list = [[] for _ in range(675)] #monte\nanother_list = list(range(200,350)) #743\n\nfor i in np.array(random.sample(range(0, 675), 400)):\n    my_list[i] = another_list\nmindex = my_list\n\n\n# mindex= [[],[],[],list(range(50,150)),[]]  # node 1\n# mindex= [list(range(10,100)),[],list(range(50,80)),[],[]] # node 2\n# mindex= [list(range(10,100)),[],list(range(50,80)),list(range(50,150)),[]] # node3\nplans_stgcn_block = {\n    'max_iteration': 30, \n    'method': ['STGCN', 'IT-STGCN'], \n    'mindex': [mindex],\n    'lags': [8], \n    'nof_filters': [12], \n    'inter_method': ['linear'],\n    'epoch': [50]\n}\n\n\nplnr = itstgcnadd.planner.PLNR_STGCN_MANUAL(plans_stgcn_block,loader,dataset_name='fivenodes')\nplnr.simulate(mindex=mindex,mtype='block')\n\n\nmy_list = [[] for _ in range(11)] #windmilsmall\nanother_list = list(range(5000,7500)) # 17470*0.8 = 13976.0\nmy_list[1] = another_list\nmy_list[3] = another_list\nmy_list[5] = another_list\nmy_list[7] = another_list\nmy_list[9] = another_list\nmindex = my_list\n\n\n# mindex= [[],[],[],list(range(50,150)),[]]  # node 1\n# mindex= [list(range(10,100)),[],list(range(50,80)),[],[]] # node 2\n# mindex= [list(range(10,100)),[],list(range(50,80)),list(range(50,150)),[]] # node3\nplans_stgcn_block = {\n    'max_iteration': 30, \n    'method': ['STGCN', 'IT-STGCN'], \n    'mindex': [mindex],\n    'lags': [8], \n    'nof_filters': [12], \n    'inter_method': ['linear'],\n    'epoch': [50]\n}\n\n\nplnr = itstgcn.planner.PLNR_STGCN_MANUAL(plans_stgcn_block,loader6,dataset_name='windmiloutputsmall')\nplnr.simulate(mindex=mindex,mtype='block')\n\n\nmy_list = [[] for _ in range(1068)] #wikimath\nanother_list = random.sample(range(0, 576), 432)\nfor i in range(0, 1068):\n    my_list[i] = another_list\nmindex = my_list\n\n\n# mindex= [[],[],[],list(range(50,150)),[]]  # node 1\n# mindex= [list(range(10,100)),[],list(range(50,80)),[],[]] # node 2\n# mindex= [list(range(10,100)),[],list(range(50,80)),list(range(50,150)),[]] # node3\nplans_stgcn_block = {\n    'max_iteration': 10, \n    'method': ['STGCN', 'IT-STGCN'], \n    'mindex': [mindex],\n    'lags': [4], \n    'nof_filters': [12], \n    'inter_method': ['nearest','linear'],\n    'epoch': [50]\n}\n\n\nplnr = itstgcn.planner.PLNR_STGCN_MANUAL(plans_stgcn_block,loader3,dataset_name='wikimath')\nplnr.simulate(mindex=mindex,mtype='block')\n\n\n# mindex= [[],[],[],list(range(50,150)),[]]  # node 1\n# mindex= [list(range(10,100)),[],list(range(50,80)),[],[]] # node 2\n# mindex= [list(range(10,100)),[],list(range(50,80)),list(range(50,150)),[]] # node3\nplans_stgcn_block = {\n    'max_iteration': 10, \n    'method': ['STGCN', 'IT-STGCN'], \n    'mindex': [mindex],\n    'lags': [4], \n    'nof_filters': [12], \n    'inter_method': ['nearest','linear'],\n    'epoch': [50]\n}\n\n\nplnr = itstgcn.planner.PLNR_STGCN_MANUAL(plans_stgcn_block,loader3,dataset_name='wikimath')\nplnr.simulate(mindex=mindex,mtype='block')\n\n\n# mindex= [[],[],[],list(range(50,150)),[]]  # node 1\n# mindex= [list(range(10,100)),[],list(range(50,80)),[],[]] # node 2\n# mindex= [list(range(10,100)),[],list(range(50,80)),list(range(50,150)),[]] # node3\nplans_stgcn_block = {\n    'max_iteration': 10, \n    'method': ['STGCN', 'IT-STGCN'], \n    'mindex': [mindex],\n    'lags': [4], \n    'nof_filters': [12], \n    'inter_method': ['nearest','linear'],\n    'epoch': [50]\n}\n\n\nplnr = itstgcn.planner.PLNR_STGCN_MANUAL(plans_stgcn_block,loader3,dataset_name='wikimath')\nplnr.simulate(mindex=mindex,mtype='block')"
  },
  {
    "objectID": "posts/2_research/2099-03-22-Baseline_SimulationPlanner-Tutorial_test_test.html#plnr_gnar_rand",
    "href": "posts/2_research/2099-03-22-Baseline_SimulationPlanner-Tutorial_test_test.html#plnr_gnar_rand",
    "title": "SimualtionPlanner-Tutorial",
    "section": "PLNR_GNAR_RAND",
    "text": "PLNR_GNAR_RAND\n\nplans_gnar_rand = {\n    'max_iteration': 30, \n#    'method': ['GNAR'], \n    'mrate': [0.8, 0.9],\n    'lags': [4], \n#    'nof_filters': [8,16], \n    'inter_method': ['cubic','linear'],\n#    'epoch': [1]\n}\n\n\nplnr = itstgcn.planner.PLNR_GNAR_RAND(plans_gnar_rand,loader1,dataset_name='chickenpox')\nplnr.simulate()"
  },
  {
    "objectID": "posts/2_research/2099-03-22-Baseline_SimulationPlanner-Tutorial_test_test.html#plnr_gnar_block",
    "href": "posts/2_research/2099-03-22-Baseline_SimulationPlanner-Tutorial_test_test.html#plnr_gnar_block",
    "title": "SimualtionPlanner-Tutorial",
    "section": "PLNR_GNAR_BLOCK",
    "text": "PLNR_GNAR_BLOCK\n\nmy_list = [[] for _ in range(20)] #chickenpox\nanother_list = list(range(100,200))\nmy_list[10] = another_list\nmindex = my_list\n\n\nmy_list = [[] for _ in range(15)] #pedalme\nanother_list = list(range(5,25))\nmy_list[1] = another_list\nmy_list[3] = another_list\nmy_list[5] = another_list\nmy_list[7] = another_list\nmy_list[9] = another_list\nmy_list[11] = another_list\nmindex = my_list\n\n\nmy_list = [[] for _ in range(1068)] #wikimath\nanother_list = list(range(10,20))\nmy_list[10] = another_list\nmindex = my_list\n\n\nmy_list = [[] for _ in range(26)] #windmilmedi\nanother_list = list(range(200,500)) # 676*0.8 = 540.8\nmy_list[10] = another_list\nmindex = my_list\n\n\nmy_list = [[] for _ in range(11)] #windmilsmall\nanother_list = list(range(5000,10000)) # 17470*0.8 = 13976.0\nmy_list[10] = another_list\nmindex = my_list\n\n\nmy_list = [[] for _ in range(675)] #monte\nanother_list = list(range(200,350)) #743\n\nfor i in np.array(random.sample(range(0, 675), 400)):\n    my_list[i] = another_list\nmindex = my_list\n\n\n# mindex = [[],[],list(range(50,250)),[],[]]\n# mindex = [list(range(10,100)),[],list(range(50,80)),[],[]]\n# mindex= [list(range(10,100)),[],list(range(50,80)),list(range(50,150)),[]]\nplans_gnar_block = {\n    'max_iteration': 3, \n    'method': ['GNAR'], \n    'mindex': [mindex],\n    'lags': [4,8], \n    'inter_method': ['linear'],\n}\n\n\nplnr = itstgcn.planner.PLNR_GNAR_MANUAL(plans_gnar_block,loader1,dataset_name='chickenpox')\nplnr.simulate(mindex,mtype='block')\n\n\n# mindex = [[],[],list(range(50,250)),[],[]]\n# mindex = [list(range(10,100)),[],list(range(50,80)),[],[]]\n# mindex= [list(range(10,100)),[],list(range(50,80)),list(range(50,150)),[]]\nplans_gnar_block = {\n    'max_iteration': 3, \n    'method': ['GNAR'], \n    'mindex': [mindex],\n    'lags': [8], \n    'inter_method': ['linear'],\n}\n\n\nplnr = itstgcn.planner.PLNR_GNAR_MANUAL(plans_gnar_block,loader3,dataset_name='wikimath')\nplnr.simulate(mindex,mtype='block')"
  },
  {
    "objectID": "posts/2_research/2022-12-29-STGCN-tutorial.html",
    "href": "posts/2_research/2022-12-29-STGCN-tutorial.html",
    "title": "[IT-STGCN] STGCN 튜토리얼",
    "section": "",
    "text": "Simulation"
  },
  {
    "objectID": "posts/2_research/2022-12-29-STGCN-tutorial.html#pyg-의-data-자료형",
    "href": "posts/2_research/2022-12-29-STGCN-tutorial.html#pyg-의-data-자료형",
    "title": "[IT-STGCN] STGCN 튜토리얼",
    "section": "PyG 의 Data 자료형",
    "text": "PyG 의 Data 자료형\n\nref: https://pytorch-geometric.readthedocs.io/en/latest/notes/introduction.html#data-handling-of-graphs\n\n- 자료는 PyG의 Data 오브젝트를 기반으로 한다.\n(예제) 아래와 같은 그래프자료를 고려하자.\n\n이러한 자료형은 아래와 같은 형식으로 저장한다.\n\nedge_index = torch.tensor([[0, 1, 1, 2],\n                           [1, 0, 2, 1]], dtype=torch.long)\nx = torch.tensor([[-1], [0], [1]], dtype=torch.float)\ndata = Data(x=x, edge_index=edge_index) # Data는 그래프자료형을 만드는 클래스\n\n\ntype(data)\n\ntorch_geometric.data.data.Data\n\n\n\ndata.x\n\ntensor([[-1.],\n        [ 0.],\n        [ 1.]])\n\n\n\ndata.edge_index\n\ntensor([[0, 1, 1, 2],\n        [1, 0, 2, 1]])"
  },
  {
    "objectID": "posts/2_research/2022-12-29-STGCN-tutorial.html#pytorch-geometric-temporal-의-자료형",
    "href": "posts/2_research/2022-12-29-STGCN-tutorial.html#pytorch-geometric-temporal-의-자료형",
    "title": "[IT-STGCN] STGCN 튜토리얼",
    "section": "PyTorch Geometric Temporal 의 자료형",
    "text": "PyTorch Geometric Temporal 의 자료형\n\nref: PyTorch Geometric Temporal Signal\n\n아래의 클래스들중 하나를 이용하여 만든다.\n## Temporal Signal Iterators\ntorch_geometric_temporal.signal.StaticGraphTemporalSignal\ntorch_geometric_temporal.signal.DynamicGraphTemporalSignal\ntorch_geometric_temporal.signal.DynamicGraphStaticSignal\n## Heterogeneous Temporal Signal Iterators\ntorch_geometric_temporal.signal.StaticHeteroGraphTemporalSignal\ntorch_geometric_temporal.signal.DynamicHeteroGraphTemporalSignal\ntorch_geometric_temporal.signal.DynamicHeteroGraphStaticSignal\n이중 “Heterogeneous Temporal Signal” 은 우리가 관심이 있는 신호가 아니므로 사실상 아래의 3개만 고려하면 된다.\n\ntorch_geometric_temporal.signal.StaticGraphTemporalSignal\ntorch_geometric_temporal.signal.DynamicGraphTemporalSignal\ntorch_geometric_temporal.signal.DynamicGraphStaticSignal\n\n여기에서 StaticGraphTemporalSignal 는 시간에 따라서 그래프 구조가 일정한 경우, 즉 \\({\\cal G}_t=\\{{\\cal V},{\\cal E}\\}\\)와 같은 구조를 의미한다.\n(예제1) StaticGraphTemporalSignal 를 이용하여 데이터 셋 만들기\n- json data \\(\\to\\) dict\n\nimport json\nimport urllib\n\n\nurl = \"https://raw.githubusercontent.com/benedekrozemberczki/pytorch_geometric_temporal/master/dataset/chickenpox.json\"\ndata_dict = json.loads(urllib.request.urlopen(url).read())\n# data_dict 출력이 김\n\n\ndata_dict.keys()\n\ndict_keys(['edges', 'node_ids', 'FX'])\n\n\n- 살펴보기\n\nnp.array(data_dict['edges']).T\n\narray([[ 0,  0,  0,  0,  0,  0,  0,  1,  1,  1,  1,  2,  2,  2,  2,  3,\n         3,  3,  3,  3,  3,  4,  4,  5,  5,  5,  5,  6,  6,  6,  6,  6,\n         6,  6,  7,  7,  7,  7,  8,  8,  8,  8,  8,  9,  9,  9,  9,  9,\n        10, 10, 10, 10, 10, 10, 10, 10, 11, 11, 11, 11, 11, 12, 12, 12,\n        12, 13, 13, 13, 13, 13, 13, 13, 13, 14, 14, 14, 14, 14, 14, 15,\n        15, 15, 16, 16, 16, 16, 16, 17, 17, 17, 17, 18, 18, 18, 18, 18,\n        18, 18, 19, 19, 19, 19],\n       [10,  6, 13,  1,  0,  5, 16,  0, 16,  1, 14, 10,  8,  2,  5,  8,\n        15, 12,  9, 10,  3,  4, 13,  0, 10,  2,  5,  0, 16,  6, 14, 13,\n        11, 18,  7, 17, 11, 18,  3,  2, 15,  8, 10,  9, 13,  3, 12, 10,\n         5,  9,  8,  3, 10,  2, 13,  0,  6, 11,  7, 13, 18,  3,  9, 13,\n        12, 13,  9,  6,  4, 12,  0, 11, 10, 18, 19,  1, 14,  6, 16,  3,\n        15,  8, 16, 14,  1,  0,  6,  7, 19, 17, 18, 14, 18, 17,  7,  6,\n        19, 11, 18, 14, 19, 17]])\n\n\n\n\\({\\cal E} = \\{(0,10),(0,6), \\dots, (19,17)\\}\\)\n혹은 \\({\\cal E} = \\{(\\tt{BACS},\\tt{JASZ}), ({\\tt BACS},{\\tt FEJER}), \\dots, (\\tt{ZALA},\\tt{VAS})\\}\\)\n\n\ndata_dict['node_ids']\n\n{'BACS': 0,\n 'BARANYA': 1,\n 'BEKES': 2,\n 'BORSOD': 3,\n 'BUDAPEST': 4,\n 'CSONGRAD': 5,\n 'FEJER': 6,\n 'GYOR': 7,\n 'HAJDU': 8,\n 'HEVES': 9,\n 'JASZ': 10,\n 'KOMAROM': 11,\n 'NOGRAD': 12,\n 'PEST': 13,\n 'SOMOGY': 14,\n 'SZABOLCS': 15,\n 'TOLNA': 16,\n 'VAS': 17,\n 'VESZPREM': 18,\n 'ZALA': 19}\n\n\n\n\\({\\cal V}=\\{\\tt{BACS},\\tt{BARANYA} \\dots, \\tt{ZALA}\\}\\)\n\n\nnp.array(data_dict['FX']), np.array(data_dict['FX']).shape\n\n(array([[-1.08135724e-03, -7.11136085e-01, -3.22808515e+00, ...,\n          1.09445310e+00, -7.08747750e-01, -1.82280792e+00],\n        [ 2.85705967e-02, -5.98430173e-01, -2.29097341e-01, ...,\n         -1.59220988e+00, -2.24597623e-01,  7.86330575e-01],\n        [ 3.54742090e-01,  1.90511208e-01,  1.61028185e+00, ...,\n          1.38183225e-01, -7.08747750e-01, -5.61724314e-01],\n        ...,\n        [-4.75512620e-01, -1.19952837e+00, -3.89043358e-01, ...,\n         -1.00023329e+00, -1.71429032e+00,  4.70746677e-02],\n        [-2.08645035e-01,  6.03766218e-01,  1.08216835e-02, ...,\n          4.71099041e-02,  2.45684924e+00, -3.44296107e-01],\n        [ 1.21464875e+00,  7.16472130e-01,  1.29038982e+00, ...,\n          4.56939849e-01,  7.43702632e-01,  1.00375878e+00]]),\n (521, 20))\n\n\n\n\\({\\bf f}=\\begin{bmatrix} {\\bf f}_1\\\\ {\\bf f}_2\\\\ \\dots \\\\ {\\bf f}_{521} \\end{bmatrix}=\\begin{bmatrix} f(t=1,v=\\tt{BACS}) & \\dots & f(t=1,v=\\tt{ZALA}) \\\\ f(t=2,v=\\tt{BACS}) & \\dots & f(t=2,v=\\tt{ZALA}) \\\\ \\dots & \\dots & \\dots \\\\ f(t=521,v=\\tt{BACS}) & \\dots & f(t=521,v=\\tt{ZALA}) \\end{bmatrix}\\)\n\n즉 data_dict는 아래와 같이 구성되어 있음\n\n\n\n\n\n\n\n\n\n\n수학 기호\n코드에 저장된 변수\n자료형\n차원\n설명\n\n\n\n\n\\({\\cal V}\\)\ndata_dict['node_ids']\ndict\n20\n20개의 노드에 대한 설명이 있음\n\n\n\\({\\cal E}\\)\ndata_dict['edges']\nlist (double list)\n(102,2)\n노드들에 대한 102개의 연결을 정의함\n\n\n\\({\\bf f}\\)\ndata_dict['node_ids']\ndict\n(521,20)\n\\(f(t,v)\\) for \\(v \\in {\\cal V}\\) and \\(t = 1,\\dots, T\\)\n\n\n\n- 주어진 자료를 정리하여 그래프신호 \\(\\big(\\{{\\cal V},{\\cal E},{\\bf W}\\},{\\bf f}\\big)\\)를 만들면 아래와 같다.\n\nedges = np.array(data_dict[\"edges\"]).T\nedge_weight = np.ones(edges.shape[1])\nf = np.array(data_dict[\"FX\"])\n\n\n여기에서 edges는 \\({\\cal E}\\)에 대한 정보를\nedges_weight는 \\({\\bf W}\\)에 대한 정보를\nf는 \\({\\bf f}\\)에 대한 정보를 저장한다.\n\n\nNote: 이때 \\({\\bf W}={\\bf E}\\) 로 정의한다. (하지만 꼭 이래야 하는건 아니야)\n\n- data_dict \\(\\to\\) dl\n\nlags = 4\nfeatures = [f[i : i + lags, :].T for i in range(f.shape[0] - lags)]\ntargets = [f[i + lags, :].T for i in range(f.shape[0] - lags)]\n\n\nnp.array(features).shape, np.array(targets).shape\n\n((517, 20, 4), (517, 20))\n\n\n\n\n\n\n\n\n\n설명변수\n반응변수\n\n\n\n\n\\({\\bf X} = {\\tt features} = \\begin{bmatrix} {\\bf f}_1 & {\\bf f}_2 & {\\bf f}_3 & {\\bf f}_4 \\\\ {\\bf f}_2 & {\\bf f}_3 & {\\bf f}_4 & {\\bf f}_5 \\\\ \\dots & \\dots & \\dots & \\dots \\\\ {\\bf f}_{517} & {\\bf f}_{518} & {\\bf f}_{519} & {\\bf f}_{520} \\end{bmatrix}\\)\n\\({\\bf y}= {\\tt targets} = \\begin{bmatrix} {\\bf f}_5 \\\\ {\\bf f}_6 \\\\ \\dots \\\\ {\\bf f}_{521} \\end{bmatrix}\\)\n\n\n\n\nAR 느낌으로 표현하면 AR(4) 임\n\n\ndataset = torch_geometric_temporal.signal.StaticGraphTemporalSignal(\n    edge_index= edges,\n    edge_weight = edge_weight,\n    features = features,\n    targets = targets\n)\n\n\ndataset\n\n&lt;torch_geometric_temporal.signal.static_graph_temporal_signal.StaticGraphTemporalSignal at 0x7f3423668bd0&gt;\n\n\n- 그런데 이 과정을 아래와 같이 할 수도 있음\n# PyTorch Geometric Temporal 공식홈페이지에 소개된 코드\nloader = torch_geometric_temporal.dataset.ChickenpoxDatasetLoader()\ndataset=loader.get_dataset(lags=4)\n- dataset은 dataset[0], \\(\\dots\\) , dataset[516]과 같은 방식으로 각 시점별 자료에 접근가능\n\ndataset[0]\n\nData(x=[20, 4], edge_index=[2, 102], edge_attr=[102], y=[20])\n\n\n각 시점에 대한 자료형은 아까 살펴보았던 PyG의 Data 자료형과 같음\n\ntype(dataset[0])\n\ntorch_geometric.data.data.Data\n\n\n\ndataset[0].x \n\ntensor([[-1.0814e-03,  2.8571e-02,  3.5474e-01,  2.9544e-01],\n        [-7.1114e-01, -5.9843e-01,  1.9051e-01,  1.0922e+00],\n        [-3.2281e+00, -2.2910e-01,  1.6103e+00, -1.5487e+00],\n        [ 6.4750e-01, -2.2117e+00, -9.6858e-01,  1.1862e+00],\n        [-1.7302e-01, -9.4717e-01,  1.0347e+00, -6.3751e-01],\n        [ 3.6345e-01, -7.5468e-01,  2.9768e-01, -1.6273e-01],\n        [-3.4174e+00,  1.7031e+00, -1.6434e+00,  1.7434e+00],\n        [-1.9641e+00,  5.5208e-01,  1.1811e+00,  6.7002e-01],\n        [-2.2133e+00,  3.0492e+00, -2.3839e+00,  1.8545e+00],\n        [-3.3141e-01,  9.5218e-01, -3.7281e-01, -8.2971e-02],\n        [-1.8380e+00, -5.8728e-01, -3.5514e-02, -7.2298e-02],\n        [-3.4669e-01, -1.9827e-01,  3.9540e-01, -2.4774e-01],\n        [ 1.4219e+00, -1.3266e+00,  5.2338e-01, -1.6374e-01],\n        [-7.7044e-01,  3.2872e-01, -1.0400e+00,  3.4945e-01],\n        [-7.8061e-01, -6.5022e-01,  1.4361e+00, -1.2864e-01],\n        [-1.0993e+00,  1.2732e-01,  5.3621e-01,  1.9023e-01],\n        [ 2.4583e+00, -1.7811e+00,  5.0732e-02, -9.4371e-01],\n        [ 1.0945e+00, -1.5922e+00,  1.3818e-01,  1.1855e+00],\n        [-7.0875e-01, -2.2460e-01, -7.0875e-01,  1.5630e+00],\n        [-1.8228e+00,  7.8633e-01, -5.6172e-01,  1.2647e+00]])\n\n\n\n이 값들은 features[0]의 값들과 같음. 즉 \\([{\\bf f}_1~ {\\bf f}_2~ {\\bf f}_3~ {\\bf f}_4]\\)를 의미함\n\n\ndataset[0].y\n\ntensor([ 0.7106, -0.0725,  2.6099,  1.7870,  0.8024, -0.2614, -0.8370,  1.9674,\n        -0.4212,  0.1655,  1.2519,  2.3743,  0.7877,  0.4531, -0.1721, -0.0614,\n         1.0452,  0.3203, -1.3791,  0.0036])\n\n\n\n이 값들은 targets[0]의 값들과 같음. 즉 \\({\\bf f}_5\\)를 의미함"
  },
  {
    "objectID": "posts/2_research/2022-12-29-STGCN-tutorial.html#summary-of-data",
    "href": "posts/2_research/2022-12-29-STGCN-tutorial.html#summary-of-data",
    "title": "[IT-STGCN] STGCN 튜토리얼",
    "section": "summary of data",
    "text": "summary of data\n\n\\(T\\) = 519\n\\(N\\) = 20 # number of nodes\n\\(|{\\cal E}|\\) = 102 # edges\n\\(f(t,v)\\)의 차원? (1,)\n시간에 따라서 Number of nodes가 변하는지? False\n시간에 따라서 Number of nodes가 변하는지? False\n\\({\\bf X}\\): (20,4)\n\\({\\bf y}\\): (20,)\n예제코드적용가능여부: Yes\n\n- Nodes : 20\n\nvertices are counties\n\n-Edges : 102\n\nedges are neighbourhoods\n\n- Time : 519\n\nbetween 2004 and 2014\nper weeks\n\n\nloader = torch_geometric_temporal.dataset.ChickenpoxDatasetLoader()\ndataset = loader.get_dataset(lags=4)\ntrain_dataset, test_dataset = temporal_signal_split(dataset, train_ratio=0.8)"
  },
  {
    "objectID": "posts/2_research/2022-12-29-STGCN-tutorial.html#learn",
    "href": "posts/2_research/2022-12-29-STGCN-tutorial.html#learn",
    "title": "[IT-STGCN] STGCN 튜토리얼",
    "section": "learn",
    "text": "learn\n\nmodel = RecurrentGCN(node_features=4, filters=32)\noptimizer = torch.optim.Adam(model.parameters(), lr=0.01)\nmodel.train()\n\nfor epoch in tqdm(range(50)):\n    for t, snapshot in enumerate(train_dataset):\n        yt_hat = model(snapshot.x, snapshot.edge_index, snapshot.edge_attr)\n        cost = torch.mean((yt_hat-snapshot.y)**2)\n        cost.backward()\n        optimizer.step()\n        optimizer.zero_grad()\n\n100%|██████████| 50/50 [00:57&lt;00:00,  1.15s/it]"
  },
  {
    "objectID": "posts/2_research/2022-12-29-STGCN-tutorial.html#visualization",
    "href": "posts/2_research/2022-12-29-STGCN-tutorial.html#visualization",
    "title": "[IT-STGCN] STGCN 튜토리얼",
    "section": "visualization",
    "text": "visualization\n\nmodel.eval()\n\nRecurrentGCN(\n  (recurrent): GConvGRU(\n    (conv_x_z): ChebConv(4, 32, K=2, normalization=sym)\n    (conv_h_z): ChebConv(32, 32, K=2, normalization=sym)\n    (conv_x_r): ChebConv(4, 32, K=2, normalization=sym)\n    (conv_h_r): ChebConv(32, 32, K=2, normalization=sym)\n    (conv_x_h): ChebConv(4, 32, K=2, normalization=sym)\n    (conv_h_h): ChebConv(32, 32, K=2, normalization=sym)\n  )\n  (linear): Linear(in_features=32, out_features=1, bias=True)\n)\n\n\n\nyhat_train = torch.stack([model(snapshot.x,snapshot.edge_index, snapshot.edge_attr) for snapshot in train_dataset]).detach().numpy()\nyhat_test = torch.stack([model(snapshot.x,snapshot.edge_index, snapshot.edge_attr) for snapshot in test_dataset]).detach().numpy()\n\n\nV = list(data_dict['node_ids'].keys())\n\n\nfig,ax = plt.subplots(20,1,figsize=(10,50))\nfor k in range(20):\n    ax[k].plot(f[:,k],'--',alpha=0.5,label='observed')\n    ax[k].set_title('node: {}'.format(V[k]))\n    ax[k].plot(yhat_train[:,k],label='predicted (tr)')\n    ax[k].plot(range(yhat_train.shape[0],yhat_train.shape[0]+yhat_test.shape[0]),yhat_test[:,k],label='predicted (test)')\n    ax[k].legend()\nfig.tight_layout()"
  },
  {
    "objectID": "posts/2_research/2022-12-29-STGCN-tutorial.html#footnotes",
    "href": "posts/2_research/2022-12-29-STGCN-tutorial.html#footnotes",
    "title": "[IT-STGCN] STGCN 튜토리얼",
    "section": "Footnotes",
    "text": "Footnotes\n\n\n일반적인 기하학적 딥러닝을 위한 파이토치 패키지↩︎\nSTGCN을 위한 패키지↩︎"
  },
  {
    "objectID": "posts/2_research/2099-05-31-Other Method.html",
    "href": "posts/2_research/2099-05-31-Other Method.html",
    "title": "ITSTGCN add Model",
    "section": "",
    "text": "summerizing it\n\n\nRANDOM\n\n\n예\n\nimport itstgcnEvolveGCNH\nimport torch\nimport itstgcnEvolveGCNH.planner \nimport pandas as pd\n\nimport numpy as np\nimport random\n\n\ndata_dict = itstgcnGCLSTM.load_data('./data/fivenodes.pkl')\nloader = itstgcnGConvLSTM.DatasetLoader(data_dict)\n\n\nfrom torch_geometric_temporal.dataset import ChickenpoxDatasetLoader\nloader1 = ChickenpoxDatasetLoader()\n\n\nfrom torch_geometric_temporal.dataset import PedalMeDatasetLoader\nloader2 = PedalMeDatasetLoader()\n\n\nfrom torch_geometric_temporal.dataset import WikiMathsDatasetLoader\nloader3 = WikiMathsDatasetLoader()\n\n\n# from torch_geometric_temporal.dataset import WindmillOutputLargeDatasetLoader\n# loader4 = WindmillOutputLargeDatasetLoader()\n\n\n# from torch_geometric_temporal.dataset import WindmillOutputMediumDatasetLoader\n# loader5 = WindmillOutputMediumDatasetLoader()\n\n\n# from torch_geometric_temporal.dataset import WindmillOutputSmallDatasetLoader\n# loader6 = WindmillOutputSmallDatasetLoader()\n\n\nloader6 = itstgcnEvolveGCNH.load_data('./data/Windmillsmall.pkl')\n\n\n# dataset6 = _a.get_dataset(lags=8)\n\n\nfrom torch_geometric_temporal.dataset import MontevideoBusDatasetLoader\nloader10 = MontevideoBusDatasetLoader()\n\n\n\nSimulation\n\nplans_stgcn_rand = {\n    'max_iteration': 1, \n    'method': ['STGCN', 'IT-STGCN'],\n    'mrate': [0.7],\n    'lags': [8], \n    'nof_filters': [12], \n    'inter_method': ['linear'],\n    'epoch': [50]\n}\n\n\nplnr = itstgcnEvolveGCNH.planner.PLNR_STGCN_RAND(plans_stgcn_rand,loader6,dataset_name='windmillsmall')\nplnr.simulate()\n\n\nplans_stgcn_rand = {\n    'max_iteration': 15, \n    'method': ['STGCN', 'IT-STGCN'], \n    'mrate': [0.7],\n    'lags': [2], \n    'nof_filters': [12], \n    'inter_method': ['linear','nearest'],\n    'epoch': [50]\n}\n\n\nplnr = itstgcnGConvLSTM.planner.PLNR_STGCN_RAND(plans_stgcn_rand,loader,dataset_name='fivenodes')\n\nplnr.simulate()\n\n\nplans_stgcn_rand = {\n    'max_iteration': 15, \n    'method': ['STGCN', 'IT-STGCN'], \n    'mrate': [0.8],\n    'lags': [2], \n    'nof_filters': [12], \n    'inter_method': ['linear','nearest'],\n    'epoch': [50]\n}\n\n\nplnr = itstgcnGConvLSTM.planner.PLNR_STGCN_RAND(plans_stgcn_rand,loader,dataset_name='fivenodes')\n\nplnr.simulate()\n\n\nplans_stgcn_rand = {\n    'max_iteration': 15, \n    'method': ['STGCN', 'IT-STGCN'], \n    'mrate': [0],\n    'lags': [2], \n    'nof_filters': [12], \n    'inter_method': ['linear','nearest'],\n    'epoch': [50]\n}\n\n\nplnr = itstgcnGConvLSTM.planner.PLNR_STGCN_RAND(plans_stgcn_rand,loader,dataset_name='fivenodes')\n\nplnr.simulate()\n\n\nmindex= [[],[],[],list(range(50,150)),[]]\n# mindex= [list(range(50,150)),[],list(range(50,90)),list(range(50,150)),[]] # node 2\nplans_stgcn_block = {\n    'max_iteration': 15, \n    'method': ['STGCN', 'IT-STGCN'],\n    'mindex': [mindex],\n    'lags': [2], \n    'nof_filters': [12], \n    'inter_method': ['linear','nearest'],\n    'epoch': [50]\n}\n\n\nplnr = itstgcnGConvLSTM.planner.PLNR_STGCN_MANUAL(plans_stgcn_block,loader,dataset_name='fivenodes')\n\nplnr.simulate(mindex=mindex,mtype='block')\n\n\nplans_stgcn_rand = {\n    'max_iteration': 15, \n    'method': ['STGCN', 'IT-STGCN'],\n    'mrate': [0.3,0.8],\n    'lags': [4], \n    'nof_filters': [32], \n    'inter_method': ['linear'],\n    'epoch': [50]\n}\n\n\nplnr = itstgcnGConvLSTM.planner.PLNR_STGCN_RAND(plans_stgcn_rand,loader1,dataset_name='chickenpox')\nplnr.simulate()\n\n\nplans_stgcn_rand = {\n    'max_iteration': 15, \n    'method': ['STGCN', 'IT-STGCN'],\n    'mrate': [0],\n    'lags': [4], \n    'nof_filters': [32], \n    'inter_method': ['linear'],\n    'epoch': [50]\n}\n\n\nplnr = itstgcnGConvLSTM.planner.PLNR_STGCN_RAND(plans_stgcn_rand,loader1,dataset_name='chickenpox')\nplnr.simulate()\n\n\nmy_list = [[] for _ in range(20)] #chickenpox\nanother_list = list(range(100,400))\nmy_list[1] = another_list\nmy_list[3] = another_list\nmy_list[5] = another_list\nmy_list[7] = another_list\nmy_list[9] = another_list\nmy_list[11] = another_list\nmy_list[13] = another_list\nmy_list[15] = another_list\nmindex = my_list\n\n\n# mindex= [[],[],[],list(range(50,150)),[]]\n# mindex= [list(range(50,150)),[],list(range(50,90)),list(range(50,150)),[]] # node 2\nplans_stgcn_block = {\n    'max_iteration': 15, \n    'method': ['STGCN', 'IT-STGCN'],\n    'mindex': [mindex],\n    'lags': [4], \n    'nof_filters': [32], \n    'inter_method': ['linear','nearest'],\n    'epoch': [50]\n}\n\n\nplnr = itstgcnGConvLSTM.planner.PLNR_STGCN_MANUAL(plans_stgcn_block,loader1,dataset_name='chickenpox')\n\nplnr.simulate(mindex=mindex,mtype='block')\n\n\nplans_stgcn_rand = {\n    'max_iteration': 30, \n    'method': ['STGCN', 'IT-STGCN'],\n    'mrate': [0,0.3,0.6],\n    'lags': [4], \n    'nof_filters': [2], \n    'inter_method': ['linear','nearest'],\n    'epoch': [50]\n}\n\n\nplnr = itstgcnGConvLSTM.planner.PLNR_STGCN_RAND(plans_stgcn_rand,loader2,dataset_name='pedalme')\n\nplnr.simulate()\n\n\nmy_list = [[] for _ in range(15)] #pedalme\nanother_list = list(range(5,25))\nmy_list[1] = another_list\nmy_list[3] = another_list\nmy_list[5] = another_list\nmy_list[7] = another_list\nmy_list[9] = another_list\nmy_list[11] = another_list\nmindex = my_list\n\n\n# mindex= [[],[],[],list(range(50,150)),[]]  # node 1\n# mindex= [list(range(10,100)),[],list(range(50,80)),[],[]] # node 2\n# mindex= [list(range(10,100)),[],list(range(50,80)),list(range(50,150)),[]] # node3\nplans_stgcn_block = {\n    'max_iteration': 30, \n    'method': ['STGCN', 'IT-STGCN'], \n    'mindex': [mindex],\n    'lags': [4], \n    'nof_filters': [2], \n    'inter_method': ['linear','nearest'],\n    'epoch': [50]\n}\n\n\nplnr = itstgcnGConvLSTM.planner.PLNR_STGCN_MANUAL(plans_stgcn_block,loader2,dataset_name='pedalme')\nplnr.simulate(mindex=mindex,mtype='block')\n\n\nplans_stgcn_rand = {\n    'max_iteration': 10, \n    'method': ['STGCN', 'IT-STGCN'],\n    'mrate': [0.3],\n    'lags': [8], \n    'nof_filters': [12], \n    'inter_method': ['linear'],\n    'epoch': [50]\n}\n\n\nplnr = itstgcnEvolveGCNH.planner.PLNR_STGCN_RAND(plans_stgcn_rand,loader3,dataset_name='wikimath')\nplnr.simulate()\n\n\nplans_stgcn_rand = {\n    'max_iteration': 15, \n    'method': ['STGCN', 'IT-STGCN'],\n    'mrate': [0.8],\n    'lags': [8], \n    'nof_filters': [12], \n    'inter_method': ['linear'],\n    'epoch': [50]\n}\n\n\nplnr = itstgcnEvolveGCNH.planner.PLNR_STGCN_RAND(plans_stgcn_rand,loader3,dataset_name='wikimath')\nplnr.simulate()\n\n\nplans_stgcn_rand = {\n    'max_iteration': 10, \n    'method': ['STGCN', 'IT-STGCN'],\n    'mrate': [0],\n    'lags': [8], \n    'nof_filters': [12], \n    'inter_method': ['linear'],\n    'epoch': [50]\n}\n\n\nplnr = itstgcnEvolveGCNH.planner.PLNR_STGCN_RAND(plans_stgcn_rand,loader3,dataset_name='wikimath')\nplnr.simulate()\n\nimport random\nmy_list = [[] for _ in range(1068)] # wikimath\nanother_list = random.sample(range(570), 72)\n# my_list에서 250개 요소 무작위 선택\nselected_indexes = random.sample(range(len(my_list)), 250)\n# 선택된 요소에 해당하는 값들을 another_list에 할당\nfor index in selected_indexes:\n    my_list[index] = another_list\n\nimport random\nmy_list = [[] for _ in range(1068)] # wikimath\nanother_list = random.sample(range(570), 150)\n# my_list에서 250개 요소 무작위 선택\nselected_indexes = random.sample(range(len(my_list)), 500)\n# 선택된 요소에 해당하는 값들을 another_list에 할당\nfor index in selected_indexes:\n    my_list[index] = another_list\nmindex = my_list\n\n\n# mindex= [[],[],[],list(range(50,150)),[]]  # node 1\n# mindex= [list(range(10,100)),[],list(range(50,80)),[],[]] # node 2\n# mindex= [list(range(10,100)),[],list(range(50,80)),list(range(50,150)),[]] # node3\nplans_stgcn_block = {\n    'max_iteration': 10, \n    'method': ['STGCN', 'IT-STGCN'], \n    'mindex': [mindex],\n    'lags': [8], \n    'nof_filters': [12], \n    'inter_method': ['linear'],\n    'epoch': [50]\n}\n\n\nplnr = itstgcnEvolveGCNH.planner.PLNR_STGCN_MANUAL(plans_stgcn_block,loader3,dataset_name='wikimath')\nplnr.simulate(mindex=mindex,mtype='block')\n\n같은 노드 같은 missing\n\nmy_list = [[] for _ in range(1068)] #wikimath\nanother_list = random.sample(range(0, 576), 300)\nfor i in range(0, 1068):\n    my_list[i] = another_list\nmindex = my_list\n\n\nplans_stgcn_block = {\n    'max_iteration': 10, \n    'method': ['STGCN', 'IT-STGCN'], \n    'mindex': [mindex],\n    'lags': [8], \n    'nof_filters': [12], \n    'inter_method': ['linear'],\n    'epoch': [50]\n}\n\n\nplnr = itstgcnEvolveGCNH.planner.PLNR_STGCN_MANUAL(plans_stgcn_block,loader3,dataset_name='wikimath')\nplnr.simulate(mindex=mindex,mtype='block')\n\n\nplans_stgcn_rand = {\n    'max_iteration': 15, \n    'method': ['STGCN', 'IT-STGCN'],\n    'mrate': [0.8],\n    'lags': [4], \n    'nof_filters': [12], \n    'inter_method': ['nearest'],\n    'epoch': [50]\n}\n\n\nplnr = itstgcnGConvLSTM.planner.PLNR_STGCN_RAND(plans_stgcn_rand,loader10,dataset_name='monte')\nplnr.simulate()\n\n\nplans_stgcn_rand = {\n    'max_iteration': 15, \n    'method': ['STGCN', 'IT-STGCN'],\n    'mrate': [0],\n    'lags': [4], \n    'nof_filters': [12], \n    'inter_method': ['nearest'],\n    'epoch': [50]\n}\n\n\nplnr = itstgcnGConvLSTM.planner.PLNR_STGCN_RAND(plans_stgcn_rand,loader10,dataset_name='monte')\nplnr.simulate()\n\n\nmy_list = [[] for _ in range(675)] #monte\nanother_list = list(range(200,350)) #743\n\nfor i in np.array(random.sample(range(0, 675), 400)):\n    my_list[i] = another_list\nmindex = my_list\n\n\n# mindex= [[],[],[],list(range(50,150)),[]]  # node 1\n# mindex= [list(range(10,100)),[],list(range(50,80)),[],[]] # node 2\n# mindex= [list(range(10,100)),[],list(range(50,80)),list(range(50,150)),[]] # node3\nplans_stgcn_block = {\n    'max_iteration': 15, \n    'method': ['STGCN', 'IT-STGCN'], \n    'mindex': [mindex],\n    'lags': [4], \n    'nof_filters': [12], \n    'inter_method': ['nearest'],\n    'epoch': [50]\n}\n\n\nplnr = itstgcnGConvLSTM.planner.PLNR_STGCN_MANUAL(plans_stgcn_block,loader10,dataset_name='monte')\nplnr.simulate(mindex=mindex,mtype='block')\n\n\nplnr = itstgcnGConvLSTM.planner.PLNR_STGCN_MANUAL(plans_stgcn_block,loader10,dataset_name='monte')\nplnr.simulate(mindex=mindex,mtype='block')"
  },
  {
    "objectID": "posts/2_research/2023-06-13-DCRNN_simulation_table_reshape.html",
    "href": "posts/2_research/2023-06-13-DCRNN_simulation_table_reshape.html",
    "title": "DCRNN_Simulation Tables_reshape",
    "section": "",
    "text": "Simulation Tables"
  },
  {
    "objectID": "posts/2_research/2023-06-13-DCRNN_simulation_table_reshape.html#baseline",
    "href": "posts/2_research/2023-06-13-DCRNN_simulation_table_reshape.html#baseline",
    "title": "DCRNN_Simulation Tables_reshape",
    "section": "Baseline",
    "text": "Baseline\n\npd.merge(data.query(\"dataset=='fivenodes' and mtype!='rand' and mtype!='block'\").groupby(['nof_filters','method','lags'])['mse'].mean().reset_index(),\n         data.query(\"dataset=='fivenodes' and mtype!='rand' and mtype!='block'\").groupby(['nof_filters','method','lags'])['mse'].std().reset_index(),\n         on=['method','nof_filters','lags']).rename(columns={'mse_x':'mean','mse_y':'std'}).round(3)\n\n\n\n\n\n\n\n\nnof_filters\nmethod\nlags\nmean\nstd\n\n\n\n\n0\n2\nIT-STGCN\n2\n1.228\n0.041\n\n\n1\n2\nSTGCN\n2\n1.230\n0.042"
  },
  {
    "objectID": "posts/2_research/2023-06-13-DCRNN_simulation_table_reshape.html#random",
    "href": "posts/2_research/2023-06-13-DCRNN_simulation_table_reshape.html#random",
    "title": "DCRNN_Simulation Tables_reshape",
    "section": "Random",
    "text": "Random\n\npd.merge(data.query(\"dataset=='fivenodes' and mtype=='rand'\").groupby(['mrate','nof_filters','method','lags'])['mse'].mean().reset_index(),\n         data.query(\"dataset=='fivenodes' and mtype=='rand'\").groupby(['mrate','nof_filters','method','lags'])['mse'].std().reset_index(),\n         on=['method','nof_filters','mrate','lags']).rename(columns={'mse_x':'mean','mse_y':'std'}).round(3).query(\"nof_filters==12\")\n\n\n\n\n\n\n\n\nmrate\nnof_filters\nmethod\nlags\nmean\nstd"
  },
  {
    "objectID": "posts/2_research/2023-06-13-DCRNN_simulation_table_reshape.html#block",
    "href": "posts/2_research/2023-06-13-DCRNN_simulation_table_reshape.html#block",
    "title": "DCRNN_Simulation Tables_reshape",
    "section": "Block",
    "text": "Block\n\npd.merge(data.query(\"dataset=='fivenodes' and mtype=='block'\").groupby(['mrate','nof_filters','method'])['mse'].mean().reset_index(),\n         data.query(\"dataset=='fivenodes' and mtype=='block'\").groupby(['mrate','nof_filters','method'])['mse'].std().reset_index(),\n         on=['method','nof_filters','mrate']).rename(columns={'mse_x':'mean','mse_y':'std'}).round(3)\n\n\n\n\n\n\n\n\nmrate\nnof_filters\nmethod\nmean\nstd\n\n\n\n\n0\n0.125\n2\nIT-STGCN\n1.227\n0.030\n\n\n1\n0.125\n2\nSTGCN\n1.254\n0.046"
  },
  {
    "objectID": "posts/2_research/2023-06-13-DCRNN_simulation_table_reshape.html#baseline-1",
    "href": "posts/2_research/2023-06-13-DCRNN_simulation_table_reshape.html#baseline-1",
    "title": "DCRNN_Simulation Tables_reshape",
    "section": "Baseline",
    "text": "Baseline\n\npd.merge(data.query(\"dataset=='chickenpox' and mtype!='rand' and mtype!='block'\").groupby(['nof_filters','method'])['mse'].mean().reset_index(),\n         data.query(\"dataset=='chickenpox' and mtype!='rand' and mtype!='block'\").groupby(['nof_filters','method'])['mse'].std().reset_index(),\n         on=['method','nof_filters']).rename(columns={'mse_x':'mean','mse_y':'std'}).round(3).query(\"nof_filters==16\")\n\n\n\n\n\n\n\n\nnof_filters\nmethod\nmean\nstd\n\n\n\n\n0\n16\nIT-STGCN\n0.726\n0.007\n\n\n1\n16\nSTGCN\n0.727\n0.011"
  },
  {
    "objectID": "posts/2_research/2023-06-13-DCRNN_simulation_table_reshape.html#random-1",
    "href": "posts/2_research/2023-06-13-DCRNN_simulation_table_reshape.html#random-1",
    "title": "DCRNN_Simulation Tables_reshape",
    "section": "Random",
    "text": "Random\n\npd.merge(data.query(\"dataset=='chickenpox' and mtype=='rand'\").groupby(['mrate','inter_method','nof_filters','method'])['mse'].mean().reset_index(),\n         data.query(\"dataset=='chickenpox' and mtype=='rand'\").groupby(['mrate','inter_method','nof_filters','method'])['mse'].std().reset_index(),\n         on=['method','inter_method','mrate','nof_filters']).rename(columns={'mse_x':'mean','mse_y':'std'}).round(3)\n\n\n\n\n\n\n\n\nmrate\ninter_method\nnof_filters\nmethod\nmean\nstd\n\n\n\n\n0\n0.3\nlinear\n16\nIT-STGCN\n0.797\n0.010\n\n\n1\n0.3\nlinear\n16\nSTGCN\n1.032\n0.039\n\n\n2\n0.8\nlinear\n16\nIT-STGCN\n1.467\n0.076\n\n\n3\n0.8\nlinear\n16\nSTGCN\n2.287\n0.074"
  },
  {
    "objectID": "posts/2_research/2023-06-13-DCRNN_simulation_table_reshape.html#block-1",
    "href": "posts/2_research/2023-06-13-DCRNN_simulation_table_reshape.html#block-1",
    "title": "DCRNN_Simulation Tables_reshape",
    "section": "Block",
    "text": "Block\n\npd.merge(data.query(\"dataset=='chickenpox' and mtype=='block'\").groupby(['inter_method','mrate','nof_filters','method'])['mse'].mean().reset_index(),\n         data.query(\"dataset=='chickenpox' and mtype=='block'\").groupby(['inter_method','mrate','nof_filters','method'])['mse'].std().reset_index(),\n         on=['method','inter_method','mrate','nof_filters']).rename(columns={'mse_x':'mean','mse_y':'std'})\n\n\n\n\n\n\n\n\ninter_method\nmrate\nnof_filters\nmethod\nmean\nstd\n\n\n\n\n0\nlinear\n0.28777\n16\nIT-STGCN\n0.739812\n0.007356\n\n\n1\nlinear\n0.28777\n16\nSTGCN\n0.812195\n0.006422\n\n\n2\nnearest\n0.28777\n16\nIT-STGCN\n0.738336\n0.007345\n\n\n3\nnearest\n0.28777\n16\nSTGCN\n0.832292\n0.009452"
  },
  {
    "objectID": "posts/2_research/2023-06-13-DCRNN_simulation_table_reshape.html#baseline-2",
    "href": "posts/2_research/2023-06-13-DCRNN_simulation_table_reshape.html#baseline-2",
    "title": "DCRNN_Simulation Tables_reshape",
    "section": "Baseline",
    "text": "Baseline\n\npd.merge(data.query(\"dataset=='pedalme' and mtype!='rand' and mtype!='block'\").groupby(['lags','nof_filters','method'])['mse'].mean().reset_index(),\n         data.query(\"dataset=='pedalme' and mtype!='rand' and mtype!='block'\").groupby(['lags','nof_filters','method'])['mse'].std().reset_index(),\n         on=['method','lags','nof_filters']).rename(columns={'mse_x':'mean','mse_y':'std'}).round(3).query(\"lags==4\")\n\n\n\n\n\n\n\n\nlags\nnof_filters\nmethod\nmean\nstd\n\n\n\n\n0\n4\n8\nIT-STGCN\n1.131\n0.015\n\n\n1\n4\n8\nSTGCN\n1.131\n0.015"
  },
  {
    "objectID": "posts/2_research/2023-06-13-DCRNN_simulation_table_reshape.html#random-2",
    "href": "posts/2_research/2023-06-13-DCRNN_simulation_table_reshape.html#random-2",
    "title": "DCRNN_Simulation Tables_reshape",
    "section": "Random",
    "text": "Random\n\npd.merge(data.query(\"dataset=='pedalme' and mtype=='rand'\").groupby(['mrate','lags','inter_method','method'])['mse'].mean().reset_index(),\n         data.query(\"dataset=='pedalme' and mtype=='rand'\").groupby(['mrate','lags','inter_method','method'])['mse'].std().reset_index(),\n         on=['method','mrate','lags','inter_method']).rename(columns={'mse_x':'mean','mse_y':'std'}).round(3)\n\n\n\n\n\n\n\n\nmrate\nlags\ninter_method\nmethod\nmean\nstd\n\n\n\n\n0\n0.3\n4\nlinear\nIT-STGCN\n1.190\n0.029\n\n\n1\n0.3\n4\nlinear\nSTGCN\n1.277\n0.064\n\n\n2\n0.3\n4\nnearest\nIT-STGCN\n1.179\n0.035\n\n\n3\n0.3\n4\nnearest\nSTGCN\n1.278\n0.060\n\n\n4\n0.6\n4\nlinear\nIT-STGCN\n1.314\n0.072\n\n\n5\n0.6\n4\nlinear\nSTGCN\n1.551\n0.092\n\n\n6\n0.6\n4\nnearest\nIT-STGCN\n1.303\n0.078\n\n\n7\n0.6\n4\nnearest\nSTGCN\n1.509\n0.068"
  },
  {
    "objectID": "posts/2_research/2023-06-13-DCRNN_simulation_table_reshape.html#block-2",
    "href": "posts/2_research/2023-06-13-DCRNN_simulation_table_reshape.html#block-2",
    "title": "DCRNN_Simulation Tables_reshape",
    "section": "Block",
    "text": "Block\n\npd.merge(data.query(\"dataset=='pedalme' and mtype=='block'\").groupby(['mrate','lags','inter_method','method'])['mse'].mean().reset_index(),\n         data.query(\"dataset=='pedalme' and mtype=='block'\").groupby(['mrate','lags','inter_method','method'])['mse'].std().reset_index(),\n         on=['method','mrate','lags','inter_method']).rename(columns={'mse_x':'mean','mse_y':'std'}).round(3).query(\"lags==4\")\n\n\n\n\n\n\n\n\nmrate\nlags\ninter_method\nmethod\nmean\nstd\n\n\n\n\n0\n0.286\n4\nlinear\nIT-STGCN\n1.154\n0.014\n\n\n1\n0.286\n4\nlinear\nSTGCN\n1.248\n0.019\n\n\n2\n0.286\n4\nnearest\nIT-STGCN\n1.150\n0.014\n\n\n3\n0.286\n4\nnearest\nSTGCN\n1.304\n0.021"
  },
  {
    "objectID": "posts/2_research/2023-06-13-DCRNN_simulation_table_reshape.html#w_st",
    "href": "posts/2_research/2023-06-13-DCRNN_simulation_table_reshape.html#w_st",
    "title": "DCRNN_Simulation Tables_reshape",
    "section": "W_st",
    "text": "W_st\n\npd.merge(data_pedal2.query(\"mtype=='rand'\").groupby(['mrate','lags','inter_method','method'])['mse'].mean().reset_index(),\n         data_pedal2.query(\"mtype=='rand'\").groupby(['mrate','lags','inter_method','method'])['mse'].std().reset_index(),\n         on=['method','mrate','lags','inter_method']).rename(columns={'mse_x':'mean','mse_y':'std'}).round(3).query(\"lags==4\")\n\n\n\n\n\n\n\n\nmrate\nlags\ninter_method\nmethod\nmean\nstd\n\n\n\n\n0\n0.3\n4\nlinear\nIT-STGCN\n1.153\n0.036\n\n\n1\n0.3\n4\nlinear\nSTGCN\n1.263\n0.053\n\n\n2\n0.3\n4\nnearest\nIT-STGCN\n1.154\n0.038\n\n\n3\n0.3\n4\nnearest\nSTGCN\n1.269\n0.068\n\n\n4\n0.6\n4\nlinear\nIT-STGCN\n1.241\n0.079\n\n\n5\n0.6\n4\nlinear\nSTGCN\n1.506\n0.065\n\n\n6\n0.6\n4\nnearest\nIT-STGCN\n1.208\n0.079\n\n\n7\n0.6\n4\nnearest\nSTGCN\n1.552\n0.087\n\n\n\n\n\n\n\n\npd.merge(data_pedal2.query(\"mtype=='block'\").groupby(['mrate','lags','inter_method','method'])['mse'].mean().reset_index(),\n         data_pedal2.query(\"mtype=='block'\").groupby(['mrate','lags','inter_method','method'])['mse'].std().reset_index(),\n         on=['method','mrate','lags','inter_method']).rename(columns={'mse_x':'mean','mse_y':'std'}).round(3).query(\"lags==4\")\n\n\n\n\n\n\n\n\nmrate\nlags\ninter_method\nmethod\nmean\nstd\n\n\n\n\n0\n0.286\n4\nlinear\nIT-STGCN\n1.145\n0.013\n\n\n1\n0.286\n4\nlinear\nSTGCN\n1.295\n0.019\n\n\n2\n0.286\n4\nnearest\nIT-STGCN\n1.143\n0.011\n\n\n3\n0.286\n4\nnearest\nSTGCN\n1.310\n0.019"
  },
  {
    "objectID": "posts/2_research/2023-06-13-DCRNN_simulation_table_reshape.html#baseline-3",
    "href": "posts/2_research/2023-06-13-DCRNN_simulation_table_reshape.html#baseline-3",
    "title": "DCRNN_Simulation Tables_reshape",
    "section": "Baseline",
    "text": "Baseline\n\npd.merge(data.query(\"dataset=='wikimath' and mrate==0\").groupby(['lags','nof_filters','method'])['mse'].mean().reset_index(),\n         data.query(\"dataset=='wikimath' and mrate==0\").groupby(['lags','nof_filters','method'])['mse'].std().reset_index(),\n         on=['lags','nof_filters','method']).rename(columns={'mse_x':'mean','mse_y':'std'}).round(3)\n\n\n\n\n\n\n\n\nlags\nnof_filters\nmethod\nmean\nstd\n\n\n\n\n0\n8\n12\nIT-STGCN\n0.582\n0.006\n\n\n1\n8\n12\nSTGCN\n0.580\n0.006"
  },
  {
    "objectID": "posts/2_research/2023-06-13-DCRNN_simulation_table_reshape.html#random-3",
    "href": "posts/2_research/2023-06-13-DCRNN_simulation_table_reshape.html#random-3",
    "title": "DCRNN_Simulation Tables_reshape",
    "section": "Random",
    "text": "Random\n\npd.merge(data.query(\"dataset=='wikimath' and mtype=='rand'\").groupby(['mrate','lags','method'])['mse'].mean().reset_index(),\n         data.query(\"dataset=='wikimath' and mtype=='rand'\").groupby(['mrate','lags','method'])['mse'].std().reset_index(),\n         on=['method','mrate','lags']).rename(columns={'mse_x':'mean','mse_y':'std'}).round(3)\n\n\n\n\n\n\n\n\nmrate\nlags\nmethod\nmean\nstd\n\n\n\n\n0\n0.3\n8\nIT-STGCN\n0.588\n0.007\n\n\n1\n0.3\n8\nSTGCN\n0.603\n0.010\n\n\n2\n0.5\n8\nIT-STGCN\n0.590\n0.006\n\n\n3\n0.5\n8\nSTGCN\n0.652\n0.015\n\n\n4\n0.6\n8\nIT-STGCN\n0.592\n0.005\n\n\n5\n0.6\n8\nSTGCN\n0.688\n0.011\n\n\n6\n0.8\n8\nIT-STGCN\n0.672\n0.007\n\n\n7\n0.8\n8\nSTGCN\n0.846\n0.031"
  },
  {
    "objectID": "posts/2_research/2023-06-13-DCRNN_simulation_table_reshape.html#block-3",
    "href": "posts/2_research/2023-06-13-DCRNN_simulation_table_reshape.html#block-3",
    "title": "DCRNN_Simulation Tables_reshape",
    "section": "Block",
    "text": "Block\n\npd.merge(data.query(\"dataset=='wikimath' and mtype=='block'\").groupby(['mrate','lags','method'])['mse'].mean().reset_index(),\n         data.query(\"dataset=='wikimath' and mtype=='block'\").groupby(['mrate','lags','method'])['mse'].std().reset_index(),\n         on=['method','mrate','lags']).rename(columns={'mse_x':'mean','mse_y':'std'})\n\n\n\n\n\n\n\n\nmrate\nlags\nmethod\nmean\nstd\n\n\n\n\n0\n0.119837\n8\nIT-STGCN\n0.582594\n0.006427\n\n\n1\n0.119837\n8\nSTGCN\n0.578406\n0.004975"
  },
  {
    "objectID": "posts/2_research/2023-06-13-DCRNN_simulation_table_reshape.html#missing-values-on-the-same-nodes",
    "href": "posts/2_research/2023-06-13-DCRNN_simulation_table_reshape.html#missing-values-on-the-same-nodes",
    "title": "DCRNN_Simulation Tables_reshape",
    "section": "missing values on the same nodes",
    "text": "missing values on the same nodes\n\npd.merge(data_wiki_GSO.groupby(['mrate','lags','method'])['mse'].mean().reset_index(),\n        data_wiki_GSO.groupby(['mrate','lags','method'])['mse'].std().reset_index(),\n         on=['method','mrate','lags']).rename(columns={'mse_x':'mean','mse_y':'std'}).round(3)\n\n\n\n\n\n\n\n\nmrate\nlags\nmethod\nmean\nstd\n\n\n\n\n0\n0.512\n8\nIT-STGCN\n0.592\n0.005\n\n\n1\n0.512\n8\nSTGCN\n0.665\n0.015"
  },
  {
    "objectID": "posts/2_research/2023-06-13-DCRNN_simulation_table_reshape.html#baseline-4",
    "href": "posts/2_research/2023-06-13-DCRNN_simulation_table_reshape.html#baseline-4",
    "title": "DCRNN_Simulation Tables_reshape",
    "section": "Baseline",
    "text": "Baseline\n\npd.merge(data.query(\"dataset=='windmillsmall' and mrate==0\").groupby(['lags','method'])['mse'].mean().reset_index(),\n         data.query(\"dataset=='windmillsmall' and mrate==0\").groupby(['lags','method'])['mse'].std().reset_index(),\n         on=['method','lags']).rename(columns={'mse_x':'mean','mse_y':'std'}).round(3)\n\n\n\n\n\n\n\n\nlags\nmethod\nmean\nstd\n\n\n\n\n0\n8\nIT-STGCN\n0.988\n0.003\n\n\n1\n8\nSTGCN\n0.987\n0.002"
  },
  {
    "objectID": "posts/2_research/2023-06-13-DCRNN_simulation_table_reshape.html#random-4",
    "href": "posts/2_research/2023-06-13-DCRNN_simulation_table_reshape.html#random-4",
    "title": "DCRNN_Simulation Tables_reshape",
    "section": "Random",
    "text": "Random\n\npd.merge(data.query(\"dataset=='windmillsmall' and mtype=='rand'\").groupby(['mrate','lags','method'])['mse'].mean().reset_index(),\n         data.query(\"dataset=='windmillsmall' and mtype=='rand'\").groupby(['mrate','lags','method'])['mse'].std().reset_index(),\n         on=['method','mrate','lags']).rename(columns={'mse_x':'mean','mse_y':'std'}).round(3)\n\n\n\n\n\n\n\n\nmrate\nlags\nmethod\nmean\nstd\n\n\n\n\n0\n0.7\n8\nIT-STGCN\n1.117\n0.034\n\n\n1\n0.7\n8\nSTGCN\n1.348\n0.057"
  },
  {
    "objectID": "posts/2_research/2023-06-13-DCRNN_simulation_table_reshape.html#block-4",
    "href": "posts/2_research/2023-06-13-DCRNN_simulation_table_reshape.html#block-4",
    "title": "DCRNN_Simulation Tables_reshape",
    "section": "Block",
    "text": "Block\n\npd.merge(data.query(\"dataset=='windmillsmall' and mtype=='block'\").groupby(['mrate','lags','method'])['mse'].mean().reset_index(),\n         data.query(\"dataset=='windmillsmall' and mtype=='block'\").groupby(['mrate','lags','method'])['mse'].std().reset_index(),\n         on=['method','mrate','lags']).rename(columns={'mse_x':'mean','mse_y':'std'}).round(3)\n\n\n\n\n\n\n\n\nmrate\nlags\nmethod\nmean\nstd\n\n\n\n\n0\n0.081\n8\nIT-STGCN\n0.983\n0.002\n\n\n1\n0.081\n8\nSTGCN\n0.994\n0.005"
  },
  {
    "objectID": "posts/2_research/2023-06-13-DCRNN_simulation_table_reshape.html#baseline-5",
    "href": "posts/2_research/2023-06-13-DCRNN_simulation_table_reshape.html#baseline-5",
    "title": "DCRNN_Simulation Tables_reshape",
    "section": "Baseline",
    "text": "Baseline\n\npd.merge(data.query(\"dataset=='monte' and mrate==0\").groupby(['lags','method'])['mse'].mean().reset_index(),\n         data.query(\"dataset=='monte' and mrate==0\").groupby(['lags','method'])['mse'].std().reset_index(),\n         on=['method','lags']).rename(columns={'mse_x':'mean','mse_y':'std'}).round(3)\n\n\n\n\n\n\n\n\nlags\nmethod\nmean\nstd\n\n\n\n\n0\n4\nIT-STGCN\n0.936\n0.002\n\n\n1\n4\nSTGCN\n0.936\n0.002"
  },
  {
    "objectID": "posts/2_research/2023-06-13-DCRNN_simulation_table_reshape.html#random-5",
    "href": "posts/2_research/2023-06-13-DCRNN_simulation_table_reshape.html#random-5",
    "title": "DCRNN_Simulation Tables_reshape",
    "section": "Random",
    "text": "Random\n\npd.merge(data.query(\"dataset=='monte' and mtype=='rand'\").groupby(['mrate','lags','inter_method','method'])['mse'].mean().reset_index(),\n         data.query(\"dataset=='monte' and mtype=='rand'\").groupby(['mrate','lags','inter_method','method'])['mse'].std().reset_index(),\n         on=['mrate','inter_method','method','mrate','lags']).rename(columns={'mse_x':'mean','mse_y':'std'})\n\n\n\n\n\n\n\n\nmrate\nlags\ninter_method\nmethod\nmean\nstd\n\n\n\n\n0\n0.3\n4\nnearest\nIT-STGCN\n0.937868\n0.001369\n\n\n1\n0.3\n4\nnearest\nSTGCN\n1.007506\n0.005817\n\n\n2\n0.5\n4\nnearest\nIT-STGCN\n0.947735\n0.002210\n\n\n3\n0.5\n4\nnearest\nSTGCN\n1.119226\n0.024755\n\n\n4\n0.7\n4\nnearest\nIT-STGCN\n1.030943\n0.017530\n\n\n5\n0.7\n4\nnearest\nSTGCN\n1.214012\n0.033267\n\n\n6\n0.8\n4\nnearest\nIT-STGCN\n1.111060\n0.036307\n\n\n7\n0.8\n4\nnearest\nSTGCN\n1.225077\n0.072743"
  },
  {
    "objectID": "posts/2_research/2023-06-13-DCRNN_simulation_table_reshape.html#block-5",
    "href": "posts/2_research/2023-06-13-DCRNN_simulation_table_reshape.html#block-5",
    "title": "DCRNN_Simulation Tables_reshape",
    "section": "Block",
    "text": "Block\n\npd.merge(data.query(\"dataset=='monte' and mtype=='block'\").groupby(['mrate','lags','inter_method','method'])['mse'].mean().reset_index(),\n         data.query(\"dataset=='monte' and mtype=='block'\").groupby(['mrate','lags','inter_method','method'])['mse'].std().reset_index(),\n         on=['method','mrate','inter_method','lags']).rename(columns={'mse_x':'mean','mse_y':'std'})\n\n\n\n\n\n\n\n\nmrate\nlags\ninter_method\nmethod\nmean\nstd\n\n\n\n\n0\n0.149142\n4\nnearest\nIT-STGCN\n0.940344\n0.001323\n\n\n1\n0.149142\n4\nnearest\nSTGCN\n0.955944\n0.003010"
  },
  {
    "objectID": "posts/2_research/2023-06-28-DYGRENCODER_Simulation_boxplot_reshape.html",
    "href": "posts/2_research/2023-06-28-DYGRENCODER_Simulation_boxplot_reshape.html",
    "title": "DYGRENCODER_Simulation_reshape",
    "section": "",
    "text": "Simulation Study"
  },
  {
    "objectID": "posts/2_research/2023-06-28-DYGRENCODER_Simulation_boxplot_reshape.html#baseline",
    "href": "posts/2_research/2023-06-28-DYGRENCODER_Simulation_boxplot_reshape.html#baseline",
    "title": "DYGRENCODER_Simulation_reshape",
    "section": "Baseline",
    "text": "Baseline\n\ndata.query(\"method!='GNAR' and mrate==0\").plot.box(backend='plotly',x='mrate',color='method',y='mse',facet_col='lags',facet_row='nof_filters',height=600)"
  },
  {
    "objectID": "posts/2_research/2023-06-28-DYGRENCODER_Simulation_boxplot_reshape.html#random",
    "href": "posts/2_research/2023-06-28-DYGRENCODER_Simulation_boxplot_reshape.html#random",
    "title": "DYGRENCODER_Simulation_reshape",
    "section": "Random",
    "text": "Random\n\ndata.query(\"method!='GNAR' and mtype =='rand'\").plot.box(backend='plotly',x='mrate',color='method',y='mse',facet_col='inter_method',facet_row='nof_filters',height=600)"
  },
  {
    "objectID": "posts/2_research/2023-06-28-DYGRENCODER_Simulation_boxplot_reshape.html#block",
    "href": "posts/2_research/2023-06-28-DYGRENCODER_Simulation_boxplot_reshape.html#block",
    "title": "DYGRENCODER_Simulation_reshape",
    "section": "Block",
    "text": "Block\n\ndata.query(\"method!='GNAR' and mtype =='block' \").plot.box(backend='plotly',x='mrate',color='method',y='mse',facet_col='inter_method',facet_row='nof_filters',height=600)"
  },
  {
    "objectID": "posts/2_research/2023-06-28-DYGRENCODER_Simulation_boxplot_reshape.html#baseline-1",
    "href": "posts/2_research/2023-06-28-DYGRENCODER_Simulation_boxplot_reshape.html#baseline-1",
    "title": "DYGRENCODER_Simulation_reshape",
    "section": "Baseline",
    "text": "Baseline\n\ndata.query(\"method!='GNAR' and mrate ==0 \").plot.box(backend='plotly',x='mrate',color='method',y='mse',facet_col='nof_filters',height=600)"
  },
  {
    "objectID": "posts/2_research/2023-06-28-DYGRENCODER_Simulation_boxplot_reshape.html#random-1",
    "href": "posts/2_research/2023-06-28-DYGRENCODER_Simulation_boxplot_reshape.html#random-1",
    "title": "DYGRENCODER_Simulation_reshape",
    "section": "Random",
    "text": "Random\n\ndata.query(\"method!='GNAR' and mtype =='rand'\").plot.box(backend='plotly',x='mrate',color='method',y='mse',facet_col='nof_filters',facet_row='inter_method',height=800)"
  },
  {
    "objectID": "posts/2_research/2023-06-28-DYGRENCODER_Simulation_boxplot_reshape.html#block-1",
    "href": "posts/2_research/2023-06-28-DYGRENCODER_Simulation_boxplot_reshape.html#block-1",
    "title": "DYGRENCODER_Simulation_reshape",
    "section": "Block",
    "text": "Block\n\ndata.query(\"method!='GNAR' and mtype =='block'\").plot.box(backend='plotly',x='mrate',color='method',y='mse',facet_col='inter_method',facet_row='lags',height=800)"
  },
  {
    "objectID": "posts/2_research/2023-06-28-DYGRENCODER_Simulation_boxplot_reshape.html#baseline-2",
    "href": "posts/2_research/2023-06-28-DYGRENCODER_Simulation_boxplot_reshape.html#baseline-2",
    "title": "DYGRENCODER_Simulation_reshape",
    "section": "Baseline",
    "text": "Baseline\n\ndata.query(\"mrate ==0 and lags!=2\").plot.box(backend='plotly',x='epoch',color='method',y='mse',facet_col='nof_filters',facet_row='lags',height=400)"
  },
  {
    "objectID": "posts/2_research/2023-06-28-DYGRENCODER_Simulation_boxplot_reshape.html#random-2",
    "href": "posts/2_research/2023-06-28-DYGRENCODER_Simulation_boxplot_reshape.html#random-2",
    "title": "DYGRENCODER_Simulation_reshape",
    "section": "random",
    "text": "random\n\ndata.query(\"method!='GNAR' and mtype =='rand' and lags!=2\").plot.box(backend='plotly',x='mrate',color='method',y='mse',facet_col='nof_filters',facet_row='inter_method',height=800)"
  },
  {
    "objectID": "posts/2_research/2023-06-28-DYGRENCODER_Simulation_boxplot_reshape.html#block-2",
    "href": "posts/2_research/2023-06-28-DYGRENCODER_Simulation_boxplot_reshape.html#block-2",
    "title": "DYGRENCODER_Simulation_reshape",
    "section": "block",
    "text": "block\n\ndata.query(\"method!='GNAR' and mtype =='block' and lags!=2 \").plot.box(backend='plotly',x='mrate',color='method',y='mse',facet_col='nof_filters',facet_row='inter_method',height=800)"
  },
  {
    "objectID": "posts/2_research/2023-06-28-DYGRENCODER_Simulation_boxplot_reshape.html#weight-matrix-time-node-고려한-결과",
    "href": "posts/2_research/2023-06-28-DYGRENCODER_Simulation_boxplot_reshape.html#weight-matrix-time-node-고려한-결과",
    "title": "DYGRENCODER_Simulation_reshape",
    "section": "weight matrix time, node 고려한 결과",
    "text": "weight matrix time, node 고려한 결과\n\ndf1 = pd.read_csv('./simulation_results/2023-06-30_13-25-56.csv')\ndf2 = pd.read_csv('./simulation_results/2023-06-30_14-00-19.csv')\n\n\ndata2 = pd.concat([df1,df2],axis=0)\n\n\ndata2.to_csv('./simulation_results/Real_simulation_reshape/DYGRENCODER_pedalme_Simulation_itstgcnsnd.csv',index=False)\n\n\ndata2 = pd.read_csv('./simulation_results/Real_simulation_reshape/DYGRENCODER_pedalme_Simulation_itstgcnsnd.csv')\n\n\ndata2.query(\"mtype=='rand'\").plot.box(backend='plotly',x='mrate',color='method',y='mse',facet_col='inter_method',facet_row='nof_filters',height=1000)\n\n                                                \n\n\n\ndata2.query(\"mtype=='block'\").plot.box(backend='plotly',x='mrate',color='method',y='mse',facet_col='inter_method',facet_row='nof_filters',height=1000)"
  },
  {
    "objectID": "posts/2_research/2023-06-28-DYGRENCODER_Simulation_boxplot_reshape.html#baseline-3",
    "href": "posts/2_research/2023-06-28-DYGRENCODER_Simulation_boxplot_reshape.html#baseline-3",
    "title": "DYGRENCODER_Simulation_reshape",
    "section": "Baseline",
    "text": "Baseline\n\ndata.query(\"mrate==0 and method!='GNAR'\").plot.box(backend='plotly',x='mrate',color='method',y='mse',height=600)"
  },
  {
    "objectID": "posts/2_research/2023-06-28-DYGRENCODER_Simulation_boxplot_reshape.html#random-3",
    "href": "posts/2_research/2023-06-28-DYGRENCODER_Simulation_boxplot_reshape.html#random-3",
    "title": "DYGRENCODER_Simulation_reshape",
    "section": "random",
    "text": "random\n\ndata.query(\"mtype=='rand' and mrate !=0 and method!='GNAR'\").plot.box(backend='plotly',x='mrate',color='method',y='mse',facet_col='lags',facet_row='nof_filters',height=800)"
  },
  {
    "objectID": "posts/2_research/2023-06-28-DYGRENCODER_Simulation_boxplot_reshape.html#block-3",
    "href": "posts/2_research/2023-06-28-DYGRENCODER_Simulation_boxplot_reshape.html#block-3",
    "title": "DYGRENCODER_Simulation_reshape",
    "section": "block",
    "text": "block\n\ndata.query(\"mtype=='block' and method!='GNAR'\").plot.box(backend='plotly',x='mrate',color='method',y='mse',facet_col='lags',facet_row='nof_filters',height=1200)"
  },
  {
    "objectID": "posts/2_research/2023-06-28-DYGRENCODER_Simulation_boxplot_reshape.html#missing-values-on-the-same-nodes",
    "href": "posts/2_research/2023-06-28-DYGRENCODER_Simulation_boxplot_reshape.html#missing-values-on-the-same-nodes",
    "title": "DYGRENCODER_Simulation_reshape",
    "section": "missing values on the same nodes",
    "text": "missing values on the same nodes\n\ndf1 = pd.read_csv('./simulation_results/2023-07-01_17-41-40.csv') # STGCN IT-STGCN block\ndf2 = pd.read_csv('./simulation_results/2023-07-01_21-00-26.csv') # STGCN IT-STGCN\ndf3 = pd.read_csv('./simulation_results/2023-07-02_00-17-30.csv') \n\n\ndata = pd.concat([df1,df2,df3],axis=0)\n\n\ndata.to_csv('./simulation_results/Real_simulation_reshape/DYGRENCODER_wikimath_GSO_st.csv',index=False)\n\n\ndata = pd.read_csv('./simulation_results/Real_simulation_reshape/DYGRENCODER_wikimath_GSO_st.csv')\n\n\ndata.query(\"method!='GNAR' and mtype =='block' \").plot.box(backend='plotly',x='mrate',color='method',y='mse',facet_col='inter_method',facet_row='lags',height=800)"
  },
  {
    "objectID": "posts/2_research/2023-06-28-DYGRENCODER_Simulation_boxplot_reshape.html#baseline-4",
    "href": "posts/2_research/2023-06-28-DYGRENCODER_Simulation_boxplot_reshape.html#baseline-4",
    "title": "DYGRENCODER_Simulation_reshape",
    "section": "Baseline",
    "text": "Baseline\n\ndata.query(\"method!='GNAR' and mrate ==0 \").plot.box(backend='plotly',x='mrate',color='method',y='mse',facet_col='lags',height=800)"
  },
  {
    "objectID": "posts/2_research/2023-06-28-DYGRENCODER_Simulation_boxplot_reshape.html#random-4",
    "href": "posts/2_research/2023-06-28-DYGRENCODER_Simulation_boxplot_reshape.html#random-4",
    "title": "DYGRENCODER_Simulation_reshape",
    "section": "random",
    "text": "random\n\ndata.query(\"method!='GNAR' and mtype =='rand' \").plot.box(backend='plotly',x='mrate',color='method',y='mse',facet_col='lags',facet_row='inter_method',height=800)"
  },
  {
    "objectID": "posts/2_research/2023-06-28-DYGRENCODER_Simulation_boxplot_reshape.html#block-4",
    "href": "posts/2_research/2023-06-28-DYGRENCODER_Simulation_boxplot_reshape.html#block-4",
    "title": "DYGRENCODER_Simulation_reshape",
    "section": "block",
    "text": "block\n\ndata.query(\"method!='GNAR' and mtype =='block' \").plot.box(backend='plotly',x='mrate',color='method',y='mse',facet_col='lags',facet_row='inter_method',height=800)"
  },
  {
    "objectID": "posts/2_research/2023-06-28-DYGRENCODER_Simulation_boxplot_reshape.html#baseline-5",
    "href": "posts/2_research/2023-06-28-DYGRENCODER_Simulation_boxplot_reshape.html#baseline-5",
    "title": "DYGRENCODER_Simulation_reshape",
    "section": "Baseline",
    "text": "Baseline\n\ndata.query(\"mrate==0 and method!='GNAR'\").plot.box(backend='plotly',x='mrate',color='method',y='mse',facet_col='lags',facet_row='nof_filters',height=600)"
  },
  {
    "objectID": "posts/2_research/2023-06-28-DYGRENCODER_Simulation_boxplot_reshape.html#random-5",
    "href": "posts/2_research/2023-06-28-DYGRENCODER_Simulation_boxplot_reshape.html#random-5",
    "title": "DYGRENCODER_Simulation_reshape",
    "section": "random",
    "text": "random\n\ndata.query(\"mtype=='rand' and mrate !=0 and method!='GNAR'\").plot.box(backend='plotly',x='mrate',color='method',y='mse',facet_col='lags',facet_row='inter_method',height=800)"
  },
  {
    "objectID": "posts/2_research/2023-06-28-DYGRENCODER_Simulation_boxplot_reshape.html#block-5",
    "href": "posts/2_research/2023-06-28-DYGRENCODER_Simulation_boxplot_reshape.html#block-5",
    "title": "DYGRENCODER_Simulation_reshape",
    "section": "block",
    "text": "block\n\ndata.query(\"mtype=='block' and method!='GNAR'\").plot.box(backend='plotly',x='mrate',color='method',y='mse',facet_col='lags',facet_row='nof_filters',height=1200)"
  },
  {
    "objectID": "2_research.html",
    "href": "2_research.html",
    "title": "Research",
    "section": "",
    "text": "Date\n\n\nTitle\n\n\nAuthor\n\n\n\n\n\n\nMay 31, 2099\n\n\nITSTGCN add Model\n\n\nSEOYEON CHOI\n\n\n\n\nFeb 1, 2024\n\n\nSelf Consistency Toy ex\n\n\nSEOYEON CHOI\n\n\n\n\nJan 28, 2024\n\n\n시뮬 데이터 정리중\n\n\nSEOYEON CHOI\n\n\n\n\nJan 28, 2024\n\n\nGNAR-R\n\n\nSEOYEON CHOI\n\n\n\n\nJan 14, 2024\n\n\nToy Example Figure(Intro)\n\n\nSEOYEON CHOI\n\n\n\n\nAug 25, 2023\n\n\nBatch\n\n\nSEOYEON CHOI\n\n\n\n\nJul 18, 2023\n\n\nEbayesThresh Toy ex\n\n\nSEOYEON CHOI\n\n\n\n\nJul 8, 2023\n\n\nToy example using GNAR\n\n\nSEOYEON CHOI\n\n\n\n\nJul 8, 2023\n\n\nToy example using GNAR\n\n\nSEOYEON CHOI\n\n\n\n\nJul 1, 2023\n\n\nEvolveGCNH_Simulation_reshape\n\n\nSEOYEON CHOI\n\n\n\n\nJul 1, 2023\n\n\nEvolveGCNH_Simulation Tables_reshape\n\n\nSEOYEON CHOI\n\n\n\n\nJun 28, 2023\n\n\nDYGRENCODER_Simulation Tables_reshape\n\n\nSEOYEON CHOI\n\n\n\n\nJun 28, 2023\n\n\nDYGRENCODER_Simulation_reshape\n\n\nSEOYEON CHOI\n\n\n\n\nJun 28, 2023\n\n\nDYGRENCODER_Simulation_reshape\n\n\nSEOYEON CHOI\n\n\n\n\nJun 25, 2023\n\n\nEvolveGCNO_Simulation_reshape\n\n\nSEOYEON CHOI\n\n\n\n\nJun 25, 2023\n\n\nEvolveGCNO_Simulation Tables_reshape\n\n\nSEOYEON CHOI\n\n\n\n\nJun 20, 2023\n\n\nTGCN_Simulation_reshape\n\n\nSEOYEON CHOI\n\n\n\n\nJun 20, 2023\n\n\nTGCN_Simulation Tables_reshape\n\n\nSEOYEON CHOI\n\n\n\n\nJun 13, 2023\n\n\nDCRNN_Simulation_reshape\n\n\nSEOYEON CHOI\n\n\n\n\nJun 13, 2023\n\n\nLRGCN_Simulation Tables_reshape\n\n\nSEOYEON CHOI\n\n\n\n\nJun 13, 2023\n\n\nLRGCN_Simulation_reshape\n\n\nSEOYEON CHOI\n\n\n\n\nJun 13, 2023\n\n\nDCRNN_Simulation Tables_reshape\n\n\nSEOYEON CHOI\n\n\n\n\nJun 6, 2023\n\n\nGCLSTM_Simulation Tables_reshape\n\n\nSEOYEON CHOI\n\n\n\n\nJun 6, 2023\n\n\nGCLSTM_Simulation_reshape\n\n\nSEOYEON CHOI\n\n\n\n\nMay 30, 2023\n\n\nGConvLSTM_Simulation_reshape\n\n\nSEOYEON CHOI\n\n\n\n\nMay 30, 2023\n\n\nGConvLSTM_Simulation Tables_reshape\n\n\nSEOYEON CHOI\n\n\n\n\nMay 27, 2023\n\n\nAdding the RecurrentGCN models\n\n\nSEOYEON CHOI\n\n\n\n\nMay 25, 2023\n\n\nGConvGRU_Simulation Boxplot_reshape\n\n\nSEOYEON CHOI\n\n\n\n\nMay 25, 2023\n\n\nGConvGRU_Simulation Tables_reshape\n\n\nSEOYEON CHOI\n\n\n\n\nMay 17, 2023\n\n\nGConvGRU and GNAR_Simulation Tables_reshape\n\n\nSEOYEON CHOI\n\n\n\n\nMay 11, 2023\n\n\nPyG Geometric Temporal Examples\n\n\nSEOYEON CHOI\n\n\n\n\nMay 6, 2023\n\n\nITSTGCN Article Refernece\n\n\nSEOYEON CHOI\n\n\n\n\nMay 4, 2023\n\n\nQuestions of PyTorch Geometric Temporal\n\n\nSEOYEON CHOI\n\n\n\n\nApr 29, 2023\n\n\nPadalme GSO_st\n\n\nSEOYEON CHOI\n\n\n\n\nApr 27, 2023\n\n\nSimulation Tables\n\n\nSEOYEON CHOI\n\n\n\n\nApr 27, 2023\n\n\nToy Example Note\n\n\nGUEBIN CHOI\n\n\n\n\nApr 25, 2023\n\n\nNote_weight amatrix\n\n\nGUEBIN CHOI\n\n\n\n\nApr 6, 2023\n\n\nMETRLADatasetLoader-Tutorial\n\n\nSEOYEON CHOI\n\n\n\n\nApr 5, 2023\n\n\nSimulation_reshape\n\n\nSEOYEON CHOI\n\n\n\n\nApr 5, 2023\n\n\nSimulation\n\n\nSEOYEON CHOI\n\n\n\n\nApr 5, 2023\n\n\nSimulation\n\n\nSEOYEON CHOI\n\n\n\n\nMar 22, 2023\n\n\nSimualtionPlanner-Tutorial\n\n\nSEOYEON CHOI\n\n\n\n\nMar 20, 2023\n\n\ndata load, data save as pickle\n\n\nSEOYEON CHOI\n\n\n\n\nMar 18, 2023\n\n\nSimualtionPlanner-Tutorial\n\n\nSEOYEON CHOI\n\n\n\n\nMar 17, 2023\n\n\nITSTGCN-Tutorial\n\n\nSEOYEON CHOI\n\n\n\n\nMar 3, 2023\n\n\nSY 1st ITSTGCN\n\n\nSEOYEON CHOI\n\n\n\n\nFeb 15, 2023\n\n\n2nd ITSTGCN\n\n\nGUEBIN CHOI\n\n\n\n\nFeb 15, 2023\n\n\n1st ITSTGCN\n\n\nGUEBIN CHOI\n\n\n\n\nFeb 7, 2023\n\n\nClass of Method(WikiMath) lag 1\n\n\nSEOYEON CHOI\n\n\n\n\nFeb 6, 2023\n\n\nClass of Method(GNAR) lag 1 80% Missing repeat\n\n\nSEOYEON CHOI\n\n\n\n\nFeb 6, 2023\n\n\nClass of Method(GNAR) lag 2\n\n\nSEOYEON CHOI\n\n\n\n\nJan 28, 2023\n\n\nClass of Method(GNAR) lag 1\n\n\nSEOYEON CHOI\n\n\n\n\nJan 28, 2023\n\n\nClass of Method(WikiMath) lag 4\n\n\nSEOYEON CHOI\n\n\n\n\nJan 26, 2023\n\n\nClass of Method\n\n\nGuebin Choi\n\n\n\n\nJan 21, 2023\n\n\nClass of Method\n\n\nSEOYEON CHOI\n\n\n\n\nJan 20, 2023\n\n\n1st ST-GCN Example dividing train and test\n\n\nSEOYEON CHOI\n\n\n\n\nJan 17, 2023\n\n\n2nd ST-GCN Example dividing train and test\n\n\nSEOYEON CHOI\n\n\n\n\nJan 11, 2023\n\n\nGCN Algorithm Example 1\n\n\nSEOYEON CHOI\n\n\n\n\nJan 5, 2023\n\n\nGNAR data\n\n\nSEOYEON CHOI\n\n\n\n\nDec 29, 2022\n\n\n[IT-STGCN] STGCN 튜토리얼\n\n\n신록예찬, SEOYEON CHOI\n\n\n\n\nDec 28, 2022\n\n\nSimulation of geometric-temporal\n\n\nSEOYEON CHOI\n\n\n\n\nDec 21, 2022\n\n\nPyTorch ST-GCN Dataset\n\n\nSEOYEON CHOI\n\n\n\n\nDec 5, 2022\n\n\nTORCH_GEOMETRIC.NN\n\n\nSEOYEON CHOI\n\n\n\n\n\nNo matching items",
    "crumbs": [
      "Main_Blog",
      "**Research**",
      "**Research**"
    ]
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "This blog provides information on GODE."
  }
]