{
 "cells": [
  {
   "cell_type": "raw",
   "id": "cbbbef98-98a2-44bb-b89f-dd5aa759f648",
   "metadata": {},
   "source": [
    "---\n",
    "title: \"논문 라이팅\"\n",
    "author: \"SEOYEON CHOI\"\n",
    "date: \"2024-05-16\"\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "048a3afe-5fa4-4947-86c9-50beb4d20133",
   "metadata": {},
   "source": [
    "# Abstract"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc4a3539-02e8-4c50-81f5-0a176dcd6ef6",
   "metadata": {},
   "source": [
    "# Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ff77f45-9576-4b3d-ba70-0659d6263894",
   "metadata": {},
   "source": [
    "In recent years, the field of the spatio temporal dataset has emerged, enabling the simultaneous consideration of both the time and space dimensions. The examples include consumer demand data\\citep{rozemberczki2021pytorch}, wind turbine records\\citep{rozemberczki2021pytorch}, neuroscience data including brain networks\\citep{atluri2016brain}, traffic data like taxi GPS traces\\citep{castro2013taxi}, and more. \n",
    "\n",
    "- STdataset 가 출현함 \n",
    "\n",
    "Classic time-series statistical methods to analyze those kind of data already exist, but they are limited by certain conditions, such as assumptions about the data. Specifically, these classic methods are hard to account for spatio temporal correlations and are not designed to work with spatio temporal data\\citep{yu2017spatio,rozemberczki2021pytorch}. \n",
    "\n",
    "- 근데 그건 전통적인 시계열로 분석이 어려움 \n",
    "\n",
    "In result, when we analize spatio temporal data to use enough information, it leads to improve accuracy during using appropriate geometric deep learning frameworks.\n",
    "\n",
    "- 그래서 GDL을 써야함\n",
    "    - 접근하는 방법은 크게 전통적인 시계열방법과 GDL이 있음. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "082f4b05-5828-4c42-8ee5-4e888d7481ad",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25346e03-84a4-4ebe-ae5d-5d7fdb9cf40a",
   "metadata": {},
   "source": [
    "However, dealing with spatio temporal datasets often presents a common challenge, which is the frequent occurrence of irregularly observed data. For instance, as highlighted by \\citet{ge2019traffic}, traffic sensor data commonly suffers from missing observations due to electronic unit failures, which can significantly impact prediction accuracy. \n",
    "\n",
    "- 그러나 STdataset은 미싱이 있으면 동작X\n",
    "\n",
    "The difficulty in handling irregular data is that many traditional data analysis procedures were designed for datasets with complete observations \\citep{schafer2002missing}. \n",
    "\n",
    "- 이유1. GDL의 대부분 방법은 fully observed 되었다고 가정함.\n",
    "\n",
    "~Second, when dealing with time-series datasets containing missing data, attempting to learn from such data can lead to challenges as it may result in the failure to capture certain time points\\citep{ge2019traffic, tian2018lstm}.~\n",
    "\n",
    "- ~이유2. -> 시계열에서 미싱이 있을 경우 분석에 어려움이 있다는 연구가 있음.~ \n",
    "\n",
    "That's why it's important to transform incomplete data into complete data before conducting any learning or analysis.\n",
    "\n",
    "- 그래서 complete data를 만들어야함. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1f52162-41d2-4efb-a42d-8d24ccc44d79",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1550287-36d7-4dd4-ace4-0c814ac3bfd8",
   "metadata": {},
   "source": [
    "To describe the geometric structures of data domain, graphs are well known as generic data representation forms\\citep{shuman2013emerging}.\n",
    "\n",
    "So in this paper, we interpret data as ${\\cal G} := (V, E)$, $V$ mean vertics and $E$ mean edges. On specific ${\\cal G}$, it has a finite collection of samples and we call it as a graph signal\\citep{shuman2013emerging}. Now, we would like to show the purpose of this paper that makes complete data when we approach the irregularly data. In our proposed method, it is important to rightly estimate the underlying function when training spatio temporal dataset because the functions define the expected pattern of the data. And that pattern would estimate expectation by reading the trend of datasets. But it is well known that it is hard to estimate when it has majority percentages of missing values."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3728385f-2a34-4142-a740-7fcba34bbf57",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "51e9e37e-a73a-435d-be31-f685c36b7a2e",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "3a4ad505-d19a-4f72-be77-7bf191b01f4c",
   "metadata": {},
   "source": [
    "    To overcome this challenge, we suggest a method called ITSTGNN(Iterative Thresholding Spatio Temporal Graph Neural Network with Graph Fourier Transform) to get higher accuracy on test dataset having missing values. Now, we focus on improving accuracy, especially in the high rate of missing values. We indicated our proposed method as a toy example on \\textbf{Animation}. There are the observed spatio temporal datasets which aussume a graph signal to handle sparse data $\\cal{G}$$_{n,t}$ $= (V_{n,t}, E_t, f_{n,t}), n=\\{1,2\\}, t=1,2, \\dots 10,000$ with an random missing rate of $60\\%$ and block missing data from $t=1,600$ to $t=4,000$. First, we employ an interpolation method to fill in missing data, enabling the utilization of Graph Convolutional Networks (GCN). Then we get $\\cal{G}$$_{n,t,inter} = (V_{n,t}, E_t, f_{n,t,inter})$. Secondary, we use the Ebayes Thresh(Empirical Bayes Thresholding)\\citep{johnstone2005ebayesthresh} to get underline function of dataset, and get $\\cal{G}$$_{n,t,trim} = (V_{n,t}, E_{t}, f_{n,t,trim})$ by using it. Next, we can have $\\cal{G}$$_{n,t,com}^{(i)} = (V_{n,t}, E_t, f_{n,t,com}^{(i)}),\\{ i=iteration\\}$ as we use classic archetectures. And we repeat this process untill to be reached convergence as called \"self consistency\" at some point which is satisfied stability by only imputate indexed missing points using iteratively estimate complete data. It is important that we only change the indexed missing value to achive self-consistency. Finally, we can have $\\hat{\\cal{G}}$$_{n,t,com} = (V_{n,t}, E_{t}, \\hat{f}_{n,t,com})$. As we can see \\textbf{Animation}, our proposed method well predicted the values.\n",
    "\n",
    "\\textbf{Contributions} In this paper, we expect to contribute as follows:\n",
    "\\begin{enumerate} \n",
    "\\item we can appropriately predict data whenever we get irregurarly data that we consider get spatio temporal dataset with higher missing rate. \n",
    "\\end{enumerate}\n",
    "\n",
    "\n",
    "\n",
    "\\section{Related work}\n",
    "\n",
    "    \\textbf{Traditional methods} When observed data is revealed incomplete data, a general approach was approached to handle this problem by using the underlying complete data. \\citet{dempster1977maximum} defined expectation-maximization algorithm as an iterative technique while computing the log-likelihood from incomplete data. Furthermore, the term of self-consistency is introduced by \\citet{flury1996self} with showing similarity between expectation-maximization algorithm and \\textit{k}-means algorithm.\n",
    "\n",
    "    \\textbf{Handling Irregular Spatio-Temporal Data} Irregular spatio temporal data, commonly encountered in the real world, poses a challenge as neural networks are more adept at handling regular data. It is often mentioned as the simple solution such as imputation algorithms\\cite{beretta2016nearest} or linear predictor\\cite{durbin2012time}. But there are also numerous methods to address the challenge of dealing with this issue.\n",
    "\n",
    "    \\textbf{Approaches to Handling Missing Values in Spatio Temporal Data} For instance, to fill missing values, \\citet{bai2020adaptive, yu2017spatio, guo2019attention} employ linear interpolation, while \\citet{cui2020graph} utilize historical data. All of them tried to convert heterogeneous graph into homogeneous graph which has the same types of nodes and edges, while heterogeneous graphs do not\\citep{zhou2020graph}. There exist three categories of interpolation techniques: spatial interpolation, temporal interpolation, and spatio-temporal interpolation\\citep{cheng2017two}, and if we generate regular data without taking into account the appropriate interpolation method, the outcome could potentially result in the loss of valuable information in either the temporal or spatial dimension\\citep{deng2016hybrid, de2001estimating}. \n",
    "\n",
    "    \\textbf{Enhancing Data Predictions through Self-Consistency} Additionally, \\citet{cini2023scalable} assume that the input data is originally complete, which is equivalent to interpreting the data as a homogeneous graph from the beginning. Furthermore, \\citet{chen2016learning, xie2020istd} proposed a general model that treats input data as a heterogeneous graph, assuming a lack of supported sensing data. It might be efficient to handle data with a heterogeneous structure in each snapshot. The real dataset frequently exhibit a homogeneous graph, and the introduction of missing values merely transforms it into a heterogeneous graph, implying that the structures of each snapshot remain consistent. In our study, our goal is to effectively manage learning data while maintaining the structures in every snapshot, even when confronted with missing values.\n",
    "\n",
    "    Various approaches have been attempted in the proposal of methods related to spatio temporal data, aiming to transform irregular patterns into regular ones. Most attempts involved the application of simple handling methods such as linear interpolation. If there is a way to estimate the optimal value other than the interpolation method, it could lead to more accurate predictions and, consequently, more effective results compared to the existing approaches used for spatio temporal data.\n",
    "\n",
    "    In this paper, we propose an approach that leverages the normal trend of data after handling missing values, utilizing the self-consistency property to enhance the accuracy of data predictions.\n",
    "\n",
    "    % The progression from Convolutional Neural Networks (CNNs) to Graph Neural Networks (GCNs) \\cite{kipf2016semi} has proven to be an effective adaptation for graph-structured data. In the domain of graphs, especially when dealing with temporal variations in input data, the concept is referred to as dynamic graphs \\citep{zhou2020graph}. If the graphs captures information that evolves over time, incorporating both temporal and spatial aspects would be effective. To delve into these architectures, a focus on spatial and temporal information is observed in DCRNN (Diffusion Convolution Recurrent Neural Network) \\cite{li2017diffusion} and STGCN (Spatio-temporal graph convolutional networks) \\cite{yu2017spatio}. DCRNN captures spatial data using Graph Neural Networks (GNNs) and subsequently transfers the output to sequence-to-sequence or sequence models like Recurrent Neural Networks (RNNs) to consider temporal dependencies. On the other hand, STGCN stacks multiple spatio temporal convolutional blocks, each comprising a spatial graph convolutional layer and two temporal gate convolutional layers. ST-GCN \\cite{yan2018spatial} concurrently captures spatial and temporal messages. To apply traditional GNNs to extended graphs, ST-GCN expands the static graph structure by incorporating temporal connections, involving the construction of spatio temporal graphs through the stacking of graph frames from each time step. Various approaches have been attempted in the proposal of methods related to spatio temporal data, aiming to transform irregular patterns into regular ones. Most attempts involved the application of simple handling methods. However, this approach involves synthetic observations rather than actual measured values, and it can adversely impact accuracy when predicting or classifying the data, leading to lower accuracy. If there are proposed to improve methods, it could lead to more accurate predictions and, consequently, more effective results compared to the existing approaches used for spatio temporal data.\n",
    "\n",
    "    % In this paper, we propose an approach that leverages the normal trend of data after handling missing values, utilizing the self-consistency property to enhance the accuracy of data predictions.\n",
    "\n",
    "\\section{Methodology}\n",
    "\n",
    "    \\subsection{Self-Consistent Estimator}\n",
    "    \n",
    "    To deal with incomplete data, we introduce the concept of \"self-consistency\" which is proposed by \\citet{efron1967two} who originally used it to address censored data. Then, \\citet{hastie1989principal} provided a definition of principal curves as smooth curves that maintain self-consistency across a distribution or dataset. Especially, the term of \"self-consistency\" is presented for regression function estimator by \\citet{flury1996self, lee2007self}. According to \\citet{lee2007self}, we can estimate $\\hat{f}_{obs}$ given underlying function of observed data with this specific self-consistent equation:\n",
    "\n",
    "    $E(\\hat{f}_{com} |x_{obs}, f = \\hat{f}_{obs}) = \\hat{f}_{obs} \\cdots (1)$\n",
    "\n",
    "    $\\hat{f}_{com}$ is an estimate of $f$ via $x_{com}$ which is assumed as complete data and consisted of $\\{ x_{obs}, x_{mis}\\}$ where $x_{obs}$ is observed data and $x_{mis}$ is missing value which is not available. Additionally, \\citet{lee2007self} obtained the \"optimal\" estimate $\\hat{f}_{com}$ for their regression function $f$, facilitating the derivation of their \"best\" incomplete data estimator $\\hat{f}_{obs}$ using the corresponding complete data procedure. Given the independence of this equation from estimation, it suggests potential applicability to our dataset assuming the presence of missing values. Under this condition, we extend the self-consistent equation with following one:\n",
    "\n",
    "    $E ( \\hat{s}_{t,com} | \\{ f \\{ n \\} : n \\in O \\}, \\{ s_t \\} = \\{ \\hat{s}_{t, obs} \\})= \\hat{s}_{t, obs}, t = 1, 2, \\dots T \\cdots (2)$\n",
    "\n",
    "    Whitin this context, $\\hat{s}_{com}$ represents an estimation of $s_t$ derived from complete data, while $\\hat{s}_{t,obs}$ is an estimation derived from observed data. We utilized $\\{ \\tilde{f} (n): n \\in M\\}$ as missing data $\\{ f(n): n \\in M\\}$ is not available. After calcurating an estimated complete dataset $\\{ \\hat{f} (n)\\} = \\{ f(n) : n \\in O \\} \\cup \\{ \\tilde{f}(n) : n \\in M\\}$, it can be explained as the corresponding decomposition followed:\n",
    "\n",
    "    $\\hat{f}(n) = \\sum^T_{t=1} \\hat{s}_t(n), t=1,2,\\dots , T\\cdots (3)$\n",
    "\n",
    "    The simplest way to obtain $\\hat{f}_{obs}$ is to update $\\{ f^{(i)}(n): n \\in M\\}$ and decompose $\\hat{f}^{(i)}(n) = \\sum^K_{t=1} \\hat{s}^{(i)}_k(n)$ iteratively. Furthermore, $f^{(i)}(n)$ is satisfied the condition of self-consistency if $f^{(i)}(n) = f^{(i+1)}(n)$, and it was varyfied by \\citet{cox1984analysis, flury1996self}. \n",
    "\n",
    "    Note that we define the $i$ as epoch and use linear implication method for part of missing data in our experiment.\n",
    "\n",
    "    \\subsection{Ebayesthresh}\n",
    "    Then we can say that if $F(\\omega)$ can properly estimated and $F_s(\\omega)$ can be properly extracted from $F(\\omega)$, the deterministic term(underlying function) of $x_t$ can be obtained. We employ the empirical Bayes thresholding bscause we would estimate $F_s(\\omega)$ as a periodogram of $x_t$ and extract $F_s(\\omega)$ from $F(\\omega)$ \\cite{johnstone2005ebayesthresh}. We assume throughout that the observations. \n",
    "\n",
    "    $$X_i \\sim N(\\mu_i,1)$$ \n",
    "\n",
    "    Within a Bayesian context, the notion of sparsity is naturally modeled by a suitable prior distribution for the parameters $\\mu_i$. We model the $\\mu_i$ as having independent prior distributions each given by the mixture \n",
    "    $$f_{\\tt{prior}}(\\mu)=(1-w)\\delta_0(\\mu)+w\\gamma(\\mu).$$ \n",
    "\n",
    "   Here the function $\\gamma$ is usually chosen as Laplace density with scale parameter $a > 0$  $\\gamma(u)=\\frac{a}{2}e^{-a|u|}.$ The empirical Bayes approach estimates each $\\mu_i$ by its posterior median. \n",
    "\n",
    "% \\subsection{Iterative Threshold}\n",
    "\n",
    "%     First, we give information about ${\\cal G} = \\{ V, E, {\\bf f} \\}$, where $| V|$ is the number of nodes, and we denote $ E$ as edges of the graph, which contains a connection of nodes. We consider an adjacency matrix $\\bf{W} \\in \\mathbf{R}^{T \\times T}$ by assuming that the nodes connect as it is a time-series domain and getting graph Laplacian. And then, we get ${\\bf D}$, which is a diagonal degree matrix, and normalize it. It is available to decompose the graph Laplacian ${\\bf \\tilde L}={\\bf V}{\\bf \\Lambda}{\\bf V}^\\top$. Now, we get Graph Fourier transforms of $\\bf f$ and calculate ${\\bf V}^\\top{\\bf f}$. It allows us to obtain a periodogram of it. We want to get $f_{\\tt trimed}$ so we used Empirical Bayes Thresholding by \\citet{johnstone2004needles} to estimate $f_{\\tt threshed}$ as known as step function from $f$ values. We can impute missing values on their index to calculate $f_{\\tt trimed}$.\n",
    "\n",
    "\n",
    " \\begin{table}[ht]\n",
    "\\centering\n",
    "    \\begin{tabular}{lcc}\n",
    "    \\toprule\n",
    "    Dataset & $V$ &  $T$\\\\\n",
    "    \\midrule\n",
    "    $\\tt{FiveVTS}$             & 5    & 200   \\\\\n",
    "    $\\tt{Chickenpox}$          & 20   & 522   \\\\\n",
    "    $\\tt{Pedalme}$             & 15   & 36    \\\\\n",
    "    $\\tt{Wikimath}$            & 1068 & 731   \\\\\n",
    "    $\\tt{Windmillsmall}$       & 11   & 17,472\\\\\n",
    "    $\\tt{MontevideoBus}$       & 675  & 744   \\\\\n",
    "    % $\\tt{FiveVTS}$\\citep{knight2019generalised}               & 5    & 200   \\\\\n",
    "    % $\\tt{Chickenpox}$\\citep{rozemberczki2021chickenpox}       & 20   & 522   \\\\\n",
    "    % $\\tt{Pedalme}$\\citep{rozemberczki2021pytorch}             & 15   & 36    \\\\\n",
    "    % $\\tt{Wikimath}$\\citep{rozemberczki2021pytorch}            & 1068 & 731   \\\\\n",
    "    % $\\tt{Windmillsmall}$\\citep{rozemberczki2021pytorch}       & 11   & 17,472\\\\\n",
    "    % $\\tt{MontevideoBus}$\\citep{rozemberczki2021pytorch}       & 675  & 744   \\\\\n",
    "    \\bottomrule\n",
    "    \\end{tabular}\n",
    "    \\caption{Information of Spatio Temporal Datasets including time period($T$) and Nodes($V$); $\\tt{FiveVTS}$\\citep{knight2019generalised}, $\\tt{Chickenpox}$\\citep{rozemberczki2021chickenpox}, $\\tt{Pedalme}$, $\\tt{Wikimath}$, $\\tt{Windmillsmall}$, $\\tt{MontevideoBus}$\\citep{rozemberczki2021pytorch}}\n",
    "    \\label{tab:datainfo}\n",
    "\\end{table}\n",
    "\n",
    "\n",
    "\n",
    "\\subsection{Overview of Method}\n",
    "\n",
    "    \\begin{algorithm}\n",
    "    \\caption{ITSTGNN algorithm}\n",
    "    \\begin{algorithmic}[1]\n",
    "        \\INPUT Graph ${\\cal G}$\n",
    "        \\ENSURE ddd\n",
    "        \\STATE dd\n",
    "        \\WHILE{d}\n",
    "            \\STATE ss\n",
    "        \\ENDWHILE\n",
    "    \\end{algorithmic}\n",
    "    \\end{algorithm}\n",
    "\n",
    "\n",
    "\n",
    "    When the graph signal ${\\bf f}$ is given on ${\\cal G} = (V, E)$ on spatio temporal data, The detailed steps of our proposed method are summarized as follows:\n",
    "    \\begin{itemize}\n",
    "    \\item First, we get the observed data given random missing data or block missing data for each node on the graph signal. We construct the complete data using interpolation methods such as linear, nearest, etc.\n",
    "\n",
    "    \\item We define the (normalized) Graph Laplacian as $${\\bf L}={\\bf D}^{-1/2}({\\bf D}-{\\bf W}){\\bf D}^{-1/2}={\\bf I}-\\widetilde{\\bf W}$$ where ${\\bf D}$ represents the degree matrix and $\\widetilde{\\bf W}={\\bf D}^{-1/2}{\\bf W}{\\bf D}^{-1/2}$. Note whether ${\\bf L}$ satisfies the conditions to be considered as a GSO(Graph Signal Operator)\\cite{djuric2018cooperative}.\n",
    "\n",
    "    \\item We perform the eigenvalue decomposition of the Graph Laplacian ${\\bf L}$ as ${\\bf L}={\\bf V}{\\boldsymbol \\Lambda}{\\bf V}^H$. Obtain the Graph Fourier Transform (GFT) of ${\\bf f}$ by calculating ${\\bf V}^H{\\bf f}$.\n",
    "\n",
    "    \\item We then compute the periodogram of ${\\bf f}$, i.e., $\\hat{\\bf p}:=\\frac{1}{R}\\sum_{r=1}^{R}|\\tilde{\\bf f}_r |^2$, where $R$ represents the number of realizations of the process ${\\bf f}$. To estimate the PSD, we obtain $\\hat{\\bf p}_{tr}$ by applying a thresholding operation to $\\hat{\\bf p}$ with Ebayesthresh.\n",
    "\n",
    "    \\item We reconstruct the original signal  ${\\bf f}$ from the thresholded periodogram $\\hat{\\bf p}_{tr}$ using the inverse GFT, and we denote the reconstructed signal as $\\hat{\\bf f}$.\n",
    "\n",
    "    \\item To get $\\hat{\\bf f}_{com}^{(1)}$, we learn the train data using the TGNN(Temporal Graph Neural Network) based on graph structure and update missing values for self-consistency. After we obtain $\\hat{\\bf f}_{obs}^{1}$ by iteratively updating missing points of $\\hat{\\bf f}_{com}^{(n)}$.\n",
    "\n",
    "    \\item After we obtain $\\hat{\\bf f}_{obs}^{1}$ by iteratively updating missing points of $\\hat{\\bf f}_{com}^{(n)}$. \n",
    "\n",
    "    \\item We now can evaluate the performance with the last $\\hat{\\bf f}_{com}^{(n)}$ by calculating MSE(mean squared error).\n",
    "\n",
    "    $${\\bf MSE} = |{\\bf f}-\\hat{\\bf f}|_2^2$$\n",
    "    \\end{itemize}\n",
    "\n",
    "\\section{Experiments}\n",
    "\n",
    "    \\subsection{Conditions}\n",
    "\n",
    "    In this section, we evaluated the performance of our proposed method in different architectures by changing the missing proportion of several datasets. Real-world datasets often include a substantial number of missing values, and as the rate of missing values rises, it becomes progressively challenging for the data to follow trends. Therefore, we aim to conduct those experiments with gradually higher rates of missing values. Furthermore, we introduced incomplete data by choosing missing data randomly and in blocks to simulate real-world scenarios. The experiments were conducted under three parts:\n",
    "    \\begin{itemize}\n",
    "\n",
    "        \\item{Baseline}: We conducted with the original observed complete data(Table \\ref{tab:datainfo}).\n",
    "        \\item{Randomly Missing}: The proportion of missing values which were selected completly at random was various and that values in every node.\n",
    "        \\item{Block Missing}: We assumed that some nodes experiences missing values during a specific interval.\n",
    "\n",
    "    \\end{itemize}\n",
    "\n",
    "    \\subsection{Models Description}\n",
    "\n",
    "    We included nine recurrent graph convolutions temporal graph neural networks methods\\cite{rozemberczki2021pytorch}, which incorporates deep learning and parametric learning approaches for processing spatio temporal signals. We also used GNAR(Generalised Network AutoRegressive) proposed by \\cite{knight2019generalised}. GNAR is known as a Graph Deviation Network based on the AR(Auto Regressive) model, a combination of a learning structure and a Graph Neural Network, and it predicts the new value for using the past one\\citep{knight2019generalised}. That is the reason why the forecasting values of GNAR converge to zero. As a result, we employed ten types of different methods in this paper and present the explanations of each model like Table \\ref{tab:modelexp} on \\textbf{Appendix}. We utilized Mean Square Error (MSE) as the evaluation metric to assess the forecasting accuracy of the datasets. The results are presented as Mean $\\pm$ Standard Deviation (SD).\n",
    "\n",
    "\n",
    "\\subsection{Datasets}\n",
    "\n",
    "    We carried out our experiments using several datasets having stability, which we can get on PyTorch Geometric temporal from \\citet{rozemberczki2021pytorch} and call as Static Graph Temporal Signal(Table \\ref{tab:datainfo}). Also, the dataset was split into training and testing subsets, allocating 80\\% for training and 20\\% for testing purposes.\n",
    "\n",
    "\\subsection{Randomly Missing values}\n",
    "\n",
    "    \\begin{ex} \n",
    "        Figure \\ref{fig:exfig1} illustrates the outcomes for the $\\tt{Chickenpox}$ dataset organized based on varying levels of missing data, with the GConvLSTM employed. As the missing data rates increase, both Classic(STGNN) and Proposed models(IT-STGNN) exhibit a tendency for the mean squared error (MSE) to rise. Particularly noteworthy is the comparison between the trendlines of Classic and Proposed: as the rate of missing values increases, the MSE of the Proposed method tends to increase more gradually. In contrast, the Classic model displays a more rapid increase in MSE. This comparison suggests that as the ratio of missing values grows, our proposed methods tend to predict better compared to the Classic models. It becomes evident that as the percentage of missing data becomes higher, our proposed method performs relatively well.\n",
    "    \\end{ex}\n",
    "\n",
    "    \\begin{figure}\n",
    "        \\centering\n",
    "        \\includegraphics[width=1\\linewidth]{figure/ex1.png}\n",
    "        \\caption{ata.}\n",
    "        \\label{fig:exfig1}\n",
    "    \\end{figure}\n",
    "\n",
    "    \\begin{ex} \n",
    "        Figure \\ref{fig:exfig2} gives information about an overview of experiment outcomes across five datasets at nine archetectures, showcasing the impact of varying rates of randomly distributed missing values. It is clear that most results has the incremental rise in MSE as the outcome of randomly generated missing values increases. Specifically, Classic(STGNN) methods were hugely on the rise compared to Proposed methods(IT-STGNN).\n",
    "    \\end{ex}\n",
    "\n",
    "    \\begin{figure}\n",
    "        \\centering\n",
    "        \\includegraphics[width=1\\linewidth]{figure/ex2.png}\n",
    "        \\caption{ata.}\n",
    "        \\label{fig:exfig2}\n",
    "    \\end{figure}\n",
    "\n",
    "    \\textbf{MSE ranking}\n",
    "    \\begin{ex} \n",
    "        MSE rankings were established independently of missing value rates, with higher MSE values positioned towards the right(Figure \\ref{fig:exfig3}). Observations across datasets consistently highlighted the proposed methods' lower MSE performance compared to the classic methods. Additionally, across the Windmillsmall dataset, all proposed methods reliably showed lower MSE values than the classic methods\n",
    "    \\end{ex}\n",
    "\n",
    "    \\begin{figure}\n",
    "        \\centering\n",
    "        \\includegraphics[width=1\\linewidth]{figure/ex3.png}\n",
    "        \\caption{ata.}\n",
    "        \\label{fig:exfig3}\n",
    "    \\end{figure}\n",
    "\n",
    "    \\textbf{Ratio of datasets' description}\n",
    "    \n",
    "    \\begin{ex} \n",
    "        Figure \\ref{fig:exfig4} illustrated the ratio($\\frac{T}{V}$) of datasets' information based on Table \\ref{tab:datainfo}($y$-axis) and MSE difference between Proposed methods and Classic methods($x$-axis). According to Figure \\ref{fig:exfig4}, as the propotion($\\frac{T}{V}$) grows, the MSE difference is also going up. It leads to a meaning that our proposed methods would outperform at the specific dataset condition which has much time($T$) data. \n",
    "    \\end{ex}\n",
    "\n",
    "    \\begin{figure}\n",
    "        \\centering\n",
    "        \\includegraphics[width=1\\linewidth]{figure/ex4.png}\n",
    "        \\caption{ata.}\n",
    "        \\label{fig:exfig4}\n",
    "    \\end{figure}\n",
    "\n",
    "    \\subsection{Block missing values}\n",
    "\n",
    "    \\begin{ex} \n",
    "        We highlighted the results when datasets has block missing values(Figure \\ref{fig:exfig5}). There was a MSE difference slightly between Proposed methods and Classic methods respectively. Even thought All of the resules had not dramatic differences, MSE of our proposed methods was lower than Classic one generally. Additionally, it would be valuable research to experiment assuming higher block missing rates in the future.\n",
    "    \\end{ex}\n",
    "\n",
    "    \\begin{figure}\n",
    "        \\centering\n",
    "        \\includegraphics[width=1\\linewidth]{figure/ex5.png}\n",
    "        \\caption{ata.}\n",
    "        \\label{fig:exfig5}\n",
    "    \\end{figure}\n",
    "\n",
    "\\section{Conclusion}\n",
    "\n",
    "    This paper aims at getting an effective way to predict real spatio temporal datasets. First, we introduce IT-STGNN, a novel method to achieve self-consistency by updating estimates to analyze spatio temporal datasets with missing values and Ebayesthresh. Briefly, our proposed method could work better than other classic methods when we aussume that thete is the incomplete spatio temporal dataset. Overall, most experiments indicate that our method outperformed other methods, regardless of whether the data has missing data randomly or in blocks.\n",
    "\n",
    "    % Additionally, it remains for a future study to conceive joint time and graph stationary when we calculate the GSO(Graph Shift operator), especially in large datasets. It is expected a better performance of our method shift when simultaneously considering spatio temporal aspects since we consider only time information when calculating GSO in this paper. In this case, we tested on $\\tt{Pedalme}$ and $\\tt{Wikimath}$ datasets by getting GSO with nodes and time-series of graph signals. All settings are the same as in each dataset's experiments without calculating GSO, and the results are in Table \\ref{tb:gsoone} on \\textbf{Appendix} and Table \\ref{tb:gsotwo} on \\textbf{Appendix}. We calculate the weight matrix when we get GSO on $\\tt{Padalme}$ dataset in Table \\ref{tb:gsotwo} on \\textbf{Appendix}. The performances of methods of incomplete data with random missing values achieve lower MSE(Mean Squared Error) compared to other methods. Specifically, GConv GRU, LRGCN, EvolveGCNH, and DCRNN show lower MSE values. \n",
    "    % The output with block missing data also records lower MSE than other methods. In this case, the methods that exhibit lower MSE are GLSTM, EvolveGCNO, EvolveGCNH, and DCRNN. Since the $\\tt{Padalme}$ dataset covers a short time range (less than 40 periods), it would perform well if it had more time points. We consider GSO considering the spatio aspect on $\\tt{Wikimath}$ dataset by supposing having missing data at the same time points. We conducted different approaches for each dataset because the $\\tt{Wikimath}$ dataset has a large number of time points, requiring a more powerful CPU with ample memory. When comparing the performance using Table \\ref{tb:gsotwo} on \\textbf{Appendix}, it shows that considering joint time and graph stationary works better, except for EvolveGCNH. \n",
    "\n",
    "    Lastly, we only used static graph temporal signals, so we expect to explore the use of dynamic graph temporal signals in future research. \n",
    "\n",
    "    In conclusion, the IT-TGNN can successfully predict spatial and temporal features and has no limitations on the extension forecasting method.\n",
    "\n",
    "% In the unusual situation where you want a paper to appear in the\n",
    "% references without citing it in the main text, use \\nocite\n",
    "\\nocite{langley00}\n",
    "\n",
    "\\bibliography{example_paper}\n",
    "\\bibliographystyle{icml2024}\n",
    "\n",
    "\n",
    "%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%\n",
    "%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%\n",
    "% APPENDIX\n",
    "%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%\n",
    "%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%\n",
    "\\newpage\n",
    "\\appendix\n",
    "\n",
    "\\onecolumn\n",
    "% \\section{You \\emph{can} have an appendix here.}\n",
    "\n",
    "\n",
    "\\section{Classic TGNN}\n",
    "\\begin{table}[ht]\n",
    "  \\centering\n",
    "  \\begin{tabular}{p{4.2cm}|p{4.5cm}|p{7cm}}\n",
    "     \\toprule\n",
    "    \\textbf{Model} & \\textbf{Full Name} & \\textbf{Explanation} \\\\\n",
    "    \\midrule\n",
    "    GConvGRU \\newline \\citep{seo2018structured} & Chebyshev Graph Convolutional Gated Recurrent Unit Cell & It combines the Gated Recurrent Unit (GRU) and Chebyshev Graph Convolutional Network to capture temporal dependencies in graph-structured data effectively. \\\\\n",
    "    GConvLSTM \\newline \\citep{seo2018structured} & Chebyshev Graph Convolutional Long Short Term Memory Cell & It integrates graph convolutions and Long Short Term Memory (LSTM) cells to process graph-structured sequences, effectively capturing node interactions and temporal dependencies. \\\\\n",
    "    GCLSTM \\newline \\citep{chen2022gc} & Graph Convolution Embedded LSTM for Dynamic Link Prediction & A model that combines GCNs and LSTM cells for dynamic link prediction in changing graphs and captures both graph structure and temporal dependencies effectively. \\\\\n",
    "    LRGCN \\newline \\citep{seo2018structured} & Long Short-Term Memory R-GCN & It takes node correlation within a graph snapshot as intra-time relation and interprets temporal dependency between adjacent graph snapshots as inter-time relations. \\\\\n",
    "    DyGrEncoder \\newline \\citep{9073186} & Dynamic Graph Auto-Encoder & It constructs embedding Graph Neural Network (GNN) to LSTM. \\\\\n",
    "    EvolveGCNH \\newline \\citep{pareja2020evolvegcn} & Evolving Graph Convolutional Hidden Layer & A modification of the GCN model that incorporates temporal information without relying on node embeddings. In EvolveGCNH, the GCN weights are treated as hidden states within the recurrent architecture. However, in EvolveGCNO (Evolving Graph Convolutional without Hidden Layer)\\citep{pareja2020evolvegcn}, these weights are used as input or output components directly, without being treated as hidden states. \\\\\n",
    "    TGCN \\newline \\cite{zhao2019t} & Temporal graph convolutional network for traffic prediction & It combines GCN and Gated Recurrent unit. \\\\\n",
    "    DCRNN \\newline \\cite{atwood2016diffusion} & Diffusion Convolutional Recurrent Neural Network & In the traffic flow, it considers both spatial and temporal dependency. \\\\\n",
    "    \\bottomrule\n",
    "  \\end{tabular}\n",
    "  \\caption{This is the explanation of Models that are used in this paper}\n",
    "  \\label{tab:modelexp}\n",
    "\\end{table}\n",
    "\n",
    "\n",
    "\\section{Results}\n",
    "\n",
    "% \\begin{table*}\n",
    "% \\vspace{-2.6mm}\n",
    "% \\centering\n",
    "% \\small\n",
    "% \\begin{tabular}{@{}clccccc@{}}  \n",
    "% \\toprule \n",
    "% & & \\multirow{2}{*}{\\textbf{Baseline}} & \\multicolumn{2}{c}{\\textbf{Random}} & \\multicolumn{2}{c}{\\textbf{Block}} \\\\ \n",
    "%                   &&             & Classic        &      Proposed         &  Classic & Proposed\\\\ \\midrule\n",
    "%   \\multirow{11}{*}{ \\makecell{ $\\tt{FiveVTS}$ \\\\ \\textbf{Random(70\\%)} \\\\ \\textbf{Block(12.5\\%)} } }        \n",
    "%  & GConvGRU         & 0.732$\\pm$0.005 & 1.858$\\pm$0.139 & \\textbf{1.180$\\pm$0.060} & 1.210$\\pm$0.039 & \\textbf{1.165$\\pm$0.043}  \\\\\n",
    "%  & GConvLSTM        & 1.131$\\pm$0.041 & 1.472$\\pm$0.125 & \\textbf{1.287$\\pm$0.075} & 1.172$\\pm$0.055 & \\textbf{1.140$\\pm$0.038}  \\\\ \n",
    "%  & GCLSTM           & 0.023$\\pm$0.023 & 1.245$\\pm$0.033 & \\textbf{1.228$\\pm$0.034} & 1.244$\\pm$0.033 & \\textbf{1.219$\\pm$0.025}  \\\\\n",
    "%  & LRGCN            & 0.024$\\pm$0.024 & 1.261$\\pm$0.047 & \\textbf{1.244$\\pm$0.041} & 1.251$\\pm$0.037 & \\textbf{1.220$\\pm$0.020}  \\\\\n",
    "%  & DyGrEncoder      & 1.114$\\pm$0.037 & 1.548$\\pm$0.158 & \\textbf{1.252$\\pm$0.060} & 1.173$\\pm$0.037 & \\textbf{1.124$\\pm$0.035}  \\\\ \n",
    "%  & EvolveGCNH       & 1.175$\\pm$0.068 & 1.228$\\pm$0.064 & \\textbf{1.188$\\pm$0.049} & 1.197$\\pm$0.076 & \\textbf{1.181$\\pm$0.055}  \\\\\n",
    "%  & EvolveGCNO       & 1.168$\\pm$0.065 & 1.198$\\pm$0.045 & \\textbf{1.162$\\pm$0.052} & 1.176$\\pm$0.056 & \\textbf{1.162$\\pm$0.040}  \\\\ \n",
    "%  & TGCN             & 1.085$\\pm$0.016 & 1.184$\\pm$0.057 & \\textbf{1.110$\\pm$0.037} & 1.107$\\pm$0.020 & \\textbf{1.090$\\pm$0.015}  \\\\\n",
    "%  & DCRNN            & 0.041$\\pm$0.041 & 1.271$\\pm$0.066 & \\textbf{1.247$\\pm$0.044} & 1.260$\\pm$0.051 & \\textbf{1.232$\\pm$0.033}  \\\\  \n",
    "%  & GNAR             & 1.407       & \\multicolumn{2}{c}{1.407} &\\multicolumn{2}{c}{1.407}   \\\\ \\midrule\n",
    "% \\multirow{11}{*}{ \\makecell{ $\\tt{Chickenpox}$\\\\\\textbf{Random(80\\%)}\\\\\\textbf{Block(28.8\\%)}}} \n",
    "%  & GConvGRU         & 0.752$\\pm$0.013 & 2.529$\\pm$0.292 & \\textbf{1.586$\\pm$0.199} & 0.828$\\pm$0.022 & \\textbf{0.807$\\pm$0.016}  \\\\\n",
    "%  & GConvLSTM        & 0.959$\\pm$0.088 & 2.522$\\pm$0.111 & \\textbf{1.433$\\pm$0.080} & \\textbf{0.900$\\pm$0.049} & 0.911$\\pm$0.069  \\\\ \n",
    "%  & GCLSTM           & 0.885$\\pm$0.051 & 2.172$\\pm$0.186 & \\textbf{1.371$\\pm$0.072} & 0.890$\\pm$0.033 & \\textbf{0.883$\\pm$0.045}  \\\\\n",
    "%  & LRGCN            & 0.868$\\pm$0.047 & 1.632$\\pm$0.156 & \\textbf{1.334$\\pm$0.071} & 0.911$\\pm$0.047 & \\textbf{0.888$\\pm$0.035}  \\\\\n",
    "%  & DyGrEncoder      & 0.906$\\pm$0.051 & 2.127$\\pm$0.240 & \\textbf{1.399$\\pm$0.063} & 0.912$\\pm$0.043 & \\textbf{0.899$\\pm$0.035}  \\\\ \n",
    "%  & EvolveGCNH       & 1.000$\\pm$0.020 & 1.203$\\pm$0.061 & \\textbf{1.140$\\pm$0.042} & 1.027$\\pm$0.023 & \\textbf{1.007$\\pm$0.021}  \\\\\n",
    "%  & EvolveGCNO       & 0.986$\\pm$0.018 & 1.234$\\pm$0.096 & \\textbf{1.161$\\pm$0.054} & 1.028$\\pm$0.016 & \\textbf{1.002$\\pm$0.015}  \\\\ \n",
    "%  & TGCN             & 1.090$\\pm$0.042 & 1.466$\\pm$0.064 & \\textbf{1.183$\\pm$0.028} & 1.082$\\pm$0.028 & \\textbf{1.065$\\pm$0.031}  \\\\\n",
    "%  & DCRNN            & 0.727$\\pm$0.009 & 2.287$\\pm$0.074 & \\textbf{1.467$\\pm$0.076} & 0.812$\\pm$0.006 & \\textbf{0.740$\\pm$0.007}  \\\\  \n",
    "%  & GNAR             & 1.427       & \\multicolumn{2}{c}{1.427} &\\multicolumn{2}{c}{1.427}   \\\\ \\midrule\n",
    "% \\multirow{11}{*}{ \\makecell{ $\\tt{Pedalme}$\\\\\\textbf{Random(60\\%)}\\\\\\textbf{Block(28.6\\%)}} }\n",
    "%  & GConvGRU         & 1.233$\\pm$0.107 & 1.851$\\pm$0.254 & \\textbf{1.625$\\pm$0.324} & \\textbf{1.270$\\pm$0.114} & 1.289$\\pm$0.115  \\\\\n",
    "%  & GConvLSTM        & 1.214$\\pm$0.055 & 1.274$\\pm$0.078 & \\textbf{1.248$\\pm$0.045} & 1.237$\\pm$0.046 & \\textbf{1.222$\\pm$0.039}  \\\\ \n",
    "%  & GCLSTM           & 1.181$\\pm$0.040 & 1.365$\\pm$0.064 & \\textbf{1.259$\\pm$0.042} & 1.248$\\pm$0.019 & \\textbf{1.195$\\pm$0.029}  \\\\\n",
    "%  & LRGCN            & 1.191$\\pm$0.054 & 1.462$\\pm$0.084 & \\textbf{1.286$\\pm$0.033} & 1.263$\\pm$0.033 & \\textbf{1.165$\\pm$0.035}  \\\\\n",
    "%  & DyGrEncoder      & 1.190$\\pm$0.047 & 1.513$\\pm$0.083 & \\textbf{1.285$\\pm$0.051} & 1.269$\\pm$0.066 & \\textbf{1.165$\\pm$0.032}  \\\\ \n",
    "%  & EvolveGCNH       & 1.213$\\pm$0.057 & 1.284$\\pm$0.066 & \\textbf{1.262$\\pm$0.091} & 1.265$\\pm$0.072 & \\textbf{1.222$\\pm$0.040}  \\\\\n",
    "%  & EvolveGCNO       & 1.223$\\pm$0.051 & 1.292$\\pm$0.075 & \\textbf{1.267$\\pm$0.067} & 1.246$\\pm$0.035 & \\textbf{1.245$\\pm$0.045}  \\\\ \n",
    "%  & TGCN             & 1.307$\\pm$0.075 & 1.301$\\pm$0.090 & \\textbf{1.260$\\pm$0.072} & \\textbf{1.232$\\pm$0.069} & 1.262$\\pm$0.066  \\\\\n",
    "%  & DCRNN            & 1.131$\\pm$0.015 & 1.509$\\pm$0.068 & \\textbf{1.303$\\pm$0.078} & 1.304$\\pm$0.021 & \\textbf{1.150$\\pm$0.014}  \\\\ \n",
    "%  & GNAR             & 1.303       & \\multicolumn{2}{c}{1.303} &\\multicolumn{2}{c}{1.303}   \\\\ \\midrule\n",
    "% \\multirow{11}{*}{ \\makecell{ $\\tt{Wikimath}$\\\\\\textbf{Random(80\\%)}\\\\\\textbf{Block(12\\%)}}} \n",
    "%  & GConvGRU         & 0.931$\\pm$0.002 & 0.932$\\pm$0.043 & \\textbf{0.687$\\pm$0.021} & 0.531$\\pm$0.002 & \\textbf{0.523$\\pm$0.002}  \\\\\n",
    "%  & GConvLSTM        & 0.960$\\pm$0.011 & 1.423$\\pm$0.121 & \\textbf{0.920$\\pm$0.069} & 0.660$\\pm$0.034 & \\textbf{0.627$\\pm$0.014}  \\\\ \n",
    "%  & GCLSTM           & 0.970$\\pm$0.011 & 1.407$\\pm$0.117 & \\textbf{0.815$\\pm$0.058} & \\textbf{0.638$\\pm$0.013} & 0.640$\\pm$0.019  \\\\\n",
    "%  & LRGCN            & 0.980$\\pm$0.024 & 1.105$\\pm$0.099 & \\textbf{0.769$\\pm$0.045} & 0.624$\\pm$0.024 & \\textbf{0.608$\\pm$0.012}  \\\\\n",
    "%  & DyGrEncoder      & 0.995$\\pm$0.034 & 0.770$\\pm$0.045 & \\textbf{0.606$\\pm$0.017} & \\textbf{0.546$\\pm$0.016} & 0.563$\\pm$0.025  \\\\ \n",
    "%  & EvolveGCNH       & 1.182$\\pm$0.192 & 0.915$\\pm$0.063 & \\textbf{0.877$\\pm$0.045} & \\textbf{0.773$\\pm$0.021} & 0.776$\\pm$0.028  \\\\\n",
    "%  & EvolveGCNO       & 1.157$\\pm$0.182 & 0.863$\\pm$0.038 & \\textbf{0.780$\\pm$0.027} & 0.735$\\pm$0.022 & \\textbf{0.732$\\pm$0.025}  \\\\ \n",
    "%  & TGCN             & 0.983$\\pm$0.006 & 0.827$\\pm$0.030 & \\textbf{0.771$\\pm$0.020} & \\textbf{0.741$\\pm$0.046} & 0.748$\\pm$0.046  \\\\\n",
    "%  & DCRNN            & 0.936$\\pm$0.002 & 0.846$\\pm$0.031 & \\textbf{0.672$\\pm$0.007} & \\textbf{0.578$\\pm$0.005} & 0.583$\\pm$0.006  \\\\  \n",
    "%  & GNAR             & 1.354       & \\multicolumn{2}{c}{1.354} &\\multicolumn{2}{c}{1.354}   \\\\ \\midrule\n",
    "% \\multirow{11}{*}{ \\makecell{ $\\tt{Windmillsmall}$\\\\\\textbf{Random(70\\%)}\\\\\\textbf{Block(12.5\\%)}} } \n",
    "%  & GConvGRU         & 1.003$\\pm$0.004 & 1.662$\\pm$0.073 & \\textbf{1.194$\\pm$0.042} & 1.008$\\pm$0.006 & \\textbf{1.007$\\pm$0.005}\\\\\n",
    "%  & GConvLSTM        & 1.019$\\pm$0.045 & 1.600$\\pm$0.056 & \\textbf{1.142$\\pm$0.021} & \\textbf{0.989$\\pm$0.009} & 0.997$\\pm$0.022  \\\\ \n",
    "%  & GCLSTM           & 0.992$\\pm$0.010 & 1.580$\\pm$0.099 & \\textbf{1.117$\\pm$0.021} & \\textbf{0.985$\\pm$0.002} & \\textbf{0.985$\\pm$0.003}  \\\\\n",
    "%  & LRGCN            & 0.987$\\pm$0.006 & 1.492$\\pm$0.087 & \\textbf{1.110$\\pm$0.012} & \\textbf{0.985$\\pm$0.003} & \\textbf{0.985$\\pm$0.002}  \\\\\n",
    "%  & DyGrEncoder      & 0.988$\\pm$0.008 & 0.985$\\pm$0.002 & \\textbf{0.984$\\pm$0.003} & \\textbf{0.985$\\pm$0.003} & \\textbf{0.985$\\pm$0.005}  \\\\ \n",
    "%  & EvolveGCNH       & 0.986$\\pm$0.002 & 1.330$\\pm$0.137 & \\textbf{1.129$\\pm$0.035} & \\textbf{0.993$\\pm$0.003} & 0.986$\\pm$0.003  \\\\\n",
    "%  & EvolveGCNO       & 0.983$\\pm$0.001 & 1.495$\\pm$0.137 & \\textbf{1.149$\\pm$0.026} & 0.990$\\pm$0.002 & \\textbf{0.983$\\pm$0.002}  \\\\ \n",
    "%  & TGCN             & 0.991$\\pm$0.010 & 1.305$\\pm$0.039 & \\textbf{1.071$\\pm$0.010} & 1.000$\\pm$0.014 & \\textbf{0.989$\\pm$0.008}  \\\\\n",
    "%  & DCRNN            & 0.988$\\pm$0.003 & 1.348$\\pm$0.057 & \\textbf{1.117$\\pm$0.034} & 0.994$\\pm$0.005 & \\textbf{0.983$\\pm$0.002}  \\\\  \n",
    "%  & GNAR             & 1.649       & \\multicolumn{2}{c}{1.649} &\\multicolumn{2}{c}{1.649}   \\\\ \\midrule\n",
    "%  \\multirow{11}{*}{ \\makecell{ $\\tt{MontevideoBus}$\\\\\\textbf{Random(80\\%)}\\\\\\textbf{Block(15\\%)}}} \n",
    "%   & GConvGRU         & 0.931$\\pm$0.002 & 1.516$\\pm$0.040 & \\textbf{1.096$\\pm$0.019} & 0.935$\\pm$0.004 & \\textbf{0.932$\\pm$0.002}  \\\\\n",
    "%  & GConvLSTM        & 0.960$\\pm$0.011 & \\textbf{1.134$\\pm$0.069} & 1.156$\\pm$0.062 & 0.950$\\pm$0.005 & \\textbf{0.949$\\pm$0.008}  \\\\ \n",
    "%  & GCLSTM           & 0.970$\\pm$0.011 & 1.140$\\pm$0.061 & \\textbf{1.032$\\pm$0.028} & \\textbf{0.956$\\pm$0.005} & 0.959$\\pm$0.008  \\\\\n",
    "%  & LRGCN            & 0.980$\\pm$0.024 & 0.989$\\pm$0.029 & \\textbf{0.982$\\pm$0.013} & \\textbf{0.977$\\pm$0.020} & 0.978$\\pm$0.024  \\\\\n",
    "%  & DyGrEncoder      & 0.995$\\pm$0.034 & 1.358$\\pm$0.149 & \\textbf{1.216$\\pm$0.118} & 1.030$\\pm$0.044 & \\textbf{1.005$\\pm$0.046}  \\\\ \n",
    "%  & EvolveGCNH       & 1.182$\\pm$0.192 & 2.158$\\pm$0.545 & \\textbf{1.845$\\pm$0.504} & 1.612$\\pm$0.216 & \\textbf{1.392$\\pm$0.110}  \\\\\n",
    "%  & EvolveGCNO       & 1.157$\\pm$0.182 & \\textbf{2.623$\\pm$0.693} & 2.263$\\pm$0.476 & 1.766$\\pm$0.123 & \\textbf{1.345$\\pm$0.110}  \\\\ \n",
    "%  & TGCN             & 0.983$\\pm$0.006 & 1.218$\\pm$0.086 & \\textbf{1.073$\\pm$0.024} & 0.956$\\pm$0.003 & \\textbf{0.940$\\pm$0.001}  \\\\\n",
    "%  & DCRNN            & 0.936$\\pm$0.002 & 1.225$\\pm$0.073 & \\textbf{1.111$\\pm$0.036} & 0.985$\\pm$0.005 & \\textbf{0.984$\\pm$0.007}  \\\\  \n",
    "%  & GNAR             & 1.062       & \\multicolumn{2}{c}{1.062} &\\multicolumn{2}{c}{1.062}   \\\\\n",
    "% \\bottomrule   \n",
    "% \\end{tabular}\n",
    "% \\caption{The performance comparison with TGNN and IT-TGNN on $\\tt{FiveVTS}$, $\\tt{Chickenpox}$, $\\tt{Pedalme}$, $\\tt{Wikimath}$, $\\tt{Windmillsmall}$, $\\tt{MontevideoBus}$ datasets.}\n",
    "% \\label{tab:results}\n",
    "% \\end{table*}\n",
    "\n",
    "\\newpage\n",
    "\n",
    "\\section{Conditions we set}\n",
    "\n",
    "\\begin{table}[H]\n",
    "\\centering\n",
    "\\small\n",
    "\\begin{tabular}{@{}lcccccc@{}}  \n",
    "\\toprule  \n",
    " & $\\tt{FiveVTS}$ &$\\tt{Chickenpox}$&$\\tt{Pedalme}$&$\\tt{Wikimath}$&$\\tt{Windmillsmall}$&$\\tt{MontevideoBus}$        \\\\ \\midrule\n",
    " \\multicolumn{1}{c}{\\textbf{Max iter.} }               &30&30&30&30&30&30 \\\\  \\midrule\n",
    "\\multicolumn{1}{c}{\\textbf{Epochs} }               &50&50&50&50&50&50 \\\\  \\midrule\n",
    "\\multicolumn{1}{c}{\\textbf{Lags}  }                 &2&4&4&8&8&4 \\\\  \\midrule\n",
    " \\multicolumn{1}{c}{\\textbf{Interpolation}} &linear&linear&linear&linear&linear&linear \\\\  \\midrule\n",
    "\\multicolumn{1}{c}{\\textbf{Filters } }&\\multicolumn{6}{l}{} \\\\  \\midrule\n",
    " GConvGRU              &   12    &     16   &  12     & 12       &  12      &   12      \\\\ \n",
    " GConvLSTM             &   12    &    32    &  2      & 64       &  16      &   12      \\\\ \n",
    " GCLSTM                &   4     &     16   &   4     & 64       &  16      &   12      \\\\\n",
    " LRGCN                 &   4     &     8    &  8      & 32       &  12      &  2        \\\\\n",
    " DyGrEncoder           &  12     &    12    &  12     & 12       &  12      &   12      \\\\ \n",
    " EvolveGCNH            &No need  & No need  & No need & No need  & No need  & No need    \\\\\n",
    " EvolveGCNO            &No need  & No need  & No need & No need  & No need  & No need    \\\\ \n",
    " TGCN                  &   12    &  12      &  12     &  12      &  12      & 8         \\\\\n",
    " DCRNN                 &   2     &   16     &  8      &  12      &  4       & 12        \\\\ \n",
    "\\bottomrule    \n",
    "\\end{tabular}\n",
    "\\caption{The information setting inclusing and number of filters by each dataset and model}\n",
    "\\label{tb:results}\n",
    "\\end{table}\n",
    "\n",
    "\\newpage\n",
    "\n",
    "\\section{Time versus Time and Graph stationary}\n",
    "\\subsection{Pedalme}\n",
    "\n",
    "\\begin{table}[H]\n",
    "\\centering\n",
    "\\small\n",
    "\\begin{threeparttable}\n",
    "  \\label{pedalme_GFT_table}\n",
    "  \\begin{tabular}{lcccccc}\n",
    "  \\toprule\n",
    "   & \\multicolumn{3}{c}{\\textbf{Random(60\\%)}}& \\multicolumn{3}{c}{\\textbf{Block(28.6\\%)}} \\\\\n",
    "                 & Classic            &      Proposed        &  Proposed\\tnote{1}           &  Classic                   & Proposed        & Proposed \\tnote{1}  \\\\ \\midrule\n",
    "    GConvGRU     & 1.851$\\pm$0.254 &          1.625$\\pm$0.324 &  \\textbf{1.410$\\pm$0.208}  & \\textbf{1.270$\\pm$0.114} & 1.289$\\pm$0.115 &  1.391$\\pm$0.151         \\\\\n",
    "    GConvLSTM    & 1.274$\\pm$0.078 & \\textbf{1.248$\\pm$0.045} &  1.313$\\pm$0.205  & 1.237$\\pm$0.046 & \\textbf{1.222$\\pm$0.039} &  1.329$\\pm$0.120         \\\\\n",
    "    GCLSTM       & 1.365$\\pm$0.064 &          1.259$\\pm$0.042 &  \\textbf{1.231$\\pm$0.044}  & 1.248$\\pm$0.019 & 1.195$\\pm$0.029 &  \\textbf{1.182$\\pm$0.045}  \\\\\n",
    "    LRGCN        & 1.462$\\pm$0.084 & \\textbf{1.286$\\pm$0.033} &  1.331$\\pm$0.120  & 1.263$\\pm$0.033 & \\textbf{1.165$\\pm$0.035} &  1.201$\\pm$0.081          \\\\\n",
    "    DyGrEncoder  & 1.513$\\pm$0.083 & \\textbf{1.285$\\pm$0.051} &  1.305$\\pm$0.131  & 1.269$\\pm$0.066 & \\textbf{1.165$\\pm$0.032} &  1.196$\\pm$0.055         \\\\\n",
    "    EvolveGCNO   & 1.284$\\pm$0.066 & 1.262$\\pm$0.091 &  \\textbf{1.248$\\pm$0.072}  & 1.265$\\pm$0.072 & 1.222$\\pm$0.040 &  \\textbf{1.204$\\pm$0.033}  \\\\\n",
    "    EvolveGCNH   & 1.292$\\pm$0.075 &          1.267$\\pm$0.067 &  \\textbf{1.246$\\pm$0.067} & 1.246$\\pm$0.035 & 1.245$\\pm$0.045 &  \\textbf{1.188$\\pm$0.042}  \\\\\n",
    "    TGCN         & 1.301$\\pm$0.090 & \\textbf{1.260$\\pm$0.072} &  1.338$\\pm$0.202  & \\textbf{1.232$\\pm$0.069} & 1.262$\\pm$0.066 &  1.243$\\pm$0.110          \\\\\n",
    "    DCRNN        & 1.509$\\pm$0.068 &          1.303$\\pm$0.078 &  \\textbf{1.208$\\pm$0.079}  & 1.304$\\pm$0.021 & 1.150$\\pm$0.014 &  \\textbf{1.145$\\pm$0.013}  \\\\\\bottomrule\n",
    "\n",
    "  \\end{tabular}\n",
    "  \\begin{tablenotes}\n",
    "    \\item[1] Joint Time and Graph Stationarity, which is considered to be jointly stationary in both the vertex and the time domain\n",
    "  \\end{tablenotes}\n",
    "    \\caption{\n",
    "  The results of the $\\tt{Pedalme}$ dataset with the Graph Shift Operator (GSO) were evaluated considering two scenarios: one considering only time stationarity and the other considering both time and graph stationarity.\n",
    "  }\n",
    "\\end{threeparttable}\n",
    "\\label{tb:gsoone}\n",
    "\\end{table} \n",
    "\n",
    "\\subsection{Wikimath}\n",
    "\n",
    "    \\begin{table}[H]\n",
    "    \\centering\n",
    "    \\small\n",
    "        \\begin{threeparttable}[H]\n",
    "        \\label{wikimath_GFT_table}\n",
    "            \\begin{tabular}{lcccc}\n",
    "                \\toprule\n",
    "                &    \\multicolumn{2}{c}{  \\textbf{Random(80\\%)}  }          & \\multicolumn{2}{c}{ \\textbf{The same missing(51.2\\%)} } \\\\\n",
    "                           & Classic            &      Proposed       &  Classic          & Proposed\\tnote{1}  \\\\\\midrule\n",
    "                GConvGRU     & 0.932$\\pm$0.043 & 0.687$\\pm$0.021 & 0.726$\\pm$0.015  &  \\textbf{0.533$\\pm$0.003}     \\\\\n",
    "                GConvLSTM    & 1.423$\\pm$0.121 & 0.920$\\pm$0.069 & 0.963$\\pm$0.098  &  \\textbf{0.653$\\pm$0.033}    \\\\\n",
    "                GCLSTM       & 1.407$\\pm$0.117 & 0.815$\\pm$0.058 & 0.824$\\pm$0.052  &  \\textbf{0.622$\\pm$0.011}    \\\\\n",
    "                LRGCN        & 1.105$\\pm$0.099 & 0.769$\\pm$0.045 & 0.810$\\pm$0.064  &  \\textbf{0.624$\\pm$0.019}   \\\\\n",
    "                DyGrEncoder  & 0.770$\\pm$0.045 & 0.606$\\pm$0.017 & 0.626$\\pm$0.027  &  \\textbf{0.561$\\pm$0.031}   \\\\\n",
    "                EvolveGCNO   & 0.915$\\pm$0.063 & 0.877$\\pm$0.045 & 0.753$\\pm$0.026  &  \\textbf{0.745$\\pm$0.017}    \\\\\n",
    "                EvolveGCNH   & 0.863$\\pm$0.038 & 0.780$\\pm$0.027 & 0.818$\\pm$0.031  &  0.794$\\pm$0.031    \\\\\n",
    "                TGCN         & 0.827$\\pm$0.030 & 0.771$\\pm$0.020 & 0.782$\\pm$0.030  &  \\textbf{0.750$\\pm$0.039}    \\\\\n",
    "                DCRNN        & 0.846$\\pm$0.031 & 0.672$\\pm$0.007 & 0.665$\\pm$0.015  &  \\textbf{0.592$\\pm$0.005}    \\\\\n",
    "                \\bottomrule\n",
    "            \\end{tabular}\n",
    "        \\begin{tablenotes}\n",
    "        \\item[1] Joint Time and Graph Stationarity, which is considered to be jointly stationary in both the vertex and the time domain\n",
    "        \\end{tablenotes}\n",
    "        \\caption{\n",
    "        The performance of the $\\tt{Wikimath}$ dataset with the Graph Shift Operator (GSO) was compared under two different situations: one that considers only time stationarity and another that includes time and graph stationarity, assuming the same index missing data at the same time points for each node.\n",
    "        }\n",
    "        \\end{threeparttable}\n",
    "    \\label{tb:gsotwo}\n",
    "    \\end{table} \n",
    "\n",
    "\n",
    "%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%\n",
    "%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%\n",
    "\n",
    "\n",
    "\\end{document}\n",
    "\n",
    "\n",
    "% This document was modified from the file originally made available by\n",
    "% Pat Langley and Andrea Danyluk for ICML-2K. This version was created\n",
    "% by Iain Murray in 2018, and modified by Alexandre Bouchard in\n",
    "% 2019 and 2021 and by Csaba Szepesvari, Gang Niu and Sivan Sabato in 2022.\n",
    "% Modified again in 2024 by Sivan Sabato and Jonathan Scarlett.\n",
    "% Previous contributors include Dan Roy, Lise Getoor and Tobias\n",
    "% Scheffer, which was slightly modified from the 2010 version by\n",
    "% Thorsten Joachims & Johannes Fuernkranz, slightly modified from the\n",
    "% 2009 version by Kiri Wagstaff and Sam Roweis's 2008 version, which is\n",
    "% slightly modified from Prasad Tadepalli's 2007 version which is a\n",
    "% lightly changed version of the previous year's version by Andrew\n",
    "% Moore, which was in turn edited from those of Kristian Kersting and\n",
    "% Codrina Lauth. Alex Smola contributed to the algorithmic style files.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee7ba81b-8e8c-41c1-aeb2-d393f912cd02",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
